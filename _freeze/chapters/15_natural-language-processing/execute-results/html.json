{
  "hash": "0bd0ec9e38b7f356078190c49cf24fde",
  "result": {
    "engine": "knitr",
    "markdown": "# Natural Language Processing\n\nThis lecture is designed to introduce you to the basics of preprocessing and\nanalyzing natural language text data.\n\n::: {.callout-note title=\"Learning Goals\" collapse=\"false\"}\nAfter this lesson, you should be able to:\n\n* Be able to identify patterns in unstructured data\n* Know the general workflow for cleaning texts in R, which entails:\n  - Tokenizing words\n  - Determining and applying stop words\n  - Normalizing, lemmatizing, and stemming texts\n  - Creating a document-term matrix\n* Understand how preprocessing steps impact analysis\n* Get high-level data about text documents (term frequencies, TF-IDF scores)\n:::\n\n::: {.callout-important collapse=\"false\"}\n#### Required Data Sets and Packages\n\nThe examples in this chapter use the following data sets and packages.\n\nData sets:\n\n* [`C19_novels_manifest.csv`][c19-manifest]\n* [`C19_novels_raw.rds`][c19-raw]\n\nFor each data set, go to the linked page and click the \"Download raw file\"\nbutton.\n\n[c19-manifest]: https://github.com/ucdavisdatalab/adventures_in_data_science/blob/main/data/C19_novels_manifest.csv\n[c19-raw]: https://github.com/ucdavisdatalab/adventures_in_data_science/blob/main/data/C19_novels_raw.rds\n\nPackages:\n\n```r\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tm\")\ninstall.packages(\"tokenizers\")\n```\n:::\n\n\n## Using a File Manifest\n\nIn this lesson, we'll be preparing a collection of texts for computational\nanalysis. Before we start that work in full, we're going to load in a file\nmanifest, which will help us a) identify what's in our collection; and b) keep\ntrack of things like the order of texts.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmanifest <- read.csv(\"data/C19_novels_manifest.csv\", row.names = 1)\nmanifest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   last_name  first_name                         title year genre\n1   Beckford     William                        Vathek 1786     G\n2  Radcliffe         Ann              ASicilianRomance 1790     G\n3  Radcliffe         Ann         TheMysteriesofUdolpho 1794     G\n4      Lewis     Matthew                       TheMonk 1795     G\n5     Austen        Jane           SenseandSensibility 1811     N\n6    Shelley        Mary                  Frankenstein 1818     G\n7      Scott      Walter                       Ivanhoe 1820     N\n8        Poe  EdgarAllen TheNarrativeofArthurGordonPym 1838     N\n9     Bronte       Emily              WutheringHeights 1847     G\n10 Hawthorne   Nathaniel      TheHouseoftheSevenGables 1851     N\n11   Gaskell   Elizabeth                 NorthandSouth 1854     N\n12   Collins      Wilkie               TheWomaninWhite 1860     N\n13   Dickens     Charles             GreatExpectations 1861     N\n14     James       Henry               PortraitofaLady 1881     N\n15 Stevenson RobertLouis                TreasureIsland 1882     N\n16 Stevenson RobertLouis                 JekyllandHyde 1886     G\n17     Wilde       Oscar        ThePictureofDorianGray 1890     G\n18    Stoker        Bram                       Dracula 1897     G\n```\n\n\n:::\n:::\n\n\n\n\nAs you can see, in addition to the author and title listings, we also have a\ngenre tag. `G` is for \"Gothic\" literature, while `N` is \"not Gothic.\" Let's\nconvert the datatype for the genre column into a factor, which will make life\neasier later on.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmanifest$genre <- as.factor(manifest$genre)\n```\n:::\n\n\n\n\n## Loading a Corpus\n\nWith our metadata loaded, it's time to bring in our files. We'll be using files\nstored in an RDS format, though you could also load straight from a directory\nwith a combination of `lapply` and `readLines`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiles <- readRDS(\"data/C19_novels_raw.rds\")\n```\n:::\n\n\n\n\nLoading our files like this will create a giant list of vectors, where each\nvector is a full text file. Those vectors are chunked by paragraph right now,\nbut for our purposes it would be easier if each vector was a single stream of\ntext (like the output of `ocr`, if you'll remember). We can collapse them\ntogether with `paste`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiles <- lapply(files, paste, collapse = \" \")\n```\n:::\n\n\n\n\nFrom here, we can wrap these files in a special \"corpus\" object, which the tm\npackage enables (a corpus is a large collection of texts). A tm corpus works\nsomewhat like a database. It has a section for \"content\", which contains text\ndata, as well as various metadata sections, which we can populate with\nadditional information about our texts, if we wished. Taken together, these\nfeatures make it easy to streamline workflows with text data.\n\nTo make a corpus with tm, we call the `Corpus` function, specifying with\n`VectorSource` (because our texts are vectors):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tm\")\ncorpus <- Corpus(VectorSource(files))\n```\n:::\n\n\n\n\nHere's a high-level glimpse at what's in this object:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<<SimpleCorpus>>\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 18\n```\n\n\n:::\n:::\n\n\n\n\nZooming in to metadata about a text in the corpus:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus[[6]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<<PlainTextDocument>>\nMetadata:  7\nContent:  chars: 424017\n```\n\n\n:::\n:::\n\n\n\n\nNot much here so far, but we'll add more later.\n\nFinally, we can get content from a text:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"stringr\")\n\nstr_sub(corpus[[6]]$content, start = 1, end = 500)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"FRANKENSTEIN:  OR,  THE MODERN PROMETHEUS.  BY MARY W. SHELLEY.  PREFACE.  The event on which this fiction is founded, has been supposed, by Dr. Darwin, and some of the physiological writers of Germany, as not of impossible occurrence. I shall not be supposed as according the remotest degree of serious faith to such an imagination; yet, in assuming it as the basis of a work of fancy, I have not considered myself as merely weaving a series of supernatural terrors. The event on which the interest \"\n```\n\n\n:::\n:::\n\n\n\n\nIn this last view, you can see that the text file is still formatted (at least\nwe didn't have to OCR it!). This formatting is unwieldy and worse, it makes it\nso we can't really access the elements that comprise each novel. We'll need to\ndo more work to preprocess our texts before we can analyze them.\n\n## Preprocessing\n\nPart of preprocessing entails making decisions about the kinds of information\nwe want to know about our data. Knowing what information we want often guides\nthe way we structure data. Put another way: _research questions drive\npreprocessing_.\n\n### Tokenizing and Bags of Words\n\nFor example, it'd be helpful to know how many words are in each novel, which\nmight enable us to study patterns and differences between authors' styles. To\nget word counts, we need to split the text vectors into individual words. One\nway to do this would be to first strip out everything in each novel that isn't\nan alphabetic character or a space. Let's grab one text to experiment with.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrankenstein <- corpus[[6]]$content\nfrankenstein <- str_replace_all(\n  frankenstein, pattern = \"[^A-Za-z]\", replacement = \" \"\n)\n```\n:::\n\n\n\n\nFrom here, it would be easy enough to count the words in a novel by splitting\nits vector on spaces, removing empty elements in the vector, and calling\n`length` on the vector. The end result is what we call a **bag of words**.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrankenstein <- str_split(frankenstein, \" \")\nfrankenstein <- lapply(frankenstein, function(x) x[x != \"\"])\nlength(frankenstein[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 76015\n```\n\n\n:::\n:::\n\n\n\n\nAnd here are the first nine words (or \"tokens\"):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrankenstein[[1]][1:9]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"FRANKENSTEIN\" \"OR\"           \"THE\"          \"MODERN\"       \"PROMETHEUS\"  \n[6] \"BY\"           \"MARY\"         \"W\"            \"SHELLEY\"     \n```\n\n\n:::\n:::\n\n\n\n\n### Text Normalization\n\nWhile easy, producing our bag of words this way is a bit clunky. And further,\nthis process can't handle contractions (\"I'm\", \"don't\", \"that's\") or\ndifferences in capitalization.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrankenstein[[1]][188:191]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Midsummer\" \"Night\"     \"s\"         \"Dream\"    \n```\n\n\n:::\n:::\n\n\n\n\nShould be:\n\n```\nMidsummer Night's Dream\n```\n\nAnd\n\n```\n\"FRANKENSTEIN\", \"Frankenstein\"\n```\n\nShould be:\n\n```\n\"Frankenstein\"\n```\n\nOr, even better:\n\n```\nfrankenstein\n```\n\nTypically, when we work with text data we want all of our words to be in the\nsame case because this makes it easier to do things like counting operations.\nRemember that, to a computer, \"Word\" and \"word\" are two separate words, and if\nwe want to count them together, we need to pick one version or the other.\nMaking all words lowercase (even proper nouns) is the standard. Doing this is\npart of what's called text **normalization**. (Other forms of normalization\nmight entail handling orthographic differences between British and American\nEnglish, like \"color\" and \"colour\".)\n\nAs for contractions, we have some decisions to make. On the one hand, it's\nimportant to retain as much information as we can about the original text, so\nkeeping \"don't\" or \"what's\" (which would be \"don t\" and \"what s\" in our current\nmethod) is important. One way corpus linguists handle these words is to\n**lemmatize** them. Lemmatizing involves removing inflectional endings to\nreturn words to their base form:\n\n* car, cars, car's, cars' => car\n* don't => do\n\nThis is a helpful step if what we're primarily interested in is doing a high-\nlevel analysis of semantics. On the other hand, though, many words that feature\ncontractions are high-frequency function words, which don't have much meaning\nbeyond the immediate context of a sentence or two. Words like \"that's\" or\n\"won't\" appear in huge numbers in text data, but they don't carry much\ninformation in and of themselves---it may in fact be the case that we could get\nrid of them entirely...\n\n### Stop Words\n\n...and indeed this is the case! When structuring text data to study it at\nscale, it's common to remove, or **stop out**, words that don't have much\nmeaning. This makes it much easier to identify significant (i.e. unique)\nfeatures in a text, without having to swim through all the noise of \"the\" or\n\"that,\" which would almost always show up as the highest-occurring words in an\nanalysis. But what words should we remove? Ultimately, this depends on your\ntext data. We can usually assume that function words will be on our list of\n**stop words**, but it may be that you'll have to add or subtract others\ndepending on your data and, of course, your research question.\n\nThe `tm` package has a good starting list. Let's look at the first 100 words.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(stopwords(\"SMART\"), 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] \"a\"            \"a's\"          \"able\"         \"about\"        \"above\"       \n  [6] \"according\"    \"accordingly\"  \"across\"       \"actually\"     \"after\"       \n [11] \"afterwards\"   \"again\"        \"against\"      \"ain't\"        \"all\"         \n [16] \"allow\"        \"allows\"       \"almost\"       \"alone\"        \"along\"       \n [21] \"already\"      \"also\"         \"although\"     \"always\"       \"am\"          \n [26] \"among\"        \"amongst\"      \"an\"           \"and\"          \"another\"     \n [31] \"any\"          \"anybody\"      \"anyhow\"       \"anyone\"       \"anything\"    \n [36] \"anyway\"       \"anyways\"      \"anywhere\"     \"apart\"        \"appear\"      \n [41] \"appreciate\"   \"appropriate\"  \"are\"          \"aren't\"       \"around\"      \n [46] \"as\"           \"aside\"        \"ask\"          \"asking\"       \"associated\"  \n [51] \"at\"           \"available\"    \"away\"         \"awfully\"      \"b\"           \n [56] \"be\"           \"became\"       \"because\"      \"become\"       \"becomes\"     \n [61] \"becoming\"     \"been\"         \"before\"       \"beforehand\"   \"behind\"      \n [66] \"being\"        \"believe\"      \"below\"        \"beside\"       \"besides\"     \n [71] \"best\"         \"better\"       \"between\"      \"beyond\"       \"both\"        \n [76] \"brief\"        \"but\"          \"by\"           \"c\"            \"c'mon\"       \n [81] \"c's\"          \"came\"         \"can\"          \"can't\"        \"cannot\"      \n [86] \"cant\"         \"cause\"        \"causes\"       \"certain\"      \"certainly\"   \n [91] \"changes\"      \"clearly\"      \"co\"           \"com\"          \"come\"        \n [96] \"comes\"        \"concerning\"   \"consequently\" \"consider\"     \"considering\" \n```\n\n\n:::\n:::\n\n\n\n\nThat looks pretty comprehensive so far, though the only way we'll know whether\nit's a good match for our corpus is to process our corpus with it. At first\nglance, the extra random letters in this list seem like they could be a big\nhelp, on the off chance there's some noise from OCR. If you look at the first\nnovel in the corpus, for example, there are a bunch of stray _p_'s, which is\nlikely from a pattern for marking pages (\"p. 7\"):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmessage(str_sub(corpus[[1]]$content, start = 1, end = 1000))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nVATHEK;  AN ARABIAN TALE,    BY  WILLIAM BECKFORD, ESQ.    p. 7VATHEK.  Vathek, ninth Caliph [7a] of the race of the Abassides, was the son of Motassem, and the grandson of Haroun Al Raschid.  From an early accession to the throne, and the talents he possessed to adorn it, his subjects were induced to expect that his reign would be long and happy.  His figure was pleasing and majestic; but when he was angry, one of his eyes became so terrible [7b] that no person could bear to behold it; and the wretch upon whom it was fixed instantly fell backward, and sometimes expired.  For fear, however, of depopulating his dominions, and making his palace desolate, he but rarely gave way to his anger.  Being much addicted to women, and the pleasures of the table, he sought by his affability to procure agreeable companions; and he succeeded the better, p. 8as his generosity was unbounded and his indulgences unrestrained; for he was by no means scrupulous: nor did he think, with the Caliph Omar Ben A\n```\n\n\n:::\n:::\n\n\n\n\nOur stop word list would take care of this. With it, we could return to our\noriginal collection of novels, split them on spaces as before, and filter out\neverything that's stored in our `stop_list` variable. Before we did the\nfiltering, though, we'd need to transform the novels into lowercase (which can\nbe done with stringr's `str_to_lower` function).\n\n### Tokenizers\n\nThis whole process is ultimately straightforward so far, but it would be nice\nto collapse all its steps. Luckily, there are packages we can use to streamline\nour process. The tokenizers package has functions that split a text vector,\nturn words into lowercase forms, and remove stop words, all in a few lines of\ncode. Further, we can combine these functions with a special `tm_map` function\nin the tm package, which will globally apply our changes.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tokenizers\")\n\ncleaned_corpus <- tm_map(\n  corpus,\n  tokenize_words,\n  stopwords = stopwords('SMART'),\n  lowercase = TRUE,\n  strip_punct = TRUE,\n  strip_numeric = TRUE\n)\n```\n:::\n\n\n\n\nYou may see a \"transformation drops documents\" warning after this. You can\ndisregard it. It has to do with the way `tm` references text changes against a\ncorpus's metadata, which we've left blank.\n\nWe can compare our tokenized output with the text data we had been working with\nearlier:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist(\n  untokenized = frankenstein[[1]][1:9],\n  tokenized = cleaned_corpus[[6]]$content[1:5]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$untokenized\n[1] \"FRANKENSTEIN\" \"OR\"           \"THE\"          \"MODERN\"       \"PROMETHEUS\"  \n[6] \"BY\"           \"MARY\"         \"W\"            \"SHELLEY\"     \n\n$tokenized\n[1] \"frankenstein\" \"modern\"       \"prometheus\"   \"mary\"         \"shelley\"     \n```\n\n\n:::\n:::\n\n\n\n\nFrom the title alone we can see how much of a difference tokenizing with stop\nwords makes. And while we lose a bit of information by doing this, what we can\nis a much clearer picture of key words we'd want to further analyze.\n\n### Document Chunking and N-grams\n\nFinally, it's possible to change the way we separate out our text data. Instead\nof tokenizing on words, we could use the tokenizers package to break apart our\ntexts on paragraphs (`tokenize_paragraphs`), sentences (`tokenize_sentences`),\nand more. There might be valuable information to be learned about the average\nsentence length of a novel, for example, so we might chunk it accordingly.\n\nWe might also want to see whether a text contains repeated phrases, or if two\nor three words often occur in the same sequence. We could investigate this by\nadjusting the window around which we tokenize individual words. So far we've\nused the \"unigram,\" or a single word, as our basic unit of counting, but we\ncould break our texts into \"bigrams\" (two word phrases), \"trigrams\" (three word\nphrases), or, well any sequence of $n$ units. Generally, you'll see these\nsequences referred to as **n-grams**:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrankenstein_bigrams <- tokenize_ngrams(\n  corpus[[6]]$content,\n  n = 2,\n  stopwords = stopwords(\"SMART\")\n)\n```\n:::\n\n\n\n\nHere, `n = 2` sets the n-gram window at two:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrankenstein_bigrams[[1]][1:20]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"frankenstein modern\"   \"modern prometheus\"     \"prometheus mary\"      \n [4] \"mary shelley\"          \"shelley preface\"       \"preface event\"        \n [7] \"event fiction\"         \"fiction founded\"       \"founded supposed\"     \n[10] \"supposed dr\"           \"dr darwin\"             \"darwin physiological\" \n[13] \"physiological writers\" \"writers germany\"       \"germany impossible\"   \n[16] \"impossible occurrence\" \"occurrence supposed\"   \"supposed remotest\"    \n[19] \"remotest degree\"       \"degree faith\"         \n```\n\n\n:::\n:::\n\n\n\n\nNote though that, for this function, we'd need to do some preprocessing on our\nown to remove numeric characters and punctuation; `tokenize_ngrams` won't do it\nfor us.\n\n## Counting Terms\n\nLet's return to our single word counts. Now that we've transformed our novels\ninto bags of single words, we can start with some analysis. Simply counting the\nnumber of times a word appears in some data can tell us a lot about a text. The\nfollowing steps should feel familiar: we did them with OCR.\n\nLet's look at _Wuthering Heights_, which is our ninth text:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\")\n\nwuthering_heights <- table(cleaned_corpus[[9]]$content)\nwuthering_heights <- data.frame(\n  word = names(wuthering_heights),\n  count = as.numeric(wuthering_heights)\n)\nwuthering_heights <- arrange(wuthering_heights, desc(count))\n\nhead(wuthering_heights, 30)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         word count\n1  heathcliff   422\n2      linton   348\n3   catherine   339\n4          mr   312\n5      master   185\n6     hareton   169\n7    answered   156\n8        till   151\n9       house   144\n10       door   133\n11        mrs   133\n12     joseph   130\n13       miss   129\n14       time   127\n15       back   121\n16    thought   118\n17      cathy   117\n18       good   117\n19    replied   117\n20   earnshaw   116\n21       eyes   116\n22      cried   114\n23      young   107\n24        day   106\n25     father   106\n26      asked   105\n27       make   105\n28      edgar   104\n29      night   104\n30       made   102\n```\n\n\n:::\n:::\n\n\n\n\nLooks good! The two main characters in this novel are named Heathcliff and\nCatherine, so it makes sense that these words would appear a lot. You can see,\nhowever, that we might want to fine tune our stop word list so that it removes\n\"mr\" and \"mrs\" from the text. Though again, it depends on our research\nquestion. If we're exploring gender roles in nineteenth-century literature,\nwe'd probably keep those words in.\n\nIn addition to fine tuning stop words, pausing here at these counts would be a\ngood way to check whether some other form of textual noise is present in your\ndata, which you haven't yet caught. There's nothing like that here, but you\nmight imagine how consistent OCR noise could make itself known in this view.\n\n### Term Frequency\n\nAfter you've done your fine tuning, it would be good to get a **term\nfrequency** number for each word in this data frame. Raw counts are nice, but\nexpressing those counts in proportion to the total words in a document will\ntell us more information about a word's contribution to the document as a\nwhole. We can get term frequencies for our words by dividing a word's count by\ndocument length (which is the sum of all words in the document).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwuthering_heights$term_frequency <- sapply(\n  wuthering_heights$count,\n  function(x) x / sum(wuthering_heights$count)\n)\nhead(wuthering_heights, 30)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         word count term_frequency\n1  heathcliff   422    0.009619549\n2      linton   348    0.007932709\n3   catherine   339    0.007727552\n4          mr   312    0.007112084\n5      master   185    0.004217101\n6     hareton   169    0.003852379\n7    answered   156    0.003556042\n8        till   151    0.003442066\n9       house   144    0.003282500\n10       door   133    0.003031754\n11        mrs   133    0.003031754\n12     joseph   130    0.002963368\n13       miss   129    0.002940573\n14       time   127    0.002894983\n15       back   121    0.002758212\n16    thought   118    0.002689827\n17      cathy   117    0.002667031\n18       good   117    0.002667031\n19    replied   117    0.002667031\n20   earnshaw   116    0.002644236\n21       eyes   116    0.002644236\n22      cried   114    0.002598646\n23      young   107    0.002439080\n24        day   106    0.002416285\n25     father   106    0.002416285\n26      asked   105    0.002393490\n27       make   105    0.002393490\n28      edgar   104    0.002370695\n29      night   104    0.002370695\n30       made   102    0.002325104\n```\n\n\n:::\n:::\n\n\n\n\n### Plotting Term Frequency\n\nLet's plot the top 50 words in _Wuthering Heights_. We'll call `fct_reorder` in\nthe `aes` layer of `ggplot` to sort words in the descending order of their term\nfrequency.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"ggplot2\")\n\nggplot(wuthering_heights[1:50, ]) +\n  aes(x = fct_reorder(word, -term_frequency), y = term_frequency) +\n  geom_bar(stat =\"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)\n  ) +\n  labs(\n    title = \"Top 50 words in Wuthering Heights\",\n    x = \"Word\",\n    y = \"Term Frequency\"\n  )\n```\n\n::: {.cell-output-display}\n![](15_natural-language-processing_files/figure-html/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n\nThis is a good start for creating a high-level view of the novel, but further\ntuning might be in order. We've already mentioned \"mrs\" and \"mr\" as two words\nthat we could cut out of the text. Another option would be to collapse these\ntwo words together into a **base form** by **stemming** them. Though this would\noverweight their base form (which in this case is \"mr\") in terms of term\nfrequency, it would also free up space to see other terms in the document.\nOther examples of stemming words would be transforming \"fishing\", \"fished\", and\n\"fisher\" all into \"fish.\"\n\nThat said, like all preprocessing, lemmatizing words is an _interpretive\ndecision_, which comes with its own consequences. Maybe it's okay to transform\n\"mr\" and \"mrs\" into \"mr\" for some analyses, but it's also the case that we'd be\nerasing potentially important gender differences in the text---and would do so\nby overweighting the masculine form of the word. Regardless of what you decide,\nit's important to keep track of these decisions as you make them because they\nwill impact the kinds of claims you make about your data later on.\n\n### Comparing Term Frequencies Across Documents\n\nTerm frequency is helpful if we want to start comparing words across two texts.\nWe can make some comparisons by transforming the above code into a function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nterm_table <- function(text) {\n  term_tab <- table(text)\n\n  term_tab <- data.frame(word = names(term_tab), count = as.numeric(term_tab))\n  term_tab$term_frequency <- sapply(\n    term_tab$count,\n    function(x) (x/sum(term_tab$count))\n  )\n\n  arrange(term_tab, desc(count))\n}\n```\n:::\n\n\n\n\nWe already have a term table for _Wuthering Heights_. Let's make one for\n_Dracula_:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndracula <- term_table(cleaned_corpus[[18]]$content)\nhead(dracula, 30)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        word count term_frequency\n1       time   387    0.007280458\n2        van   321    0.006038829\n3    helsing   299    0.005624953\n4       back   261    0.004910076\n5       room   231    0.004345699\n6       good   225    0.004232824\n7       lucy   225    0.004232824\n8        man   224    0.004214012\n9       dear   219    0.004119949\n10      mina   217    0.004082324\n11     night   217    0.004082324\n12      hand   209    0.003931823\n13      face   205    0.003856573\n14      door   201    0.003781323\n15      made   193    0.003630822\n16      poor   192    0.003612010\n17     sleep   190    0.003574385\n18      eyes   186    0.003499135\n19    looked   185    0.003480322\n20    friend   183    0.003442697\n21     great   182    0.003423884\n22  jonathan   182    0.003423884\n23        dr   178    0.003348634\n24    things   174    0.003273384\n25      make   163    0.003066446\n26       day   160    0.003010008\n27 professor   155    0.002915946\n28     count   153    0.002878320\n29     found   153    0.002878320\n30   thought   153    0.002878320\n```\n\n\n:::\n:::\n\n\n\n\nNow we can compare the relative frequency of a word across two novels:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomparison_words <- c(\"dark\", \"night\", \"ominous\")\nfor (i in comparison_words) {\n  wh <- list(wh = subset(wuthering_heights, word == i))\n  drac <- list(drac = subset(dracula, word == i))\n  print(wh)\n  print(drac)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$wh\n    word count term_frequency\n183 dark    32   0.0007294445\n\n$drac\n   word count term_frequency\n90 dark    77    0.001448566\n\n$wh\n    word count term_frequency\n29 night   104    0.002370695\n\n$drac\n    word count term_frequency\n11 night   217    0.004082324\n\n$wh\n        word count term_frequency\n7283 ominous     1   2.279514e-05\n\n$drac\n        word count term_frequency\n7217 ominous     1   1.881255e-05\n```\n\n\n:::\n:::\n\n\n\n\nNot bad! We might be able to make a few generalizations from this, but to say\nanything definitively, we'll need to scale our method. Doing so wouldn't be\neasy with this setup as it stands now. While it's true that we could write some\nfunctions to roll through these two data frames and systematically compare the\nwords in each, it would take a lot of work to do so. Luckily, the `tm` package\n(which we've used to make our stop word list) features generalized functions\nfor just this kind of thing.\n\n## Text Mining Pipepline\n\nBefore going further, we should note that `tm` has its own functions for\npreprocessing texts. To send raw files directly through those functions, you'd\ncall `tm_map` in conjunction with these functions. You can think of `tm_map` as\na cognate to the `apply` family.\n\n```r\ncorpus_2 <- Corpus(VectorSource(files))\ncorpus_2 <- tm_map(corpus_2, removeNumbers)\ncorpus_2 <- tm_map(corpus_2, removeWords, stopwords(\"SMART\"))\ncorpus_2 <- tm_map(corpus_2, removePunctuation)\ncorpus_2 <- tm_map(corpus_2, stripWhitespace)\n```\n\nNote the order of operations here: because our stop words list takes into\naccount punctuated words, like \"don't\" or \"i'm\", we want to remove stop words\n_before_ removing punctuation. If we didn't do this, `removeWords` wouldn't\ncatch the un-punctuated \"dont\" or \"im\". This won't always be the case, since we\ncan use different stop word lists, which may have a different set of terms, but\nin this instance, the order in which we preprocess matters.\n\nPreparing your text files like this would be fine, and indeed sometimes it's\npreferable to sequentially step through each part of the preprocessing\nworkflow. That said, `tokenizers` manages the order of operations above on its\nown and its preprocessing functions are generally a bit faster to run (in\nparticular, `removeWords` is quite slow in comparison to `tokenize_words`).\n\nThere is, however, one caveat to using `tokenizers`. It splits documents up to\ndo text cleaning, but other functions in `tm` require non-split documents. If\nwe use `tokenizers`, then, we need to do a quick workaround with `paste`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleaned_corpus <- lapply(cleaned_corpus, paste, collapse = \" \")\n```\n:::\n\n\n\n\nAnd then reformat that output as a corpus object:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleaned_corpus <- Corpus(VectorSource(cleaned_corpus))\n```\n:::\n\n\n\n\nUltimately, it's up to you to decide what workflow makes sense. Personally, I\n(Tyler) like to do exploratory preprocessing steps with `tokenizers`, often\nwith a sample set of all the documents. Then, once I've settled on my stop word\nlist and so forth, I reprocess all my files with the `tm`-specific functions\nabove.\n\nRegardless of what workflow you choose, preprocessing can take a while, so now\nwould be a good place to save your data. That way, you can retrieve your corpus\nlater on.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(cleaned_corpus, \"data/C19_novels_cleaned.rds\")\n```\n:::\n\n\n\n\nLoading it back in is straightforward:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleaned_corpus <- readRDS(\"data/C19_novels_cleaned.rds\")\n```\n:::\n\n\n\n\n## Document Term Matrix\n\nThe advantage of using a `tm` corpus is that it makes comparing data easier.\nRemember that, in our old workflow, looking at the respective term frequencies\nin two documents entailed a fair bit of code. And further, we left off before\ngeneralizing that code to the corpus as a whole. But what if we wanted to look\nat a term across multiple documents?\n\nTo do so, we need to create what's called a **document-term matrix**, or DTM. A\nDTM describes the frequency of terms across an entire corpus (rather than just\none document). Rows of the matrix correspond to documents, while columns\ncorrespond to the terms. For a given document, we count the number of times\nthat term appears and enter that number in the column in question. We do this\n_even if_ the count is 0; key to the way a DTM works is that it's a\n_corpus-wide_ representation of text data, so it matters if a text does or\ndoesn't contain a term.\n\nHere's a simple example with three documents:\n\n* Document 1: \"I like cats\"\n* Document 2: \"I like dogs\"\n* Document 3: \"I like both cats and dogs\"\n\nTransforming these into a document-term matrix would yield:\n\n|   n_doc  | I | like | both | cats | and | dogs |\n|:--------:|:-:|:----:|:----:|:----:|:---:|:----:|\n|     1    | 1 |  1   |  0   |  1   |  0  |   0  |\n|     2    | 1 |  1   |  0   |  0   |  0  |   1  |\n|     3    | 1 |  1   |  1   |  1   |  1  |   1  |\n\n: {.striped .hover}\n\nRepresenting texts in this way is incredibly useful because it enables us to\neasily discern similarities and differences in our corpus. For example, we can\nsee that each of the above documents contain the words \"I\" and \"like.\" Given\nthat, if we wanted to know what makes documents unique, we can ignore those two\nwords and focus on the rest of the values.\n\nNow, imagine doing this for thousands of words! What patterns might emerge?\n\nLet's try it on our corpus. We can transform a `tm` corpus object into a DTM by\ncalling `DocumentTermMatrix`.\n\n::: {.callout-warning}\n`DocumentTermMatrix` is one of the functions in the tm package that requires\nnon-split documents, so before you call it make sure you know how you've\npreprocessed your texts!\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- DocumentTermMatrix(cleaned_corpus)\n```\n:::\n\n\n\n\nThis object is quite similar to the one that results from `Corpus`: it contains\na fair bit of metadata, as well as an all-important `dimnames` field, which\nrecords the documents in the matrix and the entire term vocabulary. We access\nall of this information with the same syntax we use for data frames.\n\nLet's look around a bit and get some high-level info.\n\n## Corpus Analytics\n\nNumber of columns in the DTM (that is, the vocabulary size):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm$ncol\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 34925\n```\n\n\n:::\n:::\n\n\n\n\nNumber of rows in the DTM (that is, the number of documents this matrix\nrepresents):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm$nrow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18\n```\n\n\n:::\n:::\n\n\n\n\nRight now, the document names are just a numbers in a vector:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm$dimnames$Docs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\"\n```\n\n\n:::\n:::\n\n\n\n\nBut they're ordered according to the sequence in which the corpus was\noriginally created. This means we can use our metadata from way back when to\nassociate a document with its title:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm$dimnames$Docs <- manifest$title\ndtm$dimnames$Docs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Vathek\"                        \"ASicilianRomance\"             \n [3] \"TheMysteriesofUdolpho\"         \"TheMonk\"                      \n [5] \"SenseandSensibility\"           \"Frankenstein\"                 \n [7] \"Ivanhoe\"                       \"TheNarrativeofArthurGordonPym\"\n [9] \"WutheringHeights\"              \"TheHouseoftheSevenGables\"     \n[11] \"NorthandSouth\"                 \"TheWomaninWhite\"              \n[13] \"GreatExpectations\"             \"PortraitofaLady\"              \n[15] \"TreasureIsland\"                \"JekyllandHyde\"                \n[17] \"ThePictureofDorianGray\"        \"Dracula\"                      \n```\n\n\n:::\n:::\n\n\n\n\nWith this information associated, we can use `inspect` to get a high-level view\nof the corpus:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninspect(dtm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<<DocumentTermMatrix (documents: 18, terms: 34925)>>\nNon-/sparse entries: 145233/483417\nSparsity           : 77%\nMaximal term length: 19\nWeighting          : term frequency (tf)\nSample             :\n                          Terms\nDocs                       back day eyes good great long made man thought time\n  Dracula                   261 160  186  225   182  147  193 224     153  387\n  GreatExpectations         244 216  180  256   198  173  300 307     238  373\n  Ivanhoe                    77 138  100  298   111  154  151 235      46  182\n  NorthandSouth             184 257  197  316   179  211  234 270     332  423\n  PortraitofaLady           210 241  226  520   421  187  381 317     302  339\n  TheHouseoftheSevenGables   79 113   72  100   144  153  144 211      60  113\n  TheMonk                    81 106  184   80    66  108  167  95      72  162\n  TheMysteriesofUdolpho     117 167  225  186   164  359  316 213     341  367\n  TheWomaninWhite           417 351  233  235   112  188  244 443     183  706\n  WutheringHeights          121 106  116  117    63   97  102  88     118  127\n```\n\n\n:::\n:::\n\n\n\n\nOf special note here is **sparsity**. Sparsity measures the amount of 0s in the\ndata. This happens when a document does not contain a term that appears\nelsewhere in the corpus. In our case, of the 628,650 entries in this matrix,\n80% of them are 0. Such is the way of working with DTMs: they're big, expansive\ndata structures that have a lot of empty space.\n\nWe can zoom in and filter on term counts with `findFreqTerms`. Here are terms\nthat appear more than 1,000 times in the corpus:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfindFreqTerms(dtm, 1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"answered\" \"appeared\" \"asked\"    \"back\"     \"day\"      \"dear\"    \n [7] \"death\"    \"door\"     \"eyes\"     \"face\"     \"father\"   \"felt\"    \n[13] \"found\"    \"friend\"   \"gave\"     \"give\"     \"good\"     \"great\"   \n[19] \"half\"     \"hand\"     \"hands\"    \"head\"     \"hear\"     \"heard\"   \n[25] \"heart\"    \"hope\"     \"kind\"     \"knew\"     \"lady\"     \"leave\"   \n[31] \"left\"     \"life\"     \"light\"    \"long\"     \"looked\"   \"love\"    \n[37] \"made\"     \"make\"     \"man\"      \"men\"      \"mind\"     \"moment\"  \n[43] \"morning\"  \"mother\"   \"night\"    \"part\"     \"passed\"   \"people\"  \n[49] \"person\"   \"place\"    \"poor\"     \"present\"  \"put\"      \"replied\" \n[55] \"returned\" \"round\"    \"side\"     \"speak\"    \"stood\"    \"thing\"   \n[61] \"thou\"     \"thought\"  \"till\"     \"time\"     \"told\"     \"turned\"  \n[67] \"voice\"    \"woman\"    \"words\"    \"world\"    \"young\"    \"count\"   \n[73] \"house\"    \"madame\"   \"room\"     \"sir\"      \"emily\"    \"margaret\"\n[79] \"miss\"     \"mrs\"      \"isabel\"  \n```\n\n\n:::\n:::\n\n\n\n\nUsing `findAssocs`, we can also track which words rise and fall in usage\nalongside a given word. (The number in the third argument position of this\nfunction is a cutoff for the strength of a correlation.)\n\nHere's \"boat\":\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfindAssocs(dtm, \"boat\", .85)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$boat\n  thumping scoundrels     midday  direction \n      0.94       0.88       0.87       0.85 \n```\n\n\n:::\n:::\n\n\n\n\nHere's \"writing\" (there are a lot of terms, so we'll limit to 15):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwriting <- findAssocs(dtm, \"writing\", .85)\nwriting[[1]][1:15]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     letter        copy    disposal   inquiries    bedrooms   hindrance \n       0.99        0.97        0.97        0.97        0.97        0.97 \n   messages certificate    distrust     plainly    drawings   anonymous \n       0.97        0.97        0.96        0.96        0.96        0.96 \n   ladyship  plantation    lodgings \n       0.96        0.96        0.96 \n```\n\n\n:::\n:::\n\n\n\n\n### Corpus Term Counts\n\nFrom here, it would be useful to get a full count of all the terms in the\ncorpus. We can transform the DTM into a matrix and then a data frame:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nterm_counts <- as.matrix(dtm)\nterm_counts <- data.frame(sort(colSums(term_counts), decreasing = TRUE))\nterm_counts <- cbind(newColName = rownames(term_counts), term_counts)\ncolnames(term_counts) <- c(\"term\", \"count\")\n```\n:::\n\n\n\n\nAs before, let's plot the top 50 terms in these counts, but this time, they\nwill cover the entire corpus:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(term_counts[1:50, ]) +\n  aes(x = fct_reorder(term, -count), y = count) +\n  geom_bar(stat = \"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)\n  ) +\n  labs(\n    title = \"Top 50 words in 18 Nineteenth-Century Novels\",\n    x = \"Word\",\n    y = \"Count\"\n  )\n```\n\n::: {.cell-output-display}\n![](15_natural-language-processing_files/figure-html/unnamed-chunk-39-1.png){width=960}\n:::\n:::\n\n\n\n\nThis looks good, though the words here are all pretty common. In fact, many of\nthem are simply the most common words in the English language. \"Time\" is the\n64th-most frequent word in English; \"make\" is the 50th. As it stands, then,\nthis graph doesn't tell us very much about the _specificity_ of our particular\ncollection of texts; if we ran the same process on English novels from the\ntwentieth century, we'd probably produce very similar output.\n\n### TF-IDF Scores\n\nGiven this, if we want to know what makes our corpus special, we need a measure\nof uniqueness for the terms it contains. One of the most common ways to do this\nis to get what's called a **TF-IDF** score (short for \"term frequency-inverse\ndocument frequency\") for each term in our corpus. TF-IDF is a weighting method.\nIt increases proportionally to the number of times a word appears in a document\nbut is importantly offset by the number of documents in the corpus that contain\nthis term. This offset adjusts for common words across a corpus, pushing their\nscores down while boosting the scores of rarer terms in the corpus.\n\nInverse document frequency can be expressed as:\n\n\\begin{align*}\nidf_i = log(\\frac{n}{df_i})\n\\end{align*}\n\nWhere $idf_i$ is the idf score for term $i$, $df_i$ is the number of documents\nthat contain $i$, and $n$ is the total number of documents.\n\nA TF-IDF score can be calculated by the following:\n\n\\begin{align*}\nw_i,_j = tf_i,_j \\times idf_i\n\\end{align*}\n\nWhere $w_i,_j$ is the TF-IDF score of term $i$ in document $j$, $tf_i,_j$ is\nthe term frequency for $i$ in $j$, and $idf_i$ is the inverse document score.\n\nWhile it's good to know the underlying equations here, you won't be tested on\nthe math specifically. And as it happens, tm has a way to perform the above\nmath for each term in a corpus. We can implement TF-IDF scores when making a\ndocument-term matrix:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm_tfidf <- DocumentTermMatrix(\n  cleaned_corpus,\n  control = list(weighting = weightTfIdf)\n)\ndtm_tfidf$dimnames$Docs <- manifest$title\n```\n:::\n\n\n\n\nTo see what difference it makes, let's plot the top terms in our corpus using\ntheir TF-IDF scores:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntfidf_counts <- as.matrix(dtm_tfidf)\ntfidf_counts <- data.frame(sort(colSums(tfidf_counts), decreasing = TRUE))\ntfidf_counts <- cbind(newColName = rownames(tfidf_counts), tfidf_counts)\ncolnames(tfidf_counts) <- c(\"term\", \"tfidf\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = tfidf_counts[1:50, ]) +\n  aes(x = fct_reorder(term, -tfidf), y = tfidf) +\n  geom_bar(stat = \"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)\n  ) +\n  labs(\n    title = \"Words with the 50-highest TF-IDF scores in 18 Nineteenth-Century Novels\",\n    x = \"Word\",\n    y = \"TF-IDF\"\n  )\n```\n\n::: {.cell-output-display}\n![](15_natural-language-processing_files/figure-html/unnamed-chunk-42-1.png){width=960}\n:::\n:::\n\n\n\n\nLots of names! That makes sense: heavily weighted terms in these novels are\ngoing to be terms that are unique to each text. Main characters' names are used\na lot in novels, and the main character names in these novels are all unique.\n\nTo see in more concrete way how TF-IDF scores might make a difference in the\nway we analyze our corpus, we'll do two last things. First, we'll look again at\nterm correlations, using the same words from above with `findAssocs`, but this\ntime we'll use TF-IDF scores.\n\nHere's \"boat\":\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfindAssocs(dtm_tfidf, terms = \"boat\", corlimit = .85)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$boat\n   thumping       shore      bucket      cables         doo       geese \n       0.95        0.93        0.92        0.92        0.92        0.92 \n    pickled         sea      rudder     gunwale  scoundrels       boats \n       0.92        0.91        0.91        0.91        0.91        0.90 \n       keel      sailed        crew    baffling     biscuit    bowsprit \n       0.90        0.89        0.89        0.89        0.89        0.89 \n    hauling     muskets      ripped      splash      anchor         oar \n       0.89        0.89        0.89        0.89        0.88        0.88 \n   rattling       sandy        cook      patted     shipped       beach \n       0.88        0.88        0.88        0.88        0.88        0.87 \n    pistols      seamen     tobacco         lee    bulwarks      hauled \n       0.87        0.87        0.87        0.87        0.87        0.87 \n    inkling      musket  navigation        rags    steering      island \n       0.87        0.87        0.87        0.87        0.87        0.86 \n     bottle     tumbled       avast       belay       bilge   broadside \n       0.86        0.86        0.86        0.86        0.86        0.86 \n   cruising   cutlasses    diagonal   furtively     headway     jupiter \n       0.86        0.86        0.86        0.86        0.86        0.86 \n   mainland      marlin      midday     monthly   mutineers outnumbered \n       0.86        0.86        0.86        0.86        0.86        0.86 \n    plumped     riggers    schooner   schooners   seaworthy    swamping \n       0.86        0.86        0.86        0.86        0.86        0.86 \n     tide's      tiller     tonnage       towed       yawed        sail \n       0.86        0.86        0.86        0.86        0.86        0.85 \n       ship         tap     loading       sails         aft      berths \n       0.85        0.85        0.85        0.85        0.85        0.85 \n     pinned \n       0.85 \n```\n\n\n:::\n:::\n\n\n\n\nHere's \"writing\":\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfindAssocs(dtm_tfidf, terms = \"writing\", corlimit = .85)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$writing\n    hindrance      messages      disposal     inquiries      bedrooms \n         0.92          0.91          0.90          0.90          0.89 \n     ladyship          copy      lodgings        london    unforeseen \n         0.88          0.87          0.87          0.87          0.87 \n     drawings    plantation  explanations   certificate         dears \n         0.86          0.86          0.86          0.86          0.86 \nneighbourhood    allowances \n         0.85          0.85 \n```\n\n\n:::\n:::\n\n\n\n\nThe semantics of these results have changed. For \"boats\", we get much more\nterms related to seafaring. Most probably this is because only a few novels\ntalk about boats so these terms correlate highly with one another. For\n\"writing\", we've interestingly lost a lot of the words associated with writing\nin a strict sense (\"copy\", \"message\") but we've gained instead a list of terms\nthat seem to situate us in _where_ writing takes place in these novels, or what\ncharacters write _about_. So far though this is speculation; we'd have to look\ninto this further to see whether the hypothesis holds.\n\nFinally, we can disaggregate our giant term count graph from above to focus\nmore closely on the uniqueness of individual novels in our corpus. First, we'll\nmake a data frame from our TF-IDF DTM. We'll transpose the DTM so the documents\nare our variables (columns) and the corpus vocabulary terms are our\nobservations (or rows). Don't forget the `t`!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntfidf_df <- as.matrix(dtm_tfidf)\ntfidf_df <- as.data.frame(t(tfidf_df))\ncolnames(tfidf_df) <- manifest$title\n```\n:::\n\n\n\n\n### Unique Terms in a Document\n\nWith this data frame made, we can order our rows by the highest value for a\ngiven column. In other words, we can find out not only the top terms for a\nnovel, but the top most _unique_ terms in that novel.\n\nHere's _Dracula_:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nordering <- order(tfidf_df$Dracula, decreasing = TRUE)\nrownames(tfidf_df[ordering[1:50], ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"helsing\"      \"mina\"         \"lucy\"         \"jonathan\"     \"van\"         \n [6] \"harker\"       \"godalming\"    \"quincey\"      \"seward\"       \"professor\"   \n[11] \"morris\"       \"lucy's\"       \"harker's\"     \"diary\"        \"seward's\"    \n[16] \"arthur\"       \"renfield\"     \"westenra\"     \"whilst\"       \"undead\"      \n[21] \"tonight\"      \"whitby\"       \"dracula\"      \"varna\"        \"carfax\"      \n[26] \"journal\"      \"helsing's\"    \"count\"        \"count's\"      \"hawkins\"     \n[31] \"madam\"        \"galatz\"       \"jonathan's\"   \"mina's\"       \"pier\"        \n[36] \"wolves\"       \"tomorrow\"     \"czarina\"      \"telegram\"     \"boxes\"       \n[41] \"today\"        \"holmwood\"     \"hypnotic\"     \"garlic\"       \"vampire\"     \n[46] \"phonograph\"   \"transylvania\" \"cliff\"        \"piccadilly\"   \"slovaks\"     \n```\n\n\n:::\n:::\n\n\n\n\nNote here that some contractions have slipped through. Lemmatizing would take\ncare of this, though we could also go back to the corpus object and add in\nanother step with `tm_map` and then make another DTM:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleaned_corpus <- tm_map(\n  cleaned_corpus, str_remove_all, pattern = \"\\\\'s\", replacement = \" \"\n)\n```\n:::\n\n\n\n\nWe won't bother to do this whole process now, but it's a good example of how\niterative the preprocessing workflow is.\n\nHere's _Frankenstein_:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nordering <- order(tfidf_df$Frankenstein, decreasing = TRUE)\nrownames(tfidf_df[ordering[1:50], ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"clerval\"      \"justine\"      \"elizabeth\"    \"felix\"        \"geneva\"      \n [6] \"frankenstein\" \"safie\"        \"cottagers\"    \"dmon\"        \"ingolstadt\"  \n[11] \"kirwin\"       \"agatha\"       \"victor\"       \"ernest\"       \"mont\"        \n[16] \"krempe\"       \"lacey\"        \"waldman\"      \"agrippa\"      \"walton\"      \n[21] \"mountains\"    \"creator\"      \"cottage\"      \"sledge\"       \"hovel\"       \n[26] \"switzerland\"  \"ice\"          \"beaufort\"     \"cornelius\"    \"william\"     \n[31] \"protectors\"   \"moritz\"       \"henry\"        \"labours\"      \"chamounix\"   \n[36] \"glacier\"      \"jura\"         \"blanc\"        \"endeavoured\"  \"lake\"        \n[41] \"leghorn\"      \"monster\"      \"rhine\"        \"magistrate\"   \"belrive\"     \n[46] \"lavenza\"      \"salve\"       \"saville\"      \"strasburgh\"   \"werter\"      \n```\n\n\n:::\n:::\n\n\n\n\nAnd here's _Sense and Sensibility_:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nordering <- order(tfidf_df$SenseandSensibility, decreasing = TRUE)\nrownames(tfidf_df[ordering[1:50], ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"elinor\"       \"marianne\"     \"dashwood\"     \"jennings\"     \"willoughby\"  \n [6] \"lucy\"         \"brandon\"      \"barton\"       \"ferrars\"      \"colonel\"     \n[11] \"mrs\"          \"marianne's\"   \"edward\"       \"middleton\"    \"elinor's\"    \n[16] \"norland\"      \"palmer\"       \"steele\"       \"dashwoods\"    \"jennings's\"  \n[21] \"willoughby's\" \"edward's\"     \"delaford\"     \"steeles\"      \"cleveland\"   \n[26] \"mama\"         \"dashwood's\"   \"lucy's\"       \"brandon's\"    \"fanny\"       \n[31] \"allenham\"     \"middletons\"   \"devonshire\"   \"combe\"        \"ferrars's\"   \n[36] \"sister\"       \"morton\"       \"miss\"         \"margaret\"     \"park\"        \n[41] \"charlotte\"    \"exeter\"       \"magna\"        \"berkeley\"     \"harley\"      \n[46] \"john\"         \"middleton's\"  \"parsonage\"    \"beaux\"        \"behaviour\"   \n```\n\n\n:::\n:::\n\n\n\n\nNames still rank high, but we can see in these results other words that indeed\nseem to be particular to each novel. With this data, we now have a sense of\nwhat makes each document unique in its relationship with all other documents in\na corpus.\n",
    "supporting": [
      "15_natural-language-processing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}