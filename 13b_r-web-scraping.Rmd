Getting Data from the Web
============

The internet in general and the World Wide Web specifically serves as a 
vast source of data suitable for answering a wide range of research questions. 
In this lesson we cover several common methods for acquiring this data.

After this lesson, you should be able to:

* Explain and read hypertext markup language (HTML)
* View the HTML source of a web page
* Use Firefox or Chrome's web developer tools to locate tags within a web page
* With the rvest package:
    + Read HTML into R
    + Extract HTML tables as data frames
* With the xml2 package:
    + Use XPath to extract specific elements of a page



Overview
-------------------------

While there are many methods (and R packages) for acquiring data from 
the web, all fall into one of three general categories of data 
acquisition:

* Direct Download of data from a specified enpoint;
* Application Programming Interface (API) access;
* Scraping

_Direct Download_ describes process where a data provider has provided 
a specific URL  or web link from which you can download the data.  For 
example, when you download data from the "Files" are of Canvas you are
using the Direct Download method of data acquisition.

_Application Programming Interfaces (APIs)_ are web accessible endpoints 
that you access via a URL, just as you would any website, but that are 
designed specifically to interact with computers (as opposed to humans).  
APIs receive requests and return data to the requester in machine, as 
opposed to human, readable formats, such as JSON or XML.  We will learn 
more about working with APIs in this unit.

_Scraping_ a web page means extracting information from human readable 
internet sources so that it can be used programmatically (for instance, 
in R).  We will also learn more about Scraping in this unit.

Each of the above general methods can be accomplished by applying any 
number of sub-methods and packages.  And each brings with it its own 
degree of complexity and difficulty.  As a general rule, the various 
ways you can get data from the web can be ranked according to difficulty
from most to least convenient as follows:

1. Direct download or "data dump"
2. R or Python package (there are packages for many popular web APIs)
3. Documented web API
4. Undocumented web API
5. Scraping


APIs
-------------------------

As noted earlier, and Application Programmoing Interface (API) provides 
a machine readable gateway for accessing data from the web.  Most APIs 
provide programmatic access to the data that lives behind a human readable 
website.  For example, most social media platforms such as Twitter, Facebook, 
and Instagram provide APIs that allow computers to programmatically access 
the same data that you, as a human, see when you interct with these platforms 
via a web broswer of mobile app.  Some APIs, however, are stand alone, in that 
they provide machine access to data sources that have no human readable interface.

One of the challendges with working with APIs is that, while there are some 
standards of behavior for APIs, you need to know what and how to query a specific 
API in order to interact with it.  Some APIs are well documented, while others are 
not.  And some, for example the Twitter API, has extensive documentation that is 
frequently erroneous, incommplete, or out of date.  As a result, working with APIs 
can sometimes be challenging.

For this unit, we will work with the API to the National Oceanographic and Atmospheric 
Administration (NOAA) [FishWatch](https://www.fishwatch.gov/) database of U.S. Seafood Facts.

### Querying to an API

Many popular APIs (Twitter, Facebook, etc.) have specific R pacages designed to 
facilitate interaction with their APIs.  Because APIs are web accessible, however, all can 
be accessed programmatically using basic internet request protocols, just as if you were 
going to a human readable webpage, provided you know how to formulate your request as a
URL.  The FishWatch database publishes an easy to follow set of instructios for accessing its
API at [https://www.fishwatch.gov/developers](https://www.fishwatch.gov/developers).

We can see from the documentation that the API will allow us to query a list of 
fish species that appear in the database using a URL with the following construction:

```
https://www.fishwatch.gov/api/species
```

You can actually go to this URL in your web browser and see the response, a protion 
of which is reproduced here:

```
[{"Fishery Management":"<ul>\n<li><a href=\"https://www.fisheries.noaa.gov/region/new-england-mid-atlantic\">NOAA Fisheries</a> and the <a href=\"https://www.nefmc.org/index.html\">New England Fishery Management Council </a>manage the white hake fishery.</li>\n<li>White hake is managed as a single stock&nbsp;in U.S. waters.</li>\n<li>White hake, along with other groundfish in New England waters, is managed under the <a href=\"https://www.nefmc.org/management-plans/small-mesh-multispecies\">Northeast Multispecies Fishery Management Plan</a>, which includes:\n<ul>\n<li>Permitting requirements for commercial vessels.</li>\n<li>Separate management measures for recreational vessels.</li>\n<li>Time/area closures to protect spawning fish and habitat.</li>\n<li>Annual catch limits&nbsp;based on best available science.</li>\n</ul>\n</li>\n<li>An optional sector (<a href=\"https://www.fisheries.noaa.gov/new-england-mid-atlantic/commercial-fishing/sector-management-northeast-multispecies-fishery\">catch share</a>)
```

If you look closely at this extract form the response, you will see that it 
contains HTML but is not HTML. Remember that APIs exist to deliver machine 
readable information.  In this case, the API is delivering data in the JSON 
format, and some of the fields in the JSON object contain information provided 
as HTML.

Because the response is machine readable, we can make better use of the query if 
we run it in R rather than our web browser. Before we can do so, we need to 
setup our R environment to execute HTTP queries against the API and to process 
JSON. We'll use the `httr` package to process our http transactions and the 
`jsonlite` package to process the JSON that we receive

```{r, eval=FALSE}
install.packages("httr")
install.packages("jsonlite")
```

With our packages installed, we execture our query in R with one simple command:

```{r}
library(httr)
library(jsonlite)

response <- GET("https://www.fishwatch.gov/api/species")
```

The above executes an HTTP GET request (just like your web browser) to the identified 
query URL and loads it into an httr "response" object.  If you take some time to examine 
the response object, you will see that it is a container object that contains a lot of 
useful information in addition to the actual JSON response that you see when you load the 
URL in your browser.  For example, we can check the status of the reponse to see if it was
successfull by look at the reponse "status_code" which shoudl be 200 if the query executed 
successfully.

```{r}
cat(response$status_code)
```

The actual content of the response can be found in the response object's "content" 
element. Note, however, that we have to do some formatting on the object in order 
to access it. Go ahead and look a the actual `response$content` object

```{r}
response$content
```

Above we see that the content that is received from the API is delivered as a 
compressed binary string, which must be uncompressed and converted back to 
character encoding before processing. We co this using a built-in `httr` function:

```{r}
bdy <- content(response, "text")
```

Once we have the content extracted and converted, we can begin to process it. We 
previously examined the response and determined that it is in JSON format, so 
our step will be to load the content into a JSON object for ease of traversal:

```{r}
bdy_json <- fromJSON(bdy)
```

If you examine the class and structure of the `bdy_json` object, you will see that 
`jsonlite` has converted the json structure into a nice, R dataframe where you 
can begin the process of exploration and cleaning in preparation for research.


Web Scraping
-------------------------

### What's in a Web Page?

Modern web pages usually consist of many files:

* Hypertext markup language (HTML) for structure and formatting
* Cascading style sheets (CSS) for more formatting
* JavaScript (JS) for interactivity
* Images

HTML is the only component that always has to be there. Since HTML is what
gives a web page structure, it's what we'll focus on when scraping.

HTML is closely related to eXtensible markup language (XML). Both languages use
_tags_ to mark structural _elements_ of data. In HTML, the elements literally
correspond to the elements of a web page: paragraphs, links, tables, and so on.

Most tags come in pairs. The _opening tag_ marks the beginning of an element
and the _closing tag_ marks the end. Opening tags are written `<NAME>`, where
`NAME` is the name of the tag. Closing tags are written `</NAME>`.

A _singleton tag_ is a tag that stands alone, rather than being part of a pair.
Singleton tags are written `<NAME />`. In HTML (but not XML) they can also be
written `<NAME>`. Fortunately, HTML only has a few singleton tags, so they can
be distinguished by name regardless of which way they're written.

For example, here's some HTML that uses the `em` (emphasis, usually italic) and
`strong` (usually bold) tags, as well as the singleton `br` (line break) tag: 

```html
<em><strong>This text</strong> is emphasized.<br /></em> Not emphasized
```

A pair of tags can contain other elements (paired or singleton tags), but not a
lone opening or closing tag. This creates a strict, treelike hierarchy.

Opening and singleton tags can have _attributes_ that contain additional
information. Attributes are name-value pairs written `NAME="VALUE"` after the
tag name.

For instance, the HTML `a` (anchor) tag creates a link to the URL provided for
the `href` attribute: 

```html
<a href="http://www.google.com/" id="mytag">My Search Engine</a>
```

In this case the tag also has a value set for the `id` attribute.

Now let's look at an example of HTML for a complete, albeit simple, web page:

```html
<html>
  <head>
    <title>This is the page title!</title>
  </head>
  <body>
    <h1>This is a header!</h1>
    <p>This is a paragraph.
      <a href="http://www.r-project.org/">Here's a website!</a>
    </p>
    <p id="hello">This is another paragraph.</p>
  </body>
</html>
```

In most web browsers, you can examine the HTML for a web page by right-clicking
and choosing "View Page Source".

See [here][html_basics] for a more detailed explanation of HTML, and
[here][html_ref] for a list of valid HTML elements.

[html_basics]: https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics
[html_ref]: https://developer.mozilla.org/en-US/docs/Web/HTML/Element


R's XML Parsers
---------------

A _parser_ converts structured data into familiar data structures. R has two
popular packages for parsing XML (and HTML):

* The "XML" package
* The ["xml2" package][xml2]

The XML package has more features. The xml2 package is more user-friendly, and
as part of the Tidyverse, it's relatively well-documented. This lesson focuses
on xml2, since most of the additional features in the XML package are related
to writing (rather than parsing) XML documents.

The xml2 package is often used in conjunction with the ["rvest"
package][rvest], which provides support for CSS selectors (described later in
this lesson) and automates scraping HTML tables.

[xml2]: https://xml2.r-lib.org/ 
[rvest]: https://rvest.tidyverse.org/ 

The first time you use these packages, you'll have to install them:

```{r, eval=FALSE}
install.packages("xml2")
install.packages("rvest")
```

Let's start by parsing the example of a complete web page from earlier. The
xml2 function `read_xml` reads an XML document, and the rvest function
`read_html` reads an HTML document. Both accept an XML/HTML string or a file
path (including URLs):
```{r}
html = r"(
<html>
  <head>
    <title>This is the page title!</title>
  </head>
  <body>
    <h1>This is a header!</h1>
    <p>This is a paragraph.
      <a href="http://www.r-project.org/">Here's a website!</a>
    </p>
    <p id="hello">This is another paragraph.</p>
  </body>
</html> )"

library(xml2)
library(rvest)

doc = read_html(html)
doc
```

The `xml_children` function returns all of the immediate children of a given
element.

The top element of our document is the `html` tag, and its immediate children
are the `head` and `body` tags:
```{r}
tags = xml_children(doc)
```

The result from `xml_children` is a _node set_ (`xml_nodeset` object). Think of
a node set as a vector where the elements are tags rather than numbers or
strings. Just like a vector, you can access individual elements with the
indexing (square bracket `[`) operator:
```{r}
length(tags)
head = tags[1]
head
```

The `xml_text` function returns the text contained in a tag. Let's get the text
in the `title` tag, which is beneath the `head` tag. First we isolate the tag,
then use `xml_text`:
```{r}
title = xml_children(head)
xml_text(title)
```

Navigating through the tags by hand is tedious and easy to get wrong, but
fortunately there's a better way to find the tags we want.



XPath
-----

An XML document is a tree, similar to the file system on your computer:

```
html
├── head
│   └── title
└── body
    ├── h1
    ├── p
    └── p
        └── a
```

When we wanted to find files, we wrote file paths. We can do something similar
to find XML elements.

_XPath_ is a language for writing paths to elements in an XML document. XPath is
not R-specific. At a glance, an XPath looks similar to a file path:

XPath | Description
----- | --------------------------
`/`   | root, or element separator
`.`   | current tag
`..`  | parent tag
`*`   | any tag (wildcard)

The xml2 function `xml_find_all` finds all elements at given XPath:
```{r}
xml_find_all(doc, "/html/body/p")
```

Unlike a file path, an XPath can identify multiple elements. If you only want a
specific element, use indexing to get it from the result.

XPath also has some features that are different from file paths. The `//`
separator means "at any level beneath." It's a useful shortcut when you want to
find a specific element but don't care where it is.

Let's get all of the `p` elements at any level of the document:
```{r}
xml_find_all(doc, "//p")
```

Let's also get all `a` elements at any level beneath a `p` element:
```{r}
xml_find_all(doc, "//p/a")
```

The vertical bar `|` means "or." You can use it to get two different sets of
elements in one query.

Let's get all `h1` or `p` tags:
```{r}
xml_find_all(doc, "//h1|//p")
```


### Predicates

In XPath, the predicate operator `[]` gets elements at a position or matching a
condition. Most conditions are about the attributes of the element. In the
predicate operator, attributes are always prefixed with `@`.

For example, suppose we want to find all tags where the `id` attribute is equal
to `"hello"`:
```{r}
xml_find_all(doc, "//*[@id = 'hello']")
```

Notice that the equality operator in XPath is `=`, not `==`. Strings in XPath
can be quoted with single or double quotes.

You can combine multiple conditions in the predicate operator with `and` and
`or`. There are also several XPath functions you can use in the predicate
operator. These functions are **not** R functions, but rather built into XPath.
Here are a few:

XPath         | Description
------------- | -----------
`not()`       | negation
`contains()`  | check string x contains y
`text()`      | get tag text
`substring()` | get a substring


For instance, suppose we want to get elements that contain the word
"paragraph":
```{r}
xml_find_all(doc, "//*[contains(text(), 'paragraph')]")
```

Finally, note that you can also use the predicate operator to get elements at a
specific position. For example, to get the second `p` element anywhere in the
document:
```{r}
xml_find_all(doc, "//p[2]")
```

Notice that this is the same as if we had used R to get the second element:
```{r}
xml_find_all(doc, "//p")[2]
```

Beware that although the XPath predicate operator resembles R's indexing
operator, the syntax is not always the same.

We'll learn more XPath in the examples. There's a complete list of XPath
functions on [Wikipedia][wiki-xpath].

[wiki-xpath]: https://en.wikipedia.org/wiki/XPath#Functions_and_operators


The Web Scraping Workflow
-------------------------

Scraping a web page is part technology, part art. The goal is to find an XPath
that's concise but specific enough to identify only the elements you want. If
you plan to scrape the web page again later or want to scrape a lot of similar
web pages, the XPath also needs to be general enough that it still works even
if there are small variations.

[Firefox][] and [Chrome][] include "web developer tools" that are invaluable
for planning a web scraping strategy. Press `Ctrl + Shift + i` (`Cmd + Shift +
i` on OS X) in Firefox or Chrome to open the web developer tools.

We can also use the web developer tools to interactively identify the element
that corresponds to a specific part of a web page. Press `Ctrl + Shift + c` and
then click on the part of the web page you want to identify.

The best way to approach web scraping (and programming in general) is as an
incremental, iterative process. Use the web developer tools to come up with a
basic strategy, try it out in R, check which parts don't work, and then repeat
to adjust the strategy. Expect to go back and forth between your web browser
and R several times when you're scraping.

Most scrapers follow the same four steps, regardless of the web page and the
language of the scraper:

1. Download pages with an HTTP request (usually `GET`)
2. Parse pages to extract text
3. Clean up extracted text with string methods or regex
4. Save cleaned results

In R, xml2's `read_xml` function takes care of step 1 for you, although you can
also use httr functions to make the request yourself.

[Firefox]: https://www.mozilla.org/en-US/firefox/new/
[Chrome]: https://www.google.com/chrome/


### Being Polite

Making an HTTP request is not free! It has a real cost in CPU time and also
cash. Server administrators will not appreciate it if you make too many
requests or make requests too quickly. So:

* If you're making multiple requests, slow them down by using R's `Sys.sleep`
  function to make R do nothing for a moment. Aim for no more than 20-30
  requests per second, unless you're using an API that says more are okay.
* Avoid requesting the same page twice. One way to do this is by _caching_
  (saving) the results of the requests you make. You can do this manually, or
  use a package that does it automatically, like the [httpcache][] package.

Failing to be polite can get you banned from websites! Also check the website's
terms of service to make sure scraping is not explicitly forbidden.

[httpcache]: https://enpiar.com/r/httpcache/


Case Study: CA Cities
---------------------

[Wikipedia](https://en.wikipedia.org/) has many pages that are just tables of
data. For example, there's this [list of cities and towns in
California][wiki-cities]. Let's scrape the table to get a data frame.

[wiki-cities]: https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_California

Step 1 is to download the page:
```{r wiki_scrape, echo=FALSE, results=FALSE}
wiki_url =
  "https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_California"
wiki_doc = .read_html_cache(wiki_url)
```
```{r, eval=FALSE}
wiki_url =
  "https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_California"

wiki_doc = read_html(wiki_url)
```

Step 2 is to extract the table element from the page. We can use Firefox or
Chrome's web developer tools to identify the table. HTML tables usually use the
`table` tag. Let's see if it's the only table in the page:

```{r}
tables = xml_find_all(wiki_doc, "//table")
tables
```

The page has `r length(tables)` tables. We can either make our XPath more
specific, or use indexing to get the table we want. Refining the XPath makes
our scraper more robust, but indexing is easier.

For the sake of learning, let's refine the XPath. Going back to the browser, we
can see that the table includes `"wikitable"` and `"sortable"` in its `class`
attribute. So let's search for these among the table elements:
```{r}
tab = xml_find_all(tables, "//*[contains(@class, 'sortable')]")
tab
```

Now we get just one table! Here we used a second XPath applied only to the
results from the first, but we also could've done this all with one XPath:
`//table[contains(@class, 'sortable')]`.

The next part of extracting the data is to extract the value from each
individual cell in the table. HTML tables have a strict layout order, with tags
to indicate rows and cells. We could extract each cell by hand and then
reassemble them into a data frame, but the rvest function `html_table` can do
it for us automatically:
```{r}
cities = html_table(tab, fill = TRUE)
cities = cities[[1]]
head(cities)
```

The `fill = TRUE` argument ensures that empty cells are filled with `NA`. We've
successfully imported the data from the web page into R, so we're done with
step 2.

### Data Cleaning

Step 3 is to clean up the data frame. The column names contain symbols, the
first row is part of the header, and the column types are not correct.

```{r}
# Fix column names.
names(cities) = c("city", "type", "county", "population", "mi2", "km2", "date")

# Remove fake first row.
cities = cities[-1, ]
# Reset row names.
rownames(cities) = NULL
```

How can we clean up the `date` column? The `as.Date` function converts a string
into a date R understands. The idea is to match the date string to a _format
string_ where the components of the date are indicated by codes that start with
`%`. For example, `%m` stands for the month as a two-digit number. You can read
about the different date format codes in `?strptime`.

Here's the code to convert the dates in the data frame:
```{r}
dates = as.Date(cities$date, "%B %m, %Y")
cities$date = dates
```

We can also convert the population to a number:

```{r}
class(cities$population)

# Remove commas and footnotes (e.g., [1]) before conversion
library(stringr)

pop = str_replace_all(cities$population, ",", "")
pop = str_replace_all(pop, "\\[[0-9]+\\]", "")
pop = as.numeric(pop)

# Check for missing values, which can mean conversion failed
any(is.na(pop))

cities$population = pop
```

Case Study: The CA Aggie
------------------------

Suppose we want to scrape The California Aggie.

In particular, we want to get all the links to news articles on the features
page <https://theaggie.org/category/features/>. This could be one part of a
larger scraping project where we go on to scrape individual articles.

First, let's download the features page so we can extract the links:

```{r aggie_features, echo=FALSE, results=FALSE}
url = "https://theaggie.org/category/features/"
doc = .read_html_cache(url)
```
```{r, eval=FALSE}
url = "https://theaggie.org/category/features/"
doc = read_html(url)
```

We know that links are in `a` tags, but we only want links to articles. Looking
at the features page with the web developer tools, the links to feature
articles are all inside of a `div` tag with class `td_block_inner`. So
let's get that tag:

```{r}
xml_find_all(doc, "//div[contains(@class, 'td_block_inner')]")
# OR html_nodes(doc, "div.td-block-inner")
```

That returns a lot of results, so let's try using the `id` attribute, which is
`"tdi_113"`, instead. Usually the id of an element is unique, so this ensures
that we get the right section.

We can also add in a part about getting links now:

```{r}
div = xml_find_all(doc, "//div[@id = 'tdi_113']")
# OR html_nodes(doc, "div#tdi_113")

links = xml_find_all(div, ".//a")
# OR html_nodes(div, "a")

length(links)
```

That gives us `r length(links)` links, but there are only 15 articles on the
page, so something's still not right. Inspecting the page again, there are
actually three links to each article: on the image, on the title, and on
"Continue Reading".

Let's focus on the title link. All of the title links are inside of an `h3`
tag. Generally it's more robust to rely on tags (structure) than to rely on
attributes (other than `id` and `class`). So let's use the `h3` tag here:

```{r}
links = xml_find_all(div, ".//h3/a")
# OR html_nodes(div, "h3 > a")

length(links)
```

Now we've got the 15 links, so let's get the URLs from the `href` attribute.

```{r}
feature_urls = xml_attr(links, "href")
```

The other article listings (Sports, Science, etc) on The Aggie have a similar
structure, so we can potentially reuse our code to scrape those.

So let's turn our code into a function. The input will be a downloaded page,
and the output will be the article links.

```{r}
parse_article_links = function(page) {
  div = xml_find_all(page, "//div[@id = 'tdi_113']")
  links = xml_find_all(div, ".//h3/a")
  xml_attr(links, "href")
}
```

We can test this out on the Sports page. First we download the page:

```{r aggie_sports, echo=FALSE, results=FALSE}
sports = .read_html_cache("https://theaggie.org/category/sports")
```
```{r, eval=FALSE}
sports = read_html("https://theaggie.org/category/sports")
```
 
Then we call the function on the document:

```{r}
sports_urls = parse_article_links(sports)
head(sports_urls)
```

It looks like the function works even on other pages! We can also set up the
function to extract the link to the next page, in case we want to scrape
multiple pages of links.

The link to the next page of features (an arrow at the bottom) is an `a` tag
with attribute `aria-label` in a `div` with classs `page-nav`. Let's see if
that's specific enough to isolate the tag:

```{r}
nav = xml_find_all(doc, "//div[contains(@class, 'page-nav')]")
# OR html_nodes(doc, "div.page-nav")
next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
# OR html_nodes(nav, "a[aria-label *= 'next-page']")
```

It looks like it is. We use `contains` here rather than `=` because it is
common for the `class` attribute to have many parts. Using `contains` makes our
code robust against changes in the future.

We can now modify our parser function to return the link to the next page:

```{r}
parse_article_links = function(page) {
  # Get article URLs
  div = xml_find_all(page, "//div[@id = 'tdi_113']")
  links = xml_find_all(div, ".//h3/a")
  urls = xml_attr(links, "href")

  # Get next page URL
  nav = xml_find_all(page, "//div[contains(@class, 'page-nav')]")
  next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
  next_url = xml_attr(next_page, "href")

  # Using a list allows us to return two objects
  list(urls = urls, next_url = next_url)
}
```

Since our function gets URL for the next page, what happens on the last page?

Looking at the last page in the browser, there is no link to the next page.
Let's see what our scraper function does:

```{r aggie_last, echo=FALSE, results=FALSE}
last_page = .read_html_cache("https://theaggie.org/category/features/page/187/")
```
```{r, eval=FALSE}
last_page = read_html("https://theaggie.org/category/features/page/187/")
```
```{r}
parse_article_links(last_page)
```

We get an empty character vector as the URL for the next page. This is because
the `xml_find_all` function returns an empty node set for the next page URL, so
there aren't any `href` fields for `xml_attr` to extract. It's convenient that
the xml2 functions behave this way, but we could also add an if-statement to
the function to check (and possibly return `NA` as the next URL in this case).

Then the code becomes:

```{r}
parse_article_links = function(page) {
  # Get article URLs
  div = xml_find_all(page, "//div[@id = 'tdi_113']")
  links = xml_find_all(div, ".//h3/a")
  urls = xml_attr(links, "href")

  # Get next page URL
  nav = xml_find_all(page, "//div[contains(@class, 'page-nav')]")
  next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
  if (length(next_page) == 0) {
    next_url = NA
  } else {
    next_url = xml_attr(next_page, "href")
  }

  # Using a list allows us to return two objects
  list(urls = urls, next_url = next_url)
}
```

Now our function should work well even on the last page.

If we want to scrape links to all of the articles in the features section, we
can use our function in a loop:

```{r, eval = FALSE}
# NOTE: This code is likely to take a while to run, and is meant more for
# reading than for you to run and try out.

url = "https://theaggie.org/category/features/"
article_urls = list()
i = 1

# On the last page, the next URL will be `NA`.
while (!is.na(url)) {
  # Download and parse the page.
  page = read_html(url)
  result = parse_article_links(page)

  # Save the article URLs in the `article_urls` list. The variable `i` is the
  # page number.
  article_urls[[i]] = result$url
  i = i + 1

  # Set the URL to the next URL.
  url = result$next_url

  # Sleep for 1/30th of a second so that we never make more than 30 requests
  # per second.
  Sys.sleep(1/30)
}
```

Now we've got the basis for a simple scraper.


CSS Selectors
-------------

_Cascading style sheets_ (CSS) is a language used to control the formatting of
an XML or HTML document.

_CSS selectors_ are the CSS way to write paths to elements. CSS selectors are
more concise than XPath, so many people prefer them. Even if you prefer CSS
selectors, it's good to know XPath because CSS selectors are less flexible.

Here's the basic syntax of CSS selectors:

CSS           | Description
------------- | --------------------------
`a`           | tags `a`
`a > b`       | tags `b` directly beneath `a`
`a b`         | tags `b` anywhere beneath `a`
`a, b`        | tags `a` or `b`
`#hi`         | tags with attribute `id="hi"`
`.hi`         | tags with attribute `class` that contains `"hi"`
`[foo="hi"]`  | tags with attribute `foo="hi"`
`[foo*="hi"]` | tags with attribute `foo` that contains `"hi"`

If you want to learn more, [CSS Diner](http://flukeout.github.io/) is an
interactive tutorial that covers the entire CSS selector language.

In Firefox, you can get CSS selectors from the web developer tool. Right-click
the tag you want a selector for and choose "Copy Unique Selector". Beware that
the selectors Firefox generates are often too specific to be useful for
anything beyond the simplest web scrapers.

The rvest package uses CSS selectors by default. Behind the scenes, the package
translates these into XPath and passes them to xml2.

Here are a few examples of CSS selectors, using rvest's `html_nodes` function:

```{r}
html = r"(
<html>
  <head>
    <title>This is the page title!</title>
  </head>
  <body>
    <h1>This is a header!</h1>
    <p>This is a paragraph.
      <a href="http://www.r-project.org/">Here's a website!</a>
    </p>
    <p id="hello">This is another paragraph.</p>
  </body>
</html> )"

doc = read_html(html)

# Get all p elements
html_nodes(doc, "p")

# Get all links
html_nodes(doc, "a")

# Get all tags with id="hello"
html_nodes(doc, "#hello")
```

