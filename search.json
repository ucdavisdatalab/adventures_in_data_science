[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adventures in Data Science",
    "section": "",
    "text": "Overview\nThis is the course reader for STS 115 – Data Sense & Exploration: Critical Storytelling with Analysis. The course is designed to provide students with a basic understanding of computing and network architecture, basic programming skills, and an introduction to common methods in Data Science and Digital Humanities. The course is part one of a two part sequence we call Adventures in Data Science. This course reader provides background information that will help you to better understand the concepts that we will discuss in class and to better participate in the hands-on portion of the course.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "chapters/required-software.html",
    "href": "chapters/required-software.html",
    "title": "Required Software",
    "section": "",
    "text": "Git\nThis course uses free, open source software. You need to download this software to your computer in order to follow along with the lessons and complete the associated assignments.\nGit is version control system. It helps keep track of changes and updates to your files. You can use it locally on your machine and also on servers.\nYou can download Git for free here.",
    "crumbs": [
      "Required Software"
    ]
  },
  {
    "objectID": "chapters/required-software.html#git",
    "href": "chapters/required-software.html#git",
    "title": "Required Software",
    "section": "",
    "text": "Note\n\n\n\nOn Windows, you can install Git (and Git Bash) with the official installer linked above.\nOn macOS, you can install Git automatically together with the Xcode software development kit by opening the Terminal app, typing git --version, and pressing Enter or Return.\nOn Linux, you can install Git with your distribution’s package manager.",
    "crumbs": [
      "Required Software"
    ]
  },
  {
    "objectID": "chapters/required-software.html#r-r-studio",
    "href": "chapters/required-software.html#r-r-studio",
    "title": "Required Software",
    "section": "R & R Studio",
    "text": "R & R Studio\nR is a programming language for working with data, performing statistical analyses, and generating data visualizations.\nYou can download R for free here.\nRStudio is an integrated development environment (IDE) that provides a user-friendly interface for working with R. RStudio will not work if you do not also have R installed.\nYou can download the RStudio Desktop Open-Source Edition for free here.\nOn Windows, you’ll also need to download and install RTools in order for some of the packages we’ll use later to work.",
    "crumbs": [
      "Required Software"
    ]
  },
  {
    "objectID": "chapters/week01/research-lifecycle.html",
    "href": "chapters/week01/research-lifecycle.html",
    "title": "1  Research Lifecycle",
    "section": "",
    "text": "Important\n\n\n\nWe teach this lecture with a slide deck. This chapter does not necessarily cover everything covered in lecture.\n\n\nA research project is reproducible if a different researcher can carry out the same analysis with the same data and produce the same overall result. A reproducible project is transparent about the research process and facilitates independent verification, the gold standard for research. Best practices for reproducible research can also make it easier to collaborate and to distribute and reuse products of research.\n\n\n\n\n\n\nTip\n\n\n\nYou always have at least one collaborator: future you! Work you did in the past can be as challenging to navigate as work from someone else. So even if you don’t plan to collaborate with anyone else, it can still be helpful to adopt good practices for reproducible and collaborative research.\n\n\nAlmost all contemporary research projects involve computing—whether that means creating and storing digital documents, developing code for small analyses that run for a few minutes on a laptop, or developing code for large analyses that run for a few days on a high-performance computing cluster. Consequently, this reader focuses on practices related to computing. Many are relevant regardless of whether your research involves writing lots of code.\nOne way to think about reproducibility is in terms of five principles:\n\nDocumentation: Get in the habit of writing documentation as you work. Most details should be documented, including:\n\nThe scope of the project, in terms of goals, milestones, and a timeline\nMeetings—especially new tasks, decisions, and deferred discussions\nYour thought process as you work: what references did you find, what approaches did you try, what results did you obtain, and what do you want to try in the future?\nData you collect, especially aspects which are not clearly documented elsewhere\nCode you write (use comments!)\nAny other files and directories you create\n\nDocumentation can be for you, your collaborators, or other researchers interested in your work. Remember that not all documentation has to be for all of these people, and that even if some information is only for you, it’s usually still a good idea to document it—it’s easy to be too optimistic about what you’ll remember later.\n\n\n\n\n“Scratch” from “Piled Higher and Deeper” by Jorge Cham.\n\n\n\nArtifact preservation: Every file you produce is an artifact of your research. Keep records so that you can revisit old results or even “rewind” a project to an earlier state. Some things you might want records of include:\n\nChanges you make to (or different versions of) documents, code, and data\nSettings and other inputs you use to run computations such as analyses, experiments, and simulations\nOutputs from computations—especially outputs that are easy to overlook, such as diagnostic information printed while a computation runs\n\nYou never know what you’ll need later, and digital file storage is cheap. Keeping records is a big task, but there are many software tools available to automate most of the work.\n\n\n\n\n\n\n\nFigure 1.1: “Documents” from “xkcd” by Randall Munroe (license).\n\n\n\n\nProject organization: Establish conventions for how you’ll organize a project. Document these and then make sure to follow them. They should address details such as:\n\nHow you’ll name things like files, functions, and modules\nWhich directories you’ll create and which kinds of files belong in each directory\nWhether your code should be in notebooks, scripts, or a combination of both\nHow you’ll organize code using functions, classes, modules, and other programming abstractions\nWhich structures and file formats you’ll use to store data\nWhether you’ll package your code or data for widespread distribution\n\nTry to cover the most common cases rather than every case, and leave some flexibility for ambiguous cases. Good conventions establish single, specific “correct” places for things to be, and encourage descriptive, self-documenting names. This makes data, code, and results easier to find, in contrast to the example in Figure 1.1. Finding things quickly becomes especially important when collaborating with other people. Good conventions also encourage using programming abstractions, such as functions and modules, to make code easier to understand, verify, and reuse.\nWorkflow automation: Even when a project is well-documented and organized, reproducing particular result can be complicated: it may be necessary to run several different steps in a specific order and with specific settings. You can use programming languages and other workflow automation tools to automate a collection of steps, or workflow, so that it’s easy to run. You can also use workflow automation tools to avoid doing tedious, repetitive tasks by hand and to run commands every time you take a particular action. For example, you might want to run tests on your code each time you upload it to share with collaborators. By using programming languages and other workflow automation tools, you create an unambiguous record of the steps in each computation and make it easier to reuse parts of a project.\nEnvironment management: Make the hardware and software requirements, or environment, for your project explicit and as easy as possible to recreate. Environment management tools are especially helpful here, since they allow you to install different versions of software for different projects simultaneously, can record the software you install as you install it, and can recreate recorded environments. It’s much easier to start working on a new machine, bring a new collaborator on board, or assist other people interested in using your project when you can deploy the required computing environment in just one or two commands.\n\nAll of these principles are important. You can make sure your projects are reproducible by adopting various practices that each address one or more of these principles. The principles are often symbiotic: a good practice for one may also be a good practice for others.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research Lifecycle</span>"
    ]
  },
  {
    "objectID": "chapters/week01/interacting-with-computers.html",
    "href": "chapters/week01/interacting-with-computers.html",
    "title": "2  Interacting with Computers",
    "section": "",
    "text": "2.1 Computer Interfaces\nMost people interact with their computer through a graphical user interface (GUI), which allows them to use a mouse, keyboard, and graphical elements on screen (such as file menus, pictures of folders and files, etc.) as they do their work. We tend to conflate operating systems (such as Windows and macOS) and their GUIs, because software publishers pack these two things together as a convenience to users. But the operating system that makes your computer work and the interface that you use to interact with it are, in fact, completely different and separable software systems. It’s possible to use other methods/software to interact with your computer besides the stock GUI that launches automatically when you turn it on.\nOne such method, the command line interface (CLI), offers a text-only means of interacting with your computer. The CLI acts somewhat like a typewriter rather than a desktop with windows (as the prevailing metaphors for GUIs go). You can think of this interface as having two main parts:\nThe command line or prompt is the line in the terminal where you’ll type in commands for the shell to interpret and execute.\nIn the early days of computing, all user interaction with computers happened at the command line. But during the 1980s, computer manufacturers—with Apple at the lead—made a big push to convert their machines to the windowing systems we know today. A CLI, they felt, was too difficult for users to understand, and this, in turn, would hamper computer sales. Nowadays almost all operating systems have a GUI as the default interface. On Windows and macOS, people are by and large locked into the GUI that comes with the operating system (on Linux, this is generally not the case). Nevertheless, these operating systems also all have some kind of built-in CLI and allow others to be installed.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interacting with Computers</span>"
    ]
  },
  {
    "objectID": "chapters/week01/interacting-with-computers.html#computer-interfaces",
    "href": "chapters/week01/interacting-with-computers.html#computer-interfaces",
    "title": "2  Interacting with Computers",
    "section": "",
    "text": "The X Window System, a graphical user interface first released in 1984, with several windows open, including a command line interface. Image from Wikipedia.\n\n\n\n\nA terminal: a window (or originally, a teleprinter) with which to send/receive text to/from a computer.\nA shell: a program that interprets and executes commands you type into the terminal (common shells include Bash, Fish, and Zsh).\n\n\n\n\n2.1.1 Why Use the CLI?\nWork you do at the CLI is relatively easy to record and reproduce, because CLI commands are just text. You can write down (or copy and paste) the exact commands you entered to create an unambiguous history. Later, if you’re unsure of how you got to a particular result, you can review the history or even re-run parts of it. Be a good collaborator for future you.\nFrom a transparency perspective, having a history of computations is critical. It enables others to reproduce, assess, and verify your results. Think of the history as showing your work when your work involves a computer. This is a key component of conducting reproducible research, but is also valued outside of research contexts.\nSolving real-world problems sometimes requires computers that are larger and more powerful than a consumer-grade workstation. These high-performance computing systems do not necessarily have GUIs. Familiarity with the CLI makes it easier to get started with them.\n\n\n\n\n\n\nNoteTo the Command Line: A Mentality Shift\n\n\n\nThe following sections and chapters are a hands-on introduction to the CLI. They explain how to open the CLI and cover a host of different commands that will help you in your day-to-day work on a computer. That said, knowing the pragmatics of using the CLI is only one part of a broader change of mentality in interacting with computers that we hope to instill.\nIn order to use the CLI effectively, you’ll need to understand more about how computers work than you would if you only used a GUI. As such, we’ll cover several concepts and conventions in computing that generalize beyond the CLI. Thus learning this material is in part about preparing yourself to learn more advanced computing skills in the future, such as tracking changes to files with version control and writing code.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interacting with Computers</span>"
    ]
  },
  {
    "objectID": "chapters/week01/interacting-with-computers.html#command-line-basics",
    "href": "chapters/week01/interacting-with-computers.html#command-line-basics",
    "title": "2  Interacting with Computers",
    "section": "2.2 Command Line Basics",
    "text": "2.2 Command Line Basics\n\n\n\n\n\n\nImportant\n\n\n\nTo follow along with this section and subsequent chapters, you’ll need a compatible terminal (and shell).\nOpen a terminal by following the instructions for your computer’s operating system:\n\nWindowsmacOSLinux\n\n\nGit Bash is the terminal we recommend on Windows. It is not built into Windows, so you have to install it yourself. The Required Software chapter provides instructions.\nTo launch Git Bash, open the start menu and search for “Git Bash” or select Programs -&gt; Git Bash.\nWhen the Git Bash opens, it will look something like this:\n\n\n\nGit Bash on Windows.\n\n\n\n\nThe built-in Terminal application is what we recommend on macOS.\nTo launch Terminal, select Applications -&gt; Utilities -&gt; Terminal.\nWhen the Terminal app opens, it will look something like this:\n\n\n\nThe Terminal app on macOS.\n\n\n\n\nMany different terminals are available for Linux. Any are likely okay for this workshop, but the examples were only tested in WezTerm.\n\n\n\n\n\nWhile the command line can look intimidating to those raised on a GUI, it’s important to know that it is nevertheless an interface in the same way that your computer’s default windowing system is an interface. That is, even though a CLI is something of a bare-bones representation of your computer, it too relies on a series of assumptions and metaphors that serve to frame how you interact with your computer. Using the CLI may feel strange at first, but part of that feeling comes from not being acclimated to the way it represents a computer.\nInstead of conveying information through graphics, as in a GUI, the CLI does so through lines of text. Take a look at the terminal you just opened. It probably won’t be exactly the same due to differences between operating systems and terminal programs, but it should look something like this:\n\n\n\nTerminal window\n\n\nEverything that will happen in this window happens on a line-by-line basis. As soon as you opened the terminal, the shell started running and printed out a line of text.\nThe beginning of the line shows the names of the current user (tshoe) and current computer (ds), with an @ in between. On your screen, these will likely be different. This information may seem redundant, but with the CLI, it’s possible to interact with remote computers via a network, so it can be helpful to have this displayed as a reference point.\nThe middle of the line shows some additional status information (~), which you’ll learn about in the next chapter.\nThe end of the line shows a dollar sign ($). This is an indicator to let you know that this is a command line (or prompt) where you can type in a command. The shell will not print or do anything else until you type in a command.\nDepending on your computer’s terminal, you may or may not also see a box after the prompt character. This indicates where the cursor is in the terminal, which is helpful if you want to edit a command. In the CLI, you can’t click the mouse to move the cursor. Instead, you’ll have to use your computer’s Left and Right arrow keys.\n\n\n\n\n\n\nWarning\n\n\n\nYou do not need to type $. It will appear automatically.\nExamples online sometimes include $ at the beginning of commands to mimic what you’ll see onscreen. We do not include the $ at the beginning of any commands in this reader.\n\n\nYou can run, enter, or execute, a command in the terminal by typing the command’s name—followed by any additional information the command requires—and then pressing Enter or Return on your keyboard.\nFor example, the echo command prints text to the screen. It literally echoes your text:\necho Hello world!\nHello world!\nWhen you enter a command, your computer’s shell program interprets and runs the command, prints any output from the command to the terminal, and finally prints a new command line prompt.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interacting with Computers</span>"
    ]
  },
  {
    "objectID": "chapters/week01/browsing-files.html",
    "href": "chapters/week01/browsing-files.html",
    "title": "3  Browsing Files",
    "section": "",
    "text": "3.1 File Systems\nA key feature of the CLI is that you can browse through and inspect the files and directories on your computer. With GUIs, we tend to navigate with our mouses; on the command line, we’ll use our keyboard. This chapter begins with some background information and vocabulary about how computers organize files, then gives a hands-on introduction to working with files and directories in the CLI.\nYour computer’s file system consists of files (chunks of data) and directories (or “folders”) to organize those files. For instance, the file system on a computer shared by Ada and Charles, two pioneers of computing, might look like this:\nDon’t worry if your file system looks a bit different from the picture.\nFile systems have a tree-like structure, with a top-level directory called the root directory. On Ada and Charles’ computer, the root is called /, which is also what it’s called on all macOS and Linux computers. On Windows, the root is usually called C:/, but sometimes other letters, like D:/, are also used depending on the computer’s hardware.\nA path is a list of directories that leads to a specific file or directory on a file system (imagine giving directions to someone as they walk through the file system). Use forward slashes / to separate the directories in a path, rather than commas or spaces. The root directory includes a forward slash as part of its name, and doesn’t need an extra one.\nFor example, suppose Ada wants to write a path to the file cats.csv. She can write the path like this:\nYou can read this path from left-to-right as, “Starting from the root directory, go to the Users directory, then from there go to the ada directory, and from there go to the file cats.csv.” Alternatively, you can read the path from right-to-left as, “The file cats.csv inside of the ada directory, which is inside of the Users directory, which is in the root directory.”\nAs another example, suppose Charles wants a path to the Programs directory. He can write:\nThe / at the end of this path is reminder that Programs is a directory, not a file. Charles could also write the path like this:\nThis is still correct, but it’s not as obvious that Programs is a directory. In other words, when a path leads to a directory, including a trailing slash is optional, but makes the meaning of the path clearer. Paths that lead to files never have a trailing slash.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Browsing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/browsing-files.html#sec-file-systems",
    "href": "chapters/week01/browsing-files.html#sec-file-systems",
    "title": "3  Browsing Files",
    "section": "",
    "text": "An example of a file system.\n\n\n\n\n\n\n/Users/ada/cats.csv\n\n\n/Programs/\n\n/Programs\n\n\n\n\n\n\n\nWarning\n\n\n\nOn Windows computers, the components of a path are usually separated with backslashes \\ instead of forward slashes /.\nGit Bash is an exception: the shell commands you’ve learned so far expect and understand paths separated with forward slashes. If you instead use Windows’ built-in terminal, you’ll need to use paths separated with backslashes (and a different set of commands).",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Browsing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/browsing-files.html#sec-absolute-relative-paths",
    "href": "chapters/week01/browsing-files.html#sec-absolute-relative-paths",
    "title": "3  Browsing Files",
    "section": "3.2 Absolute & Relative Paths",
    "text": "3.2 Absolute & Relative Paths\nA path that starts from the root directory, like all of the ones we’ve seen so far, is called an absolute path. The path is “absolute” because it unambiguously describes where a file or directory is located. The downside is that absolute paths usually don’t work well if you share your code.\nFor example, suppose Ada uses the path /Programs/ada/cats.csv to load the cats.csv file in her code. If she shares her code with another pioneer of computing, say Gladys, who also has a copy of cats.csv, it might not work. Even though Gladys has the file, she might not have it in a directory called ada, and might not even have a directory called ada on her computer. Because Ada used an absolute path, her code works on her own computer, but isn’t portable to others.\nOn the other hand, a relative path is one that doesn’t start from the root directory. The path is “relative” to an unspecified starting point, which usually depends on the context.\nFor instance, suppose Ada’s code is saved in the file analysis.ipynb, which is in the same directory as cats.csv on her computer. Then instead of an absolute path, she can use a relative path in her code:\ncats.csv\nThe context is the location of analysis.ipynb, the file that contains the code. In other words, the starting point on Ada’s computer is the ada directory. On other computers, the starting point will be different, depending on where the code is stored.\nNow suppose Ada sends her corrected code in analysis.ipynb to Gladys, and tells Gladys to put it in the same directory as cats.csv. Since the path cats.csv is relative, the code will still work on Gladys’ computer, as long as the two files are in the same directory. The name of that directory and its location in the file system don’t matter, and don’t have to be the same as on Ada’s computer. Gladys can put the files in a directory /Users/gladys/from_ada/ and the path (and code) will still work.\nRelative paths can include directories. For example, suppose that Charles wants to write a relative path from the Users directory to a cool selfie he took. Then he can write:\ncharles/cool_hair_selfie.jpg\nYou can read this path as, “Starting from wherever you are, go to the charles directory, and from there go to the cool_hair_selfie.jpg file.” In other words, the relative path depends on the context of the code or program that uses it.\n\n\n\n\n\n\nTip\n\n\n\nWhen you use paths in code, they should almost always be relative paths. This ensures that the code is portable to other computers, which is an important aspect of reproducibility. Another benefit is that relative paths tend to be shorter, making your code easier to read (and write).\n\n\nWhen you write paths, there are three shortcuts you can use. These are most useful in relative paths, but also work in absolute paths:\n\n. means the current directory.\n.. means the directory above the current directory.\n~ means the home directory. Each user has their own home directory, whose location depends on the operating system and their username. Home directories are typically found inside C:/Users/ on Windows, /Users/ on macOS, and /home/ on Linux.\n\nAs an example, suppose Ada wants to write a (relative) path from the ada directory to Charles’ cool selfie. Using these shortcuts, she can write:\n../charles/cool_hair_selfie.jpg\nRead this as, “Starting from wherever you are, go up one directory, then go to the charles directory, and then go to the cool_hair_selfie.jpg file.” Since /Users/ada is Ada’s home directory, she could also write the path as:\n~/../charles/cool_hair_selfie.jpg\nThis path has the same effect, but the meaning is slightly different. You can read it as “Starting from your home directory, go up one directory, then go to the charles directory, and then go to the cool_hair_selfie.jpg file.”\nThe .. and ~ shortcut are frequently used and worth remembering. The . shortcut is included here in case you see it in someone else’s code. Since it means the current directory, a path like ./cats.csv is identical to cats.csv, and the latter is preferable for being simpler. There are a few specific situations where . is necessary, but they fall outside the scope of this text.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Browsing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/browsing-files.html#the-working-directory",
    "href": "chapters/week01/browsing-files.html#the-working-directory",
    "title": "3  Browsing Files",
    "section": "3.3 The Working Directory",
    "text": "3.3 The Working Directory\nOpening a directory with a graphical file browser (such as Explorer or Finder) is probably one of the things you do most frequently on your computer. File browsers generally open one directory at a time and display the contents. The CLI, or more specifically, the shell, works the same way. The shell always has a directory open. This is called the working directory. Think of the working directory as the directory the shell is currently “at” or watching.\nSection 3.2 explained that relative paths have a starting point that depends on the context where the path is used. The shell uses the working directory as the starting point for relative paths.\nThe shell provides commands to manipulate the working directory. The command pwd prints the absolute path for the working directory. It doesn’t require any arguments:\npwd\n/home/nick\nOn your computer, the output from pwd will likely be different.\n\n\n\n\n\n\nTip\n\n\n\nThe pwd command is very useful for getting your bearings. Run it any time you’re uncertain about what the working directory is.\nIf you write a relative path and it doesn’t work as expected, the first thing to do is run pwd to check the working directory.\n\n\nThe related cd command changes the working directory. Without any arguments, it changes the working directory to your home directory. Go ahead and try changing to the home directory and printing its path:\ncd\npwd\n/home/nick\nYou can change the working directory to a specific directory by putting the path to the directory after the cd command. Go to the directory above the home directory:\ncd ..\nNow print the path again:\npwd\n/home\nThe cd command understands both absolute and relative paths.\n\nAnother command that’s useful for dealing with the working directory and file system is ls. The ls command lists the names of all of the files and directories inside of a directory. It accepts a path to a directory as an argument, or assumes the working directory if you don’t pass a path. For instance:\nls /\nbin   dev  etc   lib    lost+found  opt   root  sbin  swapfile  tmp  var\nboot  efi  home  lib64  mnt         proc  run   srv   sys       usr  windows\nls\narchive  depot  go  haven  mill  notes.md  wharf  woods  Zotero\nAs usual, since you have a different computer, you’re likely to see different output if you run this code. If you run ls with an invalid path, the shell emits an error:\nls /this/path/is/fake/\n\"/this/path/is/fake/\": No such file or directory (os error 2)",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Browsing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/browsing-files.html#sec-making-removing-directories",
    "href": "chapters/week01/browsing-files.html#sec-making-removing-directories",
    "title": "3  Browsing Files",
    "section": "3.4 Making & Removing Directories",
    "text": "3.4 Making & Removing Directories\nWhen you start working on a project (whether it’s academic, personal, or something else), it’s a good habit to make a new directory, called a project directory or repository, where you’ll keep all of the project’s files. This way you can easily find, back up, and share the files.\n\n\n\n\n\n\nTip\n\n\n\nGive each project directory a descriptive name. This will make it easier for you to find the project in the future. At DataLab, we typically use names that include the year (or date) and title of the project. Use underscores (_) or dashes (-) to separate words and components in the name rather than spaces.\n\n\nAt the command line, you can use the mkdir command to make a directory. Navigate back to your home directory, then use mkdir to make a directory called 2026_intro-cmd:\ncd\nmkdir 2026_intro-cmd\nCheck that the new directory is there with the ls command:\nls\n2026_intro-cmd  archive  depot  go  haven  mill  notes.md  wharf  woods  Zotero\nChange to the new directory and check that it’s empty:\ncd 2026_intro-cmd\nls\nYou can make subdirectories to further organize your project directories. Try making a subdirectories called data and figures:\nmkdir data\nmkdir figures\nls\ndata  figures\nSometimes you might make a directory and then decide later that you don’t need it. The rmdir command removes an empty directory. Go ahead and remove the figures directory, since we’re not going to make any figures:\nrmdir figures\nThe rmdir command will only remove empty directories, so there’s no risk of accidentally removing important files. Later on, we’ll explain how to remove files (and directories that contain files).",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Browsing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/browsing-files.html#reference-browsing-commands",
    "href": "chapters/week01/browsing-files.html#reference-browsing-commands",
    "title": "3  Browsing Files",
    "section": "3.5 Reference: Browsing Commands",
    "text": "3.5 Reference: Browsing Commands\nThis chapter introduced five different commands you can use to browse files at the command line:\n\n\n\nCommand\nDescription\nExamples\n\n\n\n\npwd\nPrints the working directory\npwd\n\n\ncd\nChanges the working directory\ncd; cd my_directory\n\n\nls\nLists the contents of a directory\nls; ls my_directory\n\n\nmkdir\nMakes a new directory\nmkdir my_directory\n\n\nrmdir\nRemoves an empty directory\nrmdir my_directory\n\n\n\nMake sure you understand these commands before moving on, since it’s likely you’ll use them more frequently than any others.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Browsing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/other-commands.html",
    "href": "chapters/week01/other-commands.html",
    "title": "4  Going Beyond Browsing",
    "section": "",
    "text": "4.1 Command Syntax\nThe CLI is more than a mere a file browser. With the right commands, you can download files from the internet, edit text, install new software, run sophisticated computations, connect to remote computers, and more. This chapter introduces a few more commands that are especially useful. It also explains the syntax shared by most commands, and describes how to get help as you learn and use the CLI.\nMost, if not all, shell commands follow a standard syntax. Commands always begin with the name of a particular command. So, using the ls command as an example, you can write:\nSome commands require or accept additional information. For instance, you can give the ls command the path to a particular directory, such as ~:\nWe call these inputs to a command arguments. The shell treats spaces as delimiters between a command’s name and each of its arguments.\nMany commands also accept special arguments called flags that modify something about what the command does. Flags begin with a dash (-), usually followed by a single letter. For example, the ls command’s -a flag makes the command print out all files in a directory, even files that are hidden:\nSome flags can also be written in long form, as a word, to make it clearer how they modify the command. In long form, flags conventionally begin with two dashes (--). For the ls command, the long form of -a is --all. So another way to print all files in a directory is:\nWithout the -a or --all flag, the ls command does not print any files or directories whose names begin with a dot (.). Because of this, we call these files hidden files or dotfiles.\nHidden files typically contain settings or metadata for your computer’s operating system, for the shell, or for other programs. Sometimes you might want to edit a hidden file to configure your computer or even create a new hidden file for your own use.\nDoes your home directory contain any hidden files? First print the visible files:\nThen try prining all of the files, for comparison:",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Going Beyond Browsing</span>"
    ]
  },
  {
    "objectID": "chapters/week01/other-commands.html#command-syntax",
    "href": "chapters/week01/other-commands.html#command-syntax",
    "title": "4  Going Beyond Browsing",
    "section": "",
    "text": "ls\n\nls ~\n\n\n\n\n\n\n\nWarning\n\n\n\nFile and directory names with spaces can cause problems, because the shell will split the name at each space. Whatever command you’re trying to run will then treat each piece as a separate and unrelated argument. This typically causes the command to do something different from what you intended or to print an error message.\nIf you need to work with a name that has spaces, there are a few ways to work around this limitation: * Put single (') or double (\") quotation marks around the name. For example: bash   ls \"My Directory\"\n\nEscape each space by putting a backslash (\\) in front of it. For example:\nls My\\ Directory\n\nThese workarounds can be a bit of a pain to type over and over. For this reason, people who use the command line in their daily work tend to avoid spaces altogether when naming files and directories. At DataLab, we use underscores (_) and dashes (-) rather than spaces in names.\nIf you want to learn more about naming conventions, see this section of DataLab’s README, Write Me! workshop reader.\n\n\n\nls -a\n\nls --all\n\n\n\nls ~\n\nls -a ~",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Going Beyond Browsing</span>"
    ]
  },
  {
    "objectID": "chapters/week01/other-commands.html#getting-help",
    "href": "chapters/week01/other-commands.html#getting-help",
    "title": "4  Going Beyond Browsing",
    "section": "4.2 Getting Help",
    "text": "4.2 Getting Help\nMany commands have a --help flag that makes them print out some basic documentation. For example, to get help with the pwd command:\npwd --help\npwd: pwd [-LP]\n    Print the name of the current working directory.\n\n    Options:\n      -L    print the value of $PWD if it names the current working\n            directory\n      -P    print the physical directory, without any symbolic links\n\n    By default, `pwd' behaves as if `-L' were specified.\n\n    Exit Status:\n    Returns 0 unless an invalid option is given or the current directory\n    cannot be read.\nMost commands also provide detailed documentation in the form of a manual page. On macOS and Linux, you can use the man command to open the manual page for another command. For instance:\nman ls\nLS(1)                                               User Commands                                               LS(1)\n\nNAME\n       ls - list directory contents\n\nSYNOPSIS\n       ls [OPTION]... [FILE]...\n\nDESCRIPTION\n       List  information  about the FILEs (the current directory by default).  Sort entries alphabetically if none of\n       -cftuvSUX nor --sort is specified.\n\n       Mandatory arguments to long options are mandatory for short options too.\n\n       -a, --all\n              do not ignore entries starting with .\n\n       -A, --almost-all\n              do not list implied . and ..\n\n       --author\n              with -l, print the author of each file\n\n       -b, --escape\n              print C-style escapes for nongraphic characters\n\n       --block-size=SIZE\n              with -l, scale sizes by SIZE when printing them; e.g., '--block-size=M'; see SIZE format below\n\n       -B, --ignore-backups\n              do not list implied entries ending with ~\n\n       -c     with -lt: sort by, and show, ctime (time of last modification of file  status  information);  with  -l:\n              show ctime and sort by name; otherwise: sort by ctime, newest first\n\n       -C     list entries by columns\n\n       --color[=WHEN]\n              colorize the output; WHEN can be 'always' (default if omitted), 'auto', or 'never'; more info below\n\n       -d, --directory\n              list directories themselves, not their contents\n[...]\nAs opposed to the usually mute, minimalist disposition of a CLI, here you can see thorough documentation for a given command and its various flags.\n\n\n\n\n\n\nNote\n\n\n\nOn Windows, Git Bash does not include the man command. Instead, you can look up manual pages for commands by searching online. Try searching for man and the name of the command.\n\n\nSometimes it can also be helpful to know what version of a command you’re using, as commands themselves can be updated. You can find this information for some commands with --version or -v:\nfile -v\nfile-5.46\nmagic file from /usr/share/file/misc/magic\nseccomp support included\nStill unclear about what a command does? Look it up with your favorite search engine, or visit Stack Exchange and search there. CLIs are widely used software, and chances are incredibly good that someone else has had the same question you want to ask.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Going Beyond Browsing</span>"
    ]
  },
  {
    "objectID": "chapters/week01/other-commands.html#when-problems-arise",
    "href": "chapters/week01/other-commands.html#when-problems-arise",
    "title": "4  Going Beyond Browsing",
    "section": "4.3 When Problems Arise",
    "text": "4.3 When Problems Arise\nError messages offer another, admittedly less pleasant way to learn about how a command works. When you’re first starting out with any kind of console-based software (whether it be a CLI or writing code), one of the most important skills you can learn is how to read an error message. For the most part, such messages are quite clear; they’re intended to help you debug your problem and thus attempt to supply you with information about what might be going wrong.\nAs an example, if you’re at your home directory, and it looks like the following:\nls\nbackups  data.csv  jupyter_notebook.ipynb  project_folder\nAnd you decide to ls a folder within this directory that doesn’t exist:\nls no_name\nYou’ll see this error message:\nls: cannot access 'no_name': No such file or directory\nThis tells you that, while you’ve sent in a valid ls command, it can’t find what you’re looking for.\nLikewise, forgetting a space:\necho\"hello\"\nLeads to:\n-bash: echohello: command not found\nIn other words, you’ve sent in a command that’s either invalid or is unavailable. These error messages are both fairly clear, but if you’re ever confused, or if you simply want to find out more about an error, a search engine is once again your friend. Sometimes simply copy/pasting the error message and searching on that alone will lead directly to information about what went wrong.\nThat all said, sometimes you need to stop a CLI process immediately. Did you, for example, do something that causes your computer to print a million lines on screen? Did you decide you don’t want a file copied to a new location, and it’s still in the midst of transferring? You can interrupt any command with Ctrl + c (on macOS: Cmd + c). This will stop whatever current process is running in your interface.\nWhile you can stop a command, for the most part it isn’t possible to undo a command. Please take care to know exactly what you’re running and what you’re running it on, especially when it comes to modifying or deleting things on your computer.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Going Beyond Browsing</span>"
    ]
  },
  {
    "objectID": "chapters/week01/other-commands.html#sec-example-downloading-inspecting-files",
    "href": "chapters/week01/other-commands.html#sec-example-downloading-inspecting-files",
    "title": "4  Going Beyond Browsing",
    "section": "4.4 Example: Downloading & Inspecting Files",
    "text": "4.4 Example: Downloading & Inspecting Files\nTo practice using the CLI and learn some new commands, let’s download a file from the Internet.\nThe file we’ll download is called example-files.zip. It’s an archive, meaning it contains several other files. Specifically, it’s a ZIP archive (.zip), a format that uses compression to minimize the file size. This makes ZIP files great for exchanging data online, although other compressed archive formats (such as .tar.gz) are also popular. The file contains several of the files from the Ada and Charles example (Section 3.1).\nTo prepare, change the working directory to the 2026_intro-cmd/ directory you created in Section 3.4:\ncd ~/2026_intro-cmd\nRather than downloading the file with a web browser, we’ll download it with the CLI. To do this, you can use the curl command together with the URL for the file. The file is at this URL:\nhttps://ucdavis.box.com/shared/static/8nybft8ysh90vuqureczugueruhfm1lp.zip\nAnd this is the command to download the file:\ncurl -L -o example-files.zip https://ucdavis.box.com/shared/static/8nybft8ysh90vuqureczugueruhfm1lp.zip\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0   0     0   0     0     0     0  --:--:-- --:--:-- --:--:--     0\n  0     0   0     0   0     0     0     0  --:--:-- --:--:-- --:--:--     0\n100     5   0     5   0     0     4     0  --:--:--  0:00:01 --:--:--     0\n100 71957 100 71957   0     0 49205     0   0:00:01  0:00:01 --:--:-- 49205\nThe -L flag tells curl to follow redirects and is necessary because Box, the website where the file is hosted, does not provide direct links to files. The -o flag tells curl where to save the downloaded file; in this case, we save it to example-files.zip.\nConfirm that the file is in the working directory with the ls command:\nls\ndata  example-files.zip\nIn order to use the contents of the file, we have to decompress it. You can decompress (“unzip”) a ZIP archive with the unzip command:\nunzip example-files.zip\nArchive:  example-files.zip\n   creating: example-files/\n  inflating: example-files/README.md\n   creating: example-files/charles/\n  inflating: example-files/charles/cool_hair_selfie.jpg\n   creating: example-files/ada/\n  inflating: example-files/ada/analysis.ipynb\n  inflating: example-files/ada/cats.csv\nUnzipping the file creates a new directory called example-files in the working directory. Change to the example-files directory and list the files:\ncd example-files\nls\nada  charles  README.md\nArchives and projects often include a file named README, or some variation of this, intended as documentation to be read before doing anything else. You can print out the first 10 lines of a file with the head command. Take a look at the README.md file:\nhead README.md\n# README\n\nThis archive contains some of the files from the Ada and Charles example in\nDataLab's [Introduction to the Command Line][cmd] workshop reader.\n\n[cmd]: https://ucdavisdatalab.github.io/workshop_research_computing/chapters/command-line/\nFrom the output, we can see that this file contains text. In this case, the file is written in Markdown, a formatting language designed to be easy to read but convenient for adding formatting like italics and hyperlinks to text.\n\n\n\n\n\n\nNote\n\n\n\nThe cat (“concatenate”) command is similar to head, but prints out an entire file rather than just the first 10 lines. Use the cat command with care, since some files are very large (and thus take a long time to print).\nIf you want to view the contents of a large file, it’s better to use a text editor, which we introduce in the next chapter.\n\n\nThe README file is named README.md. The part after the dot, md, is called a file extension. Extensions are a convention for describing the formats of files and are usually, but not always, 1-3 characters long. In this case, .md is the standard extension for a Markdown file. A few other common extensions include .txt for text files, .zip for ZIP archives, and .png for PNG image files.\n\n\n\n\n\n\nTip\n\n\n\nSome operating systems hide extensions in their GUI file browsers by default. We recommend configuring your GUI file browser to always display extensions. If you’re not sure how, try searching for instructions online.\n\n\nSometimes the extension for a file will be missing or incorrect. If, while working with the CLI, you need to get a sense of what kind of data is stored in a file, you should use the file command. Let’s take a look at the output from the command for a few files:\nfile README.md\nREADME.md: ASCII text\nThe file command recognizes that the README.md file contains text. The ZIP archive doesn’t:\nfile ../example-files.zip\n../example-files.zip: Zip archive data, made by v4.6 UNIX, extract using at least v2.0, last modified, last modified Sun, Jan 07 2026 03:59:54, uncompressed size 0, method=store\nFor this file, the command indicates that it’s a ZIP archive and prints out some metadata about how and when it was created.\nUse the file command and the commands for browsing files from Chapter 3 to inspect the files in the example-files directory and its subdirectories.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Going Beyond Browsing</span>"
    ]
  },
  {
    "objectID": "chapters/week01/other-commands.html#reference-common-commands",
    "href": "chapters/week01/other-commands.html#reference-common-commands",
    "title": "4  Going Beyond Browsing",
    "section": "4.5 Reference: Common Commands",
    "text": "4.5 Reference: Common Commands\nThe commands introduced in this chapter, as well as a few other common commands, are listed in the following table:\n\n\n\n\n\n\n\n\nCommand\nDescription\nExamples\n\n\n\n\nman\nOpens the manual page for another command.\nman ls\n\n\ncurl\nDownloads a file from the internet.\ncurl https://datalab.ucdavis.edu/\n\n\ntar\nCompresses/decompresses files into/from a .tar or .tar.gz file.\ntar -xf my_file.tar\n\n\nzip\nCompresses files into a .zip file.\nzip my_files.zip my_files/\n\n\nunzip\nDecompresses (“unzips”) files from a .zip file.\nunzip my_file.zip\n\n\nhead\nPrints the first 10 lines of a file to the screen.\nhead README.md\n\n\ncat\nPrints the entirety of a file to the screen.\ncat README.md\n\n\nfile\nPrints the type of a file.\nfile my_dataset.csv\n\n\n\n\n\nThere are dozens of shell commands, each with their own set of flags, and it’s possible to install even more. We can’t go over everything in one workshop, but by knowing the shell command syntax and how to get help, you’ll be able to figure out how to use new commands.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Going Beyond Browsing</span>"
    ]
  },
  {
    "objectID": "chapters/week01/editing-files.html",
    "href": "chapters/week01/editing-files.html",
    "title": "5  Editing Files",
    "section": "",
    "text": "5.1 Text Editors\nThe shell commands we’ve covered so far give you ways to navigate between directories and inspect the contents of files. You can also use the CLI to edit files directly. This chapter explains one way to edit files with the CLI. It also introduces the final piece of the file browsing experience: how to copy, move, and delete files.\nA text editor is a computer program designed to edit files that contain plain text, free of images and other kinds of data. This reflects the fact that text editors originated in the CLI, although now text editors for GUIs are also available.\nText editors display the contents of a file faithfully and efficiently, with minimal processing. They stand in contrast to word processors (such as Microsoft Word), where you can freely mix text with graphical formatting, images, and other data, but what you see on the screen is a heavily processed version of what’s actually stored in the file. This minimalisim makes text editors useful for editing files that do not normally include formatting, such as code, and for examining the true contents of files, especially if their format is intended to be human-readable.\nMany different text editors exist, including several free and open-source options. A few popular CLI text editors are:\nFor the remainder of this reader, we’ll focus on nano (and Pico).",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Editing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/editing-files.html#text-editors",
    "href": "chapters/week01/editing-files.html#text-editors",
    "title": "5  Editing Files",
    "section": "",
    "text": "NotePlain Text vs. Binary\n\n\n\nUltimately, computers represent all data as numbers. They use a binary number system, where numbers are made up of the bits 0 and 1 rather than the digits 0 through 9 in the familiar decimal number system.\nTo represent text on a computer, we can encode each symbol as a specific number; the number represents the symbol. People have come up with many different standards for doing this. For instance, in the ASCII standard, which was designed for languages that use the Latin alphabet, 65 represents the letter A, 66 represents the letter B, 97 represents the letter a, and so on. Nowadays, most people use the UTF-8 standard, a superset of ASCII designed to support all languages.\nWhen we say that files or data are “plain text,” we mean that they can be decoded with some well-known standard into text that’s intelligible. We refer to files and data that cannot be decoded this way as “binary.” Note that this is a reuse of the word in a closely related but distinct way from how we defined it previously.\nExamples of plain text files include text files (.txt), Markdown files (.md), source code files for most programming languages (.R, .py, and many more), and human-readable data files (.csv, .json, and more). Plain text files can be opened in any text editor.\nExamples of binary files include image files (.jpg, .png, and more), audio and video files (.mp3, .mp4, and more), data files (.rds, .pickle, .hd5, and more), and executable programs (.exe, .o, .so, and more). To open a binary file, you generally need to have special software to decode the contents into something meaningful.\nYou can see how all of this works if you try to open a binary file with a text editor. For example, if you open the cool_hair_selfie.jpg image included with the files from Section 4.4 in a text editor, you’ll see something like this:\nÿØÿà^@^PJFIF^@^A^A^A^D°^D°^@^@ÿþ^@^SCreated with GIMPÿÛ^@C^@^C^B^B^C^B^B^C^C^C^C^    D^C^C^D^E^H^E^E^D^D^E\n^G^G^F^H^L\n^L^L^K\n^K^K^M^N^R^P^M^N^Q^N^K^K^P^V^P^Q^S^T^U^U^U^L^O^W^X^V^T^X^R^T^U^TÿÛ^@C^A^C^D^D^E^D    ^E▷^E^E▷ ^T^M^K^M^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T    ^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^T^TÿÂ^@^Q^H^Bw^AÃ^C^A^Q^@^B^Q^A^C^Q^AÿÄ^@^\\^@^@^    B^C^A^A^A^A^@^@^@^@^@^@^@^@^@^B^C^A^D^E^F^@^G^HÿÄ^@^X^A^@^C^A^A^@^@^@^@^@^@^@^@^@    ^@^@^@^@^A^B^C^DÿÚ^@^L^C^A^@^B^P^C^P^@^@^Aø¶W^C&lt;94&gt;*&lt;84&gt;1^R&lt;85&gt;(^BcSv&lt;94&gt;é+Ä¾ª%^P    *ß&lt;99&gt;^S/L4j&lt;89&gt;ò^\\é&lt;93&gt;0ß¥&lt;90&gt;&lt;96&gt;;6ê¥,(&lt;97&gt;'6áµ^H^PÊg^HÓ]&lt;8c&gt;^KPï§KD¬ä&lt;82&gt;F&lt;9a&gt;    ^BÝ¼&lt;87&gt;'êuÃR^DQ&lt;83&gt;yØÑÖ&lt;90&gt;&@$^VÆ[&lt;98&gt;^nÖ­^PNQ-&lt;94&gt;&lt;9e&gt;°Ósj&lt;88&gt;&lt;86&gt;EÑ0^A²NkÉ&lt;9d&gt;    ¶hV&lt;97&gt;(ò&lt;98&gt;³Ù«2&gt;ê«^ZÅ#Ò&lt;9a&gt;&lt;9f&gt;&lt;98&gt;Z^A&lt;8a&gt;0³uêe&lt;8c&gt;&lt;9c&gt;¯hêS&lt;9b&gt;g&lt;98&gt;Cn&lt;8d&gt;^Y&lt;8f    &gt;ÎHxºAh^B^Nd-&lt;9b&gt;`&lt;97&gt;&lt;91&gt;^R&lt;84&gt;^[»^DÇ9$6&lt;98&gt;Êrs½BG&lt;94&gt;&lt;82&gt;@Ø!&lt;92&gt;7J^hÁì¬Ü6É&lt;95&gt;k    P¦ä6^N$\nIt’s a complete mess! It looks like this because the text editor does its best to decode the file as text, but since (most of) the file isn’t text, most of the decoded symbols actually represent something else. As a result, it’s generally not a good idea to edit binary files with a text editor.\nAs a final note, word processors, like Microsoft Word, add an interesting twist to all of the above. Word documents are actually comprised of a number of different, associated files under the hood. It’s technically possible to alter the text of such files with a text editor, but finding the right place to make a change is difficult, and it’s more convenient to use a program made to to interact with them.\n\n\n\n\nnano (and its predecessor Pico), a straightforward text editor with few features beyond displaying and editing text.\nVim (and its predecessor vi), a text editor designed to be customizable and extensible. It has many features that make it easier to edit code, such as search-and-replace, keyboard shortcuts, and syntax highlighting. Vim is also infamous for being difficult to learn because its interface consists of several different “modes.”\nEmacs, another text editor designed to be customizable and extensible. It has many of the same features as Vim, and is also somewhat difficult to learn, although its interface is more like a word processor.\n\n\n\n5.1.1 Nano & Pico\n\n\n\n\n\n\nImportant\n\n\n\nOn Windows, nano is included with Git Bash.\nOn macOS, some versions have nano and some versions have Pico. You can use either one to follow along with the subsequent sections.\nOn Linux, nano is included with most distributions. If it’s missing, you can install it with your distribution’s package manager.\n\n\nLet’s use nano to create a new text file in the 2026_intro-cmd directory you created in Section 3.4. To get started, change the working directory to that directory:\ncd ~/2026_intro-cmd\nWe’ll make a file called hello.txt. You can open a file with nano by putting the name of or path to the file after the nano command. This will work even for files that don’t exist yet. Most other CLI text editors also work this way. Go ahead and open hello.txt with nano:\nnano hello.txt\nOnce nano opens the file, you can type in or delete text, and use the arrow keys to move the cursor, as you would in a word processor. For this example, type in Hello world! on the first line of the file.\nWhen you’re done editing, you can press Ctrl + o to save the file. nano will ask you to confirm that you want to save the file by pressing Enter or Return. After saving the file, you can press Ctrl + x to exit nano.\n\n\n\n\n\n\nTip\n\n\n\nnano displays all of its keyboard shortcuts at the bottom of the screen. In the dislpay, it uses ^ to mean the Ctrl key. The keyboard shortcuts in nano are not case-sensitive.\n\n\nBack at the CLI, use the head or cat command to check that the new hello.txt file contains what you wrote:\ncat hello.txt\nHello world!\nAs another example, let’s look at the README.md file included in example-files.zip (Section 4.4). To open the file in nano:\nnano example-files/README.md\nThe contents should look familiar from when you printed the file with head, but now you can edit the file if you’d like.\nA text editor is an essential tool for anyone working with the CLI. It not only provides a way to edit text files, but also a way to inspect files whether or not they contain text. After file browsing commands, your text editor is likely the CLI command you will use the most.\n\n\n\n\n\n\nTip\n\n\n\nIf you want a text editor that’s full-featured but not much harder to learn than nano, check out Micro.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Editing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/editing-files.html#moving-deleting-files",
    "href": "chapters/week01/editing-files.html#moving-deleting-files",
    "title": "5  Editing Files",
    "section": "5.2 Moving & Deleting Files",
    "text": "5.2 Moving & Deleting Files\nChapter 3 explained how to use the CLI to browse through files and directories on your computer, but we left out one important detail: how to copy, move, and delete things. We’ll cover commands for all of those in this section.\nLet’s make a copy of the hello.txt file you just created. You can use the cp command to make a copy. It requires two arguments: the path to the original file and the path to the copy (which the command will create). For the copy of hello.txt, let’s use the name another_hello.txt. So the command to make the copy is:\ncp hello.txt another_hello.txt\nCheck with ls that there’s now a new file in the directory called another_hello.txt:\nls\nanother_hello.txt  data  example-files  example-files.zip  hello.txt\nIf you open another_hello.txt with a text editor or print it with head or cat, you should see that it’s exactly the same as hello.txt. Congratulations, you’ve made a copy!\nThe mv command moves a file to a different place. Note that renaming a file is equivalent to moving the file, so this command is also useful for renaming files. The syntax of the mv command is similar to the cp command. It requires two arguments: the path to the original file and the path to the destination. Try moving another_hello.txt to a_sincere_hello.txt:\nmv another_hello.txt a_sincere_hello.txt\nNow the directory looks like this:\nls\na_sincere_hello.txt  data  example-files  example-files.zip  hello.txt\nIf you want, you can also inspect a_sincere_hello.txt to make sure that its contents are the same.\nFinally, let’s clean up by removing the a_sincere_hello.txt file. You can remove a file with the rm command. Be careful with this command, as there is no undo—removing a file is permanent and irreversible. Go ahead and remove a_sincere_hello.txt:\nrm a_sincere_hello.txt`\nTake one last look at the contents of the directory:\nls\ndata  example-files  example-files.zip  hello.txt\nThe a_sincere_hello.txt file is gone.\n\n\n\n\n\n\nWarning\n\n\n\nLike many shell commands, there’s no undo for the cp, mv, and rm commands. They’re also relatively mute, printing nothing or little to indicate what they’re doing. These characteristics make it easy to accidentally overwrite or delete a file.\nIf you’re worried about making mistakes, you can put these commands in interactive mode by passing the -i or --interactive flag. In interactive mode, the commands will prompt you with a yes/no question any time they might overwrite or delete a file.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe cp, mv, and rm commands can also copy, move, and remove directories. The mv command can do this by default. For the cp and rm commands, you have to set the -r or --recursive flag in order for them to operate on directories.\nUnlike rmdir, the rm -r command can delete directories even if they contain files.",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Editing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/editing-files.html#reference-editing-commands",
    "href": "chapters/week01/editing-files.html#reference-editing-commands",
    "title": "5  Editing Files",
    "section": "5.3 Reference: Editing Commands",
    "text": "5.3 Reference: Editing Commands\nHere’s a summary of the commands we covered in this chapter:\n\n\n\n\n\n\n\n\nCommand\nDescription\nExamples\n\n\n\n\nnano\nOpens the nano text editor.\nnano my_file.txt\n\n\ncp\nMakes a copy of a file or directory.\ncp my_file.txt my_copy.txt\n\n\nmv\nMoves (or renames) a file or directory.\nmv my_file.txt my_file_renamed.txt\n\n\nrm\nRemoves a file or directory.\nrm my_file.txt",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Editing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week01/editing-files.html#a-final-note",
    "href": "chapters/week01/editing-files.html#a-final-note",
    "title": "5  Editing Files",
    "section": "5.4 A Final Note",
    "text": "5.4 A Final Note\nAlthough we’ve only scratched the surface of what you can do with the command line, the commands and underlying concepts we’ve discussed here should prepare you to continue using and learning about the CLI. In doing so, our hope is that you’ll become a more confident and experienced computer user. If questions do arise, DataLab offers support across all levels of experience. See our website for details about office hours, as well as other resources and events for learners. Happy scripting!",
    "crumbs": [
      "Week 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Editing Files</span>"
    ]
  },
  {
    "objectID": "chapters/week02/how-ai-works.html",
    "href": "chapters/week02/how-ai-works.html",
    "title": "6  How AI Works",
    "section": "",
    "text": "Important\n\n\n\nWe teach this lecture with a slide deck.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How AI Works</span>"
    ]
  },
  {
    "objectID": "chapters/week02/version-control-systems.html",
    "href": "chapters/week02/version-control-systems.html",
    "title": "7  Version Control Systems",
    "section": "",
    "text": "7.1 What is Version Control?\nVersion control is the process of storing and organizing multiple versions (or copies) of files on your computer. Approaches to version control range from simple to complex and they can involve the use of both manual and automatic workflows.\nChances are good that you’re already doing some kind of version control yourself. You might have a directory somewhere on your computer that looks something like this:\nOr perhaps this:\nThis is a rudimentary form of version control where it’s completely up to you to name, save, and keep track of multiple versions of a file. This filesystem approach works minimally well, in that it does provide you with a history of file versions theoretically organized by their time sequence. But this system provides no information about how the file has changed from version to version, why you might have saved a particular version, or specifically how the various versions are related. This approach is also subject to human error. It’s easy to make a mistake when naming a file version, or to go back and edit a file without saving a new copy.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version Control Systems</span>"
    ]
  },
  {
    "objectID": "chapters/week02/version-control-systems.html#sec-version-control-systems",
    "href": "chapters/week02/version-control-systems.html#sec-version-control-systems",
    "title": "7  Version Control Systems",
    "section": "7.2 Version Control Systems",
    "text": "7.2 Version Control Systems\nA version control system (VCS) is software designed to automate version control. Version control systems originated in the software development community, where it’s common for many people to work on the same file, sometimes synchronously, amplifying the need to track and understand revisions. But nearly all types of computer files, not just code, can be tracked using modern version control systems.\n\n\n\n\n\n\nNoteHistorical Note\n\n\n\nIBM’s OS/360 IEBUPDTE software update tool is widely regarded as the earliest and most widely adopted precursor to modern version control systems. Its release in 1972 of the Source Code Control System (SCCS) package marked the first fully-fledged system designed specifically for software version control.\n\n\nMost version control systems keep track of changes to a collection of files called a repository. Generally, a repository (or repo) is just a directory where you’ve set up a version control system to keep track of changes to the contents. A repository can contain any number of files and subdirectories.\n\n\n\n\n\n\nTip\n\n\n\nHow many repositories to create is up to you, and depends on how you like to work, but we recommend that you create a separate repository for each distinct project.\n\n\nIn order to make it easy to collaborate, most version control systems also provide a way to create multiple copies of a repository and share changes between them. Version control systems can be divided into two categories based on how they do this:\n\nCentralized version control systems store the repository and its history on a single computer. This computer is usually a server, a computer connected to the Internet or some other network. Users can check out a copy of the repository from the server, make changes, and then check in their changes with the server. The server is the sole authority on the repository’s history. You can think of this as a “hub and spoke” model, where the server is the hub and users are the spokes. This is the oldest kind of version control system.\nDistributed version control systems treat each copy of the repository as an authority on the repository’s history, and provide ways to sync changes and resolve conflicts between copies. As two different users make changes to their copies of the repository, the copies will diverge if both users edit the same file. The divergence will remain in place until the two copies are synced, at which time the VCS merges the two different versions of the file into a single version that reflects the changes made by both users. You can think of this as a “network” model (like a social network).\n\n\n\n\n\n\n\n\n\nCentralized (hub and spoke) model\n\n\n\n\n\n\n\nDistributed (network) model\n\n\n\n\n\n\nFigure 7.1\n\n\n\nCentralized VCS provide a very ordered and controlled universe. They ensure users have access to the most recent version of every file in the repository, which reduces the potential for conflicting changes to files.\n\n\n\n\n\n\nNoteHistorical Note\n\n\n\nEarly centralized version control systems typically required users to check out individual files or directories rather than entire repositories, and only allowed one user to check out a given file at time. This prevented conflicting edits, but made it difficult to work concurrently.\n\n\nOn the other hand, distributed VCS offer greater flexibility. They allow users to work alone or in small groups, work offline, or work on experimental changes over an extended period without losing the benefits of version control. These characteristics facilitate collaborative work. Moreover, a distributed VCS can be used in a centralized way, where one copy of the repository is treated as the final authority on the repository’s history. This gives users the best of both worlds, by allowing some to sync directly with each other while others sync with this authoritative copy.\nThe most popular VCS today is Git, a distributed VCS. Some polls estimate that more than 90% of all developers use Git. A few other version control systems in use today include Mercurial, Subversion, Perforce, and Plastic SCM. Many document editors, such as Google Docs and Microsoft Word, also have built-in version control systems. Each of these systems offers a twist on version control, differing sometimes in the area of user functionality, sometimes in how they handle things on the back-end, and sometimes both. In this reader, we’ll focus on Git.\n\n\n\n\n\n\nImportant\n\n\n\nGit is available for Windows, macOS, and Linux.\nInstall Git by following the instructions for your computer’s operating system:\n\nWindowsmacOSLinux\n\n\nOn Windows, download Git from the Git downloads page. We recommend the default installation options, which also install Git Bash. You’ll need Git Bash to follow along with this workshop.\n\n\n\nOn macOS, there are many ways to install Git. The easiest is generally to install Xcode by opening a terminal and entering:\ngit --version\nThen follow the prompts to install Xcode. If you prefer not to install Xcode (it is quite large), installing Git with a package manager such as Homebrew, MacPorts, or Pixi is also okay.\n\n\nOn Linux, we recommend installing Git with your distribution’s package manager (many distributions install Git by default). Installing Git with a user-level package manager such as Pixi is also okay.\n\n\n\nYou can also find more information about how to install Git in the section Installing Git of the book Pro Git by Chacon and Straub. Pro Git is an excellent reference for all things Git, so much so that a digital version is available for free on the Git website.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version Control Systems</span>"
    ]
  },
  {
    "objectID": "chapters/week02/git-basics.html",
    "href": "chapters/week02/git-basics.html",
    "title": "8  Git Basics",
    "section": "",
    "text": "8.1 Configuring Git\nWhen you save changes to a repository, Git will automatically sign the changes with your name and email. In collaborative projects, these signatures are important for determining who made which changes. Thus the first time you use Git, you need to set your name and email.\nWe recommend that you configure Git with your real name (given name and surname) and a valid email address. These serve the same purpose in Git as they would on a publication: they ensure you receive credit for your work and give people a way to contact you about it. Git is open-source, community-developed software, so it won’t share your name and email address with spammers, but the information will be visible on any changes you make to public repositories.\nTo set your name in Git, open a terminal and type:\nReplace YOUR_NAME with your name, keeping the quotes. Then press Enter. If you make a mistake, don’t worry: you can run this command to change your name as many times as you need.\nLet’s break down what the command means. All Git commands begin with git followed by the name of a subcommand. The command to set Git’s configuration options is git config set. The --global argument makes the command set options globally (that is, for all of your repositories). Git stores your name under the user.name configuration option. The final argument is the new value for user.name. So the command sets user.name for all repositories to the name you put inside the quotes.\nThe related command git config get gets the value of an option. You can use this to check how Git is configured. For instance, to check what Git thinks your name is:\nGit stores your email address under the user.email configuration option. So to set your email, type:\nReplace YOUR_EMAIL with your preferred public email address.\nNext, we recommend that you set Git’s default text editor to nano (or on macOS, pico). These text editors are beginner-friendly. To do this, type:\nFinally, we suggest that you change the default branch name from master to main. You’ll learn more about what branches are later, but we advise making this change now. For too long the computing industry has relied on offensive terms like “master” and “slave” to describe technology, and changing such terms is part of a wider push to move away from the framework they imply. This is a small change, but we at the DataLab believe that, in all instances, language matters.\nTo change the default branch name to main, run:",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Git Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week02/git-basics.html#sec-configuring-git",
    "href": "chapters/week02/git-basics.html#sec-configuring-git",
    "title": "8  Git Basics",
    "section": "",
    "text": "Note\n\n\n\nIf you’re not comfortable attaching your real name to work you do with Git, a reasonable alternative is to use an alias you control, such as your GitHub username. Section 9.1 explains more about GitHub.\nLikewise, if you don’t want to attach your primary email address to work you do with Git, set up a new email address and use that. Don’t make up a fake email address—someone else could take credit for your work or even impersonate you.\n\n\n\ngit config set --global user.name \"YOUR_NAME\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nGit 2.46 added the set (and get) subcommands to the git config command. If you have an older version of Git, you might see an error message like this:\nerror: key does not contain a section: set\nYou can still run the git config commands in this section if you omit the set (and get) subcommands. So the command to set your name becomes:\ngit config --global user.name \"YOUR_NAME\"\nIn the long term, the best solution is to install a newer version of Git on your computer.\n\n\n\ngit config get user.name\n\n\n\n\n\n\nTip\n\n\n\nYou can view the documentation for any subcommand by adding --help to the end. For instance, to get help with the git config subcommand, run git config --help.\n\n\n\ngit config set --global user.email \"YOUR_EMAIL\"\n\n\ngit config set --global core.editor \"nano\"\n\n\ngit config set --global init.defaultBranch main\n\n\n\n\n\n\nNoteSee Also\n\n\n\nThis section is based on the section First-Time Git Setup of the book Pro Git.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Git Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week02/git-basics.html#sec-creating-a-repository",
    "href": "chapters/week02/git-basics.html#sec-creating-a-repository",
    "title": "8  Git Basics",
    "section": "8.2 Creating a Repository",
    "text": "8.2 Creating a Repository\nNow that we’ve established what version control systems are, and you’ve configured Git, it’s time to create a repository.\nOpen a terminal and navigate to your home directory:\ncd\nTo initialize a repository called my_first_repository, enter:\ngit init my_first_repository\nGit will reply with a message like:\nInitialized empty Git repository in /home/USERNAME/my_first_repository/.git/\nWhen you run the git init command, Git first checks whether the specified directory (my_first_repository/) exists, and creates it if it doesn’t. Then Git makes the directory a repository by creating a hidden .git/ subdirectory. This subdirectory is where Git will store the history of the repository.\n\n\n\n\n\n\nWarning\n\n\n\nThe .git/ subdirectory is hidden for a reason. Let Git manage its contents. Avoid creating or modifying files and directories inside .git/, as this might break your repository. If you delete .git/, your repository will no longer be a repository—it will just be an ordinary directory.\n\n\nNow let’s check that Git actually recognizes my_first_repository/ as a repository. First, navigate to the directory:\ncd my_first_repository/\nYou can use git status to check the status of a repository. Try running it for the new repository:\ngit status\nSince the directory is a repository, Git will respond with output like:\nOn branch main\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\nWe’ll save branches for a different lesson. Skipping to the second part of the message, Git says that there are no “commits” yet. A commit is a saved snapshot (or version) of the repository. You’ll learn how to make a commit soon, but right now, it makes sense that there are no commits yet, since you just created the repository. Finally, in the third part of the message, Git says that there is nothing to commit. This also makes sense, since we haven’t created any files in the repository yet.\n\n\n\n\n\n\nNote\n\n\n\nWhen Git doesn’t recognize a directory as a repository, the output from git status (and most other git subcommands) instead looks like:\nfatal: not a git repository\nIf you see this message, your working directory is not a Git repository. Most likely you meant to run the command in a different directory.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Git Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week02/git-basics.html#sec-adding-committing-changes",
    "href": "chapters/week02/git-basics.html#sec-adding-committing-changes",
    "title": "8  Git Basics",
    "section": "8.3 Adding & Committing Changes",
    "text": "8.3 Adding & Committing Changes\nOnce you’ve created a repository, you’ll want to copy some files into it, or create new files, so that you can use Git to track their versions.\nIn my_first_repository/, let’s create a file with a short self-introduction. Start by opening a text editor.\n\n\n\n\n\n\nNoteChoosing a Text Editor\n\n\n\n\n\nEven if you primarily use software with graphical user interfaces, it’s good to be familiar with a command-line text editor. You can use the editor for quick edits while at the command-line and on computers that don’t provide a graphical environment (as is often the case for cloud and high-performance computing resources).\nThe GNU nano text editor is a simple and easy-to-learn. It’s typically pre-installed on Linux and bundled with Git Bash on Windows. On macOS, Pico, nano’s almost-identical predecessor, is pre-installed. You can run nano with the nano command (or run Pico with the pico command).\nIf you like the simplicity of nano but want features like modern keyboard shortcuts and syntax highlighting by default, install the micro editor. It’s available for all major operating systems; see the website for details.\nVim is a powerful, customizable text editor, but takes some time to learn. Vim (or its predecessor vi) is typically pre-installed on macOS and Linux, and is bundled with Git Bash on Windows. You can run Vim with the vim command (or run vi with the vi command).\nPopular alternatives to Vim with similar features include GNU Emacs and Neovim.\n\n\n\nIn the text editor, enter a friendly greeting, like this one:\nHi, I'm Nick, and this is my repository!\nYour greeting doesn’t need to be identical, and you can change the name to your name. Save this as hello.txt in the my_first_repository/ directory. Then check the status of the repository again:\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        hello.txt\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nGit notices the new file, hello.txt, and says it’s untracked, which means Git doesn’t have any history for it. You just created the file and haven’t committed it yet, so it makes sense that there’s no history.\nLet’s commit hello.txt now. The first step is to add the file to Git’s staging area (or index). The staging area is a virtual space for preparing commits, where you can select which changes to include in the commit. It might help to imagine the staging area as a box 📦 that you’re packing up to store or to send to a friend.\nThe git add command adds changes to the staging area. Go ahead and add the changes to hello.txt:\ngit add hello.txt\nNow check the status of the repository again:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   hello.txt\nGit reports that the changes to hello.txt are in the staging area and ready to be committed. It also lists the command to remove the changes from the staging area.\n\n\n\n\n\n\nTip\n\n\n\nTake advantage of the staging area to curate the contents of your commits. Putting distinct work in distinct commits makes it easier to inspect (and occasionally undo) the work.\n\n\n\n\n\n\n\n\nTipTip: Unstaging Changes\n\n\n\n\n\nIn a new repository without any commits, the command to unstage changes is git rm --cached.\nIn a repository that has commits, the command to unstage changes is git restore --staged. This is the command to remember, since most repositories have commits.\nThe distinction can trip up even experienced Git users. If you want a single unstage command you can use under any circumstances, run:\ngit config set --global alias.unstage \"reset --\"\nThen you can use git unstage whenever you want to unstage changes.\n\n\n\n\nYou can make a commit with the git commit command. Enter the command:\ngit commit\nIn response, Git will open a text editor (Vim by default) with the following text:\n\n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n#\n# On branch main\n#\n# Initial commit\n#\n# Changes to be committed:\n#   new file:   hello.txt\n#\nGit will then wait for you to write a commit message, a description of what the commit changes, at the beginning of the text. The first line of the commit message must be a summary of the commit in 72 characters or less. You can optionally follow this with a blank second line and then a detailed description of the commit beginning on the third line.\n\n\n\n\n\n\nImportant\n\n\n\nStrive for clear and meaningful commit messages. If you feel like the changes in a commit are too numerous or complicated to summarize in one line, use the optional detailed description.\nCommit messages are the history of your project. Neglecting them might save time in the short run, but in the long run it will make understanding the project’s evolution much more difficult and time-consuming for future you and your collaborators.\n\n\n\n\n\n\n\n\nTipTip: The 50/72 Rule\n\n\n\n\n\nMany programmers follow the 50/72 rule, which says that the first line of a commit message should be 50 characters or less and written in the imperative mood. For example:\nFix typos in the main text.\nSubsequent lines should be 72 characters or less. The rule makes commit messages easier to read and understand, especially in the terminal and on narrow screens.\nWe recommend following the 50/72 rule, but there are situations where doing something else is justified. The best approach is to talk to your collaborators about specific conventions they want to follow, and check in with them about exceptions to the conventions.\n\n\n\nEdit the commit message to look like this:\nAdd a friendly greeting.\n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n#\n# On branch main\n#\n# Initial commit\n#\n# Changes to be committed:\n#   new file:   hello.txt\n#\nFinally, to let Git know that you’re done, save the commit message and exit the text editor.\n\n\n\n\n\n\nNote\n\n\n\nIf you exit the text editor without saving, Git will cancel the commit. This is helpful if you change your mind about making a commit or forget to add something to the staging area.\n\n\n\nGit will print some output to confirm that the commit was created:\n[main (root-commit) 0f5c79d] Add a friendly greeting.\n 1 file changed, 1 insertion(+)\n create mode 100644 hello.txt\nImportant details in the output include:\n\nA hash that uniquely identifies the commit (0f5c79d above, but yours will be different)\nThe commit message\nThe number of files changed\nA list of which files were changed (hello.txt in this case)\n\nRun git status to see how the output has changed now that you’ve made a commit:\ngit status\nOn branch main\nnothing to commit, working tree clean\nGit reports that there’s nothing to commit, and that the “working tree” is clean. The working tree consists of the files and directories you actually have in your repository. The working tree is clean if it’s identical to the most recent commit, meaning you haven’t changed anything since that commit.\n\n\n\n\n\n\nImportant\n\n\n\nRemember, saving your work in Git is a two step process:\n\ngit add (for each file or directory with changes you want to save)\ngit commit\n\nIt’s a good idea, but not required, to run git status after the first step, to check that you’ve added all of the changes you meant to add to the staging area.\n\n\n\n8.3.1 Moving a File\nTo get more practice making commits, suppose we want to move the file hello.txt to README.md, since README.md is conventionally the first file people read when they start working with an unfamiliar repository. Use the mv shell command to move the file:\nmv hello.txt README.md\nNow check the status of the repository:\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add/rm &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        deleted:    hello.txt\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nGit notices that hello.txt is gone, and also that there’s a new file README.md. Since README.md is untracked, Git doesn’t recognize that it’s the same file as the old hello.txt. Go ahead and add the changes to README.md to the staging area:\ngit add README.md\nAdd the changes to hello.txt as well:\ngit add hello.txt\nIt might seem counterintuitive to add hello.txt, since it no longer exists. What you should keep in mind is that git add adds changes to the staging area, not files, and moving (or removing) a file is a change to that file.\n\n\n\n\n\n\nImportant\n\n\n\nRemoving/deleting a file is a change to that file, just like creating, editing, or moving the file.\nIf you want to delete a file called FILE from a repository, first delete the file, then run git add FILE to add the change to the staging area, and finally run git commit to make a commit.\nDeleted files remain in the repository’s history, so it’s possible to restore them later.\n\n\nNow check the status:\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        renamed:    hello.txt -&gt; README.md\nAfter adding the changes to both files, Git correctly recognizes that the file was moved/renamed. Go ahead and commit the change with the commit message Move hello.txt to README.md.:\ngit commit\n[main 4f57023] Move hello.txt to README.md.\n 1 file changed, 0 insertions(+), 0 deletions(-)\n rename hello.txt =&gt; README.md (100%)\nIf you check the status now, you’ll see that the working tree is once again clean.\n\n\n\n\n\n\nTip\n\n\n\nYou can also make commits without opening a text editor. Use this command:\ngit commit -m \"COMMIT_MESSAGE\"\nReplace COMMIT_MESSAGE with your commit message. You can’t provide a detailed description when you commit this way, so it’s only appropriate for small, simple commits.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Git Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week02/git-basics.html#sec-exploring-history",
    "href": "chapters/week02/git-basics.html#sec-exploring-history",
    "title": "8  Git Basics",
    "section": "8.4 Exploring History",
    "text": "8.4 Exploring History\n\n\nNow that you’ve made some commits, let’s take a look at the repository’s history. To view the log of commits to a repository, enter the command:\ngit log\ncommit 4f5702364c155faa260080671b63177550347ea0 (HEAD -&gt; main)\nAuthor: YOUR_NAME &lt;YOUR_EMAIL&gt;\nDate:   Wed Jan 8 14:32:21 2025 -0800\n\n    Move hello.txt to README.md.\n\ncommit 0f5c79d0494763a31ade6a2514dd389f3f1eb1b4\nAuthor: YOUR_EMAIL &lt;YOUR_EMAIL&gt;\nDate:   Wed Jan 8 13:59:08 2025 -0800\n\n    Add a friendly greeting.\n\nFor each commit, the log lists the hash, name and email of the author, timestamp, and commit message.\n\n\n\n\n\n\nNote\n\n\n\nWhen a repository has a long history, git log will display the commits in a scrolling window. You can use the up and down arrow keys to scroll, and type q (for quit) to return to the terminal.\n\n\nLet’s make one more commit: we’ll add a title to the README.md file. Open the file with a text editor and add a title, so that it looks like this:\n# My README\n\nHi, I'm Nick, and this is my repository!\nWhen you’re finished, save the file. As usual, Git notices that something in the repository has changed:\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nYou can view the difference, or diff between the working tree and the most recent commit with git diff:\ngit diff\ndiff --git a/README.md b/README.md\nindex cd08755..4e3eb18 100644\n--- a/README.md\n+++ b/README.md\n@@ -1 +1,3 @@\n+# My README\n+\n Hello world!\nThe git diff command prints a diff for each file that’s been changed. In each diff, lines added since the last commit are prefixed with +, while lines removed since the last commit are prefixed with -. For context, each diff usually also includes a few lines that didn’t change (no prefix). It’s a good idea to check git diff before adding files to the staging area, so that you know what you’re adding.\n\n\n\n\n\n\nTip\n\n\n\nIf you’ve changed a lot of files, the output from git diff can be overwhelming. You can use the command git diff PATH to view only the changes to the file or directory at PATH.\nYou can also use git diff --staged to see the difference between files in the staging area and the last commit.\nThere are many other ways to use git diff; check the documentation (git diff --help) to learn more.\n\n\nAdd and commit the changes. After you finish, you should have a third commit in the repository history (git log) that looks something like this:\ncommit e15d8c1355f16c26fe00354855c24bff3626fc1b (HEAD -&gt; main)\nAuthor: YOUR_NAME &lt;YOUR_EMAIL&gt;\nDate:   Wed Jan 8 15:35:02 2025 -0800\n\n    Add title.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Git Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week02/git-basics.html#restoring-old-versions-of-files",
    "href": "chapters/week02/git-basics.html#restoring-old-versions-of-files",
    "title": "8  Git Basics",
    "section": "8.5 Restoring Old Versions of Files",
    "text": "8.5 Restoring Old Versions of Files\n\nSuppose you decide you don’t like the title you added to README.md in Section 8.4. If you want to change the title to something different, the best approach is to edit the file and make a new commit. On the other hand, if you want to restore an older version of the file, manual editing is tedious and error-prone.\nInstead, use the git restore --source command to restore a file to how it was in a particular commit.\nTo demonstrate this, let’s restore README.md to how it was in the commit before we added a title. First check git log to get the commit’s hash:\ncommit e15d8c1355f16c26fe00354855c24bff3626fc1b (HEAD -&gt; main)\nAuthor: YOUR_NAME &lt;YOUR_EMAIL&gt;\nDate:   Wed Jan 8 15:35:02 2025 -0800\n\n    Add title.\n\ncommit 4f5702364c155faa260080671b63177550347ea0\nAuthor: YOUR_NAME &lt;YOUR_EMAIL&gt;\nDate:   Wed Jan 8 14:32:21 2025 -0800\n\n    Move hello.txt to README.md.\n\ncommit 0f5c79d0494763a31ade6a2514dd389f3f1eb1b4\nAuthor: YOUR_NAME &lt;YOUR_EMAIL&gt;\nDate:   Wed Jan 8 13:59:08 2025 -0800\n\n    Add a friendly greeting.\n\nIn this example, the hash begins 4f5702, but it will be different for your commit. As you can see from git log, the full hash for each commit is quite long. For most Git commands that require a hash, you can just use the first few digits. Git will let you know if it needs more digits to disambiguate which commit you mean.\nTo restore README.md to how it was in commit 4f5702, run:\ngit restore --source 4f5702 README.md\nMake sure to replace 4f5702 with the actual hash for your commit.\n\n\n\n\n\n\nWarning\n\n\n\nBe careful with git restore: when you restore a file, any uncommitted changes you’ve made to the file will be erased, and there’s no undo.\nIf you just want to see what a file looked like in a previous commit, use git show HASH:FILE instead, where HASH is the commit’s hash and FILE is the path to the file.\n\n\nAfter running the command, take a look at README.md with your text editor. You should see that it no longer has the title. And if you look at the status of the repository, you’ll see that Git noticed the change:\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nAs with any other change, you can add and commit this change if you want to save it in the repository’s history.\n\n\n\n\n\n\nTipTip: Undoing Commits\n\n\n\n\n\nIf you want to undo an entire commit, use git revert rather than git restore. Specifically, run:\ngit revert HASH\nReplace HASH with the hash of the commit you want to revert.\nGit reverts a commit by creating a new commit, called a revert commit, with changes exactly the opposite of the original: lines that were added get removed and lines that were removed get added. Because of this, Git will prompt you for a commit message when you run git revert; it’s fine to use the default message.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Git Basics</span>"
    ]
  },
  {
    "objectID": "chapters/week02/remote-repositories.html",
    "href": "chapters/week02/remote-repositories.html",
    "title": "9  Remote Repositories",
    "section": "",
    "text": "9.1 GitHub\nAs a distributed version control system (Section 7.2), one of Git’s major features is that you can share commits between repositories. From the perspective of a repository, the repository is local and all other repositories are remote. Remote repositories, or remotes, are typically stored on some other computer connected to yours by a network (such as the Internet). In this chapter, you’ll learn how to use Git to send and receive commits from remote repositories.\nGitHub is a hosting service for Git repositories, much like Google Drive and Dropbox are hosting services for files. You don’t have to use GitHub or competing services (such as GitLab and BitBucket) in order to use Git, but doing so provides a convenient way to share, collaborate on, and back up repositories.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Remote Repositories</span>"
    ]
  },
  {
    "objectID": "chapters/week02/remote-repositories.html#sec-github",
    "href": "chapters/week02/remote-repositories.html#sec-github",
    "title": "9  Remote Repositories",
    "section": "",
    "text": "Important\n\n\n\nKeep in mind that Git and GitHub are different things. Git is a version control system, while GitHub is a hosting service built to support Git.\nWe’ll use a remote repository hosted on GitHub to demonstrate how to share commits, but all of the Git commands described will work with any remote repository.\n\n\n\n\n\n\n\n\nNoteNote: GitHub Desktop\n\n\n\n\n\nGitHub also offers an application called GitHub Desktop, which allows users to manage their local repositories with a point-and-click graphical user interface (GUI).\nUltimately, it’s a matter of preference whether you use the GUI or stick with the command line for your own projects, but it’s a good idea to first become proficient at interacting with Git via the command line. The primary reason for this is that not every computer you use will have GitHub Desktop installed—or even have graphics! Many computing servers offer command line-only access, and if you ever want to use Git repositories on these machines, you’ll need to do so without GitHub Desktop.\n\n\n\n\n9.1.1 Making an Account\nTo use GitHub, you need to make a (free) account. Go to GitHub and click “Sign Up” in the top-right corner of the page. This should take you to a form, which asks you to enter a username, email address, and password. After you’ve entered in this information (and completed a quick CAPTCHA), GitHub will make you an account. Then, the site will prompt you to complete an optional survey. Fill it out, or scroll to the bottom to skip it.\nEither way, you’ll need to then verify your email address. Go to your inbox and look for an email from GitHub. Click the “Verify email address” button. Doing so will take you to your profile, where, if you’d like, you can add a few details about yourself.\n\nYou now have a GitHub account!\n\n\n9.1.2 Connecting with SSH\nTo connect to GitHub from the command line, you must have a GitHub account and a way to authenticate, or establish your identity (prove that you are who you say you are). GitHub requires authentication as a security measure, so that individuals and teams can control who has access to their repositories.\nYou can establish your identity with an SSH key, a kind of cryptographic key. An SSH key consists of two separate key files:\n\nA public key file which can be used to encrypt data. The public key is meant to be freely shared, so that people (or servers) can encrypt data they want to securely send to you.\nA private key file which can be used to decrypt data that was encrypted with the associated public key. The private key is meant for you alone, so that only you can decrypt and use data that people send to you. Never share your private key with anyone else.\n\nSSH keys are much more secure than passwords, which is one reason why GitHub uses them for authentication. GitHub provides detailed documentation about how to create an SSH key and add the public key to your GitHub account.\n\n\n\n\n\n\nImportant\n\n\n\nWork through the following sections of the documentation to set up SSH key authentication with GitHub:\n\nChecking for existing SSH keys\nGenerating a new SSH key and adding it to the ssh-agent\nAdding a new SSH key to your GitHub account\nTesting your SSH connection\n\nDon’t skip this part—it’s necessary if you want to use GitHub and follow along with the subsequent examples.\n\n\n\n\n\n\n\n\nNoteNote: What is SSH?\n\n\n\n\n\nSSH stands for secure shell protocol, a protocol for communication between two computers. The “secure” in secure shell means that all messages sent between the computers are encrypted. This makes it practically impossible for a third party to see what’s being sent.\nGit uses SSH to connect GitHub. Git can also use SSH to connect to other servers hosting repositories.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Remote Repositories</span>"
    ]
  },
  {
    "objectID": "chapters/week02/remote-repositories.html#sec-sharing-a-repository",
    "href": "chapters/week02/remote-repositories.html#sec-sharing-a-repository",
    "title": "9  Remote Repositories",
    "section": "9.2 Sharing a Repository",
    "text": "9.2 Sharing a Repository\nWith a GitHub account and SSH key set up, you’re ready to upload your first repository to GitHub. Some reasons to put a repository on GitHub include:\n\nTo back up your work\nTo collaborate on a project with colleagues\nTo make your work available for other people to use and evaluate\n\nLet’s go through the steps to upload a repository, which we’ll share with other people in Section 9.3. To get started, open a terminal and navigate to your projects directory:\ncd ~/projects/\nWe’ll initialize a new repository to share, but you could also skip this step and follow the rest to share an existing repository. Name the new repository USERNAME_first_shared_repo, where USERNAME is your GitHub username:\ngit init USERNAME_first_shared_repo\nOpen a text editor and type out a short self-introduction and a question for people who access your repository. Pick something friendly and not too intrusive, like this:\n# README\n\nHi there, I'm Nick. My research interests include computer vision, programming\nlanguages, and data science pedagogy.\n\nWhat do you like about one of your hobbies?\nSave this in the repository as as README.md, then add and commit the changes:\ngit add README.md\ngit commit\nMake sure to write a descriptive commit message like Add a README.\nSo far so good! These steps should be familiar from Section 8.2 and Section 8.3. But now it’s time to do something new: we need to set up a repository on GitHub where we can push, or send, commits from the local repository.\nOpen a web browser and go to GitHub. Make sure you’re logged in, then click the “+” button in the upper-right corner and select the “New repository” option. You’ll be taken to a page like this:\n\nThe page asks for several details about the new repository:\n\nA name for the repository\nA short (1-2 sentence) description of what’s in the repository\nWhether the repository should be public (viewable by anyone) or private (viewable only by you and those you grant access)\nWhether the repository should be initalized with:\n\nA README file, to describe your project to others.\nA .gitignore file, to tell Git to ignore specific files or directories.\nA license, to governs the use or redistribution of your files\n\n\nFor this example, give the repository the same name as the one you just created on your computer (USERNAME_first_shared_repo, replacing USERNAME with your GitHub username). Leave the description blank and make sure the repository is public. Because you already initialized the repository locally, leave all of the initialization options unchecked. It should look something like this:\n\nOnce you’ve filled in the details, click the green “Create repository” button at the bottom of the page.\nGitHub will take you to a new page with “Quick setup” and instructions to “create a new repository on the command line” or “push an existing repository from the command line.” The page should look something like this:\n\nUnder “Quick setup,” click on the “SSH” button, so that the instructions show how to connect to GitHub with SSH. Since we already created a repository locally, we need to use the “push an existing repository from the command line” instructions.\nOpen a terminal, navigate to the repository you created earlier, and then run the commands listed on the page. In the screenshot above, these are:\ngit remote add origin git@github.com:nick-ulle/nick-ulle_first_shared_repo.git\ngit branch -M main\ngit push -u origin main\nThe first command, git remote add, will look slightly different for you, since your GitHub username is probably not nick-ulle. This command tells Git where to find the repository on GitHub, and to call it origin.\nThe second command, git branch, ensures that the default branch is called main.\nFinally, the third command, git push, pushes the contents of the local repository to the repository on GitHub (origin).\nYou should see some output like:\nEnumerating objects: 3, done.\nCounting objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 1.39 KiB | 1.39 MiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nTo github.com:nick-ulle/nick-ulle_first_shared_repo.git\n * [new branch]      main -&gt; main\nbranch 'main' set up to track 'origin/main'.\nYou only need to run these three commands the first time you upload a particular repository to GitHub. From now on, when you want to push commits from this repository to GitHub, you can simply run git push (without any arguments).\nNow go back to your web browser and refresh the repository’s page on GitHub. You should see the message in your README.md file:\n\n\nCongratulations, your repository is online! 🎉 GitHub automatically checks for a README file in your repository and if it finds one, displays it on the repository’s main page. If the README file is written in Markdown, GitHub will even render the formatting.\n\n\n\n\n\n\nNoteSee Also\n\n\n\nYou can find information about how to write effective README files in DataLab’s README, Write Me! workshop reader.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Remote Repositories</span>"
    ]
  },
  {
    "objectID": "chapters/week02/remote-repositories.html#sec-collaborating",
    "href": "chapters/week02/remote-repositories.html#sec-collaborating",
    "title": "9  Remote Repositories",
    "section": "9.3 Collaborating",
    "text": "9.3 Collaborating\nUploading a repository to GitHub so that it’s available to others, as in Section 9.2, is the first step towards collaboration. In this section, you’ll learn the next steps: how to download someone else’s repository, how to add someone as a collaborator to your repository, and how to send commits to and receive commits from a collaborator.\n\n\n\n\n\n\nImportant\n\n\n\nFor this part, you’ll need to work with a partner. Take a moment to find a partner and exchange GitHub usernames. Pay careful attention to their username’s spelling and capitalization.\nThroughout this section, we’ll refer to your partner’s username as PARTNER. Anywhere you see PARTNER, replace it with their actual username.\n\n\nIn your web browser, navigate to the main page for your partner’s GitHub repository, which will be at:\nhttps://github.com/PARTNER/PARTNER_first_shared_repo\nMake sure you can get to the page before proceeding. If you get a 404 error, ask your partner to check that you’ve spelled and capitalized their username correctly.\n\n9.3.1 Cloning a Repository\nIn order to use or contribute to a remote repository, you first need to clone—download a copy of—the repository to your computer. Let’s try it out with your partner’s repository. On their repository’s main page, click on the green “Code” button, select the “Local” tab, and select “SSH”. Then copy the listed URL to your clipboard. It will look something like this:\ngit@github.com:PARTNER/PARTNER_first_shared_repo.git\nThe URL will have your partner’s GitHub username rather than PARTNER.\nNext, open a terminal and navigate to your projects directory:\ncd ~/projects/\nThen use the git clone command to clone a copy of your partner’s repository. You’ll need to paste the URL you copied to the end of the command:\ngit clone git@github.com:PARTNER/PARTNER_first_shared_repo.git\nCloning into 'PARTNER_first_shared_repo'...\nremote: Enumerating objects: 3, done.\nremote: Counting objects: 100% (3/3), done.\nremote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0 (from 0)\nReceiving objects: 100% (3/3), done.\nGit will display some details about what it cloned from the remote repository. You now have a copy of your partner’s repository as PARTNER_first_shared_repo/ in your projects directory. Use a text editor to take a look at their repo’s README.\n\n\n9.3.2 Adding a Collaborator\nAnyone can clone a public repository, but only the owner of the repository and their approved collaborators can push commits. In this part, you’ll make your partner an approved collaborator on your shared repository, so that they can add a reply to the question in the README.\nOpen a web browser to your repository’s main page on GitHub. Click on the “Settings” button. You’ll be taken to a page that looks like this:\n\nOn the left side, click on “Collaborators”. GitHub might ask you to enter your password or complete two-factor authentication. Once you’ve done that, you’ll end up at a page like this:\n\nClick on the green “Add people” button near the bottom of the page, then enter your partner’s GitHub username in the popup that appears. Then tell your partner to check their email (the one they used to register with GitHub) for an invitation to collaborate on your repository.\nBoth you and your partner need to accept the invitations before proceeding.\n\n\n9.3.3 Pushing Commits\nIt’s time to answer the question your partner put in their repository’s README. Go back to the terminal, and navigate to cloned copy of your partner’s repository. Open README.md with a text editor and add add a short reply to the question, like this:\n# README\n\nHi, I'm Tyler. I research language technology, focusing on how methods in\nnatural language processing crosscut the interpretive and theoretic frameworks\nof literary and media studies.\n\nWhat kinds of tools do you use to edit code?\n\n## Reply\n\nHi Tyler! I mostly use Jupyter, RStudio, and Vim to edit code.\nSave the changes and exit the text editor. Use Git to add and commit the changes:\ngit add README.md\ngit commit\nGo back to Section 8.3 if you need to refresh your memory of these commands.\nYou can send the commit to your partner by pushing it to their repository on GitHub. To do this, run:\ngit push\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 1.46 KiB | 1.46 MiB/s, done.\nTotal 3 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nTo github.com:PARTNER/PARTNER_first_shared_repo.git\n   dcad700..be2054a  main -&gt; main\nAfter pushing the commit, check in with your partner. Confirm that they can see the commit you made on their repository’s GitHub page, and check that you can see the commit they made on your repository’s GitHub page. If anything doesn’t seem right, try working through the steps again.\n\n\n9.3.4 Pulling Commits\nSection 9.3.3 explained how to push commits from a local repository to a remote repository. The counterpart is pulling commits from a remote repository to a local one. When you’re working collaboratively, you can get commits your collaborators have pushed to GitHub (or any other remote) by pulling them to your local repository.\nIt’s time to pull the commit your partner made in Section 9.3.3. Open a terminal again and navigate to your repository. Then run git pull:\ngit pull\nGit should print output that looks something like this:\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nUnpacking objects: 100% (3/3), 918 bytes | 918.00 KiB/s, done.\nFrom github.com:nick-ulle/nick-ulle_first_shared_repo\n   c4bae61..260a2b4  main       -&gt; origin/main\nUpdating c4bae61..260a2b4\nFast-forward\n README.md | 1 +\n 1 file changed, 1 insertion(+)\nAfter pulling the commit, inspect the README.md file with a text editor to confirm that it now contains the message from your partner. If it does, congratulations! You’ve successfully used Git and GitHub to collaborate with someone.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Remote Repositories</span>"
    ]
  },
  {
    "objectID": "chapters/week02/the-git-workflow.html",
    "href": "chapters/week02/the-git-workflow.html",
    "title": "10  The Git Workflow",
    "section": "",
    "text": "A typical Git workflow—as illustrated in the figure above—is:\n\nPull recent changes from the remote with git pull.\nMake some changes locally.\nStage your changes with git add.\nCommit your changes with git commit.\nPush the changes to the remote with git push.\nRepeat steps 1-5 until the project is finished.\n\nThere are lots of steps in this process, so there are lots of places where it can go wrong. Pay attention to error messages and search online if you get stuck. Lots of people use Git, so your question has probably been asked and answered. 😃\n\n\n\n\n\n\nNoteSee Also\n\n\n\nPro Git is the definitive Git resource and an excellent reference to keep at hand as you begin to work with Git.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Git Workflow</span>"
    ]
  },
  {
    "objectID": "chapters/week02/intro-to-data.html",
    "href": "chapters/week02/intro-to-data.html",
    "title": "11  Introduction to Datasets",
    "section": "",
    "text": "11.1 Tabular Datasets\nThe structure of a dataset—its shape and organization—has enormous influence on how difficult it will be to analyze. When you start working with a dataset, examining its structure is one of the first things you should do. This way you can identify and correct potential problems, and prepare the dataset for the analyses you’d like to carry out.\nA tabular dataset is one that’s structured as a table, with rows and columns. This reader focuses on tabular datasets, since they’re common in practice and present the fewest programming challenges. Here’s an example of a tabular dataset:\nResearchers think about datasets in terms of two components:\nEvery dataset has features and observations, regardless of whether the dataset is tabular.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Datasets</span>"
    ]
  },
  {
    "objectID": "chapters/week02/intro-to-data.html#tabular-datasets",
    "href": "chapters/week02/intro-to-data.html#tabular-datasets",
    "title": "11  Introduction to Datasets",
    "section": "",
    "text": "Fruit\nQuantity\nPrice\n\n\n\n\napple\n32\n1.49\n\n\nbanana\n541\n0.79\n\n\npear\n10\n1.99\n\n\n\n\n\nA feature (also called a covariate) is measurement of something, usually across multiple subjects. For example, we might decide to measure the heights of everyone in the class. Each person in the class is a subject, and the height measurement is a feature. Features don’t have to be quantitative. If we also asked each person their favorite color, then favorite color would be another feature in our dataset. Features are usually, but not always, the columns in a tabular dataset.\nAn observation is a set of features measured for a single subject or at a single time. So in the preceding example, the combined height and favorite color measurement for one student is one observation. Observations are usually, but not always, the rows in a tabular dataset.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Datasets</span>"
    ]
  },
  {
    "objectID": "chapters/week02/intro-to-data.html#sec-tidy-data",
    "href": "chapters/week02/intro-to-data.html#sec-tidy-data",
    "title": "11  Introduction to Datasets",
    "section": "11.2 Tidy Datasets",
    "text": "11.2 Tidy Datasets\nIn 2014, Hadley Wickham refined and formalized the conventions for tabular datasets by introducing the concept of tidy datasets, which have a specific structure. A tabular dataset is tidy if and only if:\n\nEach observation has its own row.\nEach feature has its own column.\nEach value has its own cell.\n\nThese rules ensure that all of the values in a dataset are visually organized and are easy to access in programming languages. They’re specific enough to make tidiness a convenient standard for commands that operate on tabular datasets. They also reflect the way statisticians conventionally arrange tabular datasets.\n\n\n\n\n\n\nNote\n\n\n\nR’s Tidyverse packages (see Section 14.3) are designed from the ground up for working with tidy datasets. Tidy datasets have also been adopted as a standard in other software, including various packages for Python and Julia.\n\n\nWhen you first look at a dataset, think about what the observations are and what the features are. If the dataset comes with documentation, it may help you figure this out. Since this dataset is a tidy dataset, we already know each row is an observation and each column is a feature.\nChapter 21 gives examples of tidy and untidy data, as well as explanations of how to make untidy data tidy.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Datasets</span>"
    ]
  },
  {
    "objectID": "chapters/week02/intro-to-data.html#sec-types-of-data",
    "href": "chapters/week02/intro-to-data.html#sec-types-of-data",
    "title": "11  Introduction to Datasets",
    "section": "11.3 Types of Data",
    "text": "11.3 Types of Data\nDifferent types of data require different approaches and analysis methods. For example, if your data are numbers, you can summarize them by computing a sum or an average. On the other hand, if your data are categories, like genres of music (rock, pop, classical, …), you can’t use these summarization methods and have to do something else.\nResearchers categorize data into types based on sets of shared characteristics. This makes it easier to reason about how to transform, analyze, and present the data. Some widely-understood data types are:\n\nCategorical\n\nNominal - data separated into specific categories, with no order. For example, hair color (red, brown, blonde, …) is categorical.\nOrdinal - data separated into specific categories, with an order. For example, school level (elementary, middle, high, college) is ordinal.\n\nNumerical\n\nDiscrete - integers, or a finite set of decimal numbers with no values in between. Sometimes discrete values can also be treated as ordinal. For example, month as a number (1, 2, …, 12) is discrete.\nContinuous - decimal numbers. There are no specific categories, but there is an order. For example, height in inches is numerical.\n\n\nOf course, other types of data, like graphs (networks) and natural language (books, speech, and so on), are also possible. Categorizing data this way is useful for reasoning about which methods to apply to which data.\n\n\n\n\n\n\nTip\n\n\n\nBecause a feature measures something, we often think and talk about types of features rather than types of individual values. For example, for a feature that measures temperature in degrees Celsius, all of the data values will be numeric (and probably continuous). So we can say the feature is numeric.\nIn a tidy dataset, each feature has its own column, so it also makes sense to talk about the types of the columns.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Datasets</span>"
    ]
  },
  {
    "objectID": "chapters/week02/intro-to-data.html#sec-ca-least-terns",
    "href": "chapters/week02/intro-to-data.html#sec-ca-least-terns",
    "title": "11  Introduction to Datasets",
    "section": "11.4 Dataset: CA Least Terns",
    "text": "11.4 Dataset: CA Least Terns\nThe California least tern is a endangered subspecies of seabird that nests along the coast of California and Mexico. The California Department of Fish and Wildlife (CDFW) monitors least tern nesting sites across the state to estimate breeding pairs, fledglings, and predator activity in each annual breeding season.\n\n\n\nA California least tern. Original photo by Mark Pavelka, U.S. Fish & Wildlife Service (CC BY 2.0).\n\n\nThe CDFW publishes most of the data it collects to the California Open Data portal. The examples in this and subsequent chapters use a cleaned 2000-2023 version of the California least tern data.\n\n\n\n\n\n\nImportant\n\n\n\nClick here to view the 2000-2023 California least tern dataset.\n\n\n\n\n\n\n\n\nNoteDocumentation for 2000-2023 California Least Tern Dataset\n\n\n\n\n\nEach row in the dataset contains measurements from one year-site combination.\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nyear\nYear of the breeding season\n\n\nsite_name\nSite name\n\n\nsite_name_2013_2018\nSite name from 2013-2018\n\n\nsite_name_1988_2001\nSite name from 1988-2001\n\n\nsite_abbr\nAbbreviated site name\n\n\nregion_3\nRegion of state: S.F. Bay, Central, or Southern (includes Ventura)\n\n\nregion_4\nRegion of state: S.F. Bay, Central, Ventura, or Southern\n\n\nevent\nClimate events\n\n\nbp_min\nReported minimum breeding pairs\n\n\nbp_max\nReported maximum breeding pairs\n\n\nfl_min\nReported minimum fledges\n\n\nfl_max\nReported maximum fledges\n\n\ntotal_nests\nTotal reported nests (maximum if a range was reported)\n\n\nnonpred_eggs\nTotal non-predator-related mortalities of eggs\n\n\nnonpred_chicks\nTotal non-predator-related mortalities of chicks\n\n\nnonpred_fl\nTotal non-predator-related mortalities of fledges\n\n\nnonpred_ad\nTotal non-predator-related mortalities of adults\n\n\npred_control\nSite predator control (yes/no)\n\n\npred_eggs\nTotal predator-related mortalities of eggs\n\n\npred_chicks\nTotal predator-related mortalities of chicks\n\n\npred_fl\nTotal predator-related mortalities of fledges\n\n\npred_ad\nTotal predator-related mortalities of adults\n\n\npred_pefa\nPredation by peregrine falcons (yes/no)\n\n\npred_coy_fox\nPredation by coyotes or foxes (yes/no)\n\n\npred_meso\nPredation by other mesocarnivores: dogs, cats, skunks, opossums, raccoons, weasels, etc. (yes/no)\n\n\npred_owlspp\nPredation by owls (yes/no)\n\n\npred_corvid\nPredation by corvids: ravens or crows (yes/no)\n\n\npred_other_raptor\nPredation by raptors other than peregrine falcons and owls (yes/no)\n\n\npred_other_avian\nPredation by birds other than raptors and corvids (yes/no)\n\n\npred_misc\nPredation by other animals (yes/no)\n\n\ntotal_pefa\nTotal mortalities due to peregrine falcons\n\n\ntotal_coy_fox\nTotal mortalities due to coyotes and foxes\n\n\ntotal_meso\nTotal mortalities due to other mesocarnivores\n\n\ntotal_owlspp\nTotal mortalities due to owls\n\n\ntotal_corvid\nTotal mortalities due to ravens and crows\n\n\ntotal_other_raptor\nTotal mortalities due to other raptors\n\n\ntotal_other_avian\nTotal mortalities due to other birds\n\n\ntotal_misc\nTotal mortalities due to other animals\n\n\nfirst_observed\nDate CA least terns first observed at site\n\n\nlast_observed\nDate CA least terns last observed at site\n\n\nfirst_nest\nDate first egg observed at site\n\n\nfirst_chick\nDate first chick observed at site\n\n\nfirst_fledge\nDate first fledge observed at site\n\n\n\nThe messy source dataset (with more years and more columns) is available here.",
    "crumbs": [
      "Week 2",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Datasets</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html",
    "href": "chapters/week03/intro-to-r.html",
    "title": "12  Introduction to R",
    "section": "",
    "text": "12.1 Why R?\nR is a programming language for statistical computing and graphics. It provides a rich set of built-in tools for cleaning, exploring, modeling, and visualizing data.\nCompared to other programming languages, some of R’s particular strengths are its:\nThe main way you’ll interact with R is by writing R code or expressions. We’ll explain more soon, but first we need to install R and associated software.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html#why-r",
    "href": "chapters/week03/intro-to-r.html#why-r",
    "title": "12  Introduction to R",
    "section": "",
    "text": "NoteWhy should you use a programming language?\n\n\n\nCode you write is reproducible: you can share it with someone else, and if they run it with the same inputs, they’ll get the same results. By writing code, you create an unambiguous record of every step taken in your analysis. This is one of the major advantages of programming languages over point-and-click software like Tableau or Microsoft Excel.\nAnother advantage of writing code is that it’s often reusable. This means you can:\n\nAutomate repetitive tasks within an analysis\nRecycle code from one analysis into another\nPackage useful code for distribution to your colleagues or the general public\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term “R” can mean the R language (the code) or the R interpreter (the software which runs the code). Most of the time, the meaning is clear from the context, but we’ll be explicit in cases where the distinction is important.\n\n\n\n\nInteractive interpreter and debugger\nFocus on statistical computing\n\nMany statistical estimators, tests, and models are built-in\nStatisticians tend to implement new methods in R first\n\nOver 23,000 community-developed packages: reusable bundles of code, often accompanied by documentation, examples, or data sets\nFunctions and packages for creating high-quality data visualizations\nFlexible support for many different programming abstractions and paradigms\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo follow along with this and subsequent chapters, you’ll need a recent version of R.\nClick here to download the R installer. When the download is complete, run the installer to install (or update) R on your computer. We recommend keeping the installer’s default settings.\nIn addition to R, you’ll need a recent version of RStudio. RStudio is an integrated development environment (IDE), which means it’s a comprehensive program for writing, editing, searching, and running code. You can do all of these things without RStudio, but RStudio makes the process easier.\nClick here to download the free RStudio Desktop installer. When the download is complete, run the installer to install RStudio Deskop on your computer.\nOn Windows, you’ll also need to download and install RTools in order for some of the packages we’ll use later to work.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html#the-rstudio-interface",
    "href": "chapters/week03/intro-to-r.html#the-rstudio-interface",
    "title": "12  Introduction to R",
    "section": "12.2 The RStudio Interface",
    "text": "12.2 The RStudio Interface\nThe first time you open RStudio, you’ll see a window divided into several panes, like this:\n\n\n\nHow RStudio typically looks the first time you open it. Don’t worry if the text in the panes isn’t exactly the same on your computer: it depends on your operating system and versions of R and RStudio.\n\n\nThe console pane, on the left, is the main interface to R. If you type R code into the console and press the Enter key on your keyboard, R will run your code and return the result.\nOn the right are the environment pane and the plots pane. The environment pane shows data in your R workspace. The plots pane shows any plots you make, and also has tabs to browse your file system and to view R’s built-in help files. You’ll learn more about these gradually, but for now, focus on the console pane.\nLet’s start by using R to do some arithmetic. In the console, you’ll see that the cursor is on a line that begins with &gt;, called the prompt. You can make R compute the sum \\(2 + 2\\) by typing the code 2 + 2 after the prompt and then pressing the Enter key. Your code and the result from R should look like this:\n\nR always puts the result on a separate line (or lines) from your code. In this case, the result begins with the tag [1], which is a hint from R that the result is a vector and that this line starts with the element at position 1. You’ll learn more about vectors in Section 13.1, and eventually learn about other data types that are displayed differently. The result of the sum, 4, is displayed after the tag.\nIf you enter an incomplete expression, R will change the prompt to +, then wait for you to type the rest of the expression and press the Enter key. Here’s what it looks like if you only enter 2 +:\n\nYou can finish entering the expression, or you can cancel it by pressing the Esc key (or Ctrl-c if you’re using R without RStudio). R can only tell an expression is incomplete if it’s missing something, like the second operand in 2 +. So if you mean to enter 2 + 2 but accidentally enter 2, which is a complete expression by itself, don’t expect R to read your mind and wait for more input!",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html#r-basics",
    "href": "chapters/week03/intro-to-r.html#r-basics",
    "title": "12  Introduction to R",
    "section": "12.3 R Basics",
    "text": "12.3 R Basics\nTry out some other arithmetic in the R console. Besides + for addition, the other arithmetic operators are:\n\n- for subtraction\n* for multiplication\n/ for division\n%% for remainder division (modulo)\n^ or ** for exponentiation\n\nYou can combine these and use parentheses to make more complicated expressions, just as you would when writing a mathematical expression. When R computes a result, it follows the standard order of operations: parentheses, exponentiation, multiplication, division, addition, and finally subtraction. For example, to estimate the area of a circle with radius 3, you can write:\n\n3.14 * 3^2\n\n[1] 28.26\n\n\nYou can write R expressions with any number of spaces (including none) around the operators and R will still compute the result.\n\n\n\n\n\n\nTip\n\n\n\nUse spaces!\nAs with writing text, putting spaces in your code makes it easier for you and others to read, so it’s good to make it a habit. Put a single space on each side of most operators, after commas, and after keywords.\n\n\n\n12.3.1 Variables\nSince R is designed for mathematics and statistics, you might expect that it provides a better appoximation for \\(\\pi\\) than 3.14. R and most other programming languages allow you to create named values, or variables. R provides a built-in variable called pi for the value of \\(\\pi\\). You can display a variable’s value by entering its name in the console:\n\npi\n\n[1] 3.141593\n\n\nYou can also use variables in expressions. For instance, here’s a more precise expression for the area of a circle with radius 3:\n\npi * 3^2\n\n[1] 28.27433\n\n\nYou can create a variable with the assignment operator = by writing a name on the left-hand side and a value or expression on the right-hand side. For example, to save the area of the circle in a variable called area, you can write:\n\narea = pi * 3^2\n\n\n\n\n\n\n\nNote\n\n\n\nYou can use the arrow operator &lt;- instead of the assignment operator:\n\narea &lt;- pi * 3^2\n\nIn most cases, the two operators are interchangeable. For clarity, it’s best to choose one you like and use it consistently in all of your R code. In this reader, we use = because it’s the assignment operator in most programming languages and it’s easier to type.\n\n\nIn R, variable names can contain any combination of letters and dots (.). Names can also include numbers and underscores (_), but can’t start with a them. Spaces and other symbols are not allowed in variable names. So geese, top50.dogs, and nine_lives are valid variable names, but goose teeth, _fishes, and 9lives are not.\nThe main reason to use variables is to temporarily save results from expressions so that you can use them in other expressions. For instance, now you can use the area variable anywhere you want the area of the circle.\nNotice that when you assign a result to a variable, R doesn’t automatically display that result. If you want to see the result as well, you have to enter the variable’s name as a separate expression:\n\narea\n\n[1] 28.27433\n\n\nAnother reason to use variables is to make an expression clearer and more general. For instance, you might want to compute the area of several circles with different radii. Then the expression pi * 3^2 is too specific. Instead, you can create a variable r, then rewrite the expression as pi * r^2. This makes the expression easier to understand, because the reader doesn’t have to intuit that 3 is the radius in the formula. Here’s the code to compute and display the area of a circle with radius 1 this way:\n\nr = 1\narea = pi * r^2\narea\n\n[1] 3.141593\n\n\nNow if you want to compute the area for a different radius, all you have to do is change r and run the code again (R will not change area until you do this). Writing code that’s general enough to reuse across multiple problems can be a big time-saver in the long run. Later on, you’ll learn ways to make this code even easier to reuse.\n\n\n\n\n\n\nTip\n\n\n\nTry to choose descriptive variable names, so that you and your collaborators can understand the meaning and purpose of each variable when reading the code.\n\n\n\n\n12.3.2 Strings\nR treats anything inside single or double quotes as literal text rather than as an expression to evaluate. In programming jargon, a piece of literal text is called a string. You can use whichever kind of quotes you prefer, but the quote at the beginning of the string must match the quote at the end.\n\n'Hi'\n\n[1] \"Hi\"\n\n\"Hello!\"\n\n[1] \"Hello!\"\n\n\nNumbers and strings are not the same thing, so for example R considers 1 different from \"1\". As a result, you can’t use strings with most of R’s arithmetic operators. For instance, this code causes an error:\n\n\"1\" + 3\n\nError in \"1\" + 3: non-numeric argument to binary operator\n\n\nThe error message notes that + is not defined for non-numeric values.\n\n\n12.3.3 Comparisons\nBesides arithmetic, you can also use R to compare values. Programming tasks often involve comparing values. Use comparison operators to do so:\n\n&lt; for “less than”\n&gt; for “greater than”\n&lt;= for “less than or equal to”\n&gt;= for “greater than or equal to”\n== for “equal to”\n!= for “not equal to”\n\nNotice that the “equal to” operator is two equal signs. This is to distinguish it from the assignment operator =.\nLet’s look at a few examples:\n\n1.5 &lt; 3\n\n[1] TRUE\n\n\"a\" &gt; \"b\"\n\n[1] FALSE\n\npi == 3.14\n\n[1] FALSE\n\n\"hi\" == 'hi'\n\n[1] TRUE\n\n\nWhen you make a comparison, R returns a logical value, TRUE or FALSE, to indicate the result. Logical values are not the same as strings, so they are not quoted.\nLogical values are values, so you can use them in other computations. For example:\n\nTRUE\n\n[1] TRUE\n\nTRUE == FALSE\n\n[1] FALSE\n\n\nSection 15.4 describes more ways to use and combine logical values.\n\n\n\n\n\n\nWarning\n\n\n\nSome of R’s equality operators return TRUE even when comparing two different types of data:\n\n\"1\" == 1\n\n[1] TRUE\n\n\"TRUE\" &lt;= TRUE\n\n[1] TRUE\n\n\"FALSE\" &lt;= TRUE\n\n[1] TRUE\n\n\nSection 13.2.2 explains why this happens. \n\n\n\n\n12.3.4 Calling Functions\nR can do a lot more than just arithmetic. Most of R’s features are provided through functions, pieces of reusable code. You can think of a function as a machine that takes some inputs and uses them to produce some output. In programming jargon, the inputs to a function are called arguments, the output is called the return value, and when you use a function, you’re calling the function.\nTo call a function, write its name followed by parentheses. Put any arguments to the function inside the parentheses. For example, the function to round a number to a specified decimal place is named round. So you can round the number 8.153 to the nearest integer with this code:\n\nround(8.153)\n\n[1] 8\n\n\nMany functions accept more than one argument. For instance, the round function accepts at least two arguments: the number to round and the number of decimal places to keep. When you call a function with multiple arguments, separate the arguments with commas. So to round 8.153 to 1 decimal place:\n\nround(8.153, 1)\n\n[1] 8.2\n\n\nWhen you call a function, R assigns the arguments to the function’s parameter. Parameters are special variables that represent the inputs to a function and only exist while that function runs. For example, the round function has parameters x and digits. The next section, Section 12.4, explains how to look up the parameters of a function.\nSome parameters have default arguments. A parameter is automatically assigned its default argument whenever the parameter’s argument is not specified explicitly. As a result, assigning arguments to these parameters is optional. For instance, the digits parameter of round has a default argument (round to the nearest integer), so it’s okay to call round without setting digits, as in round(8.153). In contrast, the x parameter doesn’t have a default argument. Section 12.4 explains how to look up the default arguments for a function.\nBy default, R assigns arguments to parameters based on their position. The first argument is assigned to the function’s first parameter, the second to the second, and so on. So in the code above, 8.153 is assigned to x and 1 is assigned to digits.\nYou can also assign arguments to parameters by name with = (not &lt;-), overriding their positions. So some other ways you could write the call above are:\n\nround(8.153, digits = 1)\n\n[1] 8.2\n\nround(x = 8.153, digits = 1)\n\n[1] 8.2\n\nround(digits = 1, x = 8.153)\n\n[1] 8.2\n\n\nAll of these are equivalent. When you write code, choose whatever seems the clearest to you. Leaving parameter names out of calls saves typing, but including some or all of them can make the code easier to understand.\nParameters are not regular variables, and only exist while their associated function runs. You can’t set them before a call, nor can you access them after a call. So this code causes an error:\n\nx = 4.755\nround(digits = 2)\n\nError in round(digits = 2): argument \"x\" is missing, with no default\n\n\nIn the error message, R says that you forgot to assign an argument to the parameter x. You can keep the variable x and correct the call by making x an argument (for the parameter x):\n\nround(x, digits = 2)\n\n[1] 4.76\n\n\nOr, written more explicitly:\n\nround(x = x, digits = 2)\n\n[1] 4.76\n\n\nThe point is that variables and parameters are distinct, even if they happen to have the same name. The variable x is not the same thing as the parameter x.\n\n\n12.3.5 Comments\nIn R and most other programming languages, you can mark parts of your code as comments: expressions to ignore rather than run. Use comments to plan, explain, and document your code. You can also temporarily “comment out” code to prevent it from running, which is often helpful for testing and debugging.\nR comments begin with number sign # and extend to the end of the line:\n\n# This is a comment.\n\nR will ignore comments when you run your code.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html#sec-getting-help",
    "href": "chapters/week03/intro-to-r.html#sec-getting-help",
    "title": "12  Introduction to R",
    "section": "12.4 Getting Help",
    "text": "12.4 Getting Help\nLearning and using a language is hard, so it’s important to know how to get help. The first place to look for help is R’s built-in documentation. In the console, you can access a specific help page by name with ? followed by the name of the page.\nThere are help pages for all of R’s built-in functions, usually with the same name as the function itself. So the code to open the help page for the round function is:\n\n?round\n\nFor functions, help pages usually include a brief description, a list of parameters, a description of the return value, and some examples. The help page for round shows that there are two parameters x and digits. It also says that digits = 0, meaning the default argument for digits is 0.\nThere are also help pages for other topics, such as built-in mathematical constants (such as ?pi), data sets (such as ?iris), and operators. To look up the help page for an operator, put the operator’s name in single or double quotes. For example, this code opens the help page for the arithmetic operators:\n\n?\"+\"\n\nIt’s always okay to put quotes around the name of the page when you use ?, but they’re only required if it contains non-alphabetic characters. So ?sqrt, ?'sqrt', and ?\"sqrt\" all open the documentation for sqrt, the square root function.\nSometimes you might not know the name of the help page you want to look up. You can do a general search of R’s help pages with ?? followed by a string of search terms. For example, to get a list of all help pages related to linear models:\n\n??\"linear model\"\n\nThis search function doesn’t always work well, and it’s often more efficient to use an online search engine. When you search for help with R online, include “R” as a search term. Alternatively, you can use RSeek, which restricts the search to a selection of R-related websites.\n\n12.4.1 When Something Goes Wrong\nAs a programmer, sooner or later you’ll run some code and get an error message or result you didn’t expect. Don’t panic! Even experienced programmers make mistakes regularly, so learning how to diagnose and fix problems is vital.\nTry going through these steps:\n\nIf R returned a warning or error message, read it! If you’re not sure what the message means, try searching for it online.\nCheck your code for typographical errors, including incorrect capitalization and missing or extra commas, quotes, and parentheses.\nTest your code one line at a time, starting from the beginning. After each line that assigns a variable, check that the value of the variable is what you expect. Try to determine the exact line where the problem originates (which may differ from the line that emits an error!).\n\nIf none of these steps help, try asking online. Stack Overflow is a popular question and answer website for programmers. Before posting, make sure to read about how to ask a good question.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html#the-working-directory-revisited",
    "href": "chapters/week03/intro-to-r.html#the-working-directory-revisited",
    "title": "12  Introduction to R",
    "section": "12.5 The Working Directory Revisited",
    "text": "12.5 The Working Directory Revisited\n\nThis section is a review of how files on a computer work. You’ll need to understand this in order to read a data set from a file, and it’s also important for finding your saved notebooks and modules later.\nSection 3.2 explained that relative paths have a starting point that depends on the context where the path is used. The working directory is the starting point R uses for relative paths. Think of the working directory as the directory R is currently “at” or watching.\nThe function getwd returns the absolute path for the current working directory, as a string. It doesn’t require any arguments:\n\ngetwd()\n\n[1] \"/home/nick/mill/datalab/teaching/adventures_in_data_science\"\n\n\nOn your computer, the output from getwd will likely be different. This is a very useful function for getting your bearings when you write relative paths. If you write a relative path and it doesn’t work as expected, the first thing to do is check the working directory.\nThe related setwd function changes the working directory. It takes one argument: a path to the new working directory. Here’s an example:\n\nsetwd(\"..\")\n\n# Now check the working directory.\ngetwd()\n\n\n\n\n\n\n\nWarning\n\n\n\nIn your R scripts and notebooks, avoid calls to setwd. They make your code more difficult to understand and to run on other computers. Use appropriate relative paths instead.\nIn the R console, it’s okay to occasionally use setwd. You might need to change the working directory before you run some code. R’s default working directory is your home directory. In some cases, such as when you open a project, RStudio will automatically change the working directory. However, it doesn’t always change the working directory, so setwd is sometimes still necessary.\n\n\nAnother function that’s useful for dealing with the working directory and file system is list.files. The list.files function returns the names of all of the files and directories inside of a directory. It accepts a path to a directory as an argument, or assumes the working directory if you don’t pass a path. For instance:\n\n# List files and directories in /home/.\nlist.files(\"/home/\")\n\n[1] \"lost+found\" \"nick\"      \n\n# List files and directories in the working directory.\nlist.files()\n\n [1] \"_build\"          \"_freeze\"         \"_quarto.yml\"     \"chapters\"       \n [5] \"CONTRIBUTING.md\" \"data\"            \"html_cache\"      \"images\"         \n [9] \"img\"             \"index.html\"      \"index.qmd\"       \"LICENSE\"        \n[13] \"pixi.lock\"       \"pixi.toml\"       \"R\"               \"README.md\"      \n[17] \"references.bib\"  \"sandbox\"         \"site_libs\"      \n\n\nAs usual, since you have a different computer, you’re likely to see different output if you run this code. If you call list.files with an invalid path or an empty directory, the output is character(0):\n\nlist.files(\"/this/path/is/fake/\")\n\ncharacter(0)\n\n\nLater on, you’ll learn about what character(0) means more generally.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html#saving-loading-code",
    "href": "chapters/week03/intro-to-r.html#saving-loading-code",
    "title": "12  Introduction to R",
    "section": "12.6 Saving & Loading Code",
    "text": "12.6 Saving & Loading Code\n\n\n\n\n\n\nTip\n\n\n\nWhen you start a new project, it’s a good idea to create a specific directory for all of the project’s files. If you’re using R, you should also store your R code in that directory. As you work, periodically save your code.\n\n\nMost of the time, you won’t just write code directly into the R console. Reproducibility and reusability are important benefits of R over point-and-click software, and in order to realize these, you have to save your code to your computer’s hard drive.\nThe most common way to save R code is as an R script with the extension .R (see Section 12.7 for more about extensions). Editing a script is similar to editing any other text file. You can write, delete, copy, cut, and paste code.\nIn RStudio, you can create a new R script with this menu option:\nFile -&gt; New File -&gt; R Script\nThis will open a new pane in RStudio, like this:\n\n\n\nHow RStudio typically looks after opening a new R Script.\n\n\nThe new pane is the scripts pane, which displays all of the R scripts you’re editing. Each script appears in a separate tab. In the screenshot, only one script, the new script, is open.\nEvery line in an R script must be valid R code. Anything else you want to write in the script (notes, documentation, etc.) must be placed in a comment.\nArrange your code in the order of the steps to solve the problem, even if you write some parts before others. Comment out or delete any lines of code that you try but ultimately decide you don’t need. Make sure to save the file periodically so that you don’t lose your work. Following these guidelines will help you stay organized and make it easier to share your code with others later.\n\n\n\n\n\n\nTip\n\n\n\nWhile editing, you can run the current line in the R console by pressing Ctrl-Enter on Windows and Linux, or Cmd-Enter on macOS. This way you can test and correct your code as you write it.\n\n\n\n12.6.1 Running Scripts\nYou can source (that is, run) an entire R script by calling the source function with the path to the script as an argument. This is also what the “Source on Save” check box refers to in RStudio. The code runs in order, only stopping if an error occurs.\nFor instance, if you save the script as my_cool_script.R, then you can enter source(\"my_cool_script.R\") in the console to run the entire script. Pay attention to the path—it may be different on your computer.\n\n\n12.6.2 Notebooks: Quarto & R Markdown\nIn the context of data science, a notebook is an interactive file that can store a mix of code, formatted text, and images. With a notebook, you can write, run code, and view results all in one place. Viewing and editing a notebook requires a web browser or IDE. Some notebooks can also be converted to static documents, such as PDFs. Comments are a good way keep notes as you develop and run your code, but notebooks provide much more flexibility of expression. Notebooks are a kind of literate programming.\nNotebooks excel when you want to do highly interactive work and/or want to communicate results. Use notebooks to prototype code, analyze data, refine plots, generate documents and presentations, and practice programming. Scripts excel when you want to reuse code (and perhaps share it as a package) or want to run code that doesn’t require much user interaction (such as time-consuming computations you’ll run on a server or high-performance computing cluster). The remainder of this reader assumes you’re using an R script rather than the R console or a notebook, unless otherwise noted.\nQuarto is a popular notebook format and system for R. It also supports Python, Julia, and other programming languages. Quarto files have the extension .qmd. Quarto is based on an older notebook format, R Markdown, which only supports R and can’t be converted to as many kinds of documents. R Markdown is still widely used. R Markdown files have the extension .Rmd.\n\n\n\n\n\n\nImportant\n\n\n\nIn order to use Quarto, you must first download and install it. It is not included with R or RStudio.\n\n\nAfter installing Quarto, you can create a new Quarto file in RStudio with this menu option:\nFile -&gt; New -&gt; Quarto Document...\nRStudio will prompt you to provide some details about the purpose of the file.\nNotebooks are subdivided into chunks (or cells). You can create as many chunks as you like, but each chunk can contain only one kind of content. You can run a code chunk by clicking on the chunk and pressing Ctrl-Enter. The notebook will display the result.\nMarkdown is a simple language you can use to add formatting to (non-code) text in a notebook. For example, surrounding a word with asterisks, as in Let *sleeping* dogs lie, makes the surrounded word italic. You can find a short, interactive tutorial about Markdown here.\n\n\n\n\n\n\nNoteSee Also\n\n\n\nTo learn more about Quarto, see the official documentation.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html#sec-reading-files",
    "href": "chapters/week03/intro-to-r.html#sec-reading-files",
    "title": "12  Introduction to R",
    "section": "12.7 Reading Files",
    "text": "12.7 Reading Files\nAnalyzing data sets is one of the most common things to do in R. The first step is to get R to read your data. Data sets come in a variety of file formats, and you need to identify the format in order to tell R how to read the data.\nMost of the time, you can guess the format of a file by looking at its extension, the characters (usually three) after the last dot . in the filename. For example, the extension .jpg or .jpeg indicates a JPEG image file. Some operating systems hide extensions by default, but you can find instructions to change this setting online by searching for “show file extensions” and your operating system’s name. The extension is just part of the file’s name, so it should be taken as a hint about the file’s format rather than a guarantee.\nR has built-in functions for reading a variety of formats. The R community also provides packages, shareable and reusable pieces of code, to read even more formats. You’ll learn more about packages later, in Chapter 14. For now, let’s focus on data sets that can be read with R’s built-in functions.\nHere are several formats that are frequently used to distribute data, along with the name of a built-in function or contributed package that can read the format:\n\n\n\nName\nExtension\nFunction or Package\nTabular?\nText?\n\n\n\n\nComma-separated Values\n.csv\nread.csv\nYes\nYes\n\n\nTab-separated Values\n.tsv\nread.delim\nYes\nYes\n\n\nFixed-width File\n.fwf\nread.fwf\nYes\nYes\n\n\nMicrosoft Excel\n.xlsx\nreadr package\nYes\nNo\n\n\nMicrosoft Excel 1993-2007\n.xls\nreadr package\nYes\nNo\n\n\nApache Arrow\n.feather\narrow package\nYes\nNo\n\n\nR Data\n.rds\nreadRDS\nSometimes\nNo\n\n\nR Data\n.rda\nload\nSometimes\nNo\n\n\nPlaintext\n.txt\nreadLines\nSometimes\nYes\n\n\nExtensible Markup Language\n.xml\nxml2 package\nNo\nYes\n\n\nJavaScript Object Notation\n.json\njsonlite package\nNo\nYes\n\n\n\nA tabular data set is one that’s structured as a table, with rows and columns. This reader focuses on tabular data sets, since they’re common in practice and present the fewest programming challenges. Here’s an example of a tabular data set:\n\n\n\nFruit\nQuantity\nPrice\n\n\n\n\napple\n32\n1.49\n\n\nbanana\n541\n0.79\n\n\npear\n10\n1.99\n\n\n\nA text file is a file that contains human-readable lines of text. You can check this by opening the file with a text editor such as Microsoft Notepad or macOS TextEdit. Many file formats use text in order to make the format easier to work with.\nFor instance, a comma-separated values (CSV) file records a tabular data using one line per row, with commas separating columns. If you store the table above in a CSV file and open the file in a text editor, here’s what you’ll see:\nFruit,Quantity,Price\napple,32,1.49\nbanana,541,0.79\npear,10,1.99\nA binary file is one that’s not human-readable. You can’t just read off the data if you open a binary file in a text editor, but they have a number of other advantages. Compared to text files, binary files are often faster to read and take up less storage space (bytes).\nAs an example, R’s built-in binary format is called RDS (which may stand for “R data serialized”). RDS files are extremely useful for backing up work, since they can store any kind of R object, even ones that are not tabular. You can learn more about how to create an RDS file on the ?saveRDS help page, and how to read one on the ?readRDS help page.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html#example-ca-least-terns",
    "href": "chapters/week03/intro-to-r.html#example-ca-least-terns",
    "title": "12  Introduction to R",
    "section": "12.8 Example: CA Least Terns",
    "text": "12.8 Example: CA Least Terns\nLet’s use R to read the California least tern data set introduced in Section 11.4.\n\n\n\n\n\n\nImportant\n\n\n\nClick here to download the 2000-2023 California least tern data set.\nIf you haven’t already, we recommend you create a directory for this workshop. In your workshop directory, create a data/ subdirectory. Download and save the California least tern data set in the data/ subdirectory.\n\n\nThe data set is in a file called is 2000-2023_ca_least_tern.csv, which suggests it’s a CSV file. The function to read a CSV file is read.csv. The function’s first and only required argument is the path to the CSV file.\nIn the following code, the path to the California least tern data set is data/2000-2023_ca_least_tern.csv, but it might be different for you, depending on R’s working directory and where you saved the file. We’ll save the result from the read.csv function in a variable called terns. We can use this variable to access the data in subsequent code.\n\nterns = read.csv(\"data/2000-2023_ca_least_tern.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nThe variable name terns is arbitrary; you can choose something different if you want. However, in general, it’s a good habit to choose variable names that describe the contents of the variable somehow.\n\n\nIf you tried running the line of code above and got an error message, pay attention to what the error message says, and remember the strategies to get help from Section 12.4. The most common mistake when reading a file is incorrectly specifying the path, so first check that you got the path right.\nIf the code ran without errors, it’s a good idea to check that the data set looks like what the documentation describes. When working with a new data set, it usually isn’t a good idea to print the whole thing (at least until you know how big it is). Large data sets can take a long time to print, and the output can be difficult to read.\nInstead, use the head function to print only the beginning, or head, of the data set:\n\nhead(terns)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nsite_name\nsite_name_2013_2018\nsite_name_1988_2001\nsite_abbr\nregion_3\nregion_4\nevent\nbp_min\nbp_max\nfl_min\nfl_max\ntotal_nests\nnonpred_eggs\nnonpred_chicks\nnonpred_fl\nnonpred_ad\npred_control\npred_eggs\npred_chicks\npred_fl\npred_ad\npred_pefa\npred_coy_fox\npred_meso\npred_owlspp\npred_corvid\npred_other_raptor\npred_other_avian\npred_misc\ntotal_pefa\ntotal_coy_fox\ntotal_meso\ntotal_owlspp\ntotal_corvid\ntotal_other_raptor\ntotal_other_avian\ntotal_misc\nfirst_observed\nlast_observed\nfirst_nest\nfirst_chick\nfirst_fledge\n\n\n\n\n2000\nPITTSBURG POWER PLANT\nPittsburg Power Plant\nNA_2013_2018 POLYGON\nPITT_POWER\nS.F._BAY\nS.F._BAY\nLA_NINA\n15\n15\n16\n18\n15\n3\n0\n0\n0\n\n4\n2\n0\n0\nN\nN\nN\nN\nY\nY\nN\nN\n0\n0\n0\n0\n4\n2\n0\n0\n2000-05-11\n2000-08-05\n2000-05-26\n2000-06-18\n2000-07-08\n\n\n2000\nALBANY CENTRAL AVE\nNA_NO POLYGON\nAlbany Central Avenue\nAL_CENTAVE\nS.F._BAY\nS.F._BAY\nLA_NINA\n6\n12\n1\n1\n20\nNA\nNA\nNA\nNA\n\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n2000\nALAMEDA POINT\nAlameda Point\nNA_2013_2018 POLYGON\nALAM_PT\nS.F._BAY\nS.F._BAY\nLA_NINA\n282\n301\n200\n230\n312\n124\n81\n2\n1\n\n17\n0\n0\n0\nN\nN\nN\nN\nN\nY\nY\nN\n0\n0\n0\n0\n0\n6\n11\n0\n2000-05-01\n2000-08-19\n2000-05-16\n2000-06-07\n2000-06-30\n\n\n2000\nKETTLEMAN CITY\nKettleman\nNA_2013_2018 POLYGON\nKET_CTY\nKINGS\nKINGS\nLA_NINA\n2\n3\n1\n2\n3\nNA\n3\n1\n6\n\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2000-06-10\n2000-09-24\n2000-06-17\n2000-07-22\n2000-08-06\n\n\n2000\nOCEANO DUNES STATE VEHICULAR RECREATION AREA\nOceano Dunes State Vehicular Recreation Area\nNA_2013_2018 POLYGON\nOCEANO_DUNES\nCENTRAL\nCENTRAL\nLA_NINA\n4\n5\n4\n4\n5\n2\n0\n0\n0\n\n0\n4\n0\n0\nN\nN\nN\nN\nN\nN\nY\nN\n0\n0\n0\n0\n0\n0\n4\n0\n2000-05-04\n2000-08-30\n2000-05-28\n2000-06-20\n2000-07-13\n\n\n2000\nRANCHO GUADALUPE DUNES PRESERVE\nRancho Guadalupe Dunes Preserve\nNA_2013_2018 POLYGON\nRGDP\nCENTRAL\nCENTRAL\nLA_NINA\n9\n9\n17\n17\n9\n0\n1\n0\n0\n\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2000-05-07\n2000-08-13\n2000-05-31\n2000-06-22\n2000-07-20\n\n\n\n\n\nIf you run this code and see a similar table, then congratulations, you’ve read your first data set into R! ✨\nThe California least terns data set is tabular—as you might have already guessed, since it came from a CSV file. In R, it’s represented by a data frame, a table with rows and columns. R uses data frames to represent most (but not all) kinds of tabular data. The read.csv function, which you used to read this data, always returns a data frame.\nTypically, each row in a data frame corresponds to a single subject and is called an observation. Each column corresponds to a measurement of the subject and is called a feature or covariate.\n\n\n\n\n\n\nNote\n\n\n\nSometimes people also refer to columns as “variables,” but we’ll try to avoid this, because in programming contexts a variable is a name for a value (which might not be a column).\n\n\nWhen you first read an object into R, you might not know whether it’s a data frame. One way to check is visually, by printing it (as you just did with head). A better way to check is with the class function, which returns information about what an object is. For a data frame, the result will always contain data.frame:\n\nclass(terns)\n\n[1] \"data.frame\"\n\n\nYou’ll learn more about classes in Section 13.2, but for now you can use this function to identify data frames.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/intro-to-r.html#sec-inspecting-a-data-frame",
    "href": "chapters/week03/intro-to-r.html#sec-inspecting-a-data-frame",
    "title": "12  Introduction to R",
    "section": "12.9 Inspecting a Data Frame",
    "text": "12.9 Inspecting a Data Frame\nSimilar to how the head function shows the first six rows of a data frame, the tail function shows the last six:\n\ntail(terns)\n\n    year                                site_name         site_name_2013_2018\n786 2023                         NAS NORTH ISLAND         Naval Base Coronado\n787 2023           NAVAL AMPHIBIOUS BASE CORONADO         Naval Base Coronado\n788 2023        DSTREET FILL SWEETWATER MARSH NWR               D Street Fill\n789 2023             CHULA VISTA WILDLIFE RESERVE Chula Vista Wildlife Refuge\n790 2023 SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS                   Saltworks\n791 2023                     TIJUANA ESTUARY NERR             Tijuana Estuary\n     site_name_1988_2001 site_abbr region_3 region_4   event bp_min bp_max\n786 NA_2013_2018 POLYGON     NASNI SOUTHERN SOUTHERN LA_NINA      0      0\n787 NA_2013_2018 POLYGON       NAB SOUTHERN SOUTHERN LA_NINA    596    644\n788 NA_2013_2018 POLYGON      D_ST SOUTHERN SOUTHERN LA_NINA     29     38\n789 NA_2013_2018 POLYGON        CV SOUTHERN SOUTHERN LA_NINA     47     54\n790 NA_2013_2018 POLYGON      SALT SOUTHERN SOUTHERN LA_NINA     38     41\n791 NA_2013_2018 POLYGON    TJ_RIV SOUTHERN SOUTHERN LA_NINA    144    165\n    fl_min fl_max total_nests nonpred_eggs nonpred_chicks nonpred_fl nonpred_ad\n786      0      0           0            0              0          0          0\n787     90    128         717          329            185          6          6\n788      4      4          44           25              2          0          0\n789      5      6          59           32              1          0          0\n790      7      7          48           11              2          0          0\n791     35     35         171           65             44          1          1\n    pred_control pred_eggs pred_chicks pred_fl pred_ad pred_pefa pred_coy_fox\n786            Y        NA          NA      NA      NA         N            N\n787            Y        NA          NA      NA      NA         N            N\n788            Y        NA          NA      NA      NA         Y            N\n789            Y        NA          NA      NA      NA         Y            N\n790            Y        NA          NA      NA      NA         Y            Y\n791            Y        NA          NA      NA      NA         N            N\n    pred_meso pred_owlspp pred_corvid pred_other_raptor pred_other_avian\n786         N           N           N                 N                N\n787         N           N           Y                 N                Y\n788         N           N           N                 Y                Y\n789         N           N           N                 N                N\n790         N           N           N                 Y                N\n791         N           N           N                 N                Y\n    pred_misc total_pefa total_coy_fox total_meso total_owlspp total_corvid\n786         N         NA            NA         NA           NA           NA\n787         Y         NA            NA         NA           NA           NA\n788         Y         NA            NA         NA           NA           NA\n789         Y         NA            NA         NA           NA           NA\n790         Y         NA            NA         NA           NA           NA\n791         Y         NA            NA         NA           NA           NA\n    total_other_raptor total_other_avian total_misc first_observed\n786                 NA                NA         NA               \n787                 NA                NA         NA     2023-04-22\n788                 NA                NA         NA     2023-04-20\n789                 NA                NA         NA     2023-04-20\n790                 NA                NA         NA     2023-04-24\n791                 NA                NA         NA     2023-04-26\n    last_observed first_nest first_chick first_fledge\n786                                                  \n787    2023-09-09 2023-05-07  2023-05-31             \n788    2023-08-24 2023-05-12  2023-06-05             \n789    2023-09-22 2023-05-14  2023-06-05             \n790    2023-09-22 2023-05-19  2023-06-09             \n791    2023-08-28 2023-05-12  2023-06-10             \n\n\nIf there are lots of columns or the columns are wide, as is the case here, R wraps the output across lines.\n\n\n\n\n\n\nTip\n\n\n\nBoth head and tail accept an optional second argument that specifies the number of rows to print:\n\nhead(terns, 1)\n\n  year             site_name   site_name_2013_2018  site_name_1988_2001\n1 2000 PITTSBURG POWER PLANT Pittsburg Power Plant NA_2013_2018 POLYGON\n   site_abbr region_3 region_4   event bp_min bp_max fl_min fl_max total_nests\n1 PITT_POWER S.F._BAY S.F._BAY LA_NINA     15     15     16     18          15\n  nonpred_eggs nonpred_chicks nonpred_fl nonpred_ad pred_control pred_eggs\n1            3              0          0          0                      4\n  pred_chicks pred_fl pred_ad pred_pefa pred_coy_fox pred_meso pred_owlspp\n1           2       0       0         N            N         N           N\n  pred_corvid pred_other_raptor pred_other_avian pred_misc total_pefa\n1           Y                 Y                N         N          0\n  total_coy_fox total_meso total_owlspp total_corvid total_other_raptor\n1             0          0            0            4                  2\n  total_other_avian total_misc first_observed last_observed first_nest\n1                 0          0     2000-05-11    2000-08-05 2000-05-26\n  first_chick first_fledge\n1  2000-06-18   2000-07-08\n\n\n\n\nOne way to get a quick idea of what your data looks like without having to skim through all the columns and rows is by inspecting its dimensions. This is the number of rows and columns in a data frame, and you can access this information with the dim function:\n\ndim(terns)\n\n[1] 791  43\n\n\nSo this data set has 791 rows and 43 columns. As an alternative to the dim function, you can use the nrow and ncol functions to get just the number of rows and number of columns, respectively.\nSince the columns have names, you might also want to get just these. You can do that with the names or colnames functions. Both return the same result:\n\nnames(terns)\n\n [1] \"year\"                \"site_name\"           \"site_name_2013_2018\"\n [4] \"site_name_1988_2001\" \"site_abbr\"           \"region_3\"           \n [7] \"region_4\"            \"event\"               \"bp_min\"             \n[10] \"bp_max\"              \"fl_min\"              \"fl_max\"             \n[13] \"total_nests\"         \"nonpred_eggs\"        \"nonpred_chicks\"     \n[16] \"nonpred_fl\"          \"nonpred_ad\"          \"pred_control\"       \n[19] \"pred_eggs\"           \"pred_chicks\"         \"pred_fl\"            \n[22] \"pred_ad\"             \"pred_pefa\"           \"pred_coy_fox\"       \n[25] \"pred_meso\"           \"pred_owlspp\"         \"pred_corvid\"        \n[28] \"pred_other_raptor\"   \"pred_other_avian\"    \"pred_misc\"          \n[31] \"total_pefa\"          \"total_coy_fox\"       \"total_meso\"         \n[34] \"total_owlspp\"        \"total_corvid\"        \"total_other_raptor\" \n[37] \"total_other_avian\"   \"total_misc\"          \"first_observed\"     \n[40] \"last_observed\"       \"first_nest\"          \"first_chick\"        \n[43] \"first_fledge\"       \n\ncolnames(terns)\n\n [1] \"year\"                \"site_name\"           \"site_name_2013_2018\"\n [4] \"site_name_1988_2001\" \"site_abbr\"           \"region_3\"           \n [7] \"region_4\"            \"event\"               \"bp_min\"             \n[10] \"bp_max\"              \"fl_min\"              \"fl_max\"             \n[13] \"total_nests\"         \"nonpred_eggs\"        \"nonpred_chicks\"     \n[16] \"nonpred_fl\"          \"nonpred_ad\"          \"pred_control\"       \n[19] \"pred_eggs\"           \"pred_chicks\"         \"pred_fl\"            \n[22] \"pred_ad\"             \"pred_pefa\"           \"pred_coy_fox\"       \n[25] \"pred_meso\"           \"pred_owlspp\"         \"pred_corvid\"        \n[28] \"pred_other_raptor\"   \"pred_other_avian\"    \"pred_misc\"          \n[31] \"total_pefa\"          \"total_coy_fox\"       \"total_meso\"         \n[34] \"total_owlspp\"        \"total_corvid\"        \"total_other_raptor\" \n[37] \"total_other_avian\"   \"total_misc\"          \"first_observed\"     \n[40] \"last_observed\"       \"first_nest\"          \"first_chick\"        \n[43] \"first_fledge\"       \n\n\nIf the rows have names, you can get those with the rownames function. For this particular data set, the rows don’t have names.\n\n12.9.1 Summarizing Data\nAn efficient way to get a sense of what’s actually in a data set is to have R compute summary information. This works especially well for data frames, but also applies to other data. R provides two different functions to get summaries: str and summary.\nThe str function returns a structural summary of an object. This kind of summary tells us about the structure of the data—the number of rows, the number and names of columns, what kind of data is in each column, and some sample values. Here’s the structural summary for the least terns data set:\n\nstr(terns)\n\n'data.frame':   791 obs. of  43 variables:\n $ year               : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ site_name          : chr  \"PITTSBURG POWER PLANT\" \"ALBANY CENTRAL AVE\" \"ALAMEDA POINT\" \"KETTLEMAN CITY\" ...\n $ site_name_2013_2018: chr  \"Pittsburg Power Plant\" \"NA_NO POLYGON\" \"Alameda Point\" \"Kettleman\" ...\n $ site_name_1988_2001: chr  \"NA_2013_2018 POLYGON\" \"Albany Central Avenue\" \"NA_2013_2018 POLYGON\" \"NA_2013_2018 POLYGON\" ...\n $ site_abbr          : chr  \"PITT_POWER\" \"AL_CENTAVE\" \"ALAM_PT\" \"KET_CTY\" ...\n $ region_3           : chr  \"S.F._BAY\" \"S.F._BAY\" \"S.F._BAY\" \"KINGS\" ...\n $ region_4           : chr  \"S.F._BAY\" \"S.F._BAY\" \"S.F._BAY\" \"KINGS\" ...\n $ event              : chr  \"LA_NINA\" \"LA_NINA\" \"LA_NINA\" \"LA_NINA\" ...\n $ bp_min             : num  15 6 282 2 4 9 30 21 73 166 ...\n $ bp_max             : num  15 12 301 3 5 9 32 21 73 167 ...\n $ fl_min             : int  16 1 200 1 4 17 11 9 60 64 ...\n $ fl_max             : int  18 1 230 2 4 17 11 9 65 64 ...\n $ total_nests        : int  15 20 312 3 5 9 32 22 73 252 ...\n $ nonpred_eggs       : int  3 NA 124 NA 2 0 NA 4 2 NA ...\n $ nonpred_chicks     : int  0 NA 81 3 0 1 27 3 0 NA ...\n $ nonpred_fl         : int  0 NA 2 1 0 0 0 NA 0 NA ...\n $ nonpred_ad         : int  0 NA 1 6 0 0 0 NA 0 NA ...\n $ pred_control       : chr  \"\" \"\" \"\" \"\" ...\n $ pred_eggs          : int  4 NA 17 NA 0 NA 0 NA NA NA ...\n $ pred_chicks        : int  2 NA 0 NA 4 NA 3 NA NA NA ...\n $ pred_fl            : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ pred_ad            : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ pred_pefa          : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_coy_fox       : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_meso          : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_owlspp        : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_corvid        : chr  \"Y\" \"\" \"N\" \"\" ...\n $ pred_other_raptor  : chr  \"Y\" \"\" \"Y\" \"\" ...\n $ pred_other_avian   : chr  \"N\" \"\" \"Y\" \"\" ...\n $ pred_misc          : chr  \"N\" \"\" \"N\" \"\" ...\n $ total_pefa         : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_coy_fox      : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_meso         : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_owlspp       : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_corvid       : int  4 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_other_raptor : int  2 NA 6 NA 0 NA 3 NA NA NA ...\n $ total_other_avian  : int  0 NA 11 NA 4 NA 0 NA NA NA ...\n $ total_misc         : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ first_observed     : chr  \"2000-05-11\" \"\" \"2000-05-01\" \"2000-06-10\" ...\n $ last_observed      : chr  \"2000-08-05\" \"\" \"2000-08-19\" \"2000-09-24\" ...\n $ first_nest         : chr  \"2000-05-26\" \"\" \"2000-05-16\" \"2000-06-17\" ...\n $ first_chick        : chr  \"2000-06-18\" \"\" \"2000-06-07\" \"2000-07-22\" ...\n $ first_fledge       : chr  \"2000-07-08\" \"\" \"2000-06-30\" \"2000-08-06\" ...\n\n\nThis summary lists information about each column, and includes most of what you found earlier by using several different functions separately. The summary uses chr to indicate columns of text (“characters”) and int to indicate columns of integers.\nIn contrast to str, the summary function returns a statistical summary of an object. This summary includes summary statistics for each column, choosing appropriate statistics based on the kind of data in the column. For numbers, this is generally the mean, median, and quantiles. For categories, this is the frequencies. Other kinds of statistics are shown for other kinds of data. Here’s the statistical summary for the least terns data set:\n\nsummary(terns)\n\n      year       site_name         site_name_2013_2018 site_name_1988_2001\n Min.   :2000   Length:791         Length:791          Length:791         \n 1st Qu.:2008   Class :character   Class :character    Class :character   \n Median :2013   Mode  :character   Mode  :character    Mode  :character   \n Mean   :2013                                                             \n 3rd Qu.:2018                                                             \n Max.   :2023                                                             \n                                                                          \n  site_abbr           region_3           region_4            event          \n Length:791         Length:791         Length:791         Length:791        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     bp_min           bp_max           fl_min            fl_max       \n Min.   :   0.0   Min.   :   0.0   Min.   :   0.00   Min.   :   0.00  \n 1st Qu.:   3.0   1st Qu.:   5.0   1st Qu.:   0.00   1st Qu.:   0.00  \n Median :  30.0   Median :  38.0   Median :   7.00   Median :   9.00  \n Mean   : 129.3   Mean   : 151.0   Mean   :  40.82   Mean   :  50.35  \n 3rd Qu.: 127.5   3rd Qu.: 148.5   3rd Qu.:  38.00   3rd Qu.:  47.50  \n Max.   :1691.0   Max.   :1691.0   Max.   :1025.00   Max.   :1145.00  \n NA's   :8        NA's   :8        NA's   :12        NA's   :12       \n  total_nests      nonpred_eggs    nonpred_chicks      nonpred_fl     \n Min.   :   0.0   Min.   :  0.00   Min.   :   0.00   Min.   :  0.000  \n 1st Qu.:   5.0   1st Qu.:  2.00   1st Qu.:   0.00   1st Qu.:  0.000  \n Median :  42.0   Median : 12.00   Median :   3.00   Median :  0.000  \n Mean   : 162.8   Mean   : 60.29   Mean   :  44.37   Mean   :  4.181  \n 3rd Qu.: 164.0   3rd Qu.: 69.00   3rd Qu.:  22.00   3rd Qu.:  2.000  \n Max.   :1741.0   Max.   :748.00   Max.   :1063.00   Max.   :207.000  \n NA's   :8        NA's   :164      NA's   :198       NA's   :240      \n   nonpred_ad     pred_control         pred_eggs       pred_chicks     \n Min.   : 0.000   Length:791         Min.   :  0.00   Min.   :  0.000  \n 1st Qu.: 0.000   Class :character   1st Qu.:  2.00   1st Qu.:  0.000  \n Median : 0.000   Mode  :character   Median :  6.50   Median :  2.000  \n Mean   : 0.851                      Mean   : 41.57   Mean   :  8.519  \n 3rd Qu.: 1.000                      3rd Qu.: 25.50   3rd Qu.:  7.500  \n Max.   :22.000                      Max.   :417.00   Max.   :149.000  \n NA's   :234                         NA's   :737      NA's   :737      \n    pred_fl          pred_ad       pred_pefa         pred_coy_fox      \n Min.   : 0.000   Min.   : 0.00   Length:791         Length:791        \n 1st Qu.: 0.000   1st Qu.: 0.00   Class :character   Class :character  \n Median : 0.000   Median : 0.50   Mode  :character   Mode  :character  \n Mean   : 2.365   Mean   : 2.69                                        \n 3rd Qu.: 2.000   3rd Qu.: 2.00                                        \n Max.   :23.000   Max.   :41.00                                        \n NA's   :739      NA's   :733                                          \n  pred_meso         pred_owlspp        pred_corvid        pred_other_raptor \n Length:791         Length:791         Length:791         Length:791        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n pred_other_avian    pred_misc           total_pefa     total_coy_fox    \n Length:791         Length:791         Min.   : 0.000   Min.   :  0.000  \n Class :character   Class :character   1st Qu.: 0.000   1st Qu.:  0.000  \n Mode  :character   Mode  :character   Median : 0.000   Median :  0.000  \n                                       Mean   : 1.741   Mean   :  9.464  \n                                       3rd Qu.: 0.000   3rd Qu.:  0.000  \n                                       Max.   :34.000   Max.   :348.000  \n                                       NA's   :737      NA's   :735      \n   total_meso       total_owlspp     total_corvid     total_other_raptor\n Min.   :  0.000   Min.   : 0.000   Min.   :  0.000   Min.   : 0.000    \n 1st Qu.:  0.000   1st Qu.: 0.000   1st Qu.:  0.000   1st Qu.: 0.000    \n Median :  0.000   Median : 0.000   Median :  0.000   Median : 0.000    \n Mean   :  5.556   Mean   : 1.455   Mean   :  7.962   Mean   : 1.712    \n 3rd Qu.:  0.000   3rd Qu.: 0.500   3rd Qu.:  2.000   3rd Qu.: 1.000    \n Max.   :244.000   Max.   :41.000   Max.   :177.000   Max.   :43.000    \n NA's   :737       NA's   :736      NA's   :739       NA's   :739       \n total_other_avian   total_misc      first_observed     last_observed     \n Min.   :  0.000   Min.   :  0.000   Length:791         Length:791        \n 1st Qu.:  0.000   1st Qu.:  0.000   Class :character   Class :character  \n Median :  0.000   Median :  0.000   Mode  :character   Mode  :character  \n Mean   :  8.898   Mean   :  6.566                                        \n 3rd Qu.:  2.000   3rd Qu.:  0.000                                        \n Max.   :140.000   Max.   :168.000                                        \n NA's   :742       NA's   :738                                            \n  first_nest        first_chick        first_fledge      \n Length:791         Length:791         Length:791        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n\n\n\n\n12.9.2 Selecting Columns\nYou can select an individual column from a data frame by name with $, the dollar sign operator. The syntax is:\n\nVARIABLE$COLUMN_NAME\n\nFor example, for the least terns data set, terns$year selects the year column, which is the year of observation:\n\nterns$year\n\n  [1] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000\n [16] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2004\n [31] 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004\n [46] 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004\n [61] 2004 2004 2004 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005\n [76] 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005\n [91] 2005 2005 2005 2005 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006\n[106] 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006\n[121] 2006 2006 2006 2006 2006 2006 2006 2007 2007 2007 2007 2007 2007 2007 2007\n[136] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007\n[151] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2008\n[166] 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008\n[181] 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008\n[196] 2008 2008 2008 2008 2008 2008 2008 2009 2009 2009 2009 2009 2009 2009 2009\n[211] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[226] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[241] 2009 2009 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010\n[256] 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010\n[271] 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2011 2011 2011\n[286] 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011\n[301] 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011\n[316] 2011 2011 2011 2011 2011 2011 2011 2011 2012 2012 2012 2012 2012 2012 2012\n[331] 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012\n[346] 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012\n[361] 2012 2012 2012 2012 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013\n[376] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013\n[391] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013\n[406] 2013 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014\n[421] 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014\n[436] 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2015 2015\n[451] 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n[466] 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n[481] 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2016 2016 2016 2016 2016\n[496] 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016\n[511] 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016\n[526] 2016 2016 2016 2016 2016 2016 2016 2016 2017 2017 2017 2017 2017 2017 2017\n[541] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017\n[556] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017\n[571] 2017 2017 2017 2017 2017 2017 2017 2017 2018 2018 2018 2018 2018 2018 2018\n[586] 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018\n[601] 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2019 2019 2019\n[616] 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019\n[631] 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019\n[646] 2019 2019 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020\n[661] 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020\n[676] 2020 2020 2020 2020 2020 2020 2020 2021 2021 2021 2021 2021 2021 2021 2021\n[691] 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021\n[706] 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2022 2022\n[721] 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022\n[736] 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022\n[751] 2022 2022 2022 2022 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023\n[766] 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023\n[781] 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023\n\n\nR provides a variety of functions to compute on columns (and other vectors of data). For instance, what if you want to know the time period the data set covers? You can use the range function to compute the minimum and maximum of a column:\n\nrange(terns$year)\n\n[1] 2000 2023\n\n\nSo the oldest observations are from 2000 and the newest are from 2023, although this function and output doesn’t tell us whether there are observations for the years in between.\nYou can count the observations for each year with the table function:\n\ntable(terns$year)\n\n\n2000 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 \n  29   34   31   33   37   38   40   40   41   41   42   42   42   43   45   34 \n2019 2020 2021 2022 2023 \n  35   35   36   36   37 \n\n\nThe table function is great for summarizing columns of categories, where numerical statistics like means and standard deviations aren’t defined.\nOn the other hand, numerical statistics work well for summarizing columns of numbers. You can use the mean function to compute the mean of a column. For instance, let’s compute the mean of the total_nests column, which is the total number of nests seen at a site:\n\nmean(terns$total_nests)\n\n[1] NA\n\n\nThe result is NA because column is missing some values (we’ll explain this in detail in Section 13.3.1). To compute the mean with only the values that are present, set na.rm = TRUE in the call to mean:\n\nmean(terns$total_nests, na.rm = TRUE)\n\n[1] 162.8455\n\n\nYou can also use the dollar sign operator to assign values to columns. For instance, to assign 2000 to the entire year column:\n\nterns$year = 2000\n\nBe careful when you do this, as there is no undo. Fortunately, you haven’t saved this change to the least terns data set to your computer’s hard drive yet, so you can reload the data set to reset it:\n\nterns = read.csv(\"data/2000-2023_ca_least_tern.csv\")\n\nIn Chapter 15, you’ll learn how to select rows and individual elements from a data frame, as well as other ways to select columns.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "chapters/week03/data-types.html",
    "href": "chapters/week03/data-types.html",
    "title": "13  Data Types",
    "section": "",
    "text": "13.1 Vectors\nThe previous chapter introduced R and gave you enough background to do some simple computations on data sets. This chapter focuses on the foundational knowledge and skills you’ll need in order to use R effectively in the long term. Specifically, it’s a deep dive into R’s various data structures and data types.\nA vector is a collection of values. Vectors are the fundamental unit of data in R, and you’ve already used them in the previous sections.\nFor instance, each column in a data frame is a vector. So the site_name column in the California least terns data set (Section 11.4) is a vector. Take a look at it now. You can use head to avoid printing too much. Set the second argument to 10 so that exactly 10 values are printed:\nhead(terns$site_name, 10)\n\n [1] \"PITTSBURG POWER PLANT\"                       \n [2] \"ALBANY CENTRAL AVE\"                          \n [3] \"ALAMEDA POINT\"                               \n [4] \"KETTLEMAN CITY\"                              \n [5] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n [6] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n [7] \"VANDENBERG SFB\"                              \n [8] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n [9] \"ORMOND BEACH\"                                \n[10] \"NBVC POINT MUGU\"\nLike all vectors, this vector is ordered, which just means the values, or elements, have specific positions. The value of the 1st element is PITTSBURG POWER PLANT, the 2nd is ALBANY CENTRAL AVE, the 5th is OCEANO DUNES STATE VEHICULAR RECREATION AREA, and so on.\nNotice that the elements of this vector are all strings. In R, the elements of a vector must all be the same type of data (we say the elements are homogeneous). A vector can contain integers, decimal numbers, strings, or any of several other types of data, but not a mix these all at once.\nThe other columns in the least terns data frame are also vectors. For instance, the year column is a vector of integers:\nhead(terns$year)\n\n[1] 2000 2000 2000 2000 2000 2000\nVectors can contain any number of elements, including 0 or 1 element. Unlike mathematics, R does not distinguish between vectors and scalars (solitary values). As far as R is concerned, a solitary value, like 3, is a vector with 1 element.\nYou can check the length of a vector (and other objects) with the length function:\nlength(3)\n\n[1] 1\n\nlength(\"hello\")\n\n[1] 1\n\nlength(terns$year)\n\n[1] 791\nSince the last of these is a column from the data frame terns, the length is the same as the number of rows in terns.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "chapters/week03/data-types.html#sec-vectors",
    "href": "chapters/week03/data-types.html#sec-vectors",
    "title": "13  Data Types",
    "section": "",
    "text": "13.1.1 Creating Vectors\nSometimes you’ll want to create your own vectors. For instance, you might want to combine data from several columns (or datasets), need to pass a vector of arguments to a function, or want to use R as a calculator.\nYou can create vectors withe the c function. Recall that R considers individual values to be vectors, so what the c function really does is concatenate vectors. That is, it accepts any number of arguments, and combines them into a single vector:\n\nc(1, 2, 19, -3)\n\n[1]  1  2 19 -3\n\nc(\"hi\", \"hello\")\n\n[1] \"hi\"    \"hello\"\n\nc(1, 2, c(3, 4))\n\n[1] 1 2 3 4\n\n\nFor example, suppose you want to use the unique function to get all of the unique site names in terns:\n\nunique(c(terns$site_name_2013_2018, terns$site_name_1988_2001))\n\n [1] \"Pittsburg Power Plant\"                                    \n [2] \"NA_NO POLYGON\"                                            \n [3] \"Alameda Point\"                                            \n [4] \"Kettleman\"                                                \n [5] \"Oceano Dunes State Vehicular Recreation Area\"             \n [6] \"Rancho Guadalupe Dunes Preserve\"                          \n [7] \"Vandenberg AFB\"                                           \n [8] \"Santa Clara River\"                                        \n [9] \"Ormond Beach\"                                             \n[10] \"NBVC Point Mugu\"                                          \n[11] \"Venice Beach\"                                             \n[12] \"Port of LA\"                                               \n[13] \"Seal Beach National Wildlife Refuge\"                      \n[14] \"Bolsa Chica\"                                              \n[15] \"Huntington Beach State Park\"                              \n[16] \"Upper Newport Bay\"                                        \n[17] \"Camp Pendleton\"                                           \n[18] \"Batiquitos Lagoon\"                                        \n[19] \"San Elijo Lagoon\"                                         \n[20] \"FAA Island\"                                               \n[21] \"North Fiesta Island\"                                      \n[22] \"Mariner's Point\"                                          \n[23] \"Lindbergh Field/Former Naval Training Center\"             \n[24] \"Naval Base Coronado\"                                      \n[25] \"D Street Fill\"                                            \n[26] \"Chula Vista Wildlife Refuge\"                              \n[27] \"Saltworks\"                                                \n[28] \"Tijuana Estuary\"                                          \n[29] \"Coal Oil Point Reserve\"                                   \n[30] \"Hollywood Beach\"                                          \n[31] \"Burris Basin\"                                             \n[32] \"San Diego River Mouth\"                                    \n[33] \"Hayward Regional Shoreline\"                               \n[34] \"Montezuma\"                                                \n[35] \"Stony Point\"                                              \n[36] \"Napa Sonoma Marsh Wildlife Area Huichica Unit (Pond 7/7A)\"\n[37] \"Eden Landing Ecological Reserve\"                          \n[38] \"Bufferlands\"                                              \n[39] \"Fairbanks Ranch\"                                          \n[40] \"San Diequito Lagoon\"                                      \n[41] \"Salton Sea\"                                               \n[42] \"Saticoy United Water Conservation District\"               \n[43] \"Anaheim Lake\"                                             \n[44] \"Malibu Lagoon\"                                            \n[45] \"\"                                                         \n[46] \"NA_2013_2018 POLYGON\"                                     \n[47] \"Albany Central Avenue\"                                    \n\n\nIf the arguments you pass to the c function have different data types, R will attempt to convert them to a common data type that preserves the information:\n\nc(1, \"cool\", 2.3)\n\n[1] \"1\"    \"cool\" \"2.3\" \n\n\nSection 13.2.2 explains the rules for this conversion in more detail.\nThe colon operator : creates vectors that contain sequences of integers. This is useful for creating “toy” data to test things on, and Section 13.1.2 will show that it’s important for selecting elements from other vectors. Here are a few different sequences:\n\n1:3\n\n[1] 1 2 3\n\n-3:5\n\n[1] -3 -2 -1  0  1  2  3  4  5\n\n10:1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nBeware that both endpoints are included in the sequence, even in sequences like 1:0, and that the difference between elements is always -1 or 1. If you want more control over the generated sequence, use the seq function instead.\n\n\n13.1.2 Indexing Vectors\nYou can access individual elements of a vector with the indexing operator [ (also called the square bracket operator). The syntax is:\nVECTOR[INDEXES]\nHere INDEXES is a vector of positions of elements you want to get or set.\nFor example, let’s make a vector with 5 elements and get the 2nd element:\n\nx = c(4, 8, 3, 2, 1)\nx[2]\n\n[1] 8\n\n\nNow let’s get the 3rd and 1st element:\n\nx[c(3, 1)]\n\n[1] 3 4\n\n\nYou can use the indexing operator together with the assignment operator to assign elements of a vector:\n\nx[3] = 0\nx\n\n[1] 4 8 0 2 1\n\n\nIndexing is among the most frequently used operations in R, so take some time to try it out with few different vectors and indexes. We’ll revisit indexing in Chapter 15 to learn a lot more about it.\n\n\n13.1.3 Vectorization\nLet’s look at what happens if we call a mathematical function, like sqrt, on a vector:\n\nx = c(1, 3, 0, pi)\nsqrt(x)\n\n[1] 1.000000 1.732051 0.000000 1.772454\n\n\nThis gives us the same result as if we had called the function separately on each element. That is, the result is the same as:\n\nc(sqrt(1), sqrt(3), sqrt(0), sqrt(pi))\n\n[1] 1.000000 1.732051 0.000000 1.772454\n\n\nOf course, the first version is much easier to type.\nFunctions that take a vector argument and get applied element-by-element like this are said to be vectorized. Most functions in R are vectorized, especially math functions. Some examples include round, sqrt, exp, log, sin, cos, and tan.\nVectorized functions are often useful for transforming columns. For example, suppose we want to transform the total number of nests in the terns data:\n\nsqrt(terns$total_nests)\n\n  [1]  3.872983  4.472136 17.663522  1.732051  2.236068  3.000000  5.656854\n  [8]  4.690416  8.544004 15.874508 17.549929 23.769729 10.344080  7.483315\n [15] 21.931712  8.246211 32.848135 13.114877  4.000000 13.674794  5.099020\n [22] 20.493902  5.196152 11.575837 25.079872  5.830952  0.000000  6.633250\n [29] 14.491377  3.872983  0.000000 20.976177  1.414214  7.937254  2.828427\n [36]  1.000000  2.449490  9.110434  7.071068  5.385165 24.839485  4.123106\n [43] 32.726136 14.352700 15.132746 17.972201  2.236068  9.433981 37.815341\n [50] 24.331050  0.000000 17.748239  4.123106 17.291616  6.480741  8.717798\n [57] 13.114877 32.171416  1.000000 10.535654  8.124038  7.000000 22.803509\n [64]  2.000000 23.452079  2.828427  1.000000  7.681146  2.000000  6.633250\n [71]  0.000000  3.000000  5.196152 24.657656  9.486833 36.496575 12.041595\n [78] 11.618950 18.411953  5.291503 40.792156 24.413111  1.000000  2.449490\n [85]  0.000000 16.763055 10.862780 12.529964 11.575837 33.689761 10.049876\n [92]  7.549834  5.830952 21.400935  4.123106  0.000000 21.000000  3.872983\n [99]  1.414214  6.164414  0.000000  1.414214  2.236068        NA  7.280110\n[106] 21.656408 19.595918 30.116441 13.638182 14.899664 22.825424  6.000000\n[113] 39.242834 25.039968  0.000000 10.198039  5.477226 10.954451 11.661904\n[120]  3.741657 11.445523 13.416408 37.749172 10.000000  3.872983  9.055385\n[127] 19.261360  1.414214  5.656854  2.645751 19.849433  5.916080  2.449490\n[134]  1.414214  8.124038  1.000000  4.242641  2.449490  8.774964  1.000000\n[141]  7.211103 20.760539 23.366643 26.645825 12.884099 15.033296 22.022716\n[148]  3.000000  6.480741 39.115214 24.372115  0.000000  5.291503  6.244998\n[155] 10.246951  6.708204  5.477226 11.618950 11.090537 34.088121 11.401754\n[162]  6.782330  9.848858 17.058722  1.000000  4.242641  5.916080  1.000000\n[169] 18.330303  7.874008  1.414214  1.000000  7.483315  0.000000  4.242641\n[176]  1.000000  9.848858  4.898979  9.000000 22.494444 30.463092 23.000000\n[183] 14.352700 15.556349 21.307276  3.162278  5.000000 40.804412 24.698178\n[190]  0.000000  0.000000  3.162278  3.741657  1.000000  1.000000 11.789826\n[197] 12.083046 39.051248 12.165525  5.744563 10.099505 14.177447  1.000000\n[204]  7.211103  5.196152  0.000000 18.601075  8.944272  1.000000  1.000000\n[211]  5.099020  1.732051  5.567764  0.000000  9.219544  2.000000  6.633250\n[218] 25.059928 17.175564 20.856654 13.304135 17.804494 20.832667  4.000000\n[225]  3.162278 41.448764 25.475478  0.000000  1.414214  6.164414  6.708204\n[232]  3.741657  0.000000  0.000000 12.041595 11.180340 41.725292 11.489125\n[239]  6.928203  8.831761 17.146428  1.732051  1.000000  6.855655  4.123106\n[246]  0.000000 17.888544  7.280110  0.000000  1.000000  4.795832  1.000000\n[253]  5.830952  0.000000  6.000000  1.000000  6.928203 26.608269 12.806248\n[260] 14.696938 16.278821 21.118712 20.808652  2.828427  3.605551 41.496988\n[267] 21.908902  0.000000  0.000000  0.000000  4.123106  5.000000 10.816654\n[274]  1.000000  3.000000 10.770330  9.165151 33.391616 10.908712  6.324555\n[281]  8.062258 14.933185  1.000000  6.164414  3.872983  0.000000 18.841444\n[288]  8.774964  0.000000  0.000000  5.916080  0.000000  5.656854  1.000000\n[295]  5.099020  0.000000  7.745967 26.776856  5.291503  3.162278 13.304135\n[302] 12.922848 26.683328  3.741657  2.449490 39.064050 23.065125  0.000000\n[309]        NA  0.000000  4.582576  1.000000 12.041595  3.464102  3.605551\n[316]  8.831761  9.110434 32.588341 10.770330  7.280110  7.416198 16.248077\n[323]  1.732051  1.414214  5.916080  5.477226  1.732051 19.544820 13.747727\n[330]  0.000000  0.000000  6.782330  0.000000  4.242641  0.000000  6.480741\n[337]  1.000000  2.449490 29.051678  3.741657 14.525839 11.000000 17.464249\n[344] 23.280893  3.316625  4.472136 35.284558 23.727621  0.000000  0.000000\n[351]  0.000000  7.000000  1.000000 11.916375  4.123106  3.464102 11.401754\n[358]  3.162278 32.680269 10.677078  8.000000  9.486833 16.822604  0.000000\n[365]  0.000000  8.944272  5.385165  0.000000 17.088007  9.219544  0.000000\n[372]  0.000000  7.549834  0.000000  3.872983  0.000000  6.082763 14.456832\n[379]  2.645751 18.601075  0.000000  3.872983 15.937377 12.806248 12.529964\n[386] 18.627936  4.795832  5.656854 35.256205 23.622024  0.000000  0.000000\n[393]  1.732051 12.489996  0.000000  6.082763  6.403124  0.000000 10.677078\n[400]  2.645751 32.155870 12.000000  8.888194  6.708204 16.792856  1.414214\n[407]  0.000000  6.708204  4.000000  0.000000 18.055470  9.219544  0.000000\n[414]  0.000000  7.000000  0.000000  4.582576  0.000000  2.000000 10.954451\n[421]  4.690416 21.563859  0.000000  9.000000 11.224972 12.409674 17.349352\n[428] 22.715633  4.242641  1.414214 36.565011 21.863211  0.000000  0.000000\n[435]  0.000000  2.828427  3.605551 10.908712  1.414214  0.000000 10.000000\n[442]  7.549834 32.233523 12.165525  9.327379  5.916080 16.155494  1.732051\n[449]  1.000000  8.888194  4.000000  1.732051 18.734994  8.426150  0.000000\n[456]  0.000000  7.348469  0.000000  4.690416  0.000000  8.485281  4.898979\n[463]  0.000000 21.748563  0.000000  2.828427 10.440307 10.295630 14.282857\n[470] 22.891046  4.795832  4.690416 37.202150 20.371549  0.000000  0.000000\n[477]  0.000000  4.472136  4.898979 13.453624  1.000000  0.000000  4.242641\n[484]  5.744563 28.740216 11.090537  8.888194  5.385165 14.422205  1.000000\n[491]  1.000000  8.888194  2.449490  1.000000 20.074860  9.380832  0.000000\n[498]  0.000000  7.000000  0.000000  5.196152  0.000000  7.874008  0.000000\n[505]  4.242641 19.000000  0.000000  1.414214 11.874342  8.944272 11.916375\n[512] 18.654758  2.000000  3.162278  4.472136 29.949958 21.236761  0.000000\n[519]  0.000000  0.000000  7.071068  5.000000 11.269428  3.872983  2.236068\n[526]  6.082763  5.830952 29.308702 10.862780  8.717798  5.099020 13.564660\n[533]  0.000000  1.000000 11.916375  3.000000        NA 21.142375  8.485281\n[540]  4.582576  0.000000  7.211103  0.000000  0.000000  5.291503  0.000000\n[547]  3.000000  0.000000  6.164414 19.416488  0.000000  0.000000  5.567764\n[554]  2.236068 11.135529 13.341664 26.057628  0.000000  3.741657  4.123106\n[561] 37.483330 26.305893  0.000000        NA  0.000000  6.403124  0.000000\n[568] 13.266499  1.414214        NA  4.898979  4.690416 29.816103 11.269428\n[575]  9.797959  6.324555 15.000000  0.000000  5.291503  4.242641 19.364917\n[582]  5.656854 11.874342  5.916080  9.110434  4.898979  0.000000  9.165151\n[589] 17.058722  0.000000  2.236068  2.449490 11.532563 10.816654 11.532563\n[596] 26.229754  4.000000  4.690416 18.275667 26.153394  0.000000  3.464102\n[603]  1.000000 13.341664  3.316625  4.358899  4.000000 28.757608 10.440307\n[610]  9.110434  5.744563 13.820275  1.732051  3.872983 18.574176  7.483315\n[617] 11.000000  5.830952  4.123106  6.855655        NA  0.000000  9.591663\n[624] 17.748239  0.000000  2.449490  0.000000 14.106736  7.483315 11.575837\n[631] 23.043437  4.123106  4.472136 20.445048 23.409400  0.000000  6.244998\n[638]  1.414214 12.609520  1.000000  4.358899  2.000000 30.463092 10.198039\n[645]  8.124038  6.244998 12.247449  1.732051  5.385165 20.856654 10.049876\n[652]  1.000000  6.928203  7.483315  3.464102  6.244998  4.582576  4.795832\n[659] 14.317821        NA  6.782330  0.000000 13.490738  3.316625  9.539392\n[666] 20.712315  3.741657  4.242641 19.364917 11.789826  0.000000  6.082763\n[673]  1.000000 12.569805  0.000000  2.449490  2.449490 33.955854  7.874008\n[680] 11.224972  6.708204 14.247807  1.732051  8.888194  4.795832 20.493902\n[687] 10.630146  7.483315  7.280110  0.000000  5.830952  9.539392  0.000000\n[694]  5.196152 13.152946        NA  6.082763  0.000000 14.071247  7.141428\n[701] 12.961481 23.727621  4.472136  3.162278 25.059928 17.233688  0.000000\n[708]  8.660254  1.000000 11.532563  0.000000  3.316625  1.414214 33.763886\n[715] 10.392305 10.049876  6.000000 13.638182  7.348469  8.831761  2.645751\n[722] 20.615528  7.810250  6.708204  6.708204  1.000000  6.708204  7.141428\n[729]  5.099020  5.830952 17.691806  1.732051  0.000000 13.747727  7.483315\n[736] 13.114877 19.824228  4.358899  1.414214 21.771541 15.937377  0.000000\n[743]  5.385165  9.899495  0.000000 10.344080  0.000000  3.162278  0.000000\n[750] 28.372522  8.774964  9.055385  4.898979 12.288206  6.782330  3.000000\n[757]  5.567764 18.411953 12.000000  8.774964  6.480741  0.000000  6.480741\n[764]  5.477226  0.000000  4.358899 13.711309  0.000000  0.000000  7.483315\n[771]  8.426150 10.677078 17.029386  1.000000  3.000000  2.000000 23.494680\n[778] 15.099669  0.000000  6.855655 12.489996  1.000000  2.449490  0.000000\n[785]  2.828427  0.000000 26.776856  6.633250  7.681146  6.928203 13.076697\n\n\nThis square root transformation can be helpful when fitting certain kinds of statistical models, because it shrinks large values in a dataset more than small ones.\nFunctions that are not vectorized tend to be ones that combine or aggregate values in some way. For instance, the sum, mean, median, length, and class functions are not vectorized.\nNon-vectorized functions are often useful for summarizing columns. For example, we can compute the mean of the minimum breeding pairs in the terns data:\n\nmean(terns$bp_min, na.rm = TRUE)\n\n[1] 129.3199\n\n\nA function can be vectorized across multiple arguments. This is easiest to understand in terms of the arithmetic operators. Let’s see what happens if we add two vectors together:\n\nx = c(1, 2, 3, 4)\ny = c(-1, 7, 10, -10)\nx + y\n\n[1]  0  9 13 -6\n\n\nThe elements are paired up and added according to their positions. The other arithmetic operators are also vectorized:\n\nx - y\n\n[1]  2 -5 -7 14\n\nx * y\n\n[1]  -1  14  30 -40\n\nx / y\n\n[1] -1.0000000  0.2857143  0.3000000 -0.4000000\n\n\nThis means we can use arithmetic operators on whole columns. Let’s compute the average number of fledglings per nest for each row in the terns data:\n\nterns$fl_min / terns$total_nests\n\n  [1] 1.066666667 0.050000000 0.641025641 0.333333333 0.800000000 1.888888889\n  [7] 0.343750000 0.409090909 0.821917808 0.253968254 0.487012987 1.008849558\n [13] 1.682242991 0.000000000 0.904365904 0.176470588 0.949953661 0.220930233\n [19] 0.437500000 1.069518717 0.307692308 0.357142857 0.888888889 0.634328358\n [25] 0.508744038 0.794117647         NaN 0.386363636 0.376190476 0.066666667\n [31]         NaN 0.352272727 1.000000000 0.396825397 0.000000000 0.000000000\n [37] 0.000000000 0.325301205 0.540000000 0.137931034 0.196110211 0.000000000\n [43] 0.519140990 0.354368932 0.262008734 0.117647059 0.400000000          NA\n [49] 0.032867133 0.185810811         NaN 0.031746032 0.000000000 0.000000000\n [55] 0.238095238 0.131578947 0.034883721 0.032850242 0.000000000 0.036036036\n [61] 0.166666667 0.081632653 0.030769231 0.000000000 0.254545455 0.000000000\n [67] 0.000000000 0.305084746 0.000000000 0.022727273         NaN 0.444444444\n [73] 0.000000000 0.108552632 0.000000000 0.337087087 0.600000000 1.333333333\n [79] 0.209439528 0.000000000 0.179687500 0.182885906 0.000000000 0.000000000\n [85]         NaN 0.177935943 0.050847458 0.286624204 0.149253731 0.110132159\n [91] 0.089108911 0.035087719 0.058823529 0.082969432 1.647058824         NaN\n [97] 0.020408163 0.266666667 1.000000000 0.947368421         NaN 0.000000000\n[103] 1.400000000          NA 0.830188679 0.226012793 0.541666667 0.563395810\n[109] 0.580645161 1.238738739 0.251439539 0.055555556 0.337662338 0.355661882\n[115]         NaN 0.019230769 0.133333333 0.000000000 0.073529412 0.000000000\n[121] 0.412213740 0.194444444 0.120000000 0.180000000 0.133333333 0.073170732\n[127] 0.153638814          NA 0.156250000 0.000000000 0.375634518 1.400000000\n[133] 0.000000000 1.000000000 1.060606061 1.000000000 0.888888889 0.000000000\n[139] 0.987012987 2.000000000 0.673076923 0.322505800 0.758241758 0.261971831\n[145] 0.072289157 0.066371681 0.443298969 0.777777778 0.285714286 0.241176471\n[151] 0.245791246         NaN 0.071428571 0.153846154 0.190476190 0.177777778\n[157] 0.266666667 0.251851852 0.252032520 0.172117040 0.192307692 0.000000000\n[163] 0.134020619 0.099656357 0.000000000 0.055555556 0.314285714 0.000000000\n[169] 1.062500000 1.177419355 0.000000000 2.000000000 1.250000000         NaN\n[175] 1.055555556 0.000000000 0.793814433 1.166666667 0.370370370 0.156126482\n[181] 0.318965517 0.396975425 0.213592233 0.413223140 0.588105727 0.200000000\n[187] 0.800000000 0.064264264 0.201639344         NaN         NaN 0.000000000\n[193] 0.000000000 0.000000000 0.000000000 0.827338129 0.171232877 0.085245902\n[199] 0.114864865 0.060606061 0.058823529 0.223880597 2.000000000 0.038461538\n[205] 0.222222222         NaN 0.728323699 0.787500000 0.000000000 1.000000000\n[211] 1.076923077 1.000000000 1.193548387         NaN 0.658823529 1.000000000\n[217] 0.545454545 0.211783439 0.000000000 0.172413793 0.451977401 0.835962145\n[223] 0.304147465 0.312500000 0.000000000 0.104190920 0.326656394         NaN\n[229] 0.000000000 0.000000000 0.266666667 0.000000000         NaN         NaN\n[235] 0.248275862 0.240000000 0.024124067 0.143939394 0.083333333 0.051282051\n[241] 0.091836735 0.333333333 1.000000000 1.808510638 0.294117647         NaN\n[247] 0.690625000 1.415094340         NaN 2.000000000 1.260869565 2.000000000\n[253] 0.852941176         NaN 0.388888889 0.000000000 0.333333333 0.138418079\n[259] 0.000000000 0.018518519 0.120754717 0.150224215 0.688221709 0.500000000\n[265] 0.615384615 0.223577236 0.433333333         NaN         NaN         NaN\n[271] 0.000000000 0.000000000 0.341880342 0.000000000 0.666666667 0.250000000\n[277] 0.297619048 0.197309417 0.126050420 0.050000000 0.076923077 0.206278027\n[283] 3.000000000          NA          NA         NaN 0.504225352 0.311688312\n[289]         NaN         NaN 1.428571429         NaN 0.125000000 0.000000000\n[295] 0.769230769         NaN 0.200000000 0.100418410 0.000000000 0.000000000\n[301] 0.062146893 0.395209581 0.150280899 0.857142857 0.000000000 0.068152031\n[307] 0.093984962         NaN          NA         NaN 0.428571429 0.000000000\n[313] 0.344827586 0.250000000 0.384615385 0.141025641 0.144578313 0.090395480\n[319] 0.293103448 0.226415094 0.036363636 0.340909091 0.000000000 0.000000000\n[325] 0.428571429 0.133333333 0.000000000 0.044502618 0.640211640         NaN\n[331]         NaN 0.913043478         NaN 0.555555556         NaN 0.190476190\n[337] 0.000000000 0.000000000 0.017772512 0.000000000 0.165876777 0.330578512\n[343] 0.052459016 0.166051661 0.636363636 0.200000000 0.020080321 0.060390764\n[349]         NaN         NaN         NaN 0.000000000 0.000000000 0.000000000\n[355] 0.000000000 0.000000000 0.276923077 0.000000000 0.009363296 0.078947368\n[361] 0.281250000 0.011111111 0.000000000         NaN         NaN 0.137500000\n[367] 0.103448276         NaN 1.034246575 1.388235294         NaN         NaN\n[373] 0.982456140         NaN 1.266666667         NaN 0.000000000 0.148325359\n[379] 0.000000000 0.000000000         NaN 0.000000000 0.122047244 0.542682927\n[385] 0.222929936 0.288184438 0.043478261 0.250000000 0.120675784 0.209677419\n[391]         NaN         NaN 0.000000000 0.044871795         NaN 0.000000000\n[397] 0.073170732         NaN 0.298245614 1.714285714 0.150870406 0.159722222\n[403] 0.405063291 0.044444444 0.202127660 1.000000000         NaN 1.333333333\n[409] 0.062500000         NaN 1.199386503 1.388235294         NaN         NaN\n[415] 1.183673469         NaN 0.952380952         NaN 0.500000000 0.191666667\n[421] 0.000000000 0.273118280         NaN 0.925925926 0.126984127 0.025974026\n[427] 0.265780731 0.325581395 0.555555556 0.000000000 0.314136126 0.485355649\n[433]         NaN         NaN         NaN 0.250000000 0.538461538 0.504201681\n[439] 0.500000000         NaN 0.340000000 0.087719298 0.120307988 0.202702703\n[445] 0.264367816 0.285714286 0.130268199 0.000000000 1.000000000 0.303797468\n[451] 0.000000000 0.000000000 0.940170940 1.267605634         NaN         NaN\n[457] 1.277777778         NaN 1.318181818         NaN 0.375000000 0.000000000\n[463]         NaN 0.245243129         NaN 0.000000000 0.000000000 0.066037736\n[469] 0.250000000 0.238549618 0.130434783 0.045454545 0.122832370 0.216867470\n[475]         NaN         NaN         NaN 0.450000000 0.041666667 0.552486188\n[481] 0.000000000         NaN 0.444444444 0.090909091 0.202179177 0.170731707\n[487] 0.417721519 0.310344828 0.144230769 0.000000000 2.000000000 0.063291139\n[493] 0.166666667 0.000000000 1.454094293 1.784090909         NaN         NaN\n[499] 1.204081633         NaN 0.666666667         NaN 0.177419355         NaN\n[505] 0.777777778 0.155124654         NaN 0.000000000 0.326241135 0.312500000\n[511] 0.302816901 0.287356322 0.000000000 0.000000000 0.100000000 0.094760312\n[517] 0.388026608         NaN         NaN         NaN 0.100000000 0.160000000\n[523] 0.118110236 0.466666667 0.000000000 0.270270270 0.147058824 0.123399302\n[529] 0.177966102 0.197368421 0.230769231 0.179347826         NaN 0.000000000\n[535] 0.563380282 0.555555556          NA 0.407158837 0.986111111 0.952380952\n[541]         NaN 0.134615385         NaN         NaN 0.285714286         NaN\n[547] 0.000000000         NaN 0.526315789 0.071618037         NaN         NaN\n[553] 0.419354839 0.000000000 0.032258065 0.033707865 0.038291605         NaN\n[559] 0.714285714 0.764705882 0.002846975 0.252890173         NaN          NA\n[565]         NaN 0.390243902         NaN 0.170454545 0.500000000          NA\n[571] 0.541666667 0.045454545 0.312710911 0.196850394 0.177083333 0.050000000\n[577] 0.342222222         NaN 0.000000000 0.000000000 0.506666667 0.343750000\n[583] 0.007092199 1.000000000 0.421686747 0.250000000         NaN 0.523809524\n[589] 0.209621993         NaN 0.000000000 0.000000000 0.518796992 0.358974359\n[595] 0.533834586 0.093023256 0.375000000 0.045454545 0.011976048 0.137426901\n[601]         NaN 0.000000000 0.000000000 0.168539326 0.000000000 0.684210526\n[607] 0.062500000 0.035066505 0.110091743 0.072289157 0.030303030 0.078534031\n[613] 0.000000000 0.133333333 0.527536232 0.785714286 0.082644628 1.117647059\n[619] 0.235294118 0.446808511          NA         NaN 0.195652174 0.203174603\n[625]         NaN 0.000000000         NaN 0.487437186 0.000000000 0.328358209\n[631] 0.152542373 0.117647059 0.200000000 0.035885167 0.040145985         NaN\n[637] 0.025641026 0.000000000 0.226415094 0.000000000 0.315789474 0.250000000\n[643] 0.022629310 0.115384615 0.030303030 0.025641026 0.040000000 0.000000000\n[649] 0.103448276 0.664367816 1.326732673 0.000000000 0.791666667 0.107142857\n[655] 0.500000000 0.076923077 0.000000000 0.260869565 0.068292683          NA\n[661] 0.543478261         NaN 0.016483516 0.090909091 0.000000000 0.006993007\n[667] 0.214285714 0.277777778 0.266666667 0.352517986         NaN 0.162162162\n[673] 0.000000000 0.392405063         NaN 0.666666667 0.333333333 0.026886383\n[679] 0.000000000 0.126984127 0.044444444 0.073891626 0.000000000 0.215189873\n[685] 0.173913043 0.383333333 0.513274336 0.017857143 0.905660377         NaN\n[691] 0.235294118 0.384615385         NaN 0.592592593 0.248554913          NA\n[697] 0.135135135         NaN 0.191919192 0.000000000 0.083333333 0.033747780\n[703] 0.350000000 0.000000000 0.146496815 0.107744108         NaN 0.533333333\n[709] 0.000000000 0.360902256         NaN 0.545454545 0.000000000 0.071052632\n[715] 0.074074074 0.009900990 0.083333333 0.086021505 0.000000000 0.038461538\n[721] 0.000000000 0.461176471 1.081967213 0.088888889 0.822222222 0.000000000\n[727] 0.511111111 0.000000000 0.000000000 0.176470588 0.041533546 0.000000000\n[733]         NaN 0.010582011 0.035714286 0.122093023 0.203562341 0.000000000\n[739] 0.000000000 0.198312236 0.070866142         NaN 0.655172414 0.183673469\n[745]         NaN 0.299065421         NaN 0.900000000         NaN 0.026086957\n[751] 0.000000000 0.024390244 0.083333333 0.158940397 0.130434783 0.000000000\n[757] 0.064516129 0.321533923 0.881944444 0.051948052 0.833333333         NaN\n[763] 0.404761905 0.066666667         NaN 0.157894737 0.271276596         NaN\n[769]         NaN 0.000000000 0.183098592 0.315789474 0.110344828 0.000000000\n[775] 0.000000000 0.750000000 0.313405797 0.337719298         NaN 0.340425532\n[781] 0.282051282 0.000000000 0.000000000         NaN 1.250000000         NaN\n[787] 0.125523013 0.090909091 0.084745763 0.145833333 0.204678363\n\n\n\n\n13.1.4 Recycling\nWhen a function is vectorized across multiple arguments, what happens if the vectors have different lengths? Whenever you think of a question like this as you’re learning R, the best way to find out is to create some toy data and test it yourself. Let’s try that now:\n\nx = c(1, 2, 3, 4)\ny = c(-1, 1)\nx + y\n\n[1] 0 3 2 5\n\n\nThe elements of the shorter vector are recycled to match the length of the longer vector. That is, after the second element, the elements of y are repeated to make a vector with the same length as x (because x is longer), and then vectorized addition is carried out as usual.\nHere’s what that looks like written down:\n   1  2  3  4\n+ -1  1 -1  1\n  -----------\n   0  3  2  5\nIf the length of the longer vector is not a multiple of the length of the shorter vector, R issues a warning, but still returns the result. The warning as meant as a reminder, because unintended recycling is a common source of bugs:\n\nx = c(1, 2, 3, 4, 5)\ny = c(-1, 1)\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 0 3 2 5 4\n\n\nRecycling might seem strange at first, but it’s convenient if you want to use a specific value (or pattern of values) with a vector. For instance, suppose you want to multiply all the elements of a vector by 2. Recycling makes this easy:\n\n2 * c(1, 2, 3)\n\n[1] 2 4 6\n\n\nWhen you use recycling, most of the time one of the arguments will be a scalar like this.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "chapters/week03/data-types.html#sec-data-types-classes",
    "href": "chapters/week03/data-types.html#sec-data-types-classes",
    "title": "13  Data Types",
    "section": "13.2 Data Types & Classes",
    "text": "13.2 Data Types & Classes\nSection 11.3 introduced the idea of categorizing data into types based on sets of shared characteristics. Thinking about types can help you figure out which analyses might be appropriate, or at least possible, for your data. Types are such a helpful idea that most programming languages also use types.\nIn R, data objects are categorized in two different ways:\n\nThe class of an R object describes what the object does, or the role that it plays. Sometimes objects can do more than one thing, so objects can have more than one class. The class function, which debuted in Section 11.4, returns the classes of its argument.\nThe type of an R object describes what the object is. Technically, the type corresponds to how the object is stored in your computer’s memory. Each object has exactly one type. The typeof function returns the type of its argument.\n\nOf the two, classes are more important than types. If you aren’t sure what an object is, checking its classes should be the first thing you do.\nThe built-in classes you’ll use all the time correspond to vectors and lists (which we’ll learn more about in Section 13.2.1):\n\n\n\n\n\n\n\n\nClass\nExample\nDescription\n\n\n\n\nlogical\nTRUE, FALSE\nLogical (or Boolean) values\n\n\ninteger\n-1L, 1L, 2L\nInteger numbers\n\n\nnumeric\n-2.1, 7, 34.2\nReal numbers\n\n\ncomplex\n3-2i, -8+0i\nComplex numbers\n\n\ncharacter\n\"hi\", \"YAY\"\nText strings\n\n\nlist\nlist(TRUE, 1, \"hi\")\nOrdered collection of heterogeneous elements\n\n\n\nR doesn’t distinguish between scalars and vectors, so the class of a vector is the same as the class of its elements:\n\nclass(\"hi\")\n\n[1] \"character\"\n\nclass(c(\"hello\", \"hi\"))\n\n[1] \"character\"\n\n\nIn addition, for most vectors, the class and the type are the same:\n\nx = c(TRUE, FALSE)\nclass(x)\n\n[1] \"logical\"\n\ntypeof(x)\n\n[1] \"logical\"\n\n\nThe exception to this rule is numeric vectors, which have type double for historical reasons:\n\nclass(pi)\n\n[1] \"numeric\"\n\ntypeof(pi)\n\n[1] \"double\"\n\ntypeof(3)\n\n[1] \"double\"\n\n\nThe word “double” here stands for double-precision floating point number, a standard way to represent real numbers on computers.\nBy default, R assumes any numbers you enter in code are numeric, even if they’re integer-valued.\nThe class integer also represents integer numbers, but it’s not used as often as numeric. A few functions, such as the sequence operator : and the length function, return integers. You can also force R to create an integer by adding the suffix L to a number, but there are no major drawbacks to using the double default:\n\nclass(1:3)\n\n[1] \"integer\"\n\nclass(3)\n\n[1] \"numeric\"\n\nclass(3L)\n\n[1] \"integer\"\n\n\nBesides the classes for vectors and lists, there are several built-in classes that represent more sophisticated data structures:\n\n\n\n\n\n\n\nClass\nDescription\n\n\n\n\nfunction\nFunctions\n\n\nfactor\nCategorical values\n\n\nmatrix\nTwo-dimensional ordered collection of homogeneous elements\n\n\narray\nMulti-dimensional ordered collection of homogeneous elements\n\n\ndata.frame\nData frames\n\n\n\nFor these, the class is usually different from the type. We’ll learn more about most of these later on.\n\n13.2.1 Lists\nA list is an ordered data structure where the elements can have different types (they are heterogeneous). This differs from a vector, where the elements all have to have the same type, as we saw in Section 13.1. The tradeoff is that most vectorized functions do not work with lists.\nYou can make an ordinary list with the list function:\n\nx = list(1, c(\"hi\", \"bye\"))\nclass(x)\n\n[1] \"list\"\n\ntypeof(x)\n\n[1] \"list\"\n\n\nFor ordinary lists, the type and the class are both list. In Chapter 15, we’ll learn how to get and set list elements, and in later sections we’ll learn more about when and why to use lists.\nYou’ve already seen one list, the terns data frame:\n\nclass(terns)\n\n[1] \"data.frame\"\n\ntypeof(terns)\n\n[1] \"list\"\n\n\nUnder the hood, data frames are lists, and each column is a list element. Because the class is data.frame rather than list, R treats data frames differently from ordinary lists. This difference is apparent in how data frames are printed compared to ordinary lists.\n\n\n13.2.2 Implicit Coercion\nR’s types fall into a natural hierarchy of expressiveness:\n\n\n\nR’s hierarchy of types.\n\n\nEach type on the right is more expressive than the ones to its left. That is, with the convention that FALSE is 0 and TRUE is 1, we can represent any logical value as an integer. In turn, we can represent any integer as a double, and any double as a complex number. By writing the number out, we can also represent any complex number as a string.\nThe point is that no information is lost as we follow the arrows from left to right along the types in the hierarchy. In fact, R will automatically and silently convert from types on the left to types on the right as needed. This is called implicit coercion.\nAs an example, consider what happens if we add a logical value to a number:\n\nTRUE + 2\n\n[1] 3\n\n\nR automatically converts the TRUE to the numeric value 1, and then carries out the arithmetic as usual.\nWe’ve already seen implicit coercion at work once before, when we learned the c function. Since the elements of a vector all have to have the same type, if you pass several different types to c, then R tries to use implicit coercion to make them the same:\n\nx = c(TRUE, \"hi\", 1, 1+3i)\nclass(x)\n\n[1] \"character\"\n\nx\n\n[1] \"TRUE\" \"hi\"   \"1\"    \"1+3i\"\n\n\nImplicit coercion is strictly one-way; it never occurs in the other direction. If you want to coerce a type on the right to one on the left, you can do it explicitly with one of the as.TYPE functions. For instance, the as.numeric (or as.double) function coerces to numeric:\n\nas.numeric(\"3.1\")\n\n[1] 3.1\n\n\nThere are a few types that fall outside of the hierarchy entirely, like functions. Implicit coercion doesn’t apply to these. If you try to use these types where it doesn’t make sense to, R generally returns an error:\n\nsin + 3\n\nError in sin + 3: non-numeric argument to binary operator\n\n\nIf you try to use these types as elements of a vector, you get back a list instead:\n\nx = c(1, 2, sum)\nclass(x)\n\n[1] \"list\"\n\n\nUnderstanding how implicit coercion works will help you avoid bugs, and can also be a time-saver. For example, we can use implicit coercion to succinctly count how many elements of a vector satisfy a some condition:\n\nx = c(1, 3, -1, 10, -2, 3, 8, 2)\ncondition = x &lt; 4\nsum(condition)    # or sum(x &lt; 4)\n\n[1] 6\n\n\nIf you still don’t quite understand how the code above works, try inspecting each variable. In general, inspecting each step or variable is a good strategy for understanding why a piece of code works (or doesn’t work!). Here the implicit coercion happens in the third line.\n\n\n13.2.3 Matrices & Arrays\nA matrix is the two-dimensional analogue of a vector. The elements, which are arranged into rows and columns, are ordered and homogeneous.\nYou can create a matrix from a vector with the matrix function. By default, the columns are filled first:\n\n# A matrix with 2 rows and 3 columns:\nmatrix(1:6, 2, 3)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nThe class of a matrix is always matrix, and the type matches the type of the elements:\n\nx = matrix(c(\"a\", \"b\", NA, \"c\"), 2, 2)\nx\n\n     [,1] [,2]\n[1,] \"a\"  NA  \n[2,] \"b\"  \"c\" \n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\ntypeof(x)\n\n[1] \"character\"\n\n\nYou can use the matrix multiplication operator %*% to multiply two matrices with compatible dimensions.\nAn array is a further generalization of matrices to higher dimensions. You can create an array from a vector with the array function. The characteristics of arrays are almost identical to matrices, but the class of an array is always array.\n\n\n13.2.4 Factors\nA feature is categorical if it measures a qualitative category. For example, the genres rock, blues, alternative, folk, pop are categories.\nR uses the class factor to represent categorical data. Visualizations and statistical models sometimes treat factors differently than other data types, so it’s important to make sure you have the right data type. If you’re ever unsure, remember that you can check the class of an object with the class function.\nWhen it reads a data set, R usually can’t tell which features are categorical. That means identifying and converting the categorical features is up to you. For beginners, it can be difficult to understand whether a feature is categorical or not. The key is to think about whether you want to use the feature to divide the data into groups.\nFor example, if you want to know how many songs are in the rock genre, you first need to divide the songs by genre, and then count the number of songs in each group (or at least the rock group).\nAs a second example, months recorded as numbers can be categorical or not, depending on how you want to use them. You might want to treat them as categorical (for example, to compute max rainfall in each month) or you might want to treat them as numbers (for example, to compute the number of months time between two events).\nThe bottom line is that you have to think about what you’ll be doing in the analysis. In some cases, you might treat a feature as categorical only for part of the analysis.\nLet’s think about which features are categorical in least terns data set. To refresh your memory of what’s in the data set, take a look at the structural summary:\n\nstr(terns)\n\n'data.frame':   791 obs. of  43 variables:\n $ year               : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ site_name          : chr  \"PITTSBURG POWER PLANT\" \"ALBANY CENTRAL AVE\" \"ALAMEDA POINT\" \"KETTLEMAN CITY\" ...\n $ site_name_2013_2018: chr  \"Pittsburg Power Plant\" \"NA_NO POLYGON\" \"Alameda Point\" \"Kettleman\" ...\n $ site_name_1988_2001: chr  \"NA_2013_2018 POLYGON\" \"Albany Central Avenue\" \"NA_2013_2018 POLYGON\" \"NA_2013_2018 POLYGON\" ...\n $ site_abbr          : chr  \"PITT_POWER\" \"AL_CENTAVE\" \"ALAM_PT\" \"KET_CTY\" ...\n $ region_3           : chr  \"S.F._BAY\" \"S.F._BAY\" \"S.F._BAY\" \"KINGS\" ...\n $ region_4           : chr  \"S.F._BAY\" \"S.F._BAY\" \"S.F._BAY\" \"KINGS\" ...\n $ event              : chr  \"LA_NINA\" \"LA_NINA\" \"LA_NINA\" \"LA_NINA\" ...\n $ bp_min             : num  15 6 282 2 4 9 30 21 73 166 ...\n $ bp_max             : num  15 12 301 3 5 9 32 21 73 167 ...\n $ fl_min             : int  16 1 200 1 4 17 11 9 60 64 ...\n $ fl_max             : int  18 1 230 2 4 17 11 9 65 64 ...\n $ total_nests        : int  15 20 312 3 5 9 32 22 73 252 ...\n $ nonpred_eggs       : int  3 NA 124 NA 2 0 NA 4 2 NA ...\n $ nonpred_chicks     : int  0 NA 81 3 0 1 27 3 0 NA ...\n $ nonpred_fl         : int  0 NA 2 1 0 0 0 NA 0 NA ...\n $ nonpred_ad         : int  0 NA 1 6 0 0 0 NA 0 NA ...\n $ pred_control       : chr  \"\" \"\" \"\" \"\" ...\n $ pred_eggs          : int  4 NA 17 NA 0 NA 0 NA NA NA ...\n $ pred_chicks        : int  2 NA 0 NA 4 NA 3 NA NA NA ...\n $ pred_fl            : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ pred_ad            : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ pred_pefa          : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_coy_fox       : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_meso          : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_owlspp        : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_corvid        : chr  \"Y\" \"\" \"N\" \"\" ...\n $ pred_other_raptor  : chr  \"Y\" \"\" \"Y\" \"\" ...\n $ pred_other_avian   : chr  \"N\" \"\" \"Y\" \"\" ...\n $ pred_misc          : chr  \"N\" \"\" \"N\" \"\" ...\n $ total_pefa         : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_coy_fox      : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_meso         : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_owlspp       : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_corvid       : int  4 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_other_raptor : int  2 NA 6 NA 0 NA 3 NA NA NA ...\n $ total_other_avian  : int  0 NA 11 NA 4 NA 0 NA NA NA ...\n $ total_misc         : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ first_observed     : chr  \"2000-05-11\" \"\" \"2000-05-01\" \"2000-06-10\" ...\n $ last_observed      : chr  \"2000-08-05\" \"\" \"2000-08-19\" \"2000-09-24\" ...\n $ first_nest         : chr  \"2000-05-26\" \"\" \"2000-05-16\" \"2000-06-17\" ...\n $ first_chick        : chr  \"2000-06-18\" \"\" \"2000-06-07\" \"2000-07-22\" ...\n $ first_fledge       : chr  \"2000-07-08\" \"\" \"2000-06-30\" \"2000-08-06\" ...\n\n\nThe site_name, site_abbr, and event columns are all examples of categorical data. The region_ columns and some of the pred_ columns also contain categorical data.\nOne way to check whether a feature is useful for grouping (and thus effectively categorical) is to count the number of times each value appears. You can do this with the table function. For instance, to count the number of times each category of event appears:\n\ntable(terns$event)\n\n\nEL_NINO LA_NINA NEUTRAL \n    120     258     413 \n\n\nFeatures with only a few unique values, repeated many times, are ideal for grouping. Numerical features, like total_nests, usually aren’t good for grouping, both because of what they measure and because they tend to have many unique values, which leads to very small groups.\nThe year column can be treated as categorical or quantitative data. It’s easy to imagine grouping observations by year, but years are also numerical: they have an order and we might want to do math on them. The most appropriate type for year depends on how we want to use it for analysis.\nYou can convert a column to the factor class with the factor function. Try this for the event column:\n\nevent = factor(terns$event)\nevent\n\n  [1] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n [10] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n [19] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n [28] LA_NINA LA_NINA NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n [37] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n [46] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n [55] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n [64] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n [73] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n [82] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n [91] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[100] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[109] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[118] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[127] NEUTRAL EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[136] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[145] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[154] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[163] EL_NINO EL_NINO LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[172] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[181] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[190] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[199] LA_NINA LA_NINA LA_NINA LA_NINA NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[208] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[217] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[226] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[235] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL EL_NINO\n[244] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[253] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[262] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[271] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[280] EL_NINO EL_NINO EL_NINO LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[289] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[298] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[307] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[316] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[325] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[334] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[343] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[352] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[361] LA_NINA LA_NINA LA_NINA LA_NINA NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[370] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[379] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[388] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[397] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[406] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[415] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[424] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[433] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[442] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[451] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[460] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[469] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[478] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[487] NEUTRAL NEUTRAL NEUTRAL NEUTRAL EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[496] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[505] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[514] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[523] EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO EL_NINO\n[532] EL_NINO EL_NINO NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[541] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[550] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[559] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[568] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[577] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[586] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[595] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[604] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[613] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[622] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[631] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[640] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[649] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[658] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[667] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL\n[676] NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL NEUTRAL LA_NINA LA_NINA\n[685] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[694] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[703] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[712] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[721] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[730] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[739] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[748] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[757] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[766] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[775] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\n[784] LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA LA_NINA\nLevels: EL_NINO LA_NINA NEUTRAL\n\n\nNotice that factors are printed differently than strings.\nThe categories of a factor are called levels. You can list the levels with the levels function:\n\nlevels(event)\n\nA factor remembers all possible levels even if you take a subset where some of the levels aren’t present:\n\nevent[1:3]\n\n[1] LA_NINA LA_NINA LA_NINA\nLevels: EL_NINO LA_NINA NEUTRAL\n\nlevels(event[1:3])\n\n[1] \"EL_NINO\" \"LA_NINA\" \"NEUTRAL\"\n\n\nThis is an important way factors are different from strings (or character vectors). It ensures that if you plot a factor, the missing levels will still be represented on the plot.\n\n\n\n\n\n\nTip\n\n\n\nYou can make a factor forget levels that aren’t present with the droplevels function:\n\ndroplevels(event[1:3])\n\n[1] LA_NINA LA_NINA LA_NINA\nLevels: LA_NINA",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "chapters/week03/data-types.html#special-values",
    "href": "chapters/week03/data-types.html#special-values",
    "title": "13  Data Types",
    "section": "13.3 Special Values",
    "text": "13.3 Special Values\nR has four special values to represent missing or invalid data.\n\n13.3.1 Missing Values\nThe value NA, called the missing value, represents missing entries in a data set. It’s implied that the entries are missing due to how the data was collected, although there are exceptions. As an example, imagine the data came from a survey, and respondents chose not to answer some questions. In the data set, their answers for those questions can be recorded as NA.\nThe missing value is a chameleon: it can be a logical, integer, numeric, complex, or character value. By default, the missing value is logical, and the other types occur through coercion (Section 13.2.2):\n\nclass(NA)\n\n[1] \"logical\"\n\nclass(c(1, NA))\n\n[1] \"numeric\"\n\nclass(c(\"hi\", NA, NA))\n\n[1] \"character\"\n\n\nThe missing value is also contagious: it represents an unknown quantity, so using it as an argument to a function usually produces another missing value. The idea is that if the inputs to a computation are unknown, generally so is the output:\n\nNA - 3\n\n[1] NA\n\nmean(c(1, 2, NA))\n\n[1] NA\n\n\nAs a consequence, testing whether an object is equal to the missing value with == doesn’t return a meaningful result:\n\n5 == NA\n\n[1] NA\n\nNA == NA\n\n[1] NA\n\n\nYou can use the is.na function instead:\n\nis.na(5)\n\n[1] FALSE\n\nis.na(NA)\n\n[1] TRUE\n\nis.na(c(1, NA, 3))\n\n[1] FALSE  TRUE FALSE\n\n\nMissing values are a feature that sets R apart from most other programming languages.\n\n\n13.3.2 Infinity\nThe value Inf represents infinity, and can be numeric or complex. You’re most likely to encounter it as the result of certain computations:\n\n13 / 0\n\n[1] Inf\n\nclass(Inf)\n\n[1] \"numeric\"\n\n\nYou can use the is.infinite function to test whether a value is infinite:\n\nis.infinite(3)\n\n[1] FALSE\n\nis.infinite(c(-Inf, 0, Inf))\n\n[1]  TRUE FALSE  TRUE\n\n\n\n\n13.3.3 Not a Number\nThe value NaN (“not a number”) represents a quantity that’s undefined mathematically. For instance, dividing 0 by 0 is undefined:\n\n0 / 0\n\n[1] NaN\n\nclass(NaN)\n\n[1] \"numeric\"\n\n\nLike Inf, NaN can be numeric or complex.\nYou can use the is.nan function to test whether a value is NaN:\n\nis.nan(c(10.1, log(-1), 3))\n\nWarning in log(-1): NaNs produced\n\n\n[1] FALSE  TRUE FALSE\n\n\n\n\n13.3.4 Null\nThe value NULL represents a quantity that’s undefined in R. Most of the time, NULL indicates the absence of a result. For instance, vectors don’t have dimensions, so the dim function returns NULL for vectors:\n\ndim(c(1, 2))\n\nNULL\n\nclass(NULL)\n\n[1] \"NULL\"\n\ntypeof(NULL)\n\n[1] \"NULL\"\n\n\nUnlike the other special values, NULL has its own unique type and class.\nYou can use the is.null function to test whether a value is NULL:\n\nis.null(\"null\")\n\n[1] FALSE\n\nis.null(NULL)\n\n[1] TRUE",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "chapters/week04/packages.html",
    "href": "chapters/week04/packages.html",
    "title": "14  Packages",
    "section": "",
    "text": "14.1 Installing Packages\nA package is a collection of functions for use in R. Packages usually include documentation, and can also contain examples, vignettes, and data sets. Most packages are developed by members of the R community, so quality varies. There are also a few packages that are built into R but provide extra features. We’ll use packages in many of the following chapters, so we’re learning about them now.\nThe Comprehensive R Archive Network, or CRAN, is the main place people publish packages. As of writing, there were 23,094 packages posted to CRAN. This number has been steadily increasing as R has grown in popularity.\nPackages span a wide variety of topics and disciplines. There are packages related to statistics, social sciences, geography, genetics, physics, biology, pharmacology, economics, agriculture, and more. The best way to find packages is to search online, but the CRAN website also provides “task views” if you want to browse popular packages related to a specific discipline.\nThe install.packages function installs one or more packages from CRAN. Its first argument is a name or vector of names of packages to install.\nWhen you run install.packages, R will ask you to choose which mirror to download the package from. A mirror is a web server that has the same set of files as some other server. Mirrors are used to make downloads faster and to provide redundancy so that if a server stops working, files are still available somewhere else. CRAN has dozens of mirrors; you should choose one that’s geographically nearby, since that usually produces the best download speeds. If you aren’t sure which mirror to choose, you can use the 0-Cloud mirror, which attempts to automatically choose a mirror near you.\nAs an example, here’s the code to install the remotes package:\ninstall.packages(\"remotes\")\nIf you run the code above, you’ll be asked to select a mirror, and then see output that looks something like this:\nR goes through a variety of steps to install a package, even installing other packages that the package depends on. You can tell that a package installation succeeded by the final line DONE. When a package installation fails, R prints an error message explaining the problem instead.\nMost packages are periodically updated. You can update specific packages to the latest versions by reinstalling them with install.packages. Alternatively, you can update all of the R packages you’ve installed at once by calling the update.packages function. Beware that this may take a long time if you have a lot of packages installed.\nThe function to remove packages is remove.packages. Like install.packages, this function’s first argument is the packages to remove, as a character vector.\nIf you want to see which packages are installed, you can use the installed.packages function. It does not require any arguments. It returns a matrix with one row for each package and columns that contain a variety of information. Here’s an example:\npackages = installed.packages()\n# Just print the version numbers for 10 packages.\npackages[1:10, \"Version\"]\n\n   askpass assertthat       base  base64enc        bit      bit64       boot \n   \"1.2.1\"    \"0.2.1\"    \"4.5.2\"    \"0.1-3\"    \"4.6.0\"  \"4.6.0-1\"   \"1.3-32\" \n     bslib     cachem cellranger \n   \"0.9.0\"    \"1.1.0\"    \"1.1.0\"\nYou’ll see a different set of packages, since you have a different computer.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapters/week04/packages.html#installing-packages",
    "href": "chapters/week04/packages.html#installing-packages",
    "title": "14  Packages",
    "section": "",
    "text": "--- Please select a CRAN mirror for use in this session ---\ntrying URL 'https://cloud.r-project.org/src/contrib/remotes_2.5.0.tar.gz'\nContent type 'application/x-gzip' length 168405 bytes (164 KB)\n==================================================\ndownloaded 164 KB\n\n* installing *source* package ‘remotes’ ...\n** package ‘remotes’ successfully unpacked and MD5 sums checked\n** using staged installation\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (remotes)\n\nThe downloaded source packages are in\n        ‘/tmp/Rtmp8t6iGa/downloaded_packages’\n\n\n\n\n\n\n\nImportant\n\n\n\nOnce a package is installed, it stays on your computer until you remove it or remove R. This means you only need to install each package once!\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNot all R packages are published to CRAN. GitHub is another popular place to publish R packages, especially ones that are experimental or still in development. Unlike CRAN, GitHub is a general-purpose website for publishing code written in any programming language, so it contains much more than just R packages and is not specifically R-focused.\nThe remotes package that we just installed provides functions to install packages from GitHub. It is generally better to install packages from CRAN when they are available there, since the versions on CRAN tend to be more stable and intended for a wide audience. However, if you want to install a package from GitHub, you can learn more about the remotes package by reading its online documentation.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapters/week04/packages.html#loading-packages",
    "href": "chapters/week04/packages.html#loading-packages",
    "title": "14  Packages",
    "section": "14.2 Loading Packages",
    "text": "14.2 Loading Packages\nBefore you can use the functions (or other resources) in an installed package, you must load the package with the library function. R doesn’t load packages automatically because each package you load uses memory and may conflict with other packages. Thus you should only load the packages you need for whatever it is that you want to do. When you restart R, the loaded packages are cleared and you must again load any packages you want to use.\nLet’s load the remotes package we installed earlier:\n\nlibrary(\"remotes\")\n\nThe library function works with or without quotes around the package name, so you may also see people write things like library(remotes). We recommend using quotes to make it unambiguous that you are not referring to a variable.\nA handful of packages print out a message when loaded, but the vast majority do not. Thus you can assume the call to library was successful if nothing is printed. If something goes wrong while loading a package, R will print out an error message explaining the problem.\nOccasionally, you might need to check the names and versions of the packages loaded in your R session. For instance, if you go to another R programmer for help, they’ll probably ask. You can make R print out information about the session, including loaded packages, with the sessionInfo function:\nsessionInfo()\nR version 4.5.2 (2025-10-31)\nPlatform: x86_64-conda-linux-gnu\nRunning under: Arch Linux\n\nMatrix products: default\nBLAS/LAPACK: /home/nick/mill/datalab/teaching/adventures_in_data_science/.pixi/envs/default/lib/libopenblasp-r0.3.30.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C\n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8\n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8\n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C\n [9] LC_ADDRESS=C               LC_TELEPHONE=C\n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C\n\ntime zone: US/Pacific\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base\n\nother attached packages:\n[1] remotes_2.5.0\n\nloaded via a namespace (and not attached):\n[1] compiler_4.5.2 tools_4.5.2\nThe output from sessionInfo will be different on your computer, depending on your computer hardware, operating system, default language, time zone, version of R, and loaded packages.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapters/week04/packages.html#sec-the-tidyverse",
    "href": "chapters/week04/packages.html#sec-the-tidyverse",
    "title": "14  Packages",
    "section": "14.3 The Tidyverse",
    "text": "14.3 The Tidyverse\nFor cleaning and analyzing data, we recommend the Tidyverse, a popular collection of packages for doing data science. Compared to R’s built-in functions, we’ve found that the functions in Tidyverse packages are generally easier to learn and use. They also provide additional features, such as robust handling of incorrectly-formatted files and better support for characters outside of the Latin alphabet.\nAlthough they’re developed by many different members of the R community, Tidyverse packages follow a unified design philosophy, and thus have many interfaces and data structures in common. The packages provide convenient and efficient alternatives to built-in R functions for many tasks, including:\n\nReading and writing files (package readr)\nProcessing dates and times (packages lubridate, hms)\nProcessing strings (package stringr)\nReshaping data (package tidyr)\nMaking visualizations (package ggplot2)\nAnd more\n\nThink of the Tidyverse as a different dialect of R. Sometimes the syntax is different, and sometimes ideas are easier or harder to express concisely. As a consequence, the Tidyverse is sometimes polarizing in the R community. It’s useful to be literate in both base R and the Tidyverse, since both are popular.\nOne major advantage of the Tidyverse is that the packages are usually well-documented and provide lots of examples. Every package has a documentation website and the most popular ones also have cheatsheets.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "chapters/week04/indexing.html",
    "href": "chapters/week04/indexing.html",
    "title": "15  Indexing",
    "section": "",
    "text": "15.1 More Than One Dimension\nIndexing refers to getting and setting the elements of a data structure. Indexing is a fundamental operation in R, key to reasoning about how to solve problems with the language. For tabular data, two kinds of indexing—selecting columns and filtering rows—are the foundation upon which all analyses rest. This chapter is an introduction to R’s many operators and functions for indexing.\nSection 13.1.2 introduced the indexing (or square bracket) operator [ as a way to get and set the elements of a vector. You can also use the indexing operator with many other data structures, such as data frames, lists, matrices, and arrays.\nConsider data frames. A data frame is two-dimensional: it has rows and columns. In order to describe the exact position of an element in a data frame, we need to specify both a row index and a column index. You can still use the indexing operator [ to get and set elements, but now it takes two arguments:\nFor example, to get the value at row 1 and column 2 in the California least terns data (Section 11.4), you can write:\nterns[1, 2]\n\n[1] \"PITTSBURG POWER PLANT\"\nIf you swap the order of the indexes, you get the value at row 2 and column 1 instead:\nterns[2, 1]\n\n[1] 2000\nAs with indexing vectors, when indexing a data frame you can also:\nLet’s look at examples of the first two. Suppose we want to get elements from multiple rows and columns. Here’s the code to get elements from the first 3 rows in columns 2 and 7 of the least terns data:\nterns[1:3, c(2, 7)]\n\n              site_name region_4\n1 PITTSBURG POWER PLANT S.F._BAY\n2    ALBANY CENTRAL AVE S.F._BAY\n3         ALAMEDA POINT S.F._BAY\nNow suppose we want to get the elements from rows 3, 5, and 7 in all columns. Then you can write:\nterns[c(3, 5, 7), ]\n\n  year                                    site_name\n3 2000                                ALAMEDA POINT\n5 2000 OCEANO DUNES STATE VEHICULAR RECREATION AREA\n7 2000                               VANDENBERG SFB\n                           site_name_2013_2018  site_name_1988_2001\n3                                Alameda Point NA_2013_2018 POLYGON\n5 Oceano Dunes State Vehicular Recreation Area NA_2013_2018 POLYGON\n7                               Vandenberg AFB NA_2013_2018 POLYGON\n     site_abbr region_3 region_4   event bp_min bp_max fl_min fl_max\n3      ALAM_PT S.F._BAY S.F._BAY LA_NINA    282    301    200    230\n5 OCEANO_DUNES  CENTRAL  CENTRAL LA_NINA      4      5      4      4\n7      VAN_SFB  CENTRAL  CENTRAL LA_NINA     30     32     11     11\n  total_nests nonpred_eggs nonpred_chicks nonpred_fl nonpred_ad pred_control\n3         312          124             81          2          1             \n5           5            2              0          0          0             \n7          32           NA             27          0          0             \n  pred_eggs pred_chicks pred_fl pred_ad pred_pefa pred_coy_fox pred_meso\n3        17           0       0       0         N            N         N\n5         0           4       0       0         N            N         N\n7         0           3       0       0         N            N         N\n  pred_owlspp pred_corvid pred_other_raptor pred_other_avian pred_misc\n3           N           N                 Y                Y         N\n5           N           N                 N                Y         N\n7           N           N                 Y                N         N\n  total_pefa total_coy_fox total_meso total_owlspp total_corvid\n3          0             0          0            0            0\n5          0             0          0            0            0\n7          0             0          0            0            0\n  total_other_raptor total_other_avian total_misc first_observed last_observed\n3                  6                11          0     2000-05-01    2000-08-19\n5                  0                 4          0     2000-05-04    2000-08-30\n7                  3                 0          0     2000-05-07    2000-08-17\n  first_nest first_chick first_fledge\n3 2000-05-16  2000-06-07   2000-06-30\n5 2000-05-28  2000-06-20   2000-07-13\n7 2000-05-28  2000-06-20   2000-07-15\nBy leaving the column index blank, you’re telling R to select all of the columns. Likewise, you can leave the row index blank to select all of the rows.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Indexing</span>"
    ]
  },
  {
    "objectID": "chapters/week04/indexing.html#more-than-one-dimension",
    "href": "chapters/week04/indexing.html#more-than-one-dimension",
    "title": "15  Indexing",
    "section": "",
    "text": "DATA[ROW_INDEXES, COLUMN_INDEXES]\n\n\n\n\n\n\nUse a vector as an index to get multiple elements.\nLeave an index blank to get everything in that dimension.\nUse negative indexes to omit rows or columns.\nCombine indexing with assignment to set elements.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also use the indexing operator [ with data structures that have more than two dimensons. Maybe you can guess the syntax:\nDATA[DIM1_INDEXES, DIM2_INDEXES, DIM3_INDEXES, ...]\nYou won’t encounter any data structures with more than two dimensions in this course, but you might someday.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Indexing</span>"
    ]
  },
  {
    "objectID": "chapters/week04/indexing.html#selecting-columns",
    "href": "chapters/week04/indexing.html#selecting-columns",
    "title": "15  Indexing",
    "section": "15.2 Selecting Columns",
    "text": "15.2 Selecting Columns\nIn order to work with tabular data, you need to know how to select (or get) columns. Most of the time, it’s best to select columns by name. Doing so makes code easier to understand, since the reader doesn’t have to memorize the positions of the columns. It can also make code more reliable, since the code will still work even if the positions of the columns change.\nThis section explains several different ways to select columns and how to choose which one to use in any given situation. Generally, the first thing to think about is whether you want to select just one column or multiple columns.\n\n15.2.1 One Column\nThe dollar sign operator $, introduced in Section 12.9, is one way to select a single column from a data frame. For example, to select the total_nests column from the least terns dataset:\n\nterns$total_nests\n\n  [1]   15   20  312    3    5    9   32   22   73  252  308  565  107   56  481\n [16]   68 1079  172   16  187   26  420   27  134  629   34    0   44  210   15\n [31]    0  440    2   63    8    1    6   83   50   29  617   17 1071  206  229\n [46]  323    5   89 1430  592    0  315   17  299   42   76  172 1035    1  111\n [61]   66   49  520    4  550    8    1   59    4   44    0    9   27  608   90\n [76] 1332  145  135  339   28 1664  596    1    6    0  281  118  157  134 1135\n [91]  101   57   34  458   17    0  441   15    2   38    0    2    5   NA   53\n[106]  469  384  907  186  222  521   36 1540  627    0  104   30  120  136   14\n[121]  131  180 1425  100   15   82  371    2   32    7  394   35    6    2   66\n[136]    1   18    6   77    1   52  431  546  710  166  226  485    9   42 1530\n[151]  594    0   28   39  105   45   30  135  123 1162  130   46   97  291    1\n[166]   18   35    1  336   62    2    1   56    0   18    1   97   24   81  506\n[181]  928  529  206  242  454   10   25 1665  610    0    0   10   14    1    1\n[196]  139  146 1525  148   33  102  201    1   52   27    0  346   80    1    1\n[211]   26    3   31    0   85    4   44  628  295  435  177  317  434   16   10\n[226] 1718  649    0    2   38   45   14    0    0  145  125 1741  132   48   78\n[241]  294    3    1   47   17    0  320   53    0    1   23    1   34    0   36\n[256]    1   48  708  164  216  265  446  433    8   13 1722  480    0    0    0\n[271]   17   25  117    1    9  116   84 1115  119   40   65  223    1   38   15\n[286]    0  355   77    0    0   35    0   32    1   26    0   60  717   28   10\n[301]  177  167  712   14    6 1526  532    0   NA    0   21    1  145   12   13\n[316]   78   83 1062  116   53   55  264    3    2   35   30    3  382  189    0\n[331]    0   46    0   18    0   42    1    6  844   14  211  121  305  542   11\n[346]   20 1245  563    0    0    0   49    1  142   17   12  130   10 1068  114\n[361]   64   90  283    0    0   80   29    0  292   85    0    0   57    0   15\n[376]    0   37  209    7  346    0   15  254  164  157  347   23   32 1243  558\n[391]    0    0    3  156    0   37   41    0  114    7 1034  144   79   45  282\n[406]    2    0   45   16    0  326   85    0    0   49    0   21    0    4  120\n[421]   22  465    0   81  126  154  301  516   18    2 1337  478    0    0    0\n[436]    8   13  119    2    0  100   57 1039  148   87   35  261    3    1   79\n[451]   16    3  351   71    0    0   54    0   22    0   72   24    0  473    0\n[466]    8  109  106  204  524   23   22 1384  415    0    0    0   20   24  181\n[481]    1    0   18   33  826  123   79   29  208    1    1   79    6    1  403\n[496]   88    0    0   49    0   27    0   62    0   18  361    0    2  141   80\n[511]  142  348    4   10   20  897  451    0    0    0   50   25  127   15    5\n[526]   37   34  859  118   76   26  184    0    1  142    9   NA  447   72   21\n[541]    0   52    0    0   28    0    9    0   38  377    0    0   31    5  124\n[556]  178  679    0   14   17 1405  692    0   NA    0   41    0  176    2   NA\n[571]   24   22  889  127   96   40  225    0   28   18  375   32  141   35   83\n[586]   24    0   84  291    0    5    6  133  117  133  688   16   22  334  684\n[601]    0   12    1  178   11   19   16  827  109   83   33  191    3   15  345\n[616]   56  121   34   17   47   NA    0   92  315    0    6    0  199   56  134\n[631]  531   17   20  418  548    0   39    2  159    1   19    4  928  104   66\n[646]   39  150    3   29  435  101    1   48   56   12   39   21   23  205   NA\n[661]   46    0  182   11   91  429   14   18  375  139    0   37    1  158    0\n[676]    6    6 1153   62  126   45  203    3   79   23  420  113   56   53    0\n[691]   34   91    0   27  173   NA   37    0  198   51  168  563   20   10  628\n[706]  297    0   75    1  133    0   11    2 1140  108  101   36  186   54   78\n[721]    7  425   61   45   45    1   45   51   26   34  313    3    0  189   56\n[736]  172  393   19    2  474  254    0   29   98    0  107    0   10    0  805\n[751]   77   82   24  151   46    9   31  339  144   77   42    0   42   30    0\n[766]   19  188    0    0   56   71  114  290    1    9    4  552  228    0   47\n[781]  156    1    6    0    8    0  717   44   59   48  171\n\n\nThe dollar sign operator always selects the column by name (not position).\nAnother way to select a single column is with the extraction operator [[ (also called the double square bracket operator). The column name must be quoted and goes inside the brackets. For instance:\n\nterns[[\"total_nests\"]]\n\n  [1]   15   20  312    3    5    9   32   22   73  252  308  565  107   56  481\n [16]   68 1079  172   16  187   26  420   27  134  629   34    0   44  210   15\n [31]    0  440    2   63    8    1    6   83   50   29  617   17 1071  206  229\n [46]  323    5   89 1430  592    0  315   17  299   42   76  172 1035    1  111\n [61]   66   49  520    4  550    8    1   59    4   44    0    9   27  608   90\n [76] 1332  145  135  339   28 1664  596    1    6    0  281  118  157  134 1135\n [91]  101   57   34  458   17    0  441   15    2   38    0    2    5   NA   53\n[106]  469  384  907  186  222  521   36 1540  627    0  104   30  120  136   14\n[121]  131  180 1425  100   15   82  371    2   32    7  394   35    6    2   66\n[136]    1   18    6   77    1   52  431  546  710  166  226  485    9   42 1530\n[151]  594    0   28   39  105   45   30  135  123 1162  130   46   97  291    1\n[166]   18   35    1  336   62    2    1   56    0   18    1   97   24   81  506\n[181]  928  529  206  242  454   10   25 1665  610    0    0   10   14    1    1\n[196]  139  146 1525  148   33  102  201    1   52   27    0  346   80    1    1\n[211]   26    3   31    0   85    4   44  628  295  435  177  317  434   16   10\n[226] 1718  649    0    2   38   45   14    0    0  145  125 1741  132   48   78\n[241]  294    3    1   47   17    0  320   53    0    1   23    1   34    0   36\n[256]    1   48  708  164  216  265  446  433    8   13 1722  480    0    0    0\n[271]   17   25  117    1    9  116   84 1115  119   40   65  223    1   38   15\n[286]    0  355   77    0    0   35    0   32    1   26    0   60  717   28   10\n[301]  177  167  712   14    6 1526  532    0   NA    0   21    1  145   12   13\n[316]   78   83 1062  116   53   55  264    3    2   35   30    3  382  189    0\n[331]    0   46    0   18    0   42    1    6  844   14  211  121  305  542   11\n[346]   20 1245  563    0    0    0   49    1  142   17   12  130   10 1068  114\n[361]   64   90  283    0    0   80   29    0  292   85    0    0   57    0   15\n[376]    0   37  209    7  346    0   15  254  164  157  347   23   32 1243  558\n[391]    0    0    3  156    0   37   41    0  114    7 1034  144   79   45  282\n[406]    2    0   45   16    0  326   85    0    0   49    0   21    0    4  120\n[421]   22  465    0   81  126  154  301  516   18    2 1337  478    0    0    0\n[436]    8   13  119    2    0  100   57 1039  148   87   35  261    3    1   79\n[451]   16    3  351   71    0    0   54    0   22    0   72   24    0  473    0\n[466]    8  109  106  204  524   23   22 1384  415    0    0    0   20   24  181\n[481]    1    0   18   33  826  123   79   29  208    1    1   79    6    1  403\n[496]   88    0    0   49    0   27    0   62    0   18  361    0    2  141   80\n[511]  142  348    4   10   20  897  451    0    0    0   50   25  127   15    5\n[526]   37   34  859  118   76   26  184    0    1  142    9   NA  447   72   21\n[541]    0   52    0    0   28    0    9    0   38  377    0    0   31    5  124\n[556]  178  679    0   14   17 1405  692    0   NA    0   41    0  176    2   NA\n[571]   24   22  889  127   96   40  225    0   28   18  375   32  141   35   83\n[586]   24    0   84  291    0    5    6  133  117  133  688   16   22  334  684\n[601]    0   12    1  178   11   19   16  827  109   83   33  191    3   15  345\n[616]   56  121   34   17   47   NA    0   92  315    0    6    0  199   56  134\n[631]  531   17   20  418  548    0   39    2  159    1   19    4  928  104   66\n[646]   39  150    3   29  435  101    1   48   56   12   39   21   23  205   NA\n[661]   46    0  182   11   91  429   14   18  375  139    0   37    1  158    0\n[676]    6    6 1153   62  126   45  203    3   79   23  420  113   56   53    0\n[691]   34   91    0   27  173   NA   37    0  198   51  168  563   20   10  628\n[706]  297    0   75    1  133    0   11    2 1140  108  101   36  186   54   78\n[721]    7  425   61   45   45    1   45   51   26   34  313    3    0  189   56\n[736]  172  393   19    2  474  254    0   29   98    0  107    0   10    0  805\n[751]   77   82   24  151   46    9   31  339  144   77   42    0   42   30    0\n[766]   19  188    0    0   56   71  114  290    1    9    4  552  228    0   47\n[781]  156    1    6    0    8    0  717   44   59   48  171\n\n\nThe double square bracket operator is helpful if you want to select a column with a name you’ve stored in a variable:\n\nmy_column = \"site_name\"\nterns[[my_column]]\n\n  [1] \"PITTSBURG POWER PLANT\"                       \n  [2] \"ALBANY CENTRAL AVE\"                          \n  [3] \"ALAMEDA POINT\"                               \n  [4] \"KETTLEMAN CITY\"                              \n  [5] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n  [6] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n  [7] \"VANDENBERG SFB\"                              \n  [8] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n  [9] \"ORMOND BEACH\"                                \n [10] \"NBVC POINT MUGU\"                             \n [11] \"VENICE BEACH\"                                \n [12] \"LA HARBOR\"                                   \n [13] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n [14] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n [15] \"HUNTINGTON STATE BEACH\"                      \n [16] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n [17] \"MCB CAMP PENDLETON\"                          \n [18] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n [19] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n [20] \"MISSION BAY FAA ISLAND\"                      \n [21] \"MISSION BAY NORTH FIESTA ISLAND\"             \n [22] \"MISSION BAY MARINERS POINT\"                  \n [23] \"SDIA LINDBERGH FIELD\"                        \n [24] \"NAS NORTH ISLAND\"                            \n [25] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n [26] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n [27] \"CHULA VISTA WILDLIFE RESERVE\"                \n [28] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n [29] \"TIJUANA ESTUARY NERR\"                        \n [30] \"PITTSBURG POWER PLANT\"                       \n [31] \"ALBANY CENTRAL AVE\"                          \n [32] \"ALAMEDA POINT\"                               \n [33] \"KETTLEMAN CITY\"                              \n [34] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n [35] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n [36] \"VANDENBERG SFB\"                              \n [37] \"COAL OIL POINT RESERVE\"                      \n [38] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n [39] \"HOLLYWOOD BEACH\"                             \n [40] \"ORMOND BEACH\"                                \n [41] \"NBVC POINT MUGU\"                             \n [42] \"VENICE BEACH\"                                \n [43] \"LA HARBOR\"                                   \n [44] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n [45] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n [46] \"HUNTINGTON STATE BEACH\"                      \n [47] \"BURRIS ISLAND\"                               \n [48] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n [49] \"MCB CAMP PENDLETON\"                          \n [50] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n [51] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n [52] \"MISSION BAY FAA ISLAND\"                      \n [53] \"MISSION BAY NORTH FIESTA ISLAND\"             \n [54] \"MISSION BAY MARINERS POINT\"                  \n [55] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n [56] \"SDIA LINDBERGH FIELD\"                        \n [57] \"NAS NORTH ISLAND\"                            \n [58] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n [59] \"SILVER STRAND STATE BEACH\"                   \n [60] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n [61] \"CHULA VISTA WILDLIFE RESERVE\"                \n [62] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n [63] \"TIJUANA ESTUARY NERR\"                        \n [64] \"PITTSBURG POWER PLANT\"                       \n [65] \"ALAMEDA POINT\"                               \n [66] \"HAYWARD REGIONAL SHORELINE\"                  \n [67] \"KETTLEMAN CITY\"                              \n [68] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n [69] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n [70] \"VANDENBERG SFB\"                              \n [71] \"COAL OIL POINT RESERVE\"                      \n [72] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n [73] \"ORMOND BEACH\"                                \n [74] \"NBVC POINT MUGU\"                             \n [75] \"VENICE BEACH\"                                \n [76] \"LA HARBOR\"                                   \n [77] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n [78] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n [79] \"HUNTINGTON STATE BEACH\"                      \n [80] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n [81] \"MCB CAMP PENDLETON\"                          \n [82] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n [83] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n [84] \"MISSION BAY FAA ISLAND\"                      \n [85] \"MISSION BAY NORTH FIESTA ISLAND\"             \n [86] \"MISSION BAY MARINERS POINT\"                  \n [87] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n [88] \"SDIA LINDBERGH FIELD\"                        \n [89] \"NAS NORTH ISLAND\"                            \n [90] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n [91] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n [92] \"CHULA VISTA WILDLIFE RESERVE\"                \n [93] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n [94] \"TIJUANA ESTUARY NERR\"                        \n [95] \"MONTEZUMA WETLANDS\"                          \n [96] \"PITTSBURG POWER PLANT\"                       \n [97] \"ALAMEDA POINT\"                               \n [98] \"HAYWARD REGIONAL SHORELINE\"                  \n [99] \"KETTLEMAN CITY\"                              \n[100] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[101] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[102] \"VANDENBERG SFB\"                              \n[103] \"COAL OIL POINT RESERVE\"                      \n[104] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[105] \"ORMOND BEACH\"                                \n[106] \"NBVC POINT MUGU\"                             \n[107] \"VENICE BEACH\"                                \n[108] \"LA HARBOR\"                                   \n[109] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[110] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[111] \"HUNTINGTON STATE BEACH\"                      \n[112] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[113] \"MCB CAMP PENDLETON\"                          \n[114] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[115] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[116] \"MISSION BAY FAA ISLAND\"                      \n[117] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[118] \"MISSION BAY MARINERS POINT\"                  \n[119] \"MISSION BAY STONY POINT\"                     \n[120] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[121] \"SDIA LINDBERGH FIELD\"                        \n[122] \"NAS NORTH ISLAND\"                            \n[123] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[124] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[125] \"CHULA VISTA WILDLIFE RESERVE\"                \n[126] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[127] \"TIJUANA ESTUARY NERR\"                        \n[128] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[129] \"MONTEZUMA WETLANDS\"                          \n[130] \"PITTSBURG POWER PLANT\"                       \n[131] \"ALAMEDA POINT\"                               \n[132] \"HAYWARD REGIONAL SHORELINE\"                  \n[133] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[134] \"KETTLEMAN CITY\"                              \n[135] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[136] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[137] \"VANDENBERG SFB\"                              \n[138] \"COAL OIL POINT RESERVE\"                      \n[139] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[140] \"HOLLYWOOD BEACH\"                             \n[141] \"ORMOND BEACH\"                                \n[142] \"NBVC POINT MUGU\"                             \n[143] \"VENICE BEACH\"                                \n[144] \"LA HARBOR\"                                   \n[145] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[146] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[147] \"HUNTINGTON STATE BEACH\"                      \n[148] \"BURRIS ISLAND\"                               \n[149] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[150] \"MCB CAMP PENDLETON\"                          \n[151] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[152] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[153] \"MISSION BAY FAA ISLAND\"                      \n[154] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[155] \"MISSION BAY MARINERS POINT\"                  \n[156] \"MISSION BAY STONY POINT\"                     \n[157] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[158] \"SDIA LINDBERGH FIELD\"                        \n[159] \"NAS NORTH ISLAND\"                            \n[160] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[161] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[162] \"CHULA VISTA WILDLIFE RESERVE\"                \n[163] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[164] \"TIJUANA ESTUARY NERR\"                        \n[165] \"SACRAMENTO BUFFERLANDS\"                      \n[166] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[167] \"MONTEZUMA WETLANDS\"                          \n[168] \"PITTSBURG POWER PLANT\"                       \n[169] \"ALAMEDA POINT\"                               \n[170] \"HAYWARD REGIONAL SHORELINE\"                  \n[171] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[172] \"KETTLEMAN CITY\"                              \n[173] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[174] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[175] \"VANDENBERG SFB\"                              \n[176] \"COAL OIL POINT RESERVE\"                      \n[177] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[178] \"HOLLYWOOD BEACH\"                             \n[179] \"ORMOND BEACH\"                                \n[180] \"NBVC POINT MUGU\"                             \n[181] \"VENICE BEACH\"                                \n[182] \"LA HARBOR\"                                   \n[183] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[184] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[185] \"HUNTINGTON STATE BEACH\"                      \n[186] \"BURRIS ISLAND\"                               \n[187] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[188] \"MCB CAMP PENDLETON\"                          \n[189] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[190] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[191] \"MISSION BAY FAA ISLAND\"                      \n[192] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[193] \"MISSION BAY MARINERS POINT\"                  \n[194] \"MISSION BAY STONY POINT\"                     \n[195] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[196] \"SDIA LINDBERGH FIELD\"                        \n[197] \"NAS NORTH ISLAND\"                            \n[198] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[199] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[200] \"CHULA VISTA WILDLIFE RESERVE\"                \n[201] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[202] \"TIJUANA ESTUARY NERR\"                        \n[203] \"SACRAMENTO BUFFERLANDS\"                      \n[204] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[205] \"MONTEZUMA WETLANDS\"                          \n[206] \"PITTSBURG POWER PLANT\"                       \n[207] \"ALAMEDA POINT\"                               \n[208] \"HAYWARD REGIONAL SHORELINE\"                  \n[209] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[210] \"KETTLEMAN CITY\"                              \n[211] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[212] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[213] \"VANDENBERG SFB\"                              \n[214] \"COAL OIL POINT RESERVE\"                      \n[215] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[216] \"HOLLYWOOD BEACH\"                             \n[217] \"ORMOND BEACH\"                                \n[218] \"NBVC POINT MUGU\"                             \n[219] \"VENICE BEACH\"                                \n[220] \"LA HARBOR\"                                   \n[221] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[222] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[223] \"HUNTINGTON STATE BEACH\"                      \n[224] \"BURRIS ISLAND\"                               \n[225] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[226] \"MCB CAMP PENDLETON\"                          \n[227] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[228] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[229] \"FAIRBANKS RANCH\"                             \n[230] \"MISSION BAY FAA ISLAND\"                      \n[231] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[232] \"MISSION BAY MARINERS POINT\"                  \n[233] \"MISSION BAY STONY POINT\"                     \n[234] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[235] \"SDIA LINDBERGH FIELD\"                        \n[236] \"NAS NORTH ISLAND\"                            \n[237] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[238] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[239] \"CHULA VISTA WILDLIFE RESERVE\"                \n[240] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[241] \"TIJUANA ESTUARY NERR\"                        \n[242] \"ARIZONA GLENDALE\"                            \n[243] \"SACRAMENTO BUFFERLANDS\"                      \n[244] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[245] \"MONTEZUMA WETLANDS\"                          \n[246] \"PITTSBURG POWER PLANT\"                       \n[247] \"ALAMEDA POINT\"                               \n[248] \"HAYWARD REGIONAL SHORELINE\"                  \n[249] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[250] \"KETTLEMAN CITY\"                              \n[251] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[252] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[253] \"VANDENBERG SFB\"                              \n[254] \"COAL OIL POINT RESERVE\"                      \n[255] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[256] \"HOLLYWOOD BEACH\"                             \n[257] \"ORMOND BEACH\"                                \n[258] \"NBVC POINT MUGU\"                             \n[259] \"VENICE BEACH\"                                \n[260] \"LA HARBOR\"                                   \n[261] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[262] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[263] \"HUNTINGTON STATE BEACH\"                      \n[264] \"BURRIS ISLAND\"                               \n[265] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[266] \"MCB CAMP PENDLETON\"                          \n[267] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[268] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[269] \"FAIRBANKS RANCH\"                             \n[270] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[271] \"MISSION BAY FAA ISLAND\"                      \n[272] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[273] \"MISSION BAY MARINERS POINT\"                  \n[274] \"MISSION BAY STONY POINT\"                     \n[275] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[276] \"SDIA LINDBERGH FIELD\"                        \n[277] \"NAS NORTH ISLAND\"                            \n[278] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[279] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[280] \"CHULA VISTA WILDLIFE RESERVE\"                \n[281] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[282] \"TIJUANA ESTUARY NERR\"                        \n[283] \"SACRAMENTO BUFFERLANDS\"                      \n[284] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[285] \"MONTEZUMA WETLANDS\"                          \n[286] \"PITTSBURG POWER PLANT\"                       \n[287] \"ALAMEDA POINT\"                               \n[288] \"HAYWARD REGIONAL SHORELINE\"                  \n[289] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[290] \"KETTLEMAN CITY\"                              \n[291] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[292] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[293] \"VANDENBERG SFB\"                              \n[294] \"COAL OIL POINT RESERVE\"                      \n[295] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[296] \"HOLLYWOOD BEACH\"                             \n[297] \"ORMOND BEACH\"                                \n[298] \"NBVC POINT MUGU\"                             \n[299] \"VENICE BEACH\"                                \n[300] \"LA HARBOR\"                                   \n[301] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[302] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[303] \"HUNTINGTON STATE BEACH\"                      \n[304] \"BURRIS ISLAND\"                               \n[305] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[306] \"MCB CAMP PENDLETON\"                          \n[307] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[308] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[309] \"FAIRBANKS RANCH\"                             \n[310] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[311] \"MISSION BAY FAA ISLAND\"                      \n[312] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[313] \"MISSION BAY MARINERS POINT\"                  \n[314] \"MISSION BAY STONY POINT\"                     \n[315] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[316] \"SDIA LINDBERGH FIELD\"                        \n[317] \"NAS NORTH ISLAND\"                            \n[318] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[319] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[320] \"CHULA VISTA WILDLIFE RESERVE\"                \n[321] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[322] \"TIJUANA ESTUARY NERR\"                        \n[323] \"SALTON SEA\"                                  \n[324] \"SACRAMENTO BUFFERLANDS\"                      \n[325] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[326] \"MONTEZUMA WETLANDS\"                          \n[327] \"PITTSBURG POWER PLANT\"                       \n[328] \"ALAMEDA POINT\"                               \n[329] \"HAYWARD REGIONAL SHORELINE\"                  \n[330] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[331] \"KETTLEMAN CITY\"                              \n[332] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[333] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[334] \"VANDENBERG SFB\"                              \n[335] \"COAL OIL POINT RESERVE\"                      \n[336] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[337] \"HOLLYWOOD BEACH\"                             \n[338] \"ORMOND BEACH\"                                \n[339] \"NBVC POINT MUGU\"                             \n[340] \"VENICE BEACH\"                                \n[341] \"LA HARBOR\"                                   \n[342] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[343] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[344] \"HUNTINGTON STATE BEACH\"                      \n[345] \"BURRIS ISLAND\"                               \n[346] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[347] \"MCB CAMP PENDLETON\"                          \n[348] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[349] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[350] \"FAIRBANKS RANCH\"                             \n[351] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[352] \"MISSION BAY FAA ISLAND\"                      \n[353] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[354] \"MISSION BAY MARINERS POINT\"                  \n[355] \"MISSION BAY STONY POINT\"                     \n[356] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[357] \"SDIA LINDBERGH FIELD\"                        \n[358] \"NAS NORTH ISLAND\"                            \n[359] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[360] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[361] \"CHULA VISTA WILDLIFE RESERVE\"                \n[362] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[363] \"TIJUANA ESTUARY NERR\"                        \n[364] \"SALTON SEA\"                                  \n[365] \"SACRAMENTO BUFFERLANDS\"                      \n[366] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[367] \"MONTEZUMA WETLANDS\"                          \n[368] \"PITTSBURG POWER PLANT\"                       \n[369] \"ALAMEDA POINT\"                               \n[370] \"HAYWARD REGIONAL SHORELINE\"                  \n[371] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[372] \"KETTLEMAN CITY\"                              \n[373] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[374] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[375] \"VANDENBERG SFB\"                              \n[376] \"COAL OIL POINT RESERVE\"                      \n[377] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[378] \"HOLLYWOOD BEACH\"                             \n[379] \"ORMOND BEACH\"                                \n[380] \"NBVC POINT MUGU\"                             \n[381] \"SATICOY UNITED WATER CONSERVATION DISTRICT\"  \n[382] \"VENICE BEACH\"                                \n[383] \"LA HARBOR\"                                   \n[384] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[385] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[386] \"HUNTINGTON STATE BEACH\"                      \n[387] \"BURRIS ISLAND\"                               \n[388] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[389] \"MCB CAMP PENDLETON\"                          \n[390] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[391] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[392] \"FAIRBANKS RANCH\"                             \n[393] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[394] \"MISSION BAY FAA ISLAND\"                      \n[395] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[396] \"MISSION BAY MARINERS POINT\"                  \n[397] \"MISSION BAY STONY POINT\"                     \n[398] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[399] \"SDIA LINDBERGH FIELD\"                        \n[400] \"NAS NORTH ISLAND\"                            \n[401] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[402] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[403] \"CHULA VISTA WILDLIFE RESERVE\"                \n[404] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[405] \"TIJUANA ESTUARY NERR\"                        \n[406] \"SALTON SEA\"                                  \n[407] \"SACRAMENTO BUFFERLANDS\"                      \n[408] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[409] \"MONTEZUMA WETLANDS\"                          \n[410] \"PITTSBURG POWER PLANT\"                       \n[411] \"ALAMEDA POINT\"                               \n[412] \"HAYWARD REGIONAL SHORELINE\"                  \n[413] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[414] \"KETTLEMAN CITY\"                              \n[415] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[416] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[417] \"VANDENBERG SFB\"                              \n[418] \"COAL OIL POINT RESERVE\"                      \n[419] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[420] \"HOLLYWOOD BEACH\"                             \n[421] \"ORMOND BEACH\"                                \n[422] \"NBVC POINT MUGU\"                             \n[423] \"SATICOY UNITED WATER CONSERVATION DISTRICT\"  \n[424] \"VENICE BEACH\"                                \n[425] \"LA HARBOR\"                                   \n[426] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[427] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[428] \"HUNTINGTON STATE BEACH\"                      \n[429] \"BURRIS ISLAND\"                               \n[430] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[431] \"MCB CAMP PENDLETON\"                          \n[432] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[433] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[434] \"FAIRBANKS RANCH\"                             \n[435] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[436] \"MISSION BAY FAA ISLAND\"                      \n[437] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[438] \"MISSION BAY MARINERS POINT\"                  \n[439] \"MISSION BAY STONY POINT\"                     \n[440] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[441] \"SDIA LINDBERGH FIELD\"                        \n[442] \"NAS NORTH ISLAND\"                            \n[443] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[444] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[445] \"CHULA VISTA WILDLIFE RESERVE\"                \n[446] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[447] \"TIJUANA ESTUARY NERR\"                        \n[448] \"SALTON SEA\"                                  \n[449] \"SACRAMENTO BUFFERLANDS\"                      \n[450] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[451] \"MONTEZUMA WETLANDS\"                          \n[452] \"PITTSBURG POWER PLANT\"                       \n[453] \"ALAMEDA POINT\"                               \n[454] \"HAYWARD REGIONAL SHORELINE\"                  \n[455] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[456] \"KETTLEMAN CITY\"                              \n[457] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[458] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[459] \"VANDENBERG SFB\"                              \n[460] \"COAL OIL POINT RESERVE\"                      \n[461] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[462] \"HOLLYWOOD BEACH\"                             \n[463] \"ORMOND BEACH\"                                \n[464] \"NBVC POINT MUGU\"                             \n[465] \"SATICOY UNITED WATER CONSERVATION DISTRICT\"  \n[466] \"VENICE BEACH\"                                \n[467] \"LA HARBOR\"                                   \n[468] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[469] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[470] \"HUNTINGTON STATE BEACH\"                      \n[471] \"BURRIS ISLAND\"                               \n[472] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[473] \"MCB CAMP PENDLETON\"                          \n[474] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[475] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[476] \"FAIRBANKS RANCH\"                             \n[477] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[478] \"MISSION BAY FAA ISLAND\"                      \n[479] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[480] \"MISSION BAY MARINERS POINT\"                  \n[481] \"MISSION BAY STONY POINT\"                     \n[482] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[483] \"SDIA LINDBERGH FIELD\"                        \n[484] \"NAS NORTH ISLAND\"                            \n[485] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[486] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[487] \"CHULA VISTA WILDLIFE RESERVE\"                \n[488] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[489] \"TIJUANA ESTUARY NERR\"                        \n[490] \"SALTON SEA\"                                  \n[491] \"SACRAMENTO BUFFERLANDS\"                      \n[492] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[493] \"MONTEZUMA WETLANDS\"                          \n[494] \"PITTSBURG POWER PLANT\"                       \n[495] \"ALAMEDA POINT\"                               \n[496] \"HAYWARD REGIONAL SHORELINE\"                  \n[497] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[498] \"KETTLEMAN CITY\"                              \n[499] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[500] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[501] \"VANDENBERG SFB\"                              \n[502] \"COAL OIL POINT RESERVE\"                      \n[503] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[504] \"HOLLYWOOD BEACH\"                             \n[505] \"ORMOND BEACH\"                                \n[506] \"NBVC POINT MUGU\"                             \n[507] \"SATICOY UNITED WATER CONSERVATION DISTRICT\"  \n[508] \"VENICE BEACH\"                                \n[509] \"LA HARBOR\"                                   \n[510] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[511] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[512] \"HUNTINGTON STATE BEACH\"                      \n[513] \"ANAHEIM LAKE\"                                \n[514] \"BURRIS ISLAND\"                               \n[515] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[516] \"MCB CAMP PENDLETON\"                          \n[517] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[518] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[519] \"FAIRBANKS RANCH\"                             \n[520] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[521] \"MISSION BAY FAA ISLAND\"                      \n[522] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[523] \"MISSION BAY MARINERS POINT\"                  \n[524] \"MISSION BAY STONY POINT\"                     \n[525] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[526] \"SDIA LINDBERGH FIELD\"                        \n[527] \"NAS NORTH ISLAND\"                            \n[528] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[529] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[530] \"CHULA VISTA WILDLIFE RESERVE\"                \n[531] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[532] \"TIJUANA ESTUARY NERR\"                        \n[533] \"SALTON SEA\"                                  \n[534] \"SACRAMENTO BUFFERLANDS\"                      \n[535] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[536] \"MONTEZUMA WETLANDS\"                          \n[537] \"PITTSBURG POWER PLANT\"                       \n[538] \"ALAMEDA POINT\"                               \n[539] \"HAYWARD REGIONAL SHORELINE\"                  \n[540] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[541] \"KETTLEMAN CITY\"                              \n[542] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[543] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[544] \"GUADALUPE NIPOMO DUNES NWR\"                  \n[545] \"VANDENBERG SFB\"                              \n[546] \"COAL OIL POINT RESERVE\"                      \n[547] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[548] \"HOLLYWOOD BEACH\"                             \n[549] \"ORMOND BEACH\"                                \n[550] \"NBVC POINT MUGU\"                             \n[551] \"SATICOY UNITED WATER CONSERVATION DISTRICT\"  \n[552] \"VENICE BEACH\"                                \n[553] \"MALIBU LAGOON\"                               \n[554] \"LA HARBOR\"                                   \n[555] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[556] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[557] \"HUNTINGTON STATE BEACH\"                      \n[558] \"ANAHEIM LAKE\"                                \n[559] \"BURRIS ISLAND\"                               \n[560] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[561] \"MCB CAMP PENDLETON\"                          \n[562] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[563] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[564] \"FAIRBANKS RANCH\"                             \n[565] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[566] \"MISSION BAY FAA ISLAND\"                      \n[567] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[568] \"MISSION BAY MARINERS POINT\"                  \n[569] \"MISSION BAY STONY POINT\"                     \n[570] \"MISSION BAY SAN DIEGO RIVER MOUTH\"           \n[571] \"SDIA LINDBERGH FIELD\"                        \n[572] \"NAS NORTH ISLAND\"                            \n[573] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[574] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[575] \"CHULA VISTA WILDLIFE RESERVE\"                \n[576] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[577] \"TIJUANA ESTUARY NERR\"                        \n[578] \"SALTON SEA\"                                  \n[579] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[580] \"MONTEZUMA WETLANDS\"                          \n[581] \"ALAMEDA POINT\"                               \n[582] \"HAYWARD REGIONAL SHORELINE\"                  \n[583] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[584] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[585] \"VANDENBERG SFB\"                              \n[586] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[587] \"HOLLYWOOD BEACH\"                             \n[588] \"ORMOND BEACH\"                                \n[589] \"NBVC POINT MUGU\"                             \n[590] \"SATICOY UNITED WATER CONSERVATION DISTRICT\"  \n[591] \"VENICE BEACH\"                                \n[592] \"MALIBU LAGOON\"                               \n[593] \"LA HARBOR\"                                   \n[594] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[595] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[596] \"HUNTINGTON STATE BEACH\"                      \n[597] \"BURRIS ISLAND\"                               \n[598] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[599] \"MCB CAMP PENDLETON\"                          \n[600] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[601] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[602] \"MISSION BAY FAA ISLAND\"                      \n[603] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[604] \"MISSION BAY MARINERS POINT\"                  \n[605] \"MISSION BAY STONY POINT\"                     \n[606] \"SDIA LINDBERGH FIELD\"                        \n[607] \"NAS NORTH ISLAND\"                            \n[608] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[609] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[610] \"CHULA VISTA WILDLIFE RESERVE\"                \n[611] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[612] \"TIJUANA ESTUARY NERR\"                        \n[613] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[614] \"MONTEZUMA WETLANDS\"                          \n[615] \"ALAMEDA POINT\"                               \n[616] \"HAYWARD REGIONAL SHORELINE\"                  \n[617] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[618] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[619] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[620] \"VANDENBERG SFB\"                              \n[621] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[622] \"HOLLYWOOD BEACH\"                             \n[623] \"ORMOND BEACH\"                                \n[624] \"NBVC POINT MUGU\"                             \n[625] \"SATICOY UNITED WATER CONSERVATION DISTRICT\"  \n[626] \"VENICE BEACH\"                                \n[627] \"MALIBU LAGOON\"                               \n[628] \"LA HARBOR\"                                   \n[629] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[630] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[631] \"HUNTINGTON STATE BEACH\"                      \n[632] \"BURRIS ISLAND\"                               \n[633] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[634] \"MCB CAMP PENDLETON\"                          \n[635] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[636] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[637] \"MISSION BAY FAA ISLAND\"                      \n[638] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[639] \"MISSION BAY MARINERS POINT\"                  \n[640] \"MISSION BAY STONY POINT\"                     \n[641] \"SDIA LINDBERGH FIELD\"                        \n[642] \"NAS NORTH ISLAND\"                            \n[643] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[644] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[645] \"CHULA VISTA WILDLIFE RESERVE\"                \n[646] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[647] \"TIJUANA ESTUARY NERR\"                        \n[648] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[649] \"MONTEZUMA WETLANDS\"                          \n[650] \"ALAMEDA POINT\"                               \n[651] \"HAYWARD REGIONAL SHORELINE\"                  \n[652] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[653] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[654] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[655] \"VANDENBERG SFB\"                              \n[656] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[657] \"HOLLYWOOD BEACH\"                             \n[658] \"ORMOND BEACH\"                                \n[659] \"NBVC POINT MUGU\"                             \n[660] \"SATICOY UNITED WATER CONSERVATION DISTRICT\"  \n[661] \"VENICE BEACH\"                                \n[662] \"MALIBU LAGOON\"                               \n[663] \"LA HARBOR\"                                   \n[664] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[665] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[666] \"HUNTINGTON STATE BEACH\"                      \n[667] \"BURRIS ISLAND\"                               \n[668] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[669] \"MCB CAMP PENDLETON\"                          \n[670] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[671] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[672] \"MISSION BAY FAA ISLAND\"                      \n[673] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[674] \"MISSION BAY MARINERS POINT\"                  \n[675] \"MISSION BAY STONY POINT\"                     \n[676] \"SDIA LINDBERGH FIELD\"                        \n[677] \"NAS NORTH ISLAND\"                            \n[678] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[679] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[680] \"CHULA VISTA WILDLIFE RESERVE\"                \n[681] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[682] \"TIJUANA ESTUARY NERR\"                        \n[683] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[684] \"SAN PABLO BAY NWR\"                           \n[685] \"MONTEZUMA WETLANDS\"                          \n[686] \"ALAMEDA POINT\"                               \n[687] \"HAYWARD REGIONAL SHORELINE\"                  \n[688] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[689] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[690] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[691] \"VANDENBERG SFB\"                              \n[692] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[693] \"HOLLYWOOD BEACH\"                             \n[694] \"ORMOND BEACH\"                                \n[695] \"NBVC POINT MUGU\"                             \n[696] \"SATICOY UNITED WATER CONSERVATION DISTRICT\"  \n[697] \"VENICE BEACH\"                                \n[698] \"MALIBU LAGOON\"                               \n[699] \"LA HARBOR\"                                   \n[700] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[701] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[702] \"HUNTINGTON STATE BEACH\"                      \n[703] \"BURRIS ISLAND\"                               \n[704] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[705] \"MCB CAMP PENDLETON\"                          \n[706] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[707] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[708] \"MISSION BAY FAA ISLAND\"                      \n[709] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[710] \"MISSION BAY MARINERS POINT\"                  \n[711] \"MISSION BAY STONY POINT\"                     \n[712] \"SDIA LINDBERGH FIELD\"                        \n[713] \"NAS NORTH ISLAND\"                            \n[714] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[715] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[716] \"CHULA VISTA WILDLIFE RESERVE\"                \n[717] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[718] \"TIJUANA ESTUARY NERR\"                        \n[719] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[720] \"SAN PABLO BAY NWR\"                           \n[721] \"MONTEZUMA WETLANDS\"                          \n[722] \"ALAMEDA POINT\"                               \n[723] \"HAYWARD REGIONAL SHORELINE\"                  \n[724] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[725] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[726] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[727] \"VANDENBERG SFB\"                              \n[728] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[729] \"HOLLYWOOD BEACH\"                             \n[730] \"ORMOND BEACH\"                                \n[731] \"NBVC POINT MUGU\"                             \n[732] \"VENICE BEACH\"                                \n[733] \"MALIBU LAGOON\"                               \n[734] \"LA HARBOR\"                                   \n[735] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[736] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[737] \"HUNTINGTON STATE BEACH\"                      \n[738] \"BURRIS ISLAND\"                               \n[739] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[740] \"MCB CAMP PENDLETON\"                          \n[741] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[742] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[743] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[744] \"MISSION BAY FAA ISLAND\"                      \n[745] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[746] \"MISSION BAY MARINERS POINT\"                  \n[747] \"MISSION BAY STONY POINT\"                     \n[748] \"SDIA LINDBERGH FIELD\"                        \n[749] \"NAS NORTH ISLAND\"                            \n[750] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[751] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[752] \"CHULA VISTA WILDLIFE RESERVE\"                \n[753] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[754] \"TIJUANA ESTUARY NERR\"                        \n[755] \"NAPA SONOMA MARSH WILDLIFE AREA\"             \n[756] \"SAN PABLO BAY NWR\"                           \n[757] \"MONTEZUMA WETLANDS\"                          \n[758] \"ALAMEDA POINT\"                               \n[759] \"HAYWARD REGIONAL SHORELINE\"                  \n[760] \"EDEN LANDING ECOLOGICAL RESERVE\"             \n[761] \"OCEANO DUNES STATE VEHICULAR RECREATION AREA\"\n[762] \"RANCHO GUADALUPE DUNES PRESERVE\"             \n[763] \"VANDENBERG SFB\"                              \n[764] \"SANTA CLARA RIVER MCGRATH STATE BEACH\"       \n[765] \"HOLLYWOOD BEACH\"                             \n[766] \"ORMOND BEACH\"                                \n[767] \"NBVC POINT MUGU\"                             \n[768] \"VENICE BEACH\"                                \n[769] \"MALIBU LAGOON\"                               \n[770] \"LA HARBOR\"                                   \n[771] \"SEAL BEACH NWR ANAHEIM BAY\"                  \n[772] \"BOLSA CHICA ECOLOGICAL RESERVE\"              \n[773] \"HUNTINGTON STATE BEACH\"                      \n[774] \"ANAHEIM LAKE\"                                \n[775] \"BURRIS ISLAND\"                               \n[776] \"UPPER NEWPORT BAY ECOLOGICAL RESERVE\"        \n[777] \"MCB CAMP PENDLETON\"                          \n[778] \"BATIQUITOS LAGOON ECOLOGICAL RESERVE\"        \n[779] \"SAN ELIJO LAGOON ECOLOGICAL RESERVE\"         \n[780] \"SAN DIEGUITO LAGOON ECOLOGICAL RESERVE\"      \n[781] \"MISSION BAY FAA ISLAND\"                      \n[782] \"MISSION BAY NORTH FIESTA ISLAND\"             \n[783] \"MISSION BAY MARINERS POINT\"                  \n[784] \"MISSION BAY STONY POINT\"                     \n[785] \"SDIA LINDBERGH FIELD\"                        \n[786] \"NAS NORTH ISLAND\"                            \n[787] \"NAVAL AMPHIBIOUS BASE CORONADO\"              \n[788] \"DSTREET FILL SWEETWATER MARSH NWR\"           \n[789] \"CHULA VISTA WILDLIFE RESERVE\"                \n[790] \"SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS\"    \n[791] \"TIJUANA ESTUARY NERR\"                        \n\n\nThe double square bracket operator is also important for getting and setting elements of lists (Section 13.2.1). Unlike the dollar sign operator, it can also get columns by position:\n\nterns[[1]]\n\n  [1] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000\n [16] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2004\n [31] 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004\n [46] 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004\n [61] 2004 2004 2004 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005\n [76] 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005 2005\n [91] 2005 2005 2005 2005 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006\n[106] 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 2006\n[121] 2006 2006 2006 2006 2006 2006 2006 2007 2007 2007 2007 2007 2007 2007 2007\n[136] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007\n[151] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 2008\n[166] 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008\n[181] 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008 2008\n[196] 2008 2008 2008 2008 2008 2008 2008 2009 2009 2009 2009 2009 2009 2009 2009\n[211] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[226] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[241] 2009 2009 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010\n[256] 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010\n[271] 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2011 2011 2011\n[286] 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011\n[301] 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011 2011\n[316] 2011 2011 2011 2011 2011 2011 2011 2011 2012 2012 2012 2012 2012 2012 2012\n[331] 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012\n[346] 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012\n[361] 2012 2012 2012 2012 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013\n[376] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013\n[391] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013\n[406] 2013 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014\n[421] 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014\n[436] 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 2015 2015\n[451] 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n[466] 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n[481] 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2016 2016 2016 2016 2016\n[496] 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016\n[511] 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016 2016\n[526] 2016 2016 2016 2016 2016 2016 2016 2016 2017 2017 2017 2017 2017 2017 2017\n[541] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017\n[556] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017\n[571] 2017 2017 2017 2017 2017 2017 2017 2017 2018 2018 2018 2018 2018 2018 2018\n[586] 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018\n[601] 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2019 2019 2019\n[616] 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019\n[631] 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 2019\n[646] 2019 2019 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020\n[661] 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020\n[676] 2020 2020 2020 2020 2020 2020 2020 2021 2021 2021 2021 2021 2021 2021 2021\n[691] 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021\n[706] 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2021 2022 2022\n[721] 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022\n[736] 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 2022\n[751] 2022 2022 2022 2022 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023\n[766] 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023\n[781] 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023\n\n\nMost of the time, you should use the dollar sign operator when you want to get a single column.\n\n\n\n\n\n\nTip\n\n\n\nQuotes are optional with the dollar sign operator. So another way to get the total_nests column is:\nterns$\"total_nests\"\nUse quotes if you want to get a column whose name contains spaces or other special characters. For all other cases, quotes are not necessary.\n\n\n\n\n15.2.2 Multiple Columns\nThe dollar sign operator $ and double square bracket operator [[ can only get one column of a data frame at a time, and they always return columns as vectors. This is useful when we want to compute on just one column, because most functions (especially math and statistics functions) require vector arguments.\nSometimes we’ll want to get multiple columns from a data frame. Imagine a data frame with 300 columns, but only 5 that are relevant to your research question. Getting rid of the extra columns makes the data easier to inspect and might make some computations faster. When you select multiple columns from a data frame, the result is also a data frame.\nOne way to select multiple columns is with the indexing operator [. If you use the indexing operator on a data frame and only provide one index, R will select columns. For example, to select the site_name and year columns from the least terns dataset:\n\nterns[c(\"site_name\", \"year\")]\n\n                                       site_name year\n1                          PITTSBURG POWER PLANT 2000\n2                             ALBANY CENTRAL AVE 2000\n3                                  ALAMEDA POINT 2000\n4                                 KETTLEMAN CITY 2000\n5   OCEANO DUNES STATE VEHICULAR RECREATION AREA 2000\n6                RANCHO GUADALUPE DUNES PRESERVE 2000\n7                                 VANDENBERG SFB 2000\n8          SANTA CLARA RIVER MCGRATH STATE BEACH 2000\n9                                   ORMOND BEACH 2000\n10                               NBVC POINT MUGU 2000\n11                                  VENICE BEACH 2000\n12                                     LA HARBOR 2000\n13                    SEAL BEACH NWR ANAHEIM BAY 2000\n14                BOLSA CHICA ECOLOGICAL RESERVE 2000\n15                        HUNTINGTON STATE BEACH 2000\n16          UPPER NEWPORT BAY ECOLOGICAL RESERVE 2000\n17                            MCB CAMP PENDLETON 2000\n18          BATIQUITOS LAGOON ECOLOGICAL RESERVE 2000\n19           SAN ELIJO LAGOON ECOLOGICAL RESERVE 2000\n20                        MISSION BAY FAA ISLAND 2000\n21               MISSION BAY NORTH FIESTA ISLAND 2000\n22                    MISSION BAY MARINERS POINT 2000\n23                          SDIA LINDBERGH FIELD 2000\n24                              NAS NORTH ISLAND 2000\n25                NAVAL AMPHIBIOUS BASE CORONADO 2000\n26             DSTREET FILL SWEETWATER MARSH NWR 2000\n27                  CHULA VISTA WILDLIFE RESERVE 2000\n28      SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2000\n29                          TIJUANA ESTUARY NERR 2000\n30                         PITTSBURG POWER PLANT 2004\n31                            ALBANY CENTRAL AVE 2004\n32                                 ALAMEDA POINT 2004\n33                                KETTLEMAN CITY 2004\n34  OCEANO DUNES STATE VEHICULAR RECREATION AREA 2004\n35               RANCHO GUADALUPE DUNES PRESERVE 2004\n36                                VANDENBERG SFB 2004\n37                        COAL OIL POINT RESERVE 2004\n38         SANTA CLARA RIVER MCGRATH STATE BEACH 2004\n39                               HOLLYWOOD BEACH 2004\n40                                  ORMOND BEACH 2004\n41                               NBVC POINT MUGU 2004\n42                                  VENICE BEACH 2004\n43                                     LA HARBOR 2004\n44                    SEAL BEACH NWR ANAHEIM BAY 2004\n45                BOLSA CHICA ECOLOGICAL RESERVE 2004\n46                        HUNTINGTON STATE BEACH 2004\n47                                 BURRIS ISLAND 2004\n48          UPPER NEWPORT BAY ECOLOGICAL RESERVE 2004\n49                            MCB CAMP PENDLETON 2004\n50          BATIQUITOS LAGOON ECOLOGICAL RESERVE 2004\n51           SAN ELIJO LAGOON ECOLOGICAL RESERVE 2004\n52                        MISSION BAY FAA ISLAND 2004\n53               MISSION BAY NORTH FIESTA ISLAND 2004\n54                    MISSION BAY MARINERS POINT 2004\n55             MISSION BAY SAN DIEGO RIVER MOUTH 2004\n56                          SDIA LINDBERGH FIELD 2004\n57                              NAS NORTH ISLAND 2004\n58                NAVAL AMPHIBIOUS BASE CORONADO 2004\n59                     SILVER STRAND STATE BEACH 2004\n60             DSTREET FILL SWEETWATER MARSH NWR 2004\n61                  CHULA VISTA WILDLIFE RESERVE 2004\n62      SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2004\n63                          TIJUANA ESTUARY NERR 2004\n64                         PITTSBURG POWER PLANT 2005\n65                                 ALAMEDA POINT 2005\n66                    HAYWARD REGIONAL SHORELINE 2005\n67                                KETTLEMAN CITY 2005\n68  OCEANO DUNES STATE VEHICULAR RECREATION AREA 2005\n69               RANCHO GUADALUPE DUNES PRESERVE 2005\n70                                VANDENBERG SFB 2005\n71                        COAL OIL POINT RESERVE 2005\n72         SANTA CLARA RIVER MCGRATH STATE BEACH 2005\n73                                  ORMOND BEACH 2005\n74                               NBVC POINT MUGU 2005\n75                                  VENICE BEACH 2005\n76                                     LA HARBOR 2005\n77                    SEAL BEACH NWR ANAHEIM BAY 2005\n78                BOLSA CHICA ECOLOGICAL RESERVE 2005\n79                        HUNTINGTON STATE BEACH 2005\n80          UPPER NEWPORT BAY ECOLOGICAL RESERVE 2005\n81                            MCB CAMP PENDLETON 2005\n82          BATIQUITOS LAGOON ECOLOGICAL RESERVE 2005\n83           SAN ELIJO LAGOON ECOLOGICAL RESERVE 2005\n84                        MISSION BAY FAA ISLAND 2005\n85               MISSION BAY NORTH FIESTA ISLAND 2005\n86                    MISSION BAY MARINERS POINT 2005\n87             MISSION BAY SAN DIEGO RIVER MOUTH 2005\n88                          SDIA LINDBERGH FIELD 2005\n89                              NAS NORTH ISLAND 2005\n90                NAVAL AMPHIBIOUS BASE CORONADO 2005\n91             DSTREET FILL SWEETWATER MARSH NWR 2005\n92                  CHULA VISTA WILDLIFE RESERVE 2005\n93      SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2005\n94                          TIJUANA ESTUARY NERR 2005\n95                            MONTEZUMA WETLANDS 2006\n96                         PITTSBURG POWER PLANT 2006\n97                                 ALAMEDA POINT 2006\n98                    HAYWARD REGIONAL SHORELINE 2006\n99                                KETTLEMAN CITY 2006\n100 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2006\n101              RANCHO GUADALUPE DUNES PRESERVE 2006\n102                               VANDENBERG SFB 2006\n103                       COAL OIL POINT RESERVE 2006\n104        SANTA CLARA RIVER MCGRATH STATE BEACH 2006\n105                                 ORMOND BEACH 2006\n106                              NBVC POINT MUGU 2006\n107                                 VENICE BEACH 2006\n108                                    LA HARBOR 2006\n109                   SEAL BEACH NWR ANAHEIM BAY 2006\n110               BOLSA CHICA ECOLOGICAL RESERVE 2006\n111                       HUNTINGTON STATE BEACH 2006\n112         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2006\n113                           MCB CAMP PENDLETON 2006\n114         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2006\n115          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2006\n116                       MISSION BAY FAA ISLAND 2006\n117              MISSION BAY NORTH FIESTA ISLAND 2006\n118                   MISSION BAY MARINERS POINT 2006\n119                      MISSION BAY STONY POINT 2006\n120            MISSION BAY SAN DIEGO RIVER MOUTH 2006\n121                         SDIA LINDBERGH FIELD 2006\n122                             NAS NORTH ISLAND 2006\n123               NAVAL AMPHIBIOUS BASE CORONADO 2006\n124            DSTREET FILL SWEETWATER MARSH NWR 2006\n125                 CHULA VISTA WILDLIFE RESERVE 2006\n126     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2006\n127                         TIJUANA ESTUARY NERR 2006\n128              NAPA SONOMA MARSH WILDLIFE AREA 2007\n129                           MONTEZUMA WETLANDS 2007\n130                        PITTSBURG POWER PLANT 2007\n131                                ALAMEDA POINT 2007\n132                   HAYWARD REGIONAL SHORELINE 2007\n133              EDEN LANDING ECOLOGICAL RESERVE 2007\n134                               KETTLEMAN CITY 2007\n135 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2007\n136              RANCHO GUADALUPE DUNES PRESERVE 2007\n137                               VANDENBERG SFB 2007\n138                       COAL OIL POINT RESERVE 2007\n139        SANTA CLARA RIVER MCGRATH STATE BEACH 2007\n140                              HOLLYWOOD BEACH 2007\n141                                 ORMOND BEACH 2007\n142                              NBVC POINT MUGU 2007\n143                                 VENICE BEACH 2007\n144                                    LA HARBOR 2007\n145                   SEAL BEACH NWR ANAHEIM BAY 2007\n146               BOLSA CHICA ECOLOGICAL RESERVE 2007\n147                       HUNTINGTON STATE BEACH 2007\n148                                BURRIS ISLAND 2007\n149         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2007\n150                           MCB CAMP PENDLETON 2007\n151         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2007\n152          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2007\n153                       MISSION BAY FAA ISLAND 2007\n154              MISSION BAY NORTH FIESTA ISLAND 2007\n155                   MISSION BAY MARINERS POINT 2007\n156                      MISSION BAY STONY POINT 2007\n157            MISSION BAY SAN DIEGO RIVER MOUTH 2007\n158                         SDIA LINDBERGH FIELD 2007\n159                             NAS NORTH ISLAND 2007\n160               NAVAL AMPHIBIOUS BASE CORONADO 2007\n161            DSTREET FILL SWEETWATER MARSH NWR 2007\n162                 CHULA VISTA WILDLIFE RESERVE 2007\n163     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2007\n164                         TIJUANA ESTUARY NERR 2007\n165                       SACRAMENTO BUFFERLANDS 2008\n166              NAPA SONOMA MARSH WILDLIFE AREA 2008\n167                           MONTEZUMA WETLANDS 2008\n168                        PITTSBURG POWER PLANT 2008\n169                                ALAMEDA POINT 2008\n170                   HAYWARD REGIONAL SHORELINE 2008\n171              EDEN LANDING ECOLOGICAL RESERVE 2008\n172                               KETTLEMAN CITY 2008\n173 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2008\n174              RANCHO GUADALUPE DUNES PRESERVE 2008\n175                               VANDENBERG SFB 2008\n176                       COAL OIL POINT RESERVE 2008\n177        SANTA CLARA RIVER MCGRATH STATE BEACH 2008\n178                              HOLLYWOOD BEACH 2008\n179                                 ORMOND BEACH 2008\n180                              NBVC POINT MUGU 2008\n181                                 VENICE BEACH 2008\n182                                    LA HARBOR 2008\n183                   SEAL BEACH NWR ANAHEIM BAY 2008\n184               BOLSA CHICA ECOLOGICAL RESERVE 2008\n185                       HUNTINGTON STATE BEACH 2008\n186                                BURRIS ISLAND 2008\n187         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2008\n188                           MCB CAMP PENDLETON 2008\n189         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2008\n190          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2008\n191                       MISSION BAY FAA ISLAND 2008\n192              MISSION BAY NORTH FIESTA ISLAND 2008\n193                   MISSION BAY MARINERS POINT 2008\n194                      MISSION BAY STONY POINT 2008\n195            MISSION BAY SAN DIEGO RIVER MOUTH 2008\n196                         SDIA LINDBERGH FIELD 2008\n197                             NAS NORTH ISLAND 2008\n198               NAVAL AMPHIBIOUS BASE CORONADO 2008\n199            DSTREET FILL SWEETWATER MARSH NWR 2008\n200                 CHULA VISTA WILDLIFE RESERVE 2008\n201     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2008\n202                         TIJUANA ESTUARY NERR 2008\n203                       SACRAMENTO BUFFERLANDS 2009\n204              NAPA SONOMA MARSH WILDLIFE AREA 2009\n205                           MONTEZUMA WETLANDS 2009\n206                        PITTSBURG POWER PLANT 2009\n207                                ALAMEDA POINT 2009\n208                   HAYWARD REGIONAL SHORELINE 2009\n209              EDEN LANDING ECOLOGICAL RESERVE 2009\n210                               KETTLEMAN CITY 2009\n211 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2009\n212              RANCHO GUADALUPE DUNES PRESERVE 2009\n213                               VANDENBERG SFB 2009\n214                       COAL OIL POINT RESERVE 2009\n215        SANTA CLARA RIVER MCGRATH STATE BEACH 2009\n216                              HOLLYWOOD BEACH 2009\n217                                 ORMOND BEACH 2009\n218                              NBVC POINT MUGU 2009\n219                                 VENICE BEACH 2009\n220                                    LA HARBOR 2009\n221                   SEAL BEACH NWR ANAHEIM BAY 2009\n222               BOLSA CHICA ECOLOGICAL RESERVE 2009\n223                       HUNTINGTON STATE BEACH 2009\n224                                BURRIS ISLAND 2009\n225         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2009\n226                           MCB CAMP PENDLETON 2009\n227         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2009\n228          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2009\n229                              FAIRBANKS RANCH 2009\n230                       MISSION BAY FAA ISLAND 2009\n231              MISSION BAY NORTH FIESTA ISLAND 2009\n232                   MISSION BAY MARINERS POINT 2009\n233                      MISSION BAY STONY POINT 2009\n234            MISSION BAY SAN DIEGO RIVER MOUTH 2009\n235                         SDIA LINDBERGH FIELD 2009\n236                             NAS NORTH ISLAND 2009\n237               NAVAL AMPHIBIOUS BASE CORONADO 2009\n238            DSTREET FILL SWEETWATER MARSH NWR 2009\n239                 CHULA VISTA WILDLIFE RESERVE 2009\n240     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2009\n241                         TIJUANA ESTUARY NERR 2009\n242                             ARIZONA GLENDALE 2009\n243                       SACRAMENTO BUFFERLANDS 2010\n244              NAPA SONOMA MARSH WILDLIFE AREA 2010\n245                           MONTEZUMA WETLANDS 2010\n246                        PITTSBURG POWER PLANT 2010\n247                                ALAMEDA POINT 2010\n248                   HAYWARD REGIONAL SHORELINE 2010\n249              EDEN LANDING ECOLOGICAL RESERVE 2010\n250                               KETTLEMAN CITY 2010\n251 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2010\n252              RANCHO GUADALUPE DUNES PRESERVE 2010\n253                               VANDENBERG SFB 2010\n254                       COAL OIL POINT RESERVE 2010\n255        SANTA CLARA RIVER MCGRATH STATE BEACH 2010\n256                              HOLLYWOOD BEACH 2010\n257                                 ORMOND BEACH 2010\n258                              NBVC POINT MUGU 2010\n259                                 VENICE BEACH 2010\n260                                    LA HARBOR 2010\n261                   SEAL BEACH NWR ANAHEIM BAY 2010\n262               BOLSA CHICA ECOLOGICAL RESERVE 2010\n263                       HUNTINGTON STATE BEACH 2010\n264                                BURRIS ISLAND 2010\n265         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2010\n266                           MCB CAMP PENDLETON 2010\n267         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2010\n268          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2010\n269                              FAIRBANKS RANCH 2010\n270       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2010\n271                       MISSION BAY FAA ISLAND 2010\n272              MISSION BAY NORTH FIESTA ISLAND 2010\n273                   MISSION BAY MARINERS POINT 2010\n274                      MISSION BAY STONY POINT 2010\n275            MISSION BAY SAN DIEGO RIVER MOUTH 2010\n276                         SDIA LINDBERGH FIELD 2010\n277                             NAS NORTH ISLAND 2010\n278               NAVAL AMPHIBIOUS BASE CORONADO 2010\n279            DSTREET FILL SWEETWATER MARSH NWR 2010\n280                 CHULA VISTA WILDLIFE RESERVE 2010\n281     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2010\n282                         TIJUANA ESTUARY NERR 2010\n283                       SACRAMENTO BUFFERLANDS 2011\n284              NAPA SONOMA MARSH WILDLIFE AREA 2011\n285                           MONTEZUMA WETLANDS 2011\n286                        PITTSBURG POWER PLANT 2011\n287                                ALAMEDA POINT 2011\n288                   HAYWARD REGIONAL SHORELINE 2011\n289              EDEN LANDING ECOLOGICAL RESERVE 2011\n290                               KETTLEMAN CITY 2011\n291 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2011\n292              RANCHO GUADALUPE DUNES PRESERVE 2011\n293                               VANDENBERG SFB 2011\n294                       COAL OIL POINT RESERVE 2011\n295        SANTA CLARA RIVER MCGRATH STATE BEACH 2011\n296                              HOLLYWOOD BEACH 2011\n297                                 ORMOND BEACH 2011\n298                              NBVC POINT MUGU 2011\n299                                 VENICE BEACH 2011\n300                                    LA HARBOR 2011\n301                   SEAL BEACH NWR ANAHEIM BAY 2011\n302               BOLSA CHICA ECOLOGICAL RESERVE 2011\n303                       HUNTINGTON STATE BEACH 2011\n304                                BURRIS ISLAND 2011\n305         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2011\n306                           MCB CAMP PENDLETON 2011\n307         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2011\n308          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2011\n309                              FAIRBANKS RANCH 2011\n310       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2011\n311                       MISSION BAY FAA ISLAND 2011\n312              MISSION BAY NORTH FIESTA ISLAND 2011\n313                   MISSION BAY MARINERS POINT 2011\n314                      MISSION BAY STONY POINT 2011\n315            MISSION BAY SAN DIEGO RIVER MOUTH 2011\n316                         SDIA LINDBERGH FIELD 2011\n317                             NAS NORTH ISLAND 2011\n318               NAVAL AMPHIBIOUS BASE CORONADO 2011\n319            DSTREET FILL SWEETWATER MARSH NWR 2011\n320                 CHULA VISTA WILDLIFE RESERVE 2011\n321     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2011\n322                         TIJUANA ESTUARY NERR 2011\n323                                   SALTON SEA 2011\n324                       SACRAMENTO BUFFERLANDS 2012\n325              NAPA SONOMA MARSH WILDLIFE AREA 2012\n326                           MONTEZUMA WETLANDS 2012\n327                        PITTSBURG POWER PLANT 2012\n328                                ALAMEDA POINT 2012\n329                   HAYWARD REGIONAL SHORELINE 2012\n330              EDEN LANDING ECOLOGICAL RESERVE 2012\n331                               KETTLEMAN CITY 2012\n332 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2012\n333              RANCHO GUADALUPE DUNES PRESERVE 2012\n334                               VANDENBERG SFB 2012\n335                       COAL OIL POINT RESERVE 2012\n336        SANTA CLARA RIVER MCGRATH STATE BEACH 2012\n337                              HOLLYWOOD BEACH 2012\n338                                 ORMOND BEACH 2012\n339                              NBVC POINT MUGU 2012\n340                                 VENICE BEACH 2012\n341                                    LA HARBOR 2012\n342                   SEAL BEACH NWR ANAHEIM BAY 2012\n343               BOLSA CHICA ECOLOGICAL RESERVE 2012\n344                       HUNTINGTON STATE BEACH 2012\n345                                BURRIS ISLAND 2012\n346         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2012\n347                           MCB CAMP PENDLETON 2012\n348         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2012\n349          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2012\n350                              FAIRBANKS RANCH 2012\n351       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2012\n352                       MISSION BAY FAA ISLAND 2012\n353              MISSION BAY NORTH FIESTA ISLAND 2012\n354                   MISSION BAY MARINERS POINT 2012\n355                      MISSION BAY STONY POINT 2012\n356            MISSION BAY SAN DIEGO RIVER MOUTH 2012\n357                         SDIA LINDBERGH FIELD 2012\n358                             NAS NORTH ISLAND 2012\n359               NAVAL AMPHIBIOUS BASE CORONADO 2012\n360            DSTREET FILL SWEETWATER MARSH NWR 2012\n361                 CHULA VISTA WILDLIFE RESERVE 2012\n362     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2012\n363                         TIJUANA ESTUARY NERR 2012\n364                                   SALTON SEA 2012\n365                       SACRAMENTO BUFFERLANDS 2013\n366              NAPA SONOMA MARSH WILDLIFE AREA 2013\n367                           MONTEZUMA WETLANDS 2013\n368                        PITTSBURG POWER PLANT 2013\n369                                ALAMEDA POINT 2013\n370                   HAYWARD REGIONAL SHORELINE 2013\n371              EDEN LANDING ECOLOGICAL RESERVE 2013\n372                               KETTLEMAN CITY 2013\n373 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2013\n374              RANCHO GUADALUPE DUNES PRESERVE 2013\n375                               VANDENBERG SFB 2013\n376                       COAL OIL POINT RESERVE 2013\n377        SANTA CLARA RIVER MCGRATH STATE BEACH 2013\n378                              HOLLYWOOD BEACH 2013\n379                                 ORMOND BEACH 2013\n380                              NBVC POINT MUGU 2013\n381   SATICOY UNITED WATER CONSERVATION DISTRICT 2013\n382                                 VENICE BEACH 2013\n383                                    LA HARBOR 2013\n384                   SEAL BEACH NWR ANAHEIM BAY 2013\n385               BOLSA CHICA ECOLOGICAL RESERVE 2013\n386                       HUNTINGTON STATE BEACH 2013\n387                                BURRIS ISLAND 2013\n388         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2013\n389                           MCB CAMP PENDLETON 2013\n390         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2013\n391          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2013\n392                              FAIRBANKS RANCH 2013\n393       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2013\n394                       MISSION BAY FAA ISLAND 2013\n395              MISSION BAY NORTH FIESTA ISLAND 2013\n396                   MISSION BAY MARINERS POINT 2013\n397                      MISSION BAY STONY POINT 2013\n398            MISSION BAY SAN DIEGO RIVER MOUTH 2013\n399                         SDIA LINDBERGH FIELD 2013\n400                             NAS NORTH ISLAND 2013\n401               NAVAL AMPHIBIOUS BASE CORONADO 2013\n402            DSTREET FILL SWEETWATER MARSH NWR 2013\n403                 CHULA VISTA WILDLIFE RESERVE 2013\n404     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2013\n405                         TIJUANA ESTUARY NERR 2013\n406                                   SALTON SEA 2013\n407                       SACRAMENTO BUFFERLANDS 2014\n408              NAPA SONOMA MARSH WILDLIFE AREA 2014\n409                           MONTEZUMA WETLANDS 2014\n410                        PITTSBURG POWER PLANT 2014\n411                                ALAMEDA POINT 2014\n412                   HAYWARD REGIONAL SHORELINE 2014\n413              EDEN LANDING ECOLOGICAL RESERVE 2014\n414                               KETTLEMAN CITY 2014\n415 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2014\n416              RANCHO GUADALUPE DUNES PRESERVE 2014\n417                               VANDENBERG SFB 2014\n418                       COAL OIL POINT RESERVE 2014\n419        SANTA CLARA RIVER MCGRATH STATE BEACH 2014\n420                              HOLLYWOOD BEACH 2014\n421                                 ORMOND BEACH 2014\n422                              NBVC POINT MUGU 2014\n423   SATICOY UNITED WATER CONSERVATION DISTRICT 2014\n424                                 VENICE BEACH 2014\n425                                    LA HARBOR 2014\n426                   SEAL BEACH NWR ANAHEIM BAY 2014\n427               BOLSA CHICA ECOLOGICAL RESERVE 2014\n428                       HUNTINGTON STATE BEACH 2014\n429                                BURRIS ISLAND 2014\n430         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2014\n431                           MCB CAMP PENDLETON 2014\n432         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2014\n433          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2014\n434                              FAIRBANKS RANCH 2014\n435       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2014\n436                       MISSION BAY FAA ISLAND 2014\n437              MISSION BAY NORTH FIESTA ISLAND 2014\n438                   MISSION BAY MARINERS POINT 2014\n439                      MISSION BAY STONY POINT 2014\n440            MISSION BAY SAN DIEGO RIVER MOUTH 2014\n441                         SDIA LINDBERGH FIELD 2014\n442                             NAS NORTH ISLAND 2014\n443               NAVAL AMPHIBIOUS BASE CORONADO 2014\n444            DSTREET FILL SWEETWATER MARSH NWR 2014\n445                 CHULA VISTA WILDLIFE RESERVE 2014\n446     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2014\n447                         TIJUANA ESTUARY NERR 2014\n448                                   SALTON SEA 2014\n449                       SACRAMENTO BUFFERLANDS 2015\n450              NAPA SONOMA MARSH WILDLIFE AREA 2015\n451                           MONTEZUMA WETLANDS 2015\n452                        PITTSBURG POWER PLANT 2015\n453                                ALAMEDA POINT 2015\n454                   HAYWARD REGIONAL SHORELINE 2015\n455              EDEN LANDING ECOLOGICAL RESERVE 2015\n456                               KETTLEMAN CITY 2015\n457 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2015\n458              RANCHO GUADALUPE DUNES PRESERVE 2015\n459                               VANDENBERG SFB 2015\n460                       COAL OIL POINT RESERVE 2015\n461        SANTA CLARA RIVER MCGRATH STATE BEACH 2015\n462                              HOLLYWOOD BEACH 2015\n463                                 ORMOND BEACH 2015\n464                              NBVC POINT MUGU 2015\n465   SATICOY UNITED WATER CONSERVATION DISTRICT 2015\n466                                 VENICE BEACH 2015\n467                                    LA HARBOR 2015\n468                   SEAL BEACH NWR ANAHEIM BAY 2015\n469               BOLSA CHICA ECOLOGICAL RESERVE 2015\n470                       HUNTINGTON STATE BEACH 2015\n471                                BURRIS ISLAND 2015\n472         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2015\n473                           MCB CAMP PENDLETON 2015\n474         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2015\n475          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2015\n476                              FAIRBANKS RANCH 2015\n477       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2015\n478                       MISSION BAY FAA ISLAND 2015\n479              MISSION BAY NORTH FIESTA ISLAND 2015\n480                   MISSION BAY MARINERS POINT 2015\n481                      MISSION BAY STONY POINT 2015\n482            MISSION BAY SAN DIEGO RIVER MOUTH 2015\n483                         SDIA LINDBERGH FIELD 2015\n484                             NAS NORTH ISLAND 2015\n485               NAVAL AMPHIBIOUS BASE CORONADO 2015\n486            DSTREET FILL SWEETWATER MARSH NWR 2015\n487                 CHULA VISTA WILDLIFE RESERVE 2015\n488     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2015\n489                         TIJUANA ESTUARY NERR 2015\n490                                   SALTON SEA 2015\n491                       SACRAMENTO BUFFERLANDS 2016\n492              NAPA SONOMA MARSH WILDLIFE AREA 2016\n493                           MONTEZUMA WETLANDS 2016\n494                        PITTSBURG POWER PLANT 2016\n495                                ALAMEDA POINT 2016\n496                   HAYWARD REGIONAL SHORELINE 2016\n497              EDEN LANDING ECOLOGICAL RESERVE 2016\n498                               KETTLEMAN CITY 2016\n499 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2016\n500              RANCHO GUADALUPE DUNES PRESERVE 2016\n501                               VANDENBERG SFB 2016\n502                       COAL OIL POINT RESERVE 2016\n503        SANTA CLARA RIVER MCGRATH STATE BEACH 2016\n504                              HOLLYWOOD BEACH 2016\n505                                 ORMOND BEACH 2016\n506                              NBVC POINT MUGU 2016\n507   SATICOY UNITED WATER CONSERVATION DISTRICT 2016\n508                                 VENICE BEACH 2016\n509                                    LA HARBOR 2016\n510                   SEAL BEACH NWR ANAHEIM BAY 2016\n511               BOLSA CHICA ECOLOGICAL RESERVE 2016\n512                       HUNTINGTON STATE BEACH 2016\n513                                 ANAHEIM LAKE 2016\n514                                BURRIS ISLAND 2016\n515         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2016\n516                           MCB CAMP PENDLETON 2016\n517         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2016\n518          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2016\n519                              FAIRBANKS RANCH 2016\n520       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2016\n521                       MISSION BAY FAA ISLAND 2016\n522              MISSION BAY NORTH FIESTA ISLAND 2016\n523                   MISSION BAY MARINERS POINT 2016\n524                      MISSION BAY STONY POINT 2016\n525            MISSION BAY SAN DIEGO RIVER MOUTH 2016\n526                         SDIA LINDBERGH FIELD 2016\n527                             NAS NORTH ISLAND 2016\n528               NAVAL AMPHIBIOUS BASE CORONADO 2016\n529            DSTREET FILL SWEETWATER MARSH NWR 2016\n530                 CHULA VISTA WILDLIFE RESERVE 2016\n531     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2016\n532                         TIJUANA ESTUARY NERR 2016\n533                                   SALTON SEA 2016\n534                       SACRAMENTO BUFFERLANDS 2017\n535              NAPA SONOMA MARSH WILDLIFE AREA 2017\n536                           MONTEZUMA WETLANDS 2017\n537                        PITTSBURG POWER PLANT 2017\n538                                ALAMEDA POINT 2017\n539                   HAYWARD REGIONAL SHORELINE 2017\n540              EDEN LANDING ECOLOGICAL RESERVE 2017\n541                               KETTLEMAN CITY 2017\n542 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2017\n543              RANCHO GUADALUPE DUNES PRESERVE 2017\n544                   GUADALUPE NIPOMO DUNES NWR 2017\n545                               VANDENBERG SFB 2017\n546                       COAL OIL POINT RESERVE 2017\n547        SANTA CLARA RIVER MCGRATH STATE BEACH 2017\n548                              HOLLYWOOD BEACH 2017\n549                                 ORMOND BEACH 2017\n550                              NBVC POINT MUGU 2017\n551   SATICOY UNITED WATER CONSERVATION DISTRICT 2017\n552                                 VENICE BEACH 2017\n553                                MALIBU LAGOON 2017\n554                                    LA HARBOR 2017\n555                   SEAL BEACH NWR ANAHEIM BAY 2017\n556               BOLSA CHICA ECOLOGICAL RESERVE 2017\n557                       HUNTINGTON STATE BEACH 2017\n558                                 ANAHEIM LAKE 2017\n559                                BURRIS ISLAND 2017\n560         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2017\n561                           MCB CAMP PENDLETON 2017\n562         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2017\n563          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2017\n564                              FAIRBANKS RANCH 2017\n565       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2017\n566                       MISSION BAY FAA ISLAND 2017\n567              MISSION BAY NORTH FIESTA ISLAND 2017\n568                   MISSION BAY MARINERS POINT 2017\n569                      MISSION BAY STONY POINT 2017\n570            MISSION BAY SAN DIEGO RIVER MOUTH 2017\n571                         SDIA LINDBERGH FIELD 2017\n572                             NAS NORTH ISLAND 2017\n573               NAVAL AMPHIBIOUS BASE CORONADO 2017\n574            DSTREET FILL SWEETWATER MARSH NWR 2017\n575                 CHULA VISTA WILDLIFE RESERVE 2017\n576     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2017\n577                         TIJUANA ESTUARY NERR 2017\n578                                   SALTON SEA 2017\n579              NAPA SONOMA MARSH WILDLIFE AREA 2018\n580                           MONTEZUMA WETLANDS 2018\n581                                ALAMEDA POINT 2018\n582                   HAYWARD REGIONAL SHORELINE 2018\n583              EDEN LANDING ECOLOGICAL RESERVE 2018\n584 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2018\n585                               VANDENBERG SFB 2018\n586        SANTA CLARA RIVER MCGRATH STATE BEACH 2018\n587                              HOLLYWOOD BEACH 2018\n588                                 ORMOND BEACH 2018\n589                              NBVC POINT MUGU 2018\n590   SATICOY UNITED WATER CONSERVATION DISTRICT 2018\n591                                 VENICE BEACH 2018\n592                                MALIBU LAGOON 2018\n593                                    LA HARBOR 2018\n594                   SEAL BEACH NWR ANAHEIM BAY 2018\n595               BOLSA CHICA ECOLOGICAL RESERVE 2018\n596                       HUNTINGTON STATE BEACH 2018\n597                                BURRIS ISLAND 2018\n598         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2018\n599                           MCB CAMP PENDLETON 2018\n600         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2018\n601          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2018\n602                       MISSION BAY FAA ISLAND 2018\n603              MISSION BAY NORTH FIESTA ISLAND 2018\n604                   MISSION BAY MARINERS POINT 2018\n605                      MISSION BAY STONY POINT 2018\n606                         SDIA LINDBERGH FIELD 2018\n607                             NAS NORTH ISLAND 2018\n608               NAVAL AMPHIBIOUS BASE CORONADO 2018\n609            DSTREET FILL SWEETWATER MARSH NWR 2018\n610                 CHULA VISTA WILDLIFE RESERVE 2018\n611     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2018\n612                         TIJUANA ESTUARY NERR 2018\n613              NAPA SONOMA MARSH WILDLIFE AREA 2019\n614                           MONTEZUMA WETLANDS 2019\n615                                ALAMEDA POINT 2019\n616                   HAYWARD REGIONAL SHORELINE 2019\n617              EDEN LANDING ECOLOGICAL RESERVE 2019\n618 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2019\n619              RANCHO GUADALUPE DUNES PRESERVE 2019\n620                               VANDENBERG SFB 2019\n621        SANTA CLARA RIVER MCGRATH STATE BEACH 2019\n622                              HOLLYWOOD BEACH 2019\n623                                 ORMOND BEACH 2019\n624                              NBVC POINT MUGU 2019\n625   SATICOY UNITED WATER CONSERVATION DISTRICT 2019\n626                                 VENICE BEACH 2019\n627                                MALIBU LAGOON 2019\n628                                    LA HARBOR 2019\n629                   SEAL BEACH NWR ANAHEIM BAY 2019\n630               BOLSA CHICA ECOLOGICAL RESERVE 2019\n631                       HUNTINGTON STATE BEACH 2019\n632                                BURRIS ISLAND 2019\n633         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2019\n634                           MCB CAMP PENDLETON 2019\n635         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2019\n636          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2019\n637                       MISSION BAY FAA ISLAND 2019\n638              MISSION BAY NORTH FIESTA ISLAND 2019\n639                   MISSION BAY MARINERS POINT 2019\n640                      MISSION BAY STONY POINT 2019\n641                         SDIA LINDBERGH FIELD 2019\n642                             NAS NORTH ISLAND 2019\n643               NAVAL AMPHIBIOUS BASE CORONADO 2019\n644            DSTREET FILL SWEETWATER MARSH NWR 2019\n645                 CHULA VISTA WILDLIFE RESERVE 2019\n646     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2019\n647                         TIJUANA ESTUARY NERR 2019\n648              NAPA SONOMA MARSH WILDLIFE AREA 2020\n649                           MONTEZUMA WETLANDS 2020\n650                                ALAMEDA POINT 2020\n651                   HAYWARD REGIONAL SHORELINE 2020\n652              EDEN LANDING ECOLOGICAL RESERVE 2020\n653 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2020\n654              RANCHO GUADALUPE DUNES PRESERVE 2020\n655                               VANDENBERG SFB 2020\n656        SANTA CLARA RIVER MCGRATH STATE BEACH 2020\n657                              HOLLYWOOD BEACH 2020\n658                                 ORMOND BEACH 2020\n659                              NBVC POINT MUGU 2020\n660   SATICOY UNITED WATER CONSERVATION DISTRICT 2020\n661                                 VENICE BEACH 2020\n662                                MALIBU LAGOON 2020\n663                                    LA HARBOR 2020\n664                   SEAL BEACH NWR ANAHEIM BAY 2020\n665               BOLSA CHICA ECOLOGICAL RESERVE 2020\n666                       HUNTINGTON STATE BEACH 2020\n667                                BURRIS ISLAND 2020\n668         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2020\n669                           MCB CAMP PENDLETON 2020\n670         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2020\n671          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2020\n672                       MISSION BAY FAA ISLAND 2020\n673              MISSION BAY NORTH FIESTA ISLAND 2020\n674                   MISSION BAY MARINERS POINT 2020\n675                      MISSION BAY STONY POINT 2020\n676                         SDIA LINDBERGH FIELD 2020\n677                             NAS NORTH ISLAND 2020\n678               NAVAL AMPHIBIOUS BASE CORONADO 2020\n679            DSTREET FILL SWEETWATER MARSH NWR 2020\n680                 CHULA VISTA WILDLIFE RESERVE 2020\n681     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2020\n682                         TIJUANA ESTUARY NERR 2020\n683              NAPA SONOMA MARSH WILDLIFE AREA 2021\n684                            SAN PABLO BAY NWR 2021\n685                           MONTEZUMA WETLANDS 2021\n686                                ALAMEDA POINT 2021\n687                   HAYWARD REGIONAL SHORELINE 2021\n688              EDEN LANDING ECOLOGICAL RESERVE 2021\n689 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2021\n690              RANCHO GUADALUPE DUNES PRESERVE 2021\n691                               VANDENBERG SFB 2021\n692        SANTA CLARA RIVER MCGRATH STATE BEACH 2021\n693                              HOLLYWOOD BEACH 2021\n694                                 ORMOND BEACH 2021\n695                              NBVC POINT MUGU 2021\n696   SATICOY UNITED WATER CONSERVATION DISTRICT 2021\n697                                 VENICE BEACH 2021\n698                                MALIBU LAGOON 2021\n699                                    LA HARBOR 2021\n700                   SEAL BEACH NWR ANAHEIM BAY 2021\n701               BOLSA CHICA ECOLOGICAL RESERVE 2021\n702                       HUNTINGTON STATE BEACH 2021\n703                                BURRIS ISLAND 2021\n704         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2021\n705                           MCB CAMP PENDLETON 2021\n706         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2021\n707          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2021\n708                       MISSION BAY FAA ISLAND 2021\n709              MISSION BAY NORTH FIESTA ISLAND 2021\n710                   MISSION BAY MARINERS POINT 2021\n711                      MISSION BAY STONY POINT 2021\n712                         SDIA LINDBERGH FIELD 2021\n713                             NAS NORTH ISLAND 2021\n714               NAVAL AMPHIBIOUS BASE CORONADO 2021\n715            DSTREET FILL SWEETWATER MARSH NWR 2021\n716                 CHULA VISTA WILDLIFE RESERVE 2021\n717     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2021\n718                         TIJUANA ESTUARY NERR 2021\n719              NAPA SONOMA MARSH WILDLIFE AREA 2022\n720                            SAN PABLO BAY NWR 2022\n721                           MONTEZUMA WETLANDS 2022\n722                                ALAMEDA POINT 2022\n723                   HAYWARD REGIONAL SHORELINE 2022\n724              EDEN LANDING ECOLOGICAL RESERVE 2022\n725 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2022\n726              RANCHO GUADALUPE DUNES PRESERVE 2022\n727                               VANDENBERG SFB 2022\n728        SANTA CLARA RIVER MCGRATH STATE BEACH 2022\n729                              HOLLYWOOD BEACH 2022\n730                                 ORMOND BEACH 2022\n731                              NBVC POINT MUGU 2022\n732                                 VENICE BEACH 2022\n733                                MALIBU LAGOON 2022\n734                                    LA HARBOR 2022\n735                   SEAL BEACH NWR ANAHEIM BAY 2022\n736               BOLSA CHICA ECOLOGICAL RESERVE 2022\n737                       HUNTINGTON STATE BEACH 2022\n738                                BURRIS ISLAND 2022\n739         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2022\n740                           MCB CAMP PENDLETON 2022\n741         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2022\n742          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2022\n743       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2022\n744                       MISSION BAY FAA ISLAND 2022\n745              MISSION BAY NORTH FIESTA ISLAND 2022\n746                   MISSION BAY MARINERS POINT 2022\n747                      MISSION BAY STONY POINT 2022\n748                         SDIA LINDBERGH FIELD 2022\n749                             NAS NORTH ISLAND 2022\n750               NAVAL AMPHIBIOUS BASE CORONADO 2022\n751            DSTREET FILL SWEETWATER MARSH NWR 2022\n752                 CHULA VISTA WILDLIFE RESERVE 2022\n753     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2022\n754                         TIJUANA ESTUARY NERR 2022\n755              NAPA SONOMA MARSH WILDLIFE AREA 2023\n756                            SAN PABLO BAY NWR 2023\n757                           MONTEZUMA WETLANDS 2023\n758                                ALAMEDA POINT 2023\n759                   HAYWARD REGIONAL SHORELINE 2023\n760              EDEN LANDING ECOLOGICAL RESERVE 2023\n761 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2023\n762              RANCHO GUADALUPE DUNES PRESERVE 2023\n763                               VANDENBERG SFB 2023\n764        SANTA CLARA RIVER MCGRATH STATE BEACH 2023\n765                              HOLLYWOOD BEACH 2023\n766                                 ORMOND BEACH 2023\n767                              NBVC POINT MUGU 2023\n768                                 VENICE BEACH 2023\n769                                MALIBU LAGOON 2023\n770                                    LA HARBOR 2023\n771                   SEAL BEACH NWR ANAHEIM BAY 2023\n772               BOLSA CHICA ECOLOGICAL RESERVE 2023\n773                       HUNTINGTON STATE BEACH 2023\n774                                 ANAHEIM LAKE 2023\n775                                BURRIS ISLAND 2023\n776         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2023\n777                           MCB CAMP PENDLETON 2023\n778         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2023\n779          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2023\n780       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2023\n781                       MISSION BAY FAA ISLAND 2023\n782              MISSION BAY NORTH FIESTA ISLAND 2023\n783                   MISSION BAY MARINERS POINT 2023\n784                      MISSION BAY STONY POINT 2023\n785                         SDIA LINDBERGH FIELD 2023\n786                             NAS NORTH ISLAND 2023\n787               NAVAL AMPHIBIOUS BASE CORONADO 2023\n788            DSTREET FILL SWEETWATER MARSH NWR 2023\n789                 CHULA VISTA WILDLIFE RESERVE 2023\n790     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2023\n791                         TIJUANA ESTUARY NERR 2023\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn R, omitting an argument is different from leaving the argument blank. So these two expressions do different things despite looking similar:\nterns[c(\"site_name\", \"year\")]\nterns[c(\"site_name\", \"year\"), ]\nThe first selects the site_name and year columns. The second selects the site_name and year rows (which causes an error, since the least terns dataset doesn’t have row names).\nThere’s one more possibility:\nterns[, c(\"site_name\", \"year\")]\nLike the first expression, this selects the site_name and year columns. You can write the code either way. Some people prefer the first expression because it’s shorter, while others prefer this expression because it makes it clear that terns is two-dimensional.\n\n\nYou can select whatever combination of columns you want with the indexing operator, but it’s somewhat inconvenient if you need to select a lot of columns, because you typically have to type out all of their names. For example, suppose we want to select all of the columns in the least terns dataset whose name starts with nonpred_ (these are mortalities caused by something other than predation).\nThe dplyr package is a collection of functions that make working with data frames more convenient. Install and load the package if you want to follow along with this example:\n\n# install.packages(\"dplyr\")\nlibrary(\"dplyr\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nRemember that you only need to install each package once.\nThe dplyr package’s select function selects columns from a data frame. Unlike most of R’s built-in functions, the functions in dplyr don’t require quotes around column names. To select the site_name and year columns from the least terns dataset, you can write:\n\nselect(terns, site_name, year)\n\n                                       site_name year\n1                          PITTSBURG POWER PLANT 2000\n2                             ALBANY CENTRAL AVE 2000\n3                                  ALAMEDA POINT 2000\n4                                 KETTLEMAN CITY 2000\n5   OCEANO DUNES STATE VEHICULAR RECREATION AREA 2000\n6                RANCHO GUADALUPE DUNES PRESERVE 2000\n7                                 VANDENBERG SFB 2000\n8          SANTA CLARA RIVER MCGRATH STATE BEACH 2000\n9                                   ORMOND BEACH 2000\n10                               NBVC POINT MUGU 2000\n11                                  VENICE BEACH 2000\n12                                     LA HARBOR 2000\n13                    SEAL BEACH NWR ANAHEIM BAY 2000\n14                BOLSA CHICA ECOLOGICAL RESERVE 2000\n15                        HUNTINGTON STATE BEACH 2000\n16          UPPER NEWPORT BAY ECOLOGICAL RESERVE 2000\n17                            MCB CAMP PENDLETON 2000\n18          BATIQUITOS LAGOON ECOLOGICAL RESERVE 2000\n19           SAN ELIJO LAGOON ECOLOGICAL RESERVE 2000\n20                        MISSION BAY FAA ISLAND 2000\n21               MISSION BAY NORTH FIESTA ISLAND 2000\n22                    MISSION BAY MARINERS POINT 2000\n23                          SDIA LINDBERGH FIELD 2000\n24                              NAS NORTH ISLAND 2000\n25                NAVAL AMPHIBIOUS BASE CORONADO 2000\n26             DSTREET FILL SWEETWATER MARSH NWR 2000\n27                  CHULA VISTA WILDLIFE RESERVE 2000\n28      SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2000\n29                          TIJUANA ESTUARY NERR 2000\n30                         PITTSBURG POWER PLANT 2004\n31                            ALBANY CENTRAL AVE 2004\n32                                 ALAMEDA POINT 2004\n33                                KETTLEMAN CITY 2004\n34  OCEANO DUNES STATE VEHICULAR RECREATION AREA 2004\n35               RANCHO GUADALUPE DUNES PRESERVE 2004\n36                                VANDENBERG SFB 2004\n37                        COAL OIL POINT RESERVE 2004\n38         SANTA CLARA RIVER MCGRATH STATE BEACH 2004\n39                               HOLLYWOOD BEACH 2004\n40                                  ORMOND BEACH 2004\n41                               NBVC POINT MUGU 2004\n42                                  VENICE BEACH 2004\n43                                     LA HARBOR 2004\n44                    SEAL BEACH NWR ANAHEIM BAY 2004\n45                BOLSA CHICA ECOLOGICAL RESERVE 2004\n46                        HUNTINGTON STATE BEACH 2004\n47                                 BURRIS ISLAND 2004\n48          UPPER NEWPORT BAY ECOLOGICAL RESERVE 2004\n49                            MCB CAMP PENDLETON 2004\n50          BATIQUITOS LAGOON ECOLOGICAL RESERVE 2004\n51           SAN ELIJO LAGOON ECOLOGICAL RESERVE 2004\n52                        MISSION BAY FAA ISLAND 2004\n53               MISSION BAY NORTH FIESTA ISLAND 2004\n54                    MISSION BAY MARINERS POINT 2004\n55             MISSION BAY SAN DIEGO RIVER MOUTH 2004\n56                          SDIA LINDBERGH FIELD 2004\n57                              NAS NORTH ISLAND 2004\n58                NAVAL AMPHIBIOUS BASE CORONADO 2004\n59                     SILVER STRAND STATE BEACH 2004\n60             DSTREET FILL SWEETWATER MARSH NWR 2004\n61                  CHULA VISTA WILDLIFE RESERVE 2004\n62      SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2004\n63                          TIJUANA ESTUARY NERR 2004\n64                         PITTSBURG POWER PLANT 2005\n65                                 ALAMEDA POINT 2005\n66                    HAYWARD REGIONAL SHORELINE 2005\n67                                KETTLEMAN CITY 2005\n68  OCEANO DUNES STATE VEHICULAR RECREATION AREA 2005\n69               RANCHO GUADALUPE DUNES PRESERVE 2005\n70                                VANDENBERG SFB 2005\n71                        COAL OIL POINT RESERVE 2005\n72         SANTA CLARA RIVER MCGRATH STATE BEACH 2005\n73                                  ORMOND BEACH 2005\n74                               NBVC POINT MUGU 2005\n75                                  VENICE BEACH 2005\n76                                     LA HARBOR 2005\n77                    SEAL BEACH NWR ANAHEIM BAY 2005\n78                BOLSA CHICA ECOLOGICAL RESERVE 2005\n79                        HUNTINGTON STATE BEACH 2005\n80          UPPER NEWPORT BAY ECOLOGICAL RESERVE 2005\n81                            MCB CAMP PENDLETON 2005\n82          BATIQUITOS LAGOON ECOLOGICAL RESERVE 2005\n83           SAN ELIJO LAGOON ECOLOGICAL RESERVE 2005\n84                        MISSION BAY FAA ISLAND 2005\n85               MISSION BAY NORTH FIESTA ISLAND 2005\n86                    MISSION BAY MARINERS POINT 2005\n87             MISSION BAY SAN DIEGO RIVER MOUTH 2005\n88                          SDIA LINDBERGH FIELD 2005\n89                              NAS NORTH ISLAND 2005\n90                NAVAL AMPHIBIOUS BASE CORONADO 2005\n91             DSTREET FILL SWEETWATER MARSH NWR 2005\n92                  CHULA VISTA WILDLIFE RESERVE 2005\n93      SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2005\n94                          TIJUANA ESTUARY NERR 2005\n95                            MONTEZUMA WETLANDS 2006\n96                         PITTSBURG POWER PLANT 2006\n97                                 ALAMEDA POINT 2006\n98                    HAYWARD REGIONAL SHORELINE 2006\n99                                KETTLEMAN CITY 2006\n100 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2006\n101              RANCHO GUADALUPE DUNES PRESERVE 2006\n102                               VANDENBERG SFB 2006\n103                       COAL OIL POINT RESERVE 2006\n104        SANTA CLARA RIVER MCGRATH STATE BEACH 2006\n105                                 ORMOND BEACH 2006\n106                              NBVC POINT MUGU 2006\n107                                 VENICE BEACH 2006\n108                                    LA HARBOR 2006\n109                   SEAL BEACH NWR ANAHEIM BAY 2006\n110               BOLSA CHICA ECOLOGICAL RESERVE 2006\n111                       HUNTINGTON STATE BEACH 2006\n112         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2006\n113                           MCB CAMP PENDLETON 2006\n114         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2006\n115          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2006\n116                       MISSION BAY FAA ISLAND 2006\n117              MISSION BAY NORTH FIESTA ISLAND 2006\n118                   MISSION BAY MARINERS POINT 2006\n119                      MISSION BAY STONY POINT 2006\n120            MISSION BAY SAN DIEGO RIVER MOUTH 2006\n121                         SDIA LINDBERGH FIELD 2006\n122                             NAS NORTH ISLAND 2006\n123               NAVAL AMPHIBIOUS BASE CORONADO 2006\n124            DSTREET FILL SWEETWATER MARSH NWR 2006\n125                 CHULA VISTA WILDLIFE RESERVE 2006\n126     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2006\n127                         TIJUANA ESTUARY NERR 2006\n128              NAPA SONOMA MARSH WILDLIFE AREA 2007\n129                           MONTEZUMA WETLANDS 2007\n130                        PITTSBURG POWER PLANT 2007\n131                                ALAMEDA POINT 2007\n132                   HAYWARD REGIONAL SHORELINE 2007\n133              EDEN LANDING ECOLOGICAL RESERVE 2007\n134                               KETTLEMAN CITY 2007\n135 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2007\n136              RANCHO GUADALUPE DUNES PRESERVE 2007\n137                               VANDENBERG SFB 2007\n138                       COAL OIL POINT RESERVE 2007\n139        SANTA CLARA RIVER MCGRATH STATE BEACH 2007\n140                              HOLLYWOOD BEACH 2007\n141                                 ORMOND BEACH 2007\n142                              NBVC POINT MUGU 2007\n143                                 VENICE BEACH 2007\n144                                    LA HARBOR 2007\n145                   SEAL BEACH NWR ANAHEIM BAY 2007\n146               BOLSA CHICA ECOLOGICAL RESERVE 2007\n147                       HUNTINGTON STATE BEACH 2007\n148                                BURRIS ISLAND 2007\n149         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2007\n150                           MCB CAMP PENDLETON 2007\n151         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2007\n152          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2007\n153                       MISSION BAY FAA ISLAND 2007\n154              MISSION BAY NORTH FIESTA ISLAND 2007\n155                   MISSION BAY MARINERS POINT 2007\n156                      MISSION BAY STONY POINT 2007\n157            MISSION BAY SAN DIEGO RIVER MOUTH 2007\n158                         SDIA LINDBERGH FIELD 2007\n159                             NAS NORTH ISLAND 2007\n160               NAVAL AMPHIBIOUS BASE CORONADO 2007\n161            DSTREET FILL SWEETWATER MARSH NWR 2007\n162                 CHULA VISTA WILDLIFE RESERVE 2007\n163     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2007\n164                         TIJUANA ESTUARY NERR 2007\n165                       SACRAMENTO BUFFERLANDS 2008\n166              NAPA SONOMA MARSH WILDLIFE AREA 2008\n167                           MONTEZUMA WETLANDS 2008\n168                        PITTSBURG POWER PLANT 2008\n169                                ALAMEDA POINT 2008\n170                   HAYWARD REGIONAL SHORELINE 2008\n171              EDEN LANDING ECOLOGICAL RESERVE 2008\n172                               KETTLEMAN CITY 2008\n173 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2008\n174              RANCHO GUADALUPE DUNES PRESERVE 2008\n175                               VANDENBERG SFB 2008\n176                       COAL OIL POINT RESERVE 2008\n177        SANTA CLARA RIVER MCGRATH STATE BEACH 2008\n178                              HOLLYWOOD BEACH 2008\n179                                 ORMOND BEACH 2008\n180                              NBVC POINT MUGU 2008\n181                                 VENICE BEACH 2008\n182                                    LA HARBOR 2008\n183                   SEAL BEACH NWR ANAHEIM BAY 2008\n184               BOLSA CHICA ECOLOGICAL RESERVE 2008\n185                       HUNTINGTON STATE BEACH 2008\n186                                BURRIS ISLAND 2008\n187         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2008\n188                           MCB CAMP PENDLETON 2008\n189         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2008\n190          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2008\n191                       MISSION BAY FAA ISLAND 2008\n192              MISSION BAY NORTH FIESTA ISLAND 2008\n193                   MISSION BAY MARINERS POINT 2008\n194                      MISSION BAY STONY POINT 2008\n195            MISSION BAY SAN DIEGO RIVER MOUTH 2008\n196                         SDIA LINDBERGH FIELD 2008\n197                             NAS NORTH ISLAND 2008\n198               NAVAL AMPHIBIOUS BASE CORONADO 2008\n199            DSTREET FILL SWEETWATER MARSH NWR 2008\n200                 CHULA VISTA WILDLIFE RESERVE 2008\n201     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2008\n202                         TIJUANA ESTUARY NERR 2008\n203                       SACRAMENTO BUFFERLANDS 2009\n204              NAPA SONOMA MARSH WILDLIFE AREA 2009\n205                           MONTEZUMA WETLANDS 2009\n206                        PITTSBURG POWER PLANT 2009\n207                                ALAMEDA POINT 2009\n208                   HAYWARD REGIONAL SHORELINE 2009\n209              EDEN LANDING ECOLOGICAL RESERVE 2009\n210                               KETTLEMAN CITY 2009\n211 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2009\n212              RANCHO GUADALUPE DUNES PRESERVE 2009\n213                               VANDENBERG SFB 2009\n214                       COAL OIL POINT RESERVE 2009\n215        SANTA CLARA RIVER MCGRATH STATE BEACH 2009\n216                              HOLLYWOOD BEACH 2009\n217                                 ORMOND BEACH 2009\n218                              NBVC POINT MUGU 2009\n219                                 VENICE BEACH 2009\n220                                    LA HARBOR 2009\n221                   SEAL BEACH NWR ANAHEIM BAY 2009\n222               BOLSA CHICA ECOLOGICAL RESERVE 2009\n223                       HUNTINGTON STATE BEACH 2009\n224                                BURRIS ISLAND 2009\n225         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2009\n226                           MCB CAMP PENDLETON 2009\n227         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2009\n228          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2009\n229                              FAIRBANKS RANCH 2009\n230                       MISSION BAY FAA ISLAND 2009\n231              MISSION BAY NORTH FIESTA ISLAND 2009\n232                   MISSION BAY MARINERS POINT 2009\n233                      MISSION BAY STONY POINT 2009\n234            MISSION BAY SAN DIEGO RIVER MOUTH 2009\n235                         SDIA LINDBERGH FIELD 2009\n236                             NAS NORTH ISLAND 2009\n237               NAVAL AMPHIBIOUS BASE CORONADO 2009\n238            DSTREET FILL SWEETWATER MARSH NWR 2009\n239                 CHULA VISTA WILDLIFE RESERVE 2009\n240     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2009\n241                         TIJUANA ESTUARY NERR 2009\n242                             ARIZONA GLENDALE 2009\n243                       SACRAMENTO BUFFERLANDS 2010\n244              NAPA SONOMA MARSH WILDLIFE AREA 2010\n245                           MONTEZUMA WETLANDS 2010\n246                        PITTSBURG POWER PLANT 2010\n247                                ALAMEDA POINT 2010\n248                   HAYWARD REGIONAL SHORELINE 2010\n249              EDEN LANDING ECOLOGICAL RESERVE 2010\n250                               KETTLEMAN CITY 2010\n251 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2010\n252              RANCHO GUADALUPE DUNES PRESERVE 2010\n253                               VANDENBERG SFB 2010\n254                       COAL OIL POINT RESERVE 2010\n255        SANTA CLARA RIVER MCGRATH STATE BEACH 2010\n256                              HOLLYWOOD BEACH 2010\n257                                 ORMOND BEACH 2010\n258                              NBVC POINT MUGU 2010\n259                                 VENICE BEACH 2010\n260                                    LA HARBOR 2010\n261                   SEAL BEACH NWR ANAHEIM BAY 2010\n262               BOLSA CHICA ECOLOGICAL RESERVE 2010\n263                       HUNTINGTON STATE BEACH 2010\n264                                BURRIS ISLAND 2010\n265         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2010\n266                           MCB CAMP PENDLETON 2010\n267         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2010\n268          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2010\n269                              FAIRBANKS RANCH 2010\n270       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2010\n271                       MISSION BAY FAA ISLAND 2010\n272              MISSION BAY NORTH FIESTA ISLAND 2010\n273                   MISSION BAY MARINERS POINT 2010\n274                      MISSION BAY STONY POINT 2010\n275            MISSION BAY SAN DIEGO RIVER MOUTH 2010\n276                         SDIA LINDBERGH FIELD 2010\n277                             NAS NORTH ISLAND 2010\n278               NAVAL AMPHIBIOUS BASE CORONADO 2010\n279            DSTREET FILL SWEETWATER MARSH NWR 2010\n280                 CHULA VISTA WILDLIFE RESERVE 2010\n281     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2010\n282                         TIJUANA ESTUARY NERR 2010\n283                       SACRAMENTO BUFFERLANDS 2011\n284              NAPA SONOMA MARSH WILDLIFE AREA 2011\n285                           MONTEZUMA WETLANDS 2011\n286                        PITTSBURG POWER PLANT 2011\n287                                ALAMEDA POINT 2011\n288                   HAYWARD REGIONAL SHORELINE 2011\n289              EDEN LANDING ECOLOGICAL RESERVE 2011\n290                               KETTLEMAN CITY 2011\n291 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2011\n292              RANCHO GUADALUPE DUNES PRESERVE 2011\n293                               VANDENBERG SFB 2011\n294                       COAL OIL POINT RESERVE 2011\n295        SANTA CLARA RIVER MCGRATH STATE BEACH 2011\n296                              HOLLYWOOD BEACH 2011\n297                                 ORMOND BEACH 2011\n298                              NBVC POINT MUGU 2011\n299                                 VENICE BEACH 2011\n300                                    LA HARBOR 2011\n301                   SEAL BEACH NWR ANAHEIM BAY 2011\n302               BOLSA CHICA ECOLOGICAL RESERVE 2011\n303                       HUNTINGTON STATE BEACH 2011\n304                                BURRIS ISLAND 2011\n305         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2011\n306                           MCB CAMP PENDLETON 2011\n307         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2011\n308          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2011\n309                              FAIRBANKS RANCH 2011\n310       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2011\n311                       MISSION BAY FAA ISLAND 2011\n312              MISSION BAY NORTH FIESTA ISLAND 2011\n313                   MISSION BAY MARINERS POINT 2011\n314                      MISSION BAY STONY POINT 2011\n315            MISSION BAY SAN DIEGO RIVER MOUTH 2011\n316                         SDIA LINDBERGH FIELD 2011\n317                             NAS NORTH ISLAND 2011\n318               NAVAL AMPHIBIOUS BASE CORONADO 2011\n319            DSTREET FILL SWEETWATER MARSH NWR 2011\n320                 CHULA VISTA WILDLIFE RESERVE 2011\n321     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2011\n322                         TIJUANA ESTUARY NERR 2011\n323                                   SALTON SEA 2011\n324                       SACRAMENTO BUFFERLANDS 2012\n325              NAPA SONOMA MARSH WILDLIFE AREA 2012\n326                           MONTEZUMA WETLANDS 2012\n327                        PITTSBURG POWER PLANT 2012\n328                                ALAMEDA POINT 2012\n329                   HAYWARD REGIONAL SHORELINE 2012\n330              EDEN LANDING ECOLOGICAL RESERVE 2012\n331                               KETTLEMAN CITY 2012\n332 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2012\n333              RANCHO GUADALUPE DUNES PRESERVE 2012\n334                               VANDENBERG SFB 2012\n335                       COAL OIL POINT RESERVE 2012\n336        SANTA CLARA RIVER MCGRATH STATE BEACH 2012\n337                              HOLLYWOOD BEACH 2012\n338                                 ORMOND BEACH 2012\n339                              NBVC POINT MUGU 2012\n340                                 VENICE BEACH 2012\n341                                    LA HARBOR 2012\n342                   SEAL BEACH NWR ANAHEIM BAY 2012\n343               BOLSA CHICA ECOLOGICAL RESERVE 2012\n344                       HUNTINGTON STATE BEACH 2012\n345                                BURRIS ISLAND 2012\n346         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2012\n347                           MCB CAMP PENDLETON 2012\n348         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2012\n349          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2012\n350                              FAIRBANKS RANCH 2012\n351       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2012\n352                       MISSION BAY FAA ISLAND 2012\n353              MISSION BAY NORTH FIESTA ISLAND 2012\n354                   MISSION BAY MARINERS POINT 2012\n355                      MISSION BAY STONY POINT 2012\n356            MISSION BAY SAN DIEGO RIVER MOUTH 2012\n357                         SDIA LINDBERGH FIELD 2012\n358                             NAS NORTH ISLAND 2012\n359               NAVAL AMPHIBIOUS BASE CORONADO 2012\n360            DSTREET FILL SWEETWATER MARSH NWR 2012\n361                 CHULA VISTA WILDLIFE RESERVE 2012\n362     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2012\n363                         TIJUANA ESTUARY NERR 2012\n364                                   SALTON SEA 2012\n365                       SACRAMENTO BUFFERLANDS 2013\n366              NAPA SONOMA MARSH WILDLIFE AREA 2013\n367                           MONTEZUMA WETLANDS 2013\n368                        PITTSBURG POWER PLANT 2013\n369                                ALAMEDA POINT 2013\n370                   HAYWARD REGIONAL SHORELINE 2013\n371              EDEN LANDING ECOLOGICAL RESERVE 2013\n372                               KETTLEMAN CITY 2013\n373 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2013\n374              RANCHO GUADALUPE DUNES PRESERVE 2013\n375                               VANDENBERG SFB 2013\n376                       COAL OIL POINT RESERVE 2013\n377        SANTA CLARA RIVER MCGRATH STATE BEACH 2013\n378                              HOLLYWOOD BEACH 2013\n379                                 ORMOND BEACH 2013\n380                              NBVC POINT MUGU 2013\n381   SATICOY UNITED WATER CONSERVATION DISTRICT 2013\n382                                 VENICE BEACH 2013\n383                                    LA HARBOR 2013\n384                   SEAL BEACH NWR ANAHEIM BAY 2013\n385               BOLSA CHICA ECOLOGICAL RESERVE 2013\n386                       HUNTINGTON STATE BEACH 2013\n387                                BURRIS ISLAND 2013\n388         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2013\n389                           MCB CAMP PENDLETON 2013\n390         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2013\n391          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2013\n392                              FAIRBANKS RANCH 2013\n393       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2013\n394                       MISSION BAY FAA ISLAND 2013\n395              MISSION BAY NORTH FIESTA ISLAND 2013\n396                   MISSION BAY MARINERS POINT 2013\n397                      MISSION BAY STONY POINT 2013\n398            MISSION BAY SAN DIEGO RIVER MOUTH 2013\n399                         SDIA LINDBERGH FIELD 2013\n400                             NAS NORTH ISLAND 2013\n401               NAVAL AMPHIBIOUS BASE CORONADO 2013\n402            DSTREET FILL SWEETWATER MARSH NWR 2013\n403                 CHULA VISTA WILDLIFE RESERVE 2013\n404     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2013\n405                         TIJUANA ESTUARY NERR 2013\n406                                   SALTON SEA 2013\n407                       SACRAMENTO BUFFERLANDS 2014\n408              NAPA SONOMA MARSH WILDLIFE AREA 2014\n409                           MONTEZUMA WETLANDS 2014\n410                        PITTSBURG POWER PLANT 2014\n411                                ALAMEDA POINT 2014\n412                   HAYWARD REGIONAL SHORELINE 2014\n413              EDEN LANDING ECOLOGICAL RESERVE 2014\n414                               KETTLEMAN CITY 2014\n415 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2014\n416              RANCHO GUADALUPE DUNES PRESERVE 2014\n417                               VANDENBERG SFB 2014\n418                       COAL OIL POINT RESERVE 2014\n419        SANTA CLARA RIVER MCGRATH STATE BEACH 2014\n420                              HOLLYWOOD BEACH 2014\n421                                 ORMOND BEACH 2014\n422                              NBVC POINT MUGU 2014\n423   SATICOY UNITED WATER CONSERVATION DISTRICT 2014\n424                                 VENICE BEACH 2014\n425                                    LA HARBOR 2014\n426                   SEAL BEACH NWR ANAHEIM BAY 2014\n427               BOLSA CHICA ECOLOGICAL RESERVE 2014\n428                       HUNTINGTON STATE BEACH 2014\n429                                BURRIS ISLAND 2014\n430         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2014\n431                           MCB CAMP PENDLETON 2014\n432         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2014\n433          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2014\n434                              FAIRBANKS RANCH 2014\n435       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2014\n436                       MISSION BAY FAA ISLAND 2014\n437              MISSION BAY NORTH FIESTA ISLAND 2014\n438                   MISSION BAY MARINERS POINT 2014\n439                      MISSION BAY STONY POINT 2014\n440            MISSION BAY SAN DIEGO RIVER MOUTH 2014\n441                         SDIA LINDBERGH FIELD 2014\n442                             NAS NORTH ISLAND 2014\n443               NAVAL AMPHIBIOUS BASE CORONADO 2014\n444            DSTREET FILL SWEETWATER MARSH NWR 2014\n445                 CHULA VISTA WILDLIFE RESERVE 2014\n446     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2014\n447                         TIJUANA ESTUARY NERR 2014\n448                                   SALTON SEA 2014\n449                       SACRAMENTO BUFFERLANDS 2015\n450              NAPA SONOMA MARSH WILDLIFE AREA 2015\n451                           MONTEZUMA WETLANDS 2015\n452                        PITTSBURG POWER PLANT 2015\n453                                ALAMEDA POINT 2015\n454                   HAYWARD REGIONAL SHORELINE 2015\n455              EDEN LANDING ECOLOGICAL RESERVE 2015\n456                               KETTLEMAN CITY 2015\n457 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2015\n458              RANCHO GUADALUPE DUNES PRESERVE 2015\n459                               VANDENBERG SFB 2015\n460                       COAL OIL POINT RESERVE 2015\n461        SANTA CLARA RIVER MCGRATH STATE BEACH 2015\n462                              HOLLYWOOD BEACH 2015\n463                                 ORMOND BEACH 2015\n464                              NBVC POINT MUGU 2015\n465   SATICOY UNITED WATER CONSERVATION DISTRICT 2015\n466                                 VENICE BEACH 2015\n467                                    LA HARBOR 2015\n468                   SEAL BEACH NWR ANAHEIM BAY 2015\n469               BOLSA CHICA ECOLOGICAL RESERVE 2015\n470                       HUNTINGTON STATE BEACH 2015\n471                                BURRIS ISLAND 2015\n472         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2015\n473                           MCB CAMP PENDLETON 2015\n474         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2015\n475          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2015\n476                              FAIRBANKS RANCH 2015\n477       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2015\n478                       MISSION BAY FAA ISLAND 2015\n479              MISSION BAY NORTH FIESTA ISLAND 2015\n480                   MISSION BAY MARINERS POINT 2015\n481                      MISSION BAY STONY POINT 2015\n482            MISSION BAY SAN DIEGO RIVER MOUTH 2015\n483                         SDIA LINDBERGH FIELD 2015\n484                             NAS NORTH ISLAND 2015\n485               NAVAL AMPHIBIOUS BASE CORONADO 2015\n486            DSTREET FILL SWEETWATER MARSH NWR 2015\n487                 CHULA VISTA WILDLIFE RESERVE 2015\n488     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2015\n489                         TIJUANA ESTUARY NERR 2015\n490                                   SALTON SEA 2015\n491                       SACRAMENTO BUFFERLANDS 2016\n492              NAPA SONOMA MARSH WILDLIFE AREA 2016\n493                           MONTEZUMA WETLANDS 2016\n494                        PITTSBURG POWER PLANT 2016\n495                                ALAMEDA POINT 2016\n496                   HAYWARD REGIONAL SHORELINE 2016\n497              EDEN LANDING ECOLOGICAL RESERVE 2016\n498                               KETTLEMAN CITY 2016\n499 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2016\n500              RANCHO GUADALUPE DUNES PRESERVE 2016\n501                               VANDENBERG SFB 2016\n502                       COAL OIL POINT RESERVE 2016\n503        SANTA CLARA RIVER MCGRATH STATE BEACH 2016\n504                              HOLLYWOOD BEACH 2016\n505                                 ORMOND BEACH 2016\n506                              NBVC POINT MUGU 2016\n507   SATICOY UNITED WATER CONSERVATION DISTRICT 2016\n508                                 VENICE BEACH 2016\n509                                    LA HARBOR 2016\n510                   SEAL BEACH NWR ANAHEIM BAY 2016\n511               BOLSA CHICA ECOLOGICAL RESERVE 2016\n512                       HUNTINGTON STATE BEACH 2016\n513                                 ANAHEIM LAKE 2016\n514                                BURRIS ISLAND 2016\n515         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2016\n516                           MCB CAMP PENDLETON 2016\n517         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2016\n518          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2016\n519                              FAIRBANKS RANCH 2016\n520       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2016\n521                       MISSION BAY FAA ISLAND 2016\n522              MISSION BAY NORTH FIESTA ISLAND 2016\n523                   MISSION BAY MARINERS POINT 2016\n524                      MISSION BAY STONY POINT 2016\n525            MISSION BAY SAN DIEGO RIVER MOUTH 2016\n526                         SDIA LINDBERGH FIELD 2016\n527                             NAS NORTH ISLAND 2016\n528               NAVAL AMPHIBIOUS BASE CORONADO 2016\n529            DSTREET FILL SWEETWATER MARSH NWR 2016\n530                 CHULA VISTA WILDLIFE RESERVE 2016\n531     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2016\n532                         TIJUANA ESTUARY NERR 2016\n533                                   SALTON SEA 2016\n534                       SACRAMENTO BUFFERLANDS 2017\n535              NAPA SONOMA MARSH WILDLIFE AREA 2017\n536                           MONTEZUMA WETLANDS 2017\n537                        PITTSBURG POWER PLANT 2017\n538                                ALAMEDA POINT 2017\n539                   HAYWARD REGIONAL SHORELINE 2017\n540              EDEN LANDING ECOLOGICAL RESERVE 2017\n541                               KETTLEMAN CITY 2017\n542 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2017\n543              RANCHO GUADALUPE DUNES PRESERVE 2017\n544                   GUADALUPE NIPOMO DUNES NWR 2017\n545                               VANDENBERG SFB 2017\n546                       COAL OIL POINT RESERVE 2017\n547        SANTA CLARA RIVER MCGRATH STATE BEACH 2017\n548                              HOLLYWOOD BEACH 2017\n549                                 ORMOND BEACH 2017\n550                              NBVC POINT MUGU 2017\n551   SATICOY UNITED WATER CONSERVATION DISTRICT 2017\n552                                 VENICE BEACH 2017\n553                                MALIBU LAGOON 2017\n554                                    LA HARBOR 2017\n555                   SEAL BEACH NWR ANAHEIM BAY 2017\n556               BOLSA CHICA ECOLOGICAL RESERVE 2017\n557                       HUNTINGTON STATE BEACH 2017\n558                                 ANAHEIM LAKE 2017\n559                                BURRIS ISLAND 2017\n560         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2017\n561                           MCB CAMP PENDLETON 2017\n562         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2017\n563          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2017\n564                              FAIRBANKS RANCH 2017\n565       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2017\n566                       MISSION BAY FAA ISLAND 2017\n567              MISSION BAY NORTH FIESTA ISLAND 2017\n568                   MISSION BAY MARINERS POINT 2017\n569                      MISSION BAY STONY POINT 2017\n570            MISSION BAY SAN DIEGO RIVER MOUTH 2017\n571                         SDIA LINDBERGH FIELD 2017\n572                             NAS NORTH ISLAND 2017\n573               NAVAL AMPHIBIOUS BASE CORONADO 2017\n574            DSTREET FILL SWEETWATER MARSH NWR 2017\n575                 CHULA VISTA WILDLIFE RESERVE 2017\n576     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2017\n577                         TIJUANA ESTUARY NERR 2017\n578                                   SALTON SEA 2017\n579              NAPA SONOMA MARSH WILDLIFE AREA 2018\n580                           MONTEZUMA WETLANDS 2018\n581                                ALAMEDA POINT 2018\n582                   HAYWARD REGIONAL SHORELINE 2018\n583              EDEN LANDING ECOLOGICAL RESERVE 2018\n584 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2018\n585                               VANDENBERG SFB 2018\n586        SANTA CLARA RIVER MCGRATH STATE BEACH 2018\n587                              HOLLYWOOD BEACH 2018\n588                                 ORMOND BEACH 2018\n589                              NBVC POINT MUGU 2018\n590   SATICOY UNITED WATER CONSERVATION DISTRICT 2018\n591                                 VENICE BEACH 2018\n592                                MALIBU LAGOON 2018\n593                                    LA HARBOR 2018\n594                   SEAL BEACH NWR ANAHEIM BAY 2018\n595               BOLSA CHICA ECOLOGICAL RESERVE 2018\n596                       HUNTINGTON STATE BEACH 2018\n597                                BURRIS ISLAND 2018\n598         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2018\n599                           MCB CAMP PENDLETON 2018\n600         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2018\n601          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2018\n602                       MISSION BAY FAA ISLAND 2018\n603              MISSION BAY NORTH FIESTA ISLAND 2018\n604                   MISSION BAY MARINERS POINT 2018\n605                      MISSION BAY STONY POINT 2018\n606                         SDIA LINDBERGH FIELD 2018\n607                             NAS NORTH ISLAND 2018\n608               NAVAL AMPHIBIOUS BASE CORONADO 2018\n609            DSTREET FILL SWEETWATER MARSH NWR 2018\n610                 CHULA VISTA WILDLIFE RESERVE 2018\n611     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2018\n612                         TIJUANA ESTUARY NERR 2018\n613              NAPA SONOMA MARSH WILDLIFE AREA 2019\n614                           MONTEZUMA WETLANDS 2019\n615                                ALAMEDA POINT 2019\n616                   HAYWARD REGIONAL SHORELINE 2019\n617              EDEN LANDING ECOLOGICAL RESERVE 2019\n618 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2019\n619              RANCHO GUADALUPE DUNES PRESERVE 2019\n620                               VANDENBERG SFB 2019\n621        SANTA CLARA RIVER MCGRATH STATE BEACH 2019\n622                              HOLLYWOOD BEACH 2019\n623                                 ORMOND BEACH 2019\n624                              NBVC POINT MUGU 2019\n625   SATICOY UNITED WATER CONSERVATION DISTRICT 2019\n626                                 VENICE BEACH 2019\n627                                MALIBU LAGOON 2019\n628                                    LA HARBOR 2019\n629                   SEAL BEACH NWR ANAHEIM BAY 2019\n630               BOLSA CHICA ECOLOGICAL RESERVE 2019\n631                       HUNTINGTON STATE BEACH 2019\n632                                BURRIS ISLAND 2019\n633         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2019\n634                           MCB CAMP PENDLETON 2019\n635         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2019\n636          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2019\n637                       MISSION BAY FAA ISLAND 2019\n638              MISSION BAY NORTH FIESTA ISLAND 2019\n639                   MISSION BAY MARINERS POINT 2019\n640                      MISSION BAY STONY POINT 2019\n641                         SDIA LINDBERGH FIELD 2019\n642                             NAS NORTH ISLAND 2019\n643               NAVAL AMPHIBIOUS BASE CORONADO 2019\n644            DSTREET FILL SWEETWATER MARSH NWR 2019\n645                 CHULA VISTA WILDLIFE RESERVE 2019\n646     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2019\n647                         TIJUANA ESTUARY NERR 2019\n648              NAPA SONOMA MARSH WILDLIFE AREA 2020\n649                           MONTEZUMA WETLANDS 2020\n650                                ALAMEDA POINT 2020\n651                   HAYWARD REGIONAL SHORELINE 2020\n652              EDEN LANDING ECOLOGICAL RESERVE 2020\n653 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2020\n654              RANCHO GUADALUPE DUNES PRESERVE 2020\n655                               VANDENBERG SFB 2020\n656        SANTA CLARA RIVER MCGRATH STATE BEACH 2020\n657                              HOLLYWOOD BEACH 2020\n658                                 ORMOND BEACH 2020\n659                              NBVC POINT MUGU 2020\n660   SATICOY UNITED WATER CONSERVATION DISTRICT 2020\n661                                 VENICE BEACH 2020\n662                                MALIBU LAGOON 2020\n663                                    LA HARBOR 2020\n664                   SEAL BEACH NWR ANAHEIM BAY 2020\n665               BOLSA CHICA ECOLOGICAL RESERVE 2020\n666                       HUNTINGTON STATE BEACH 2020\n667                                BURRIS ISLAND 2020\n668         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2020\n669                           MCB CAMP PENDLETON 2020\n670         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2020\n671          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2020\n672                       MISSION BAY FAA ISLAND 2020\n673              MISSION BAY NORTH FIESTA ISLAND 2020\n674                   MISSION BAY MARINERS POINT 2020\n675                      MISSION BAY STONY POINT 2020\n676                         SDIA LINDBERGH FIELD 2020\n677                             NAS NORTH ISLAND 2020\n678               NAVAL AMPHIBIOUS BASE CORONADO 2020\n679            DSTREET FILL SWEETWATER MARSH NWR 2020\n680                 CHULA VISTA WILDLIFE RESERVE 2020\n681     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2020\n682                         TIJUANA ESTUARY NERR 2020\n683              NAPA SONOMA MARSH WILDLIFE AREA 2021\n684                            SAN PABLO BAY NWR 2021\n685                           MONTEZUMA WETLANDS 2021\n686                                ALAMEDA POINT 2021\n687                   HAYWARD REGIONAL SHORELINE 2021\n688              EDEN LANDING ECOLOGICAL RESERVE 2021\n689 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2021\n690              RANCHO GUADALUPE DUNES PRESERVE 2021\n691                               VANDENBERG SFB 2021\n692        SANTA CLARA RIVER MCGRATH STATE BEACH 2021\n693                              HOLLYWOOD BEACH 2021\n694                                 ORMOND BEACH 2021\n695                              NBVC POINT MUGU 2021\n696   SATICOY UNITED WATER CONSERVATION DISTRICT 2021\n697                                 VENICE BEACH 2021\n698                                MALIBU LAGOON 2021\n699                                    LA HARBOR 2021\n700                   SEAL BEACH NWR ANAHEIM BAY 2021\n701               BOLSA CHICA ECOLOGICAL RESERVE 2021\n702                       HUNTINGTON STATE BEACH 2021\n703                                BURRIS ISLAND 2021\n704         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2021\n705                           MCB CAMP PENDLETON 2021\n706         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2021\n707          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2021\n708                       MISSION BAY FAA ISLAND 2021\n709              MISSION BAY NORTH FIESTA ISLAND 2021\n710                   MISSION BAY MARINERS POINT 2021\n711                      MISSION BAY STONY POINT 2021\n712                         SDIA LINDBERGH FIELD 2021\n713                             NAS NORTH ISLAND 2021\n714               NAVAL AMPHIBIOUS BASE CORONADO 2021\n715            DSTREET FILL SWEETWATER MARSH NWR 2021\n716                 CHULA VISTA WILDLIFE RESERVE 2021\n717     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2021\n718                         TIJUANA ESTUARY NERR 2021\n719              NAPA SONOMA MARSH WILDLIFE AREA 2022\n720                            SAN PABLO BAY NWR 2022\n721                           MONTEZUMA WETLANDS 2022\n722                                ALAMEDA POINT 2022\n723                   HAYWARD REGIONAL SHORELINE 2022\n724              EDEN LANDING ECOLOGICAL RESERVE 2022\n725 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2022\n726              RANCHO GUADALUPE DUNES PRESERVE 2022\n727                               VANDENBERG SFB 2022\n728        SANTA CLARA RIVER MCGRATH STATE BEACH 2022\n729                              HOLLYWOOD BEACH 2022\n730                                 ORMOND BEACH 2022\n731                              NBVC POINT MUGU 2022\n732                                 VENICE BEACH 2022\n733                                MALIBU LAGOON 2022\n734                                    LA HARBOR 2022\n735                   SEAL BEACH NWR ANAHEIM BAY 2022\n736               BOLSA CHICA ECOLOGICAL RESERVE 2022\n737                       HUNTINGTON STATE BEACH 2022\n738                                BURRIS ISLAND 2022\n739         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2022\n740                           MCB CAMP PENDLETON 2022\n741         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2022\n742          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2022\n743       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2022\n744                       MISSION BAY FAA ISLAND 2022\n745              MISSION BAY NORTH FIESTA ISLAND 2022\n746                   MISSION BAY MARINERS POINT 2022\n747                      MISSION BAY STONY POINT 2022\n748                         SDIA LINDBERGH FIELD 2022\n749                             NAS NORTH ISLAND 2022\n750               NAVAL AMPHIBIOUS BASE CORONADO 2022\n751            DSTREET FILL SWEETWATER MARSH NWR 2022\n752                 CHULA VISTA WILDLIFE RESERVE 2022\n753     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2022\n754                         TIJUANA ESTUARY NERR 2022\n755              NAPA SONOMA MARSH WILDLIFE AREA 2023\n756                            SAN PABLO BAY NWR 2023\n757                           MONTEZUMA WETLANDS 2023\n758                                ALAMEDA POINT 2023\n759                   HAYWARD REGIONAL SHORELINE 2023\n760              EDEN LANDING ECOLOGICAL RESERVE 2023\n761 OCEANO DUNES STATE VEHICULAR RECREATION AREA 2023\n762              RANCHO GUADALUPE DUNES PRESERVE 2023\n763                               VANDENBERG SFB 2023\n764        SANTA CLARA RIVER MCGRATH STATE BEACH 2023\n765                              HOLLYWOOD BEACH 2023\n766                                 ORMOND BEACH 2023\n767                              NBVC POINT MUGU 2023\n768                                 VENICE BEACH 2023\n769                                MALIBU LAGOON 2023\n770                                    LA HARBOR 2023\n771                   SEAL BEACH NWR ANAHEIM BAY 2023\n772               BOLSA CHICA ECOLOGICAL RESERVE 2023\n773                       HUNTINGTON STATE BEACH 2023\n774                                 ANAHEIM LAKE 2023\n775                                BURRIS ISLAND 2023\n776         UPPER NEWPORT BAY ECOLOGICAL RESERVE 2023\n777                           MCB CAMP PENDLETON 2023\n778         BATIQUITOS LAGOON ECOLOGICAL RESERVE 2023\n779          SAN ELIJO LAGOON ECOLOGICAL RESERVE 2023\n780       SAN DIEGUITO LAGOON ECOLOGICAL RESERVE 2023\n781                       MISSION BAY FAA ISLAND 2023\n782              MISSION BAY NORTH FIESTA ISLAND 2023\n783                   MISSION BAY MARINERS POINT 2023\n784                      MISSION BAY STONY POINT 2023\n785                         SDIA LINDBERGH FIELD 2023\n786                             NAS NORTH ISLAND 2023\n787               NAVAL AMPHIBIOUS BASE CORONADO 2023\n788            DSTREET FILL SWEETWATER MARSH NWR 2023\n789                 CHULA VISTA WILDLIFE RESERVE 2023\n790     SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS 2023\n791                         TIJUANA ESTUARY NERR 2023\n\n\nThe advantage of select over the indexing operator [ is that it provides concise ways to a lot of columns. For instance, to select all of the columns whose names begin with nonpred_, you can write:\n\nselect(terns, starts_with(\"nonpred_\"))\n\n    nonpred_eggs nonpred_chicks nonpred_fl nonpred_ad\n1              3              0          0          0\n2             NA             NA         NA         NA\n3            124             81          2          1\n4             NA              3          1          6\n5              2              0          0          0\n6              0              1          0          0\n7             NA             27          0          0\n8              4              3         NA         NA\n9              2              0          0          0\n10            NA             NA         NA         NA\n11            26             NA         NA         NA\n12            77             NA         NA         NA\n13            10              3         NA         NA\n14            NA             NA         NA         NA\n15            74              4          0          0\n16            NA             NA         NA         NA\n17            72            118         24         16\n18            10             35          3          6\n19             1              0          0          0\n20            28             NA         NA         NA\n21             8              0          0          1\n22            32             11          1          0\n23             7              1          0          0\n24            20              3          1          0\n25            67             19          2          0\n26             8              2          0          1\n27            NA             NA         NA         NA\n28             9              3          0          0\n29            56              7          1          0\n30             0              0          0          0\n31            NA             NA         NA         NA\n32           142            156          1          2\n33            NA             NA         NA         NA\n34            NA             NA         NA         NA\n35             2              0         NA          0\n36             1              0         NA          0\n37            NA              0         NA          2\n38             6              1          0          0\n39             7              2          1         NA\n40             6              1          0          1\n41           132             10          0          0\n42             0              0         NA          0\n43           179            453         31         12\n44            69             85          2          2\n45             0              5          0          1\n46            24             NA         NA         NA\n47            NA             NA         NA         NA\n48             5              0          0          0\n49           521           1063         11          7\n50           146            417          0          0\n51            NA             NA         NA         NA\n52             0             NA         NA         NA\n53             1              0         NA          0\n54           144              2         NA          0\n55             9             NA         NA         NA\n56            24             17          0          0\n57           120             14          0          0\n58           306             48          0          2\n59            NA             NA         NA         NA\n60            30             11          0          0\n61            17              1          0          0\n62             4              2          0          2\n63           272              2          1          2\n64            NA             NA         NA         NA\n65           186            286          0          0\n66            NA             NA         NA         NA\n67            NA             NA         NA         NA\n68            25              9          0          0\n69            NA             NA         NA         NA\n70            42             29          1          0\n71            NA             NA         NA         NA\n72             1              0          0          0\n73            24              1          0          0\n74           156             22          2          9\n75             0              0          0          1\n76           217            868         13          3\n77            NA              4          2          2\n78            13             13          1          1\n79            NA             80          0          7\n80            NA              1          0          0\n81           556            928         16         11\n82           112            307         15          6\n83             0              0          4          3\n84             0              0          0          0\n85             0              0          0          0\n86           186             43          4          0\n87            37              2          1          1\n88            31             21         10          1\n89            76             18          2          2\n90           225             27          6          1\n91            23             17          1          1\n92            13              5          0          0\n93            10              0         NA         NA\n94            99             NA         NA         NA\n95            NA             NA         NA         NA\n96            NA             NA         NA         NA\n97           248            202          1          1\n98             0              0          0          0\n99            NA             NA         NA         NA\n100           12              1          2          2\n101           NA             NA         NA         NA\n102            4              0         NA          0\n103           NA             NA         NA         NA\n104           NA             NA         NA         NA\n105            5              2          0          0\n106           85             19          0          0\n107           60             57          0          1\n108          434            253          7          7\n109           27             15          0          0\n110           40             14          0          1\n111           NA             NA         NA         NA\n112            0              0          0          0\n113          307            420         63          7\n114           81            189         21          5\n115           NA             NA         NA         NA\n116            4              0          0          2\n117            3              2          0          0\n118            1             NA         NA         NA\n119           11              1         NA         NA\n120           NA             NA         NA         NA\n121           NA             NA         NA         NA\n122           65             15          0          0\n123          230             23          1          7\n124           NA             NA         NA         NA\n125           NA             NA         NA         NA\n126           24              2          0          0\n127           98              9          2          0\n128           NA             NA         NA         NA\n129           NA             NA         NA         NA\n130           NA             NA         NA         NA\n131          122            127         28          0\n132           NA             NA          0          0\n133           NA             NA         NA         NA\n134           NA             NA         NA         NA\n135           20              4          4          0\n136            0              0          0          0\n137            8              2          0          0\n138            0              0         NA          0\n139           13              0          0          0\n140            0              0          0          0\n141            9              4          0          0\n142           77             NA         NA         NA\n143           89            125         NA          3\n144          385             NA         NA         NA\n145           20              6          0          0\n146           18              2          0          0\n147           82              8          3          1\n148           NA             NA         NA         NA\n149           NA             NA         NA         NA\n150          430            236         35         11\n151          181            109          6          3\n152           NA             NA         NA         NA\n153            1              2          0          0\n154           15              3          5          0\n155           24              8          1          0\n156            4              3          0          0\n157           16              0          5          0\n158           53             12          3          1\n159           56             11          3          0\n160          238             52         10          0\n161           31              7          5          0\n162            3              0         NA          0\n163           28              4          0          0\n164          139              8          2          0\n165           NA             NA         NA         NA\n166           NA             NA         NA         NA\n167           NA             NA         NA         NA\n168           NA             NA         NA         NA\n169           54             22          0          0\n170           15              3          0          0\n171           NA             NA         NA         NA\n172           NA             NA         NA         NA\n173            9              7          1          1\n174           NA             NA         NA         NA\n175            2              4          4          0\n176           NA             NA         NA         NA\n177            5              2         NA         NA\n178            3             NA         NA         NA\n179           10              2         NA         NA\n180          157              4          1          0\n181           35            129          5          3\n182          277            153          1          2\n183           NA             29          2          3\n184           17             18          1          1\n185          167             24          7         10\n186           NA             NA         NA         NA\n187           NA             NA         NA         NA\n188          600            680        207          4\n189          131            167         70          3\n190           NA             NA         NA         NA\n191           NA             NA         NA         NA\n192            0              0         NA          0\n193            0              0         NA          0\n194           NA             NA         NA         NA\n195            0              0          0          0\n196           29             19          5          0\n197           46              5          0          0\n198          296             46          8          3\n199           29             12          0          0\n200            4              0          0          0\n201           31              0          0          0\n202           63              3          2          1\n203           NA             NA         NA         NA\n204           NA             NA         NA         NA\n205           NA             NA         NA         NA\n206           NA             NA         NA          0\n207          106             50          4          1\n208           NA              1          0          0\n209            0              0         NA          0\n210           NA             NA         NA         NA\n211            8              0          1          0\n212            0              0          0          0\n213            6              2          5          0\n214           NA             NA         NA         NA\n215           11              1         NA         NA\n216            1             NA         NA         NA\n217           13              1         NA         NA\n218          214             25          8          0\n219            0              0         NA          0\n220          328            123          3          1\n221           NA             NA         NA         NA\n222           NA             14          0          0\n223          168             57          1          2\n224           NA             NA         NA         NA\n225           NA             NA         NA         NA\n226          474            769         32          4\n227          123            278         33          2\n228           NA             NA         NA          0\n229           NA             NA         NA         NA\n230           40              3         NA          0\n231           14              3         NA         NA\n232           12             NA         NA         NA\n233           NA             NA         NA         NA\n234           NA             NA         NA         NA\n235           58             46          2          0\n236           NA             NA          1         NA\n237           NA             NA          0         NA\n238           22             12          0          0\n239            8              2          0          0\n240            7              2          0          0\n241           90              0          3          0\n242           NA             NA         NA         NA\n243            0              0          0          0\n244           17              3          0          0\n245           NA             NA         NA         NA\n246            0              0          0          0\n247           94             68          5          0\n248            9              2          2          0\n249           NA             NA         NA         NA\n250           NA             NA         NA         NA\n251            6              1          1          0\n252            0              0          0          0\n253            9              5          0          0\n254           NA             NA         NA         NA\n255            3             NA         NA         NA\n256            0              0          0          0\n257           12              2         NA          2\n258          222             91         27          1\n259            1             NA         NA         NA\n260          158              6          0          0\n261           NA            116          2          2\n262           NA             NA         NA         NA\n263          120             17          3          1\n264            0              0          0          0\n265           NA             NA         NA         NA\n266          327            679        182          6\n267           98            131        104          1\n268           NA             NA         NA         NA\n269           NA             NA         NA         NA\n270           NA             NA         NA         NA\n271            3              2          0          0\n272            5              0          0          0\n273           68             27         14          1\n274            0              0          0          0\n275            6              2          6          1\n276           33             36          5          2\n277           32             27          5          0\n278          221            365        168          4\n279           13             28         11          0\n280            2              1          1          0\n281           14              1          1          1\n282           83              9         12          2\n283           NA             NA         NA         NA\n284           10              0          0          1\n285           NA             NA         NA         NA\n286           NA             NA         NA         NA\n287          171            207          1          5\n288            3             15          3          0\n289           NA             NA         NA         NA\n290           NA             NA         NA         NA\n291            5              0          0          0\n292           NA             NA         NA         NA\n293           10              3          1          0\n294           NA             NA         NA         NA\n295           NA              1         NA         NA\n296           NA             NA         NA         NA\n297            7              2          0          0\n298          316             41          4          0\n299            0              0          0          0\n300           11              5          0          0\n301           91              9         NA          1\n302           20              2          0          0\n303          231            112         16          5\n304            0             NA         NA         NA\n305           NA             NA         NA         NA\n306          748            547          3          6\n307          138             NA         NA         NA\n308           NA             NA         NA         NA\n309           NA             NA         NA         NA\n310           NA             NA         NA         NA\n311            8              4          2          0\n312           NA             NA         NA         NA\n313           40             36          1          1\n314            7             NA          1         NA\n315            5              3         NA         NA\n316           21              9          0          1\n317           27             12          0          0\n318          249            143         24          4\n319           22             40          9          0\n320            6              2          0          0\n321           10              3          0          0\n322           86              1          4          1\n323           NA             NA         NA         NA\n324           NA             NA         NA         NA\n325           27              2         NA         NA\n326           NA             NA         NA         NA\n327           NA             NA         NA         NA\n328          184             37          0          0\n329           83             14          2          1\n330           NA             NA         NA         NA\n331           NA             NA         NA         NA\n332           12              1          0          0\n333           NA             NA         NA         NA\n334            9              4          4          0\n335           NA             NA         NA         NA\n336           19              2         NA         NA\n337           NA             NA         NA         NA\n338            5              0          0          0\n339          709            110         10          0\n340            0              0          0          1\n341           77             84          1          1\n342           30             42          3          1\n343           21              7          0          1\n344          127            150          0          0\n345            4              0          0          0\n346            1             NA         NA         NA\n347          590            676          0          6\n348          189            400          4          1\n349           NA             NA         NA         NA\n350           NA             NA         NA         NA\n351           NA             NA         NA         NA\n352           48             10          0          1\n353            2             NA         NA         NA\n354          163             22          0          0\n355           12              1         NA         NA\n356            2             NA         NA         NA\n357           44             44         NA          1\n358            0              0          0          0\n359          607             52          0          0\n360           56             16         NA          1\n361           23              3         NA         NA\n362           21             NA         NA         NA\n363          104             NA         NA         NA\n364           NA             NA         NA         NA\n365           NA             NA         NA         NA\n366           58              6          0          0\n367            0              1          2          0\n368           NA             NA         NA         NA\n369            0             48          8          0\n370            0              0          0          0\n371           NA             NA         NA         NA\n372           NA             NA         NA         NA\n373           10              3          2          1\n374           NA             NA         NA         NA\n375            4              2          0          0\n376           NA             NA         NA         NA\n377            2              0          0          0\n378           43             24          0          0\n379            5              0          0          0\n380          350              0          0          0\n381           NA             NA         NA         NA\n382            0              0          0          0\n383          116            107          0          0\n384           46              8          0          0\n385           65              0          0          0\n386          121             18          0          0\n387            0              0          0          0\n388            2              2          0          0\n389          536            291          1          2\n390          168            294          8          1\n391           NA             NA         NA         NA\n392           NA             NA         NA         NA\n393            2              0          0          0\n394          127             53          0          1\n395           NA             NA         NA         NA\n396           41              2          0          0\n397           24              5          0          0\n398           NA             NA         NA         NA\n399           42             12          3          2\n400            2              0          0          0\n401          403            211         23          1\n402           33             57         10          0\n403           21             29          3          0\n404           14              3          0          1\n405          139             38          2          1\n406            0              0          0          0\n407           NA             NA         NA         NA\n408           28              4         NA          1\n409            0             NA         NA         NA\n410           NA             NA         NA         NA\n411           84             13         NA         NA\n412            7              1         NA         NA\n413           NA             NA         NA         NA\n414           NA             NA         NA         NA\n415            6             NA          6          1\n416           NA             NA         NA         NA\n417            2             NA         NA         NA\n418           NA             NA         NA         NA\n419            2             NA         NA         NA\n420           21              5         NA         NA\n421           11              1         NA         NA\n422          176             19          3          1\n423           NA             NA         NA         NA\n424           NA              2         NA         NA\n425           59             12         NA         NA\n426           63             NA         NA         NA\n427           49             15         NA         NA\n428          141             76         26          2\n429            3             NA         NA         NA\n430            0             NA         NA         NA\n431          203            578        102          5\n432          108            271         17         NA\n433           NA             NA         NA         NA\n434           NA             NA         NA         NA\n435           NA             NA         NA         NA\n436            6             NA         NA         NA\n437            3              5         NA         NA\n438           54             13          2         NA\n439            2              1         NA         NA\n440           NA             NA         NA         NA\n441           42             21          2         NA\n442           39              8          1         NA\n443          269            428         13          6\n444           31             65          7         NA\n445           15             35          2         NA\n446            9              5         NA         NA\n447           66              7         NA         NA\n448            0             NA         NA         NA\n449            0             NA         NA         NA\n450           18              2          0          0\n451            6              1          0          0\n452            0             NA         NA         NA\n453           80             11         NA         NA\n454           15             NA         NA         NA\n455           NA             NA         NA         NA\n456           NA             NA         NA         NA\n457            3             NA          2         NA\n458           NA             NA         NA         NA\n459            1              1         NA         NA\n460           NA             NA         NA         NA\n461            3             NA         NA         NA\n462            4             NA         NA         NA\n463            0             NA         NA         NA\n464          161             11          0          0\n465           NA             NA         NA         NA\n466            0             NA         NA         NA\n467          130             NA         NA         NA\n468           68             NA         NA         NA\n469           66             12          0          0\n470          186             83         22         11\n471            0             NA         NA         NA\n472            2              4         NA         NA\n473          392            525         17          5\n474          115            162          7          0\n475           NA             NA         NA         NA\n476           NA             NA         NA         NA\n477           NA             NA         NA         NA\n478            4             10         NA         NA\n479            5              2         NA         NA\n480           52             30          2          1\n481            0             NA         NA         NA\n482           NA             NA         NA         NA\n483            7             NA          1         NA\n484           18             NA          1         NA\n485          182            306         16          4\n486           29             22          2         NA\n487           13             17          4         NA\n488            5              6          2         NA\n489           46              7          3          0\n490           NA             NA         NA         NA\n491            0              0          0          0\n492           35              0          0          0\n493            1              0          0          0\n494           NA             NA         NA         NA\n495           83             40          0          0\n496           14              0          0          0\n497           NA             NA         NA         NA\n498           NA             NA         NA         NA\n499            8              1          1          0\n500           NA             NA         NA         NA\n501            3              0          0          0\n502           NA             NA         NA         NA\n503            6              0          0          0\n504           NA             NA         NA         NA\n505            3             NA          1          1\n506          197             14          0          0\n507           NA             NA         NA         NA\n508            1              0          0          0\n509           87             10          0          0\n510           30             17          1          2\n511           55             16          0          0\n512          133             89          0          1\n513            1              0          0          0\n514            2              0          0          0\n515           11              0          0          0\n516          342            423         16          2\n517          107            310         25          3\n518           NA             NA         NA         NA\n519           NA             NA         NA         NA\n520           NA             NA         NA         NA\n521           10              0          0          0\n522           23              0          0          0\n523           42             13          1          0\n524           11              1          0          0\n525            0              0          0          0\n526           12              5          1          0\n527           18              5          1          0\n528          302            402         10          2\n529           33             58          1          1\n530           16             27          1          0\n531            3              9          1          0\n532           48             10          5          3\n533           NA             NA         NA         NA\n534            0              3          0          0\n535           NA             NA         NA         NA\n536            1              3          0          0\n537           NA             NA         NA         NA\n538          180            130          1          5\n539            9             17          0          0\n540            3              0          0          0\n541            0              0          0          0\n542           12              0          0          0\n543            0              0          0          0\n544            0              0          0          0\n545           10              3          0          0\n546            0              0          0          0\n547            5              0          0          1\n548            0              0          0          0\n549           10              2          0          0\n550          162              5          0          0\n551            0              0          0          0\n552            0              0          0          0\n553           NA             NA         NA         NA\n554            0              0          0          0\n555           47             14          0          0\n556           78              3          0          0\n557          250            152         18          3\n558            0              0          0          0\n559            7              0          1          0\n560            6              6          0          0\n561          208              5          0          2\n562          308            345        173          4\n563            0              0          0          0\n564           NA              0          0          0\n565            0              0          0          0\n566           20              3          0          0\n567            0              0          0          0\n568           96             52          9          1\n569            2              0          0          0\n570           NA             NA         NA         NA\n571            9              1          0          0\n572            7              4          0          1\n573          281            474         30          6\n574           40             34          3          0\n575           23             23          2          0\n576           13              2          0          0\n577           69             43         11          1\n578            0              0          0          0\n579            0              0          0          0\n580            3              0          0          0\n581          143             86          4          0\n582            0              0          0          0\n583            1              0          0          0\n584           11              0          0          0\n585            6              5          0          0\n586            3              0          0          0\n587            0              0          0          0\n588           20              2          0          0\n589           76              1          0          1\n590            0              0          0          0\n591            0              0          0          0\n592            7              0          0          0\n593           58              7          0          0\n594           22             31          0          0\n595           54             11          0          0\n596          308            141         11          5\n597            0              0          0          0\n598           12              1          0          0\n599           54              4          0          0\n600          176            618          8          2\n601            0              0          0          0\n602            6              0          0          0\n603            0              0          0          0\n604           50             67          1          0\n605            1              0          0          0\n606            8              5          0          0\n607            9              0          0          0\n608          260            423          1          3\n609           26             46          0          0\n610           35             28          0          0\n611           11              4          0          0\n612           88             37          0          2\n613            0              0          0          6\n614            6              1          0          0\n615          113            141          1          0\n616           17              0          0          0\n617            1              0          0          0\n618            3              0          2          0\n619            0              0          0          0\n620           12             15          0          0\n621           NA             NA         NA         NA\n622            0              0          0          0\n623           20              0          0          0\n624           69              4          1          0\n625            0              0          0          0\n626            2              0          0          0\n627            0              0          0          0\n628           71             28          0          0\n629            2              0          0          0\n630           71              2          0          0\n631          184            309          3          4\n632            2              3          0          1\n633            0              0          0          0\n634          258             40          0          0\n635          138            208          1          3\n636            0              0          0          0\n637            4              2          0          0\n638            0              0          0          0\n639           83             48          1          0\n640            0              0          0          0\n641            6              0          0          3\n642            0              2          0          0\n643          361            192          4          1\n644           54             16          0          0\n645           43              4          0          0\n646           23              2          0          0\n647           72              6          1          0\n648            0              0          0          0\n649            5              0          0          0\n650          116              0          0          0\n651            0              2          1          0\n652            0              0          0          0\n653           13              0          0          0\n654            1              0          0          0\n655            9              0          0          0\n656            5              3          0          1\n657            1              2          0          0\n658            9              0          0          0\n659          134              2          0          2\n660           NA             NA         NA         NA\n661            5              8          4          0\n662            0              0          0          0\n663            0             18          0          0\n664            0              0          0          0\n665           30              0          0          0\n666          336             18          2          7\n667            3              6          0          0\n668            2              0          0          0\n669          231            119         33          0\n670           78             29          3          0\n671            0              0          0          0\n672           14              6          5          0\n673            1              0          0          0\n674           85             80         11          0\n675            0              0          0          0\n676            5              0          0          0\n677            6              1          0          0\n678          412            170          3          1\n679            2              0          0          0\n680           49              5          0          0\n681            8              1          0          0\n682           56              7          0          1\n683            0              0          0          0\n684            0              0          0          0\n685            0              0          0          0\n686          118             98          3          0\n687           23              1          0          1\n688            5              0          0          0\n689            9              8          2          1\n690            0              0          0          0\n691            7             11          0          0\n692            6              4          0          0\n693            0              0          0          0\n694            9              1          0          0\n695           80              4          0          0\n696           NA             NA         NA         NA\n697           16              8          0          0\n698            0              0          0          0\n699          112             48          0          0\n700            1              0          0          0\n701           11              0          0          1\n702          157            101         50         13\n703            6              3          0          0\n704            4              0          0          0\n705          330            178          7          3\n706          133            171          2          3\n707            0              0          0          0\n708           43             20          1          1\n709            0              0          0          0\n710           70             31          1          0\n711            0              0          0          0\n712            4              0          0          1\n713            0              0          0          0\n714          351              0          0          0\n715           45              8          0          0\n716           39             11          0          0\n717           32              2          0          1\n718          137             20          1          1\n719            8              0          0          0\n720           10              2          0          0\n721            0              0          0          0\n722          127            124          1          1\n723           12              0          0          0\n724            4              1          0          1\n725           12              4          1          0\n726            1              0          0          0\n727            5              6          0          0\n728            6              0          0          0\n729           14              0          0          0\n730            7              0          0          3\n731           94              4          0          0\n732            0              0          0          0\n733            0              0          0          0\n734          146              9          0          0\n735            1              4          0          0\n736           59              9          0          0\n737          249            141         17          9\n738            0              0          0          0\n739            0              0          0          0\n740          247             83          5          2\n741          100             70         12          0\n742            0              0          0          0\n743           10              0          0          0\n744           38             16          1          0\n745            0              0          0          0\n746           53             47          2          1\n747            0              0          0          0\n748            2              1          0          0\n749            0              0          0          0\n750          338             66          3          0\n751           32              3          0          0\n752           40              4          0          0\n753            9              0          0          0\n754          106              4          0          0\n755            2              1          0          0\n756            1              0          0          0\n757            0              0          0          1\n758          112             56          5          0\n759           17              1          0          0\n760            1              1          0          0\n761            8              2          0          0\n762            0              0          0          0\n763            3             28          0          0\n764           13              0          0          0\n765            0              0          0          0\n766            1              1          0          0\n767           40              3          0          1\n768            0              0          0          0\n769            0              0          0          0\n770           37              0          0          0\n771           31              7          2          0\n772           29              0          0          0\n773          176             90          3         22\n774            0              0          0          0\n775            4              0          0          0\n776            4              0          0          0\n777          317            169         15          1\n778           67             94          7          0\n779            0              0          0          0\n780            0              0          0          0\n781           50             36          4          0\n782            1              0          0          0\n783            2              0          0          0\n784            0              0          0          0\n785            1              2          0          0\n786            0              0          0          0\n787          329            185          6          6\n788           25              2          0          0\n789           32              1          0          0\n790           11              2          0          0\n791           65             44          1          1\n\n\nYou can select the same columns with the indexing operator, but you’d either have to type out all of their names or use a more complicated expression.\nUse the indexing operator when you only need to select a few columns. Use dplyr’s select function when you need to select lots of columns that follow some kind of pattern. The dplyr documentation provides more details about how to use the select function.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Indexing</span>"
    ]
  },
  {
    "objectID": "chapters/week04/indexing.html#filtering-rows",
    "href": "chapters/week04/indexing.html#filtering-rows",
    "title": "15  Indexing",
    "section": "15.3 Filtering Rows",
    "text": "15.3 Filtering Rows\nIn addition to selecting columns, you also need to know how to filter rows in order to work with tabular data. In this context, to “filter” is to get a subset of rows that satisfy a condition. As a motivating example, suppose we want to get all of the rows in the least terns dataset that correspond to the Los Angeles Harbor (LA HARBOR) site.\nThe indexing operator [ is one way to filter the rows of a data frame. The first step is to translate the condition on the rows into code. To test whether rows correspond to Los Angeles Harbor, we can write:\n\nterns$site_name == \"LA HARBOR\"\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[181] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[217] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[337] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[397] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[421] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[433] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[445] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[469] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[493] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[505] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[553] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[565] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[577] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[589] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[601] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[613] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[625] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[661] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[673] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[685] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[697] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[721] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[733] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[745] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[769] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nThis returns a vector of logical (TRUE and FALSE) values. Now that we have the code for the condition, we can use indexing to get rows where the condition is TRUE:\n\nterns[terns$site_name == \"LA HARBOR\", ]\n\n    year site_name site_name_2013_2018  site_name_1988_2001 site_abbr region_3\n12  2000 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n43  2004 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n76  2005 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n108 2006 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n144 2007 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n182 2008 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n220 2009 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n260 2010 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n300 2011 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n341 2012 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n383 2013 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n425 2014 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n467 2015 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n509 2016 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n554 2017 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n593 2018 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n628 2019 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n663 2020 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n699 2021 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n734 2022 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n770 2023 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n    region_4   event bp_min bp_max fl_min fl_max total_nests nonpred_eggs\n12  SOUTHERN LA_NINA    437    437    570    570         565           77\n43  SOUTHERN NEUTRAL    951    951    556    556        1071          179\n76  SOUTHERN NEUTRAL   1254   1254    449    687        1332          217\n108 SOUTHERN NEUTRAL    835    835    511    771         907          434\n144 SOUTHERN EL_NINO    669    669    186    186         710          385\n182 SOUTHERN LA_NINA    486    515    210    210         529          277\n220 SOUTHERN NEUTRAL    371    385     75     75         435          328\n260 SOUTHERN EL_NINO    149    195      4      4         216          158\n300 SOUTHERN LA_NINA      7      8      0      0          10           11\n341 SOUTHERN LA_NINA    144    207     35     35         211           77\n383 SOUTHERN NEUTRAL    237    239     31    147         254          116\n425 SOUTHERN NEUTRAL    110    112     16    112         126           59\n467 SOUTHERN NEUTRAL    103    109      0      0         109          130\n509 SOUTHERN EL_NINO    109    141     46     70         141           87\n554 SOUTHERN NEUTRAL      0      5      0      0           5            0\n593 SOUTHERN NEUTRAL    105    117     69     69         133           58\n628 SOUTHERN NEUTRAL    161    184     97     97         199           71\n663 SOUTHERN NEUTRAL     91    182      3    111         182            0\n699 SOUTHERN LA_NINA    170    188     38    142         198          112\n734 SOUTHERN LA_NINA    172    177      2      4         189          146\n770 SOUTHERN LA_NINA      1     55      0      0          56           37\n    nonpred_chicks nonpred_fl nonpred_ad pred_control pred_eggs pred_chicks\n12              NA         NA         NA                     24          NA\n43             453         31         12                     NA          NA\n76             868         13          3                      6           0\n108            253          7          7                     27           0\n144             NA         NA         NA                     NA          NA\n182            153          1          2                     NA          NA\n220            123          3          1                     NA          NA\n260              6          0          0                     NA          NA\n300              5          0          0            Y        NA          NA\n341             84          1          1            Y        NA          NA\n383            107          0          0            Y        NA          NA\n425             12         NA         NA            Y        NA          NA\n467             NA         NA         NA            Y        NA          NA\n509             10          0          0            Y        NA          NA\n554              0          0          0            Y        NA          NA\n593              7          0          0            Y        NA          NA\n628             28          0          0            Y        NA          NA\n663             18          0          0            Y        NA          NA\n699             48          0          0            Y        NA          NA\n734              9          0          0            Y        NA          NA\n770              0          0          0            Y        NA          NA\n    pred_fl pred_ad pred_pefa pred_coy_fox pred_meso pred_owlspp pred_corvid\n12       15      NA         Y            N         N           N           N\n43       NA      NA         Y            N         Y           Y           Y\n76        0       0         N            N         N           Y           Y\n108       0       7         Y            N         N           Y           Y\n144      NA      NA                                                         \n182      NA      NA         Y            N         N           Y           Y\n220      NA      NA         Y            N         N           Y           Y\n260      NA      NA         N            N         N           N           Y\n300      NA      NA         N            N         N           N           Y\n341      NA      NA         N            N         N           N           N\n383      NA      NA         Y            N         Y           N           N\n425      NA      NA         N            N         Y           N           N\n467      NA      NA         N            N         N           N           Y\n509      NA      NA         Y            N         N           Y           Y\n554      NA      NA         N            N         N           N           Y\n593      NA      NA                      N         N           N           N\n628      NA      NA         Y            N         N           N           Y\n663      NA      NA         N            N         N           N           Y\n699      NA      NA         N            N         Y           N           Y\n734      NA      NA         N            N         N           N           Y\n770      NA      NA         N            N         N           N           Y\n    pred_other_raptor pred_other_avian pred_misc total_pefa total_coy_fox\n12                  Y                Y         N         17             0\n43                  Y                N         Y         NA            NA\n76                  N                N         N          0             0\n108                 N                Y         N         NA            NA\n144                                                      NA            NA\n182                 Y                Y         N         NA            NA\n220                 Y                N         N         NA            NA\n260                 N                N         N         NA            NA\n300                 N                Y         N         NA            NA\n341                 Y                Y         N         NA            NA\n383                 N                N         N         NA            NA\n425                 N                N         Y         NA            NA\n467                 N                N         N         NA            NA\n509                 N                Y         N         NA            NA\n554                 N                N         N         NA            NA\n593                                  Y         N         NA            NA\n628                 N                N                   NA            NA\n663                 N                N         N         NA            NA\n699                 Y                N         N         NA            NA\n734                 Y                N         Y         NA            NA\n770                 N                N         N         NA            NA\n    total_meso total_owlspp total_corvid total_other_raptor total_other_avian\n12           0            0            0                  4                24\n43          NA           NA           NA                 NA                NA\n76           0           NA            6                  0                 0\n108         NA           NA           NA                 NA                NA\n144         NA           NA           NA                 NA                NA\n182         NA           NA           NA                 NA                NA\n220         NA           NA           NA                 NA                NA\n260         NA           NA           NA                 NA                NA\n300         NA           NA           NA                 NA                NA\n341         NA           NA           NA                 NA                NA\n383         NA           NA           NA                 NA                NA\n425         NA           NA           NA                 NA                NA\n467         NA           NA           NA                 NA                NA\n509         NA           NA           NA                 NA                NA\n554         NA           NA           NA                 NA                NA\n593         NA           NA           NA                 NA                NA\n628         NA           NA           NA                 NA                NA\n663         NA           NA           NA                 NA                NA\n699         NA           NA           NA                 NA                NA\n734         NA           NA           NA                 NA                NA\n770         NA           NA           NA                 NA                NA\n    total_misc first_observed last_observed first_nest first_chick first_fledge\n12           0     2000-04-28    2000-08-20 2000-05-10  3000-06-02   2000-06-22\n43          NA     2004-05-03    2004-08-19 2004-05-19  2004-06-09   2004-06-30\n76           0                                          2005-06-01   2005-06-24\n108         NA     2006-04-22    2006-08-17 2006-05-14  2006-06-11   2006-07-04\n144         NA                                                                 \n182         NA     2008-05-01    2008-08-17 2008-05-21  2008-06-04   2008-06-29\n220         NA     2009-04-26    2009-08-15 2009-05-21  2009-06-17   2009-07-05\n260         NA     2010-05-13    2010-07-29 2010-05-22  2010-06-16   2010-07-09\n300         NA     2011-04-27    2011-08-02 2011-06-08  2011-07-10             \n341         NA     2012-04-20    2012-07-25 2012-05-14  2012-06-06   2012-06-27\n383         NA     2013-06-01    2013-07-21 2013-05-26  2013-06-13   2013-07-11\n425         NA     2014-05-22    2014-07-27 2014-05-22  2014-06-09   2014-07-03\n467         NA     2015-05-14    2015-06-22 2015-05-14  2015-06-08             \n509         NA     2016-04-20    2016-08-18 2016-05-16  2016-06-10   2016-06-27\n554         NA     2017-05-10    2017-07-17 2017-05-19                         \n593         NA     2018-04-20    2018-08-04 2018-05-14  2018-06-08   2018-06-27\n628         NA     2019-04-26    2019-08-10 2019-05-10  2019-06-03   2019-06-28\n663         NA     2020-04-26    2020-08-16 2020-07-03  2020-07-20   2020-08-13\n699         NA     2021-04-21    2021-08-05 2021-05-17  2021-06-14             \n734         NA     2022-04-30    2022-07-22 2022-05-21  2022-06-19             \n770         NA     2023-04-28    2023-06-14 2023-05-24                         \n\n\nSection 15.4 describes various operators and functions you can use to write more complicated conditions.\nNotice that in the condition, we had to write terns$site_name rather than just the column name site_name. We have to specify that we mean a column in terns because the indexing operator doesn’t make any assumptions about what is or isn’t a column name. But most of the time, we filter rows by testing conditions on the columns. Typing the name of the data frame (for example, terns$) in front of every column name makes writing complicated conditions tedious.\nThe dplyr package’s filter function is another way to filter the rows of a data frame. By assuming that names refer to column names, filter makes writing conditions less tedious. The first argument to filter is the data frame to filter, while the second is the condition. Here’s how to get the Los Angeles Harbor rows from the least terns dataset:\n\nfilter(terns, site_name == \"LA HARBOR\")\n\n   year site_name site_name_2013_2018  site_name_1988_2001 site_abbr region_3\n1  2000 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n2  2004 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n3  2005 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n4  2006 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n5  2007 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n6  2008 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n7  2009 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n8  2010 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n9  2011 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n10 2012 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n11 2013 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n12 2014 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n13 2015 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n14 2016 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n15 2017 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n16 2018 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n17 2019 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n18 2020 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n19 2021 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n20 2022 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n21 2023 LA HARBOR          Port of LA NA_2013_2018 POLYGON   LA_HARB SOUTHERN\n   region_4   event bp_min bp_max fl_min fl_max total_nests nonpred_eggs\n1  SOUTHERN LA_NINA    437    437    570    570         565           77\n2  SOUTHERN NEUTRAL    951    951    556    556        1071          179\n3  SOUTHERN NEUTRAL   1254   1254    449    687        1332          217\n4  SOUTHERN NEUTRAL    835    835    511    771         907          434\n5  SOUTHERN EL_NINO    669    669    186    186         710          385\n6  SOUTHERN LA_NINA    486    515    210    210         529          277\n7  SOUTHERN NEUTRAL    371    385     75     75         435          328\n8  SOUTHERN EL_NINO    149    195      4      4         216          158\n9  SOUTHERN LA_NINA      7      8      0      0          10           11\n10 SOUTHERN LA_NINA    144    207     35     35         211           77\n11 SOUTHERN NEUTRAL    237    239     31    147         254          116\n12 SOUTHERN NEUTRAL    110    112     16    112         126           59\n13 SOUTHERN NEUTRAL    103    109      0      0         109          130\n14 SOUTHERN EL_NINO    109    141     46     70         141           87\n15 SOUTHERN NEUTRAL      0      5      0      0           5            0\n16 SOUTHERN NEUTRAL    105    117     69     69         133           58\n17 SOUTHERN NEUTRAL    161    184     97     97         199           71\n18 SOUTHERN NEUTRAL     91    182      3    111         182            0\n19 SOUTHERN LA_NINA    170    188     38    142         198          112\n20 SOUTHERN LA_NINA    172    177      2      4         189          146\n21 SOUTHERN LA_NINA      1     55      0      0          56           37\n   nonpred_chicks nonpred_fl nonpred_ad pred_control pred_eggs pred_chicks\n1              NA         NA         NA                     24          NA\n2             453         31         12                     NA          NA\n3             868         13          3                      6           0\n4             253          7          7                     27           0\n5              NA         NA         NA                     NA          NA\n6             153          1          2                     NA          NA\n7             123          3          1                     NA          NA\n8               6          0          0                     NA          NA\n9               5          0          0            Y        NA          NA\n10             84          1          1            Y        NA          NA\n11            107          0          0            Y        NA          NA\n12             12         NA         NA            Y        NA          NA\n13             NA         NA         NA            Y        NA          NA\n14             10          0          0            Y        NA          NA\n15              0          0          0            Y        NA          NA\n16              7          0          0            Y        NA          NA\n17             28          0          0            Y        NA          NA\n18             18          0          0            Y        NA          NA\n19             48          0          0            Y        NA          NA\n20              9          0          0            Y        NA          NA\n21              0          0          0            Y        NA          NA\n   pred_fl pred_ad pred_pefa pred_coy_fox pred_meso pred_owlspp pred_corvid\n1       15      NA         Y            N         N           N           N\n2       NA      NA         Y            N         Y           Y           Y\n3        0       0         N            N         N           Y           Y\n4        0       7         Y            N         N           Y           Y\n5       NA      NA                                                         \n6       NA      NA         Y            N         N           Y           Y\n7       NA      NA         Y            N         N           Y           Y\n8       NA      NA         N            N         N           N           Y\n9       NA      NA         N            N         N           N           Y\n10      NA      NA         N            N         N           N           N\n11      NA      NA         Y            N         Y           N           N\n12      NA      NA         N            N         Y           N           N\n13      NA      NA         N            N         N           N           Y\n14      NA      NA         Y            N         N           Y           Y\n15      NA      NA         N            N         N           N           Y\n16      NA      NA                      N         N           N           N\n17      NA      NA         Y            N         N           N           Y\n18      NA      NA         N            N         N           N           Y\n19      NA      NA         N            N         Y           N           Y\n20      NA      NA         N            N         N           N           Y\n21      NA      NA         N            N         N           N           Y\n   pred_other_raptor pred_other_avian pred_misc total_pefa total_coy_fox\n1                  Y                Y         N         17             0\n2                  Y                N         Y         NA            NA\n3                  N                N         N          0             0\n4                  N                Y         N         NA            NA\n5                                                       NA            NA\n6                  Y                Y         N         NA            NA\n7                  Y                N         N         NA            NA\n8                  N                N         N         NA            NA\n9                  N                Y         N         NA            NA\n10                 Y                Y         N         NA            NA\n11                 N                N         N         NA            NA\n12                 N                N         Y         NA            NA\n13                 N                N         N         NA            NA\n14                 N                Y         N         NA            NA\n15                 N                N         N         NA            NA\n16                                  Y         N         NA            NA\n17                 N                N                   NA            NA\n18                 N                N         N         NA            NA\n19                 Y                N         N         NA            NA\n20                 Y                N         Y         NA            NA\n21                 N                N         N         NA            NA\n   total_meso total_owlspp total_corvid total_other_raptor total_other_avian\n1           0            0            0                  4                24\n2          NA           NA           NA                 NA                NA\n3           0           NA            6                  0                 0\n4          NA           NA           NA                 NA                NA\n5          NA           NA           NA                 NA                NA\n6          NA           NA           NA                 NA                NA\n7          NA           NA           NA                 NA                NA\n8          NA           NA           NA                 NA                NA\n9          NA           NA           NA                 NA                NA\n10         NA           NA           NA                 NA                NA\n11         NA           NA           NA                 NA                NA\n12         NA           NA           NA                 NA                NA\n13         NA           NA           NA                 NA                NA\n14         NA           NA           NA                 NA                NA\n15         NA           NA           NA                 NA                NA\n16         NA           NA           NA                 NA                NA\n17         NA           NA           NA                 NA                NA\n18         NA           NA           NA                 NA                NA\n19         NA           NA           NA                 NA                NA\n20         NA           NA           NA                 NA                NA\n21         NA           NA           NA                 NA                NA\n   total_misc first_observed last_observed first_nest first_chick first_fledge\n1           0     2000-04-28    2000-08-20 2000-05-10  3000-06-02   2000-06-22\n2          NA     2004-05-03    2004-08-19 2004-05-19  2004-06-09   2004-06-30\n3           0                                          2005-06-01   2005-06-24\n4          NA     2006-04-22    2006-08-17 2006-05-14  2006-06-11   2006-07-04\n5          NA                                                                 \n6          NA     2008-05-01    2008-08-17 2008-05-21  2008-06-04   2008-06-29\n7          NA     2009-04-26    2009-08-15 2009-05-21  2009-06-17   2009-07-05\n8          NA     2010-05-13    2010-07-29 2010-05-22  2010-06-16   2010-07-09\n9          NA     2011-04-27    2011-08-02 2011-06-08  2011-07-10             \n10         NA     2012-04-20    2012-07-25 2012-05-14  2012-06-06   2012-06-27\n11         NA     2013-06-01    2013-07-21 2013-05-26  2013-06-13   2013-07-11\n12         NA     2014-05-22    2014-07-27 2014-05-22  2014-06-09   2014-07-03\n13         NA     2015-05-14    2015-06-22 2015-05-14  2015-06-08             \n14         NA     2016-04-20    2016-08-18 2016-05-16  2016-06-10   2016-06-27\n15         NA     2017-05-10    2017-07-17 2017-05-19                         \n16         NA     2018-04-20    2018-08-04 2018-05-14  2018-06-08   2018-06-27\n17         NA     2019-04-26    2019-08-10 2019-05-10  2019-06-03   2019-06-28\n18         NA     2020-04-26    2020-08-16 2020-07-03  2020-07-20   2020-08-13\n19         NA     2021-04-21    2021-08-05 2021-05-17  2021-06-14             \n20         NA     2022-04-30    2022-07-22 2022-05-21  2022-06-19             \n21         NA     2023-04-28    2023-06-14 2023-05-24                         \n\n\nYou can pass additional conditions to filter as additional arguments. All of them must be satisfied (TRUE) for a row to be kept.\n\n\n\n\n\n\nNote\n\n\n\nR’s built-in subset function is similar to and might have inspired dplyr’s filter function. Here’s how to use it to get all of the Los Angeles Harbor rows:\nsubset(terns, site_name == \"LA HARBOR\")\nThe subset function is a good alternative if you prefer not to use dplyr.\n\n\nBoth the indexing operator and dplyr’s filter function are good ways to filter rows in a data frame. The indexing operator has the advantage that you can simultaneously use it to select columns by filling in the second argument. On the other hand, the filter function makes complicated conditions easier to type and read—this might seem like a small advantage, but code clarity is important.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Indexing</span>"
    ]
  },
  {
    "objectID": "chapters/week04/indexing.html#sec-logic",
    "href": "chapters/week04/indexing.html#sec-logic",
    "title": "15  Indexing",
    "section": "15.4 Logic Operators",
    "text": "15.4 Logic Operators\nAll of the conditions we’ve seen so far have been written in terms of a single test. If you want to use more sophisticated conditions, R provides operators to negate and combine logical vectors. These operators are useful for working with logical vectors even outside the context of indexing.\n\n15.4.1 Negation\nThe NOT operator ! converts TRUE to FALSE and FALSE to TRUE:\n\nx = c(TRUE, FALSE, TRUE, TRUE, NA)\nx\n\n[1]  TRUE FALSE  TRUE  TRUE    NA\n\n!x\n\n[1] FALSE  TRUE FALSE FALSE    NA\n\n\nYou can use ! with a condition:\n\ny = c(\"hi\", \"hello\")\n!(y == \"hi\")\n\n[1] FALSE  TRUE\n\n\nThe NOT operator is vectorized.\n\n\n15.4.2 Combinations\nR also has operators for combining logical values.\nThe AND operator & returns TRUE only when both arguments are TRUE. Here are some examples:\n\nFALSE & FALSE\n\n[1] FALSE\n\nTRUE & FALSE\n\n[1] FALSE\n\nFALSE & TRUE\n\n[1] FALSE\n\nTRUE & TRUE\n\n[1] TRUE\n\nc(TRUE, FALSE, TRUE) & c(TRUE, TRUE, FALSE)\n\n[1]  TRUE FALSE FALSE\n\n\nThe OR operator | returns TRUE when at least one argument is TRUE. Let’s see some examples:\n\nFALSE | FALSE\n\n[1] FALSE\n\nTRUE | FALSE\n\n[1] TRUE\n\nFALSE | TRUE\n\n[1] TRUE\n\nTRUE | TRUE\n\n[1] TRUE\n\nc(TRUE, FALSE) | c(TRUE, TRUE)\n\n[1] TRUE TRUE\n\n\nThe combination operators are vectorized.\n\n\n\n\n\n\nWarning\n\n\n\nBe careful: everyday English is less precise than logic. You might say:\n\nI want all subjects with age over 50 and all subjects that like cats.\n\nBut in logic this means:\n(subject age over 50) OR (subject likes cats)\nSo think carefully about whether you need both conditions to be true (AND) or at least one (OR).\n\n\n\n\n\n\n\n\nNoteShort-circuit Operators\n\n\n\n\n\nThe second argument is irrelevant in some conditions:\n\nFALSE & is always FALSE\nTRUE | is always TRUE\n\nNow imagine you have FALSE & long_computation(). You can save time by skipping long_computation(). A short-circuit operator does exactly that.\nR has two short-circuit operators:\n\n&& is a short-circuited &\n|| is a short-circuited |\n\nThese operators only evaluate the second argument if it is necessary to determine the result. Here are some of these:\n\nTRUE && FALSE\n\n[1] FALSE\n\nTRUE && TRUE\n\n[1] TRUE\n\nTRUE || TRUE\n\n[1] TRUE\n\n\nThe short-circuit operators are not vectorized—they only accept length-1 arguments:\n\nc(TRUE, FALSE) && c(TRUE, TRUE)\n\nError in c(TRUE, FALSE) && c(TRUE, TRUE): 'length = 2' in coercion to 'logical(1)'\n\n\nBecause of this, you can’t use short-circuit operators for indexing. Their main use is in writing conditions for if-expressions, which we’ll learn about later on.\nPrior to R 4.3.0, short-circuit operators didn’t raise an error for inputs with length greater than 1 (and thus were a common source of bugs).\n\n\n\n\n\n\n\n\n\nNoteExclusive OR\n\n\n\n\n\nRarely, you might want exactly one condition to be true. The XOR (eXclusive OR) function xor() returns TRUE when exactly one argument is TRUE. For example:\n\nxor(FALSE, FALSE)\n\n[1] FALSE\n\nxor(TRUE, FALSE)\n\n[1] TRUE\n\nxor(TRUE, TRUE)\n\n[1] FALSE",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Indexing</span>"
    ]
  },
  {
    "objectID": "chapters/week04/data-forensics.html",
    "href": "chapters/week04/data-forensics.html",
    "title": "16  Data Forensics",
    "section": "",
    "text": "16.1 Structural Summaries\nThis lesson covers a variety of ways to investigate and summarize tabular data in order to understand the data better and identify potential problems. The lesson also describes how to fix some of the most common data problems.\nWhenever you load a data set into R, your next step should be to investigate the data’s structure. This step is important because it can help you identify whether:\nSection 12.9 demonstrated several functions for getting structural summaries of data. Some of these are:\nThat section gave examples with the California Least Terns dataset (Section 11.4). For instance, the str function shows the classes of the columns:\nstr(terns)\n\n'data.frame':   791 obs. of  43 variables:\n $ year               : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ site_name          : chr  \"PITTSBURG POWER PLANT\" \"ALBANY CENTRAL AVE\" \"ALAMEDA POINT\" \"KETTLEMAN CITY\" ...\n $ site_name_2013_2018: chr  \"Pittsburg Power Plant\" \"NA_NO POLYGON\" \"Alameda Point\" \"Kettleman\" ...\n $ site_name_1988_2001: chr  \"NA_2013_2018 POLYGON\" \"Albany Central Avenue\" \"NA_2013_2018 POLYGON\" \"NA_2013_2018 POLYGON\" ...\n $ site_abbr          : chr  \"PITT_POWER\" \"AL_CENTAVE\" \"ALAM_PT\" \"KET_CTY\" ...\n $ region_3           : chr  \"S.F._BAY\" \"S.F._BAY\" \"S.F._BAY\" \"KINGS\" ...\n $ region_4           : chr  \"S.F._BAY\" \"S.F._BAY\" \"S.F._BAY\" \"KINGS\" ...\n $ event              : chr  \"LA_NINA\" \"LA_NINA\" \"LA_NINA\" \"LA_NINA\" ...\n $ bp_min             : num  15 6 282 2 4 9 30 21 73 166 ...\n $ bp_max             : num  15 12 301 3 5 9 32 21 73 167 ...\n $ fl_min             : int  16 1 200 1 4 17 11 9 60 64 ...\n $ fl_max             : int  18 1 230 2 4 17 11 9 65 64 ...\n $ total_nests        : int  15 20 312 3 5 9 32 22 73 252 ...\n $ nonpred_eggs       : int  3 NA 124 NA 2 0 NA 4 2 NA ...\n $ nonpred_chicks     : int  0 NA 81 3 0 1 27 3 0 NA ...\n $ nonpred_fl         : int  0 NA 2 1 0 0 0 NA 0 NA ...\n $ nonpred_ad         : int  0 NA 1 6 0 0 0 NA 0 NA ...\n $ pred_control       : chr  \"\" \"\" \"\" \"\" ...\n $ pred_eggs          : int  4 NA 17 NA 0 NA 0 NA NA NA ...\n $ pred_chicks        : int  2 NA 0 NA 4 NA 3 NA NA NA ...\n $ pred_fl            : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ pred_ad            : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ pred_pefa          : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_coy_fox       : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_meso          : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_owlspp        : chr  \"N\" \"\" \"N\" \"\" ...\n $ pred_corvid        : chr  \"Y\" \"\" \"N\" \"\" ...\n $ pred_other_raptor  : chr  \"Y\" \"\" \"Y\" \"\" ...\n $ pred_other_avian   : chr  \"N\" \"\" \"Y\" \"\" ...\n $ pred_misc          : chr  \"N\" \"\" \"N\" \"\" ...\n $ total_pefa         : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_coy_fox      : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_meso         : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_owlspp       : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_corvid       : int  4 NA 0 NA 0 NA 0 NA NA NA ...\n $ total_other_raptor : int  2 NA 6 NA 0 NA 3 NA NA NA ...\n $ total_other_avian  : int  0 NA 11 NA 4 NA 0 NA NA NA ...\n $ total_misc         : int  0 NA 0 NA 0 NA 0 NA NA NA ...\n $ first_observed     : chr  \"2000-05-11\" \"\" \"2000-05-01\" \"2000-06-10\" ...\n $ last_observed      : chr  \"2000-08-05\" \"\" \"2000-08-19\" \"2000-09-24\" ...\n $ first_nest         : chr  \"2000-05-26\" \"\" \"2000-05-16\" \"2000-06-17\" ...\n $ first_chick        : chr  \"2000-06-18\" \"\" \"2000-06-07\" \"2000-07-22\" ...\n $ first_fledge       : chr  \"2000-07-08\" \"\" \"2000-06-30\" \"2000-08-06\" ...\nOften when you load a new dataset, some of the columns won’t have the correct data type (or class) for what you want to do. For instance, in the least terns dataset, the site_name, region_3, and event columns all contain categorical data, so they should be factors.\nYou can convert these columns to factors with the factor function from Section 13.2.4:\nterns$site_name = factor(terns$site_name)\nterns$region_3 = factor(terns$region_3)\nterns$event = factor(terns$event)\nR provides as. functions to convert to the most common data types. For instance, as.character converts an object to a string:\nx = 3.1\nclass(x)\n\n[1] \"numeric\"\n\ny = as.character(x)\ny\n\n[1] \"3.1\"\n\nclass(y)\n\n[1] \"character\"\nThe read.csv function does a good job at identifying columns of numbers, so it’s rarely necessary to convert columns of numbers manually. However, you may have to do this for data you got some other way (rather than loading a file). For instance, it’s common to make these conversions when scraping data from the web.\nIt’s also a good idea to convert categorical columns into factors with the factor function, and to convert columns of dates into dates (more about this in Chapter 20).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Forensics</span>"
    ]
  },
  {
    "objectID": "chapters/week04/data-forensics.html#structural-summaries",
    "href": "chapters/week04/data-forensics.html#structural-summaries",
    "title": "16  Data Forensics",
    "section": "",
    "text": "The data was loaded correctly\nThere are structural problems with the data that will make it difficult to use if they aren’t fixed\n\n\n\nstr to get a detailed structural summary\nhead, tail to preview the data\nnrow, ncol, dim, length to get dimension information\nnames, colnames, rownames to get element names\nclass, typeof to get classes and types\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere’s another way we could’ve done this that uses only two lines of code, no matter how many columns there are:\n\ncols = c(\"site_name\", \"region_3\", \"event\")\nterns[cols] = lapply(terns[cols], factor)\n\nWe’ll learn more about the lapply function in Section 17.1.\nYou can use whichever approach is more convenient and makes more sense to you. If there were other columns to convert, we’d go through the same steps with the appropriate conversion function.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Forensics</span>"
    ]
  },
  {
    "objectID": "chapters/week04/data-forensics.html#statistical-summaries",
    "href": "chapters/week04/data-forensics.html#statistical-summaries",
    "title": "16  Data Forensics",
    "section": "16.2 Statistical Summaries",
    "text": "16.2 Statistical Summaries\nAfter investigating the data’s structure, it’s a good idea to check some basic statistical properties. This step is important because it can help you identify limitations of and patterns in the data.\nWhich statistics are appropriate for a given feature often depends on the type of the feature. Recall from Section 13.2 that the types statisticians typically think about are:\n\nCategorical\n\nNominal - data separated into specific categories, with no order. For example, hair color (red, brown, blonde, …) is categorical.\nOrdinal - data separated into specific categories, with an order. For example, school level (elementary, middle, high, college) is ordinal.\n\nNumerical\n\nDiscrete - integers, or a finite set of decimal numbers with no values in between. Sometimes discrete values can also be treated as ordinal. For example, month as a number (1, 2, …, 12) is discrete.\nContinuous - decimal numbers. There are no specific categories, but there is an order. For example, height in inches is numerical.\n\n\nThe table function, which was introduced in Section 12.9, is great for summarizing categorical (and sometimes discrete) data. For example:\n\ntable(terns$region_3)\n\n\n   ARIZONA    CENTRAL      KINGS   S.F._BAY SACRAMENTO   SOUTHERN \n         1         77         15        112         10        576 \n\n\nWhat about numerical data?\nTwo important questions to ask about data are:\n\nWhere is it? This is the location of the data.\nHow spread out is it? This is the scale of the data.\n\nLet’s use the data\n\nx = c(-2, -1, -1, -1, 0, 2, 6)\n\nas an example.\nLocation is generally summarized with a number near the middle or center of the data. A few options are:\n\nMode - the value that appears most frequently. The mode can be calculated for any kind of data, but doesn’t work well for continuous data.\nFor our example, the mode of x is -1. You can compute the mode with table:\n\ntable(x)\n\nx\n-2 -1  0  2  6 \n 1  3  1  1  1 \n\n\nMedian - sort the data, then find the value in the middle. The median can be calculated for ordinal or numerical data.\nFor our example, the median is -1. Compute this with median:\n\nmedian(x)\n\n[1] -1\n\n\nMean - the balancing point of the data, if a waiter was trying to balance the data on a tray. The mean can only be calculated for numerical data.\nFor our example the mean is 0.4285. Compute this with mean:\n\nmean(x)\n\n[1] 0.4285714\n\n\n\nAdding large values to the data affects the mean more than the median:\n\ny = c(x, 100)\nmean(y)\n\n[1] 12.875\n\nmedian(y)\n\n[1] -0.5\n\n\nBecause of this, we say that the median is robust.\nThe mean is good for getting a general idea of where the center of the data is, while comparing it with the median reveals whether there are any unusually large or small values.\nScale is generally summarized by a number that says how far the data is from the center (mean, median, etc…). Two options are:\n\nStandard Deviation - square root of the average squared distance to the mean (the distance from a point to a mean is called a deviation). You can think of this as approximately the average distance from a data point to the mean. As a rule of thumb, most of the data will be within 3 standard deviations of the mean.\nYou can compute the standard deviation with sd:\n\nsd(x)\n\n[1] 2.760262\n\n\nInterquartile Range (IQR) - difference between the 75th and 25th percentile. The median is the 50th percentile of the data; it’s at the middle of the sorted data. We can also consider other percentiles. For instance, the 25th percentile is the value one-quarter of the way through the sorted data.\nQuantile is another word for percentile. Quartile specifically refers to the 25th, 50th, and 75th percentiles because they separate the data into four parts (hence “quart-”).\nYou can compute quantiles with quantile, or compute the IQR directly with IQR:\n\nquantile(x)\n\n  0%  25%  50%  75% 100% \n  -2   -1   -1    1    6 \n\n# IQR\nIQR(x)\n\n[1] 2\n\n\n\nThe IQR is more robust than the standard deviation.\nMany of the functions for computing statistical summaries have a parameter na.rm to ignore missing values. Setting na.rm = TRUE is often useful when you’re just trying to do an initial investigation of the data. However, in a more complete analysis, you should think carefully about what the missing values mean, whether they follow any patterns, and whether there are enough non-missing values for statistical summaries to be good representatives of the data.\nFinally, the summary function computes a detailed statistical summary of an R object. For data frames, the function computes a summary of each column, guessing an appropriate statistic based on the column’s data type.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Forensics</span>"
    ]
  },
  {
    "objectID": "chapters/week04/data-forensics.html#missing-values",
    "href": "chapters/week04/data-forensics.html#missing-values",
    "title": "16  Data Forensics",
    "section": "16.3 Missing Values",
    "text": "16.3 Missing Values\nIf your data contains missing values, it’s important to think about why the values are missing. Statisticians use two different terms to describe why data is missing:\n\nmissing at random (MAR)\nmissing not at random (MNAR) - causes bias!\n\nWhen values are missing at random, the cause for missingness is one or more features in the data set. For example, if a soil moisture sensor overheats and doesn’t work on hot days, but air temperature is recorded in the data set, values are missing at random.\nWhen values are missing not at random, the cause for missingness is one or more features not in the data set. Think of this as a form of censorship. For example, if people in a food survey refuse to report how much sugar they ate on days where they ate junk food, values are missing not at random. Values MNAR can bias an analysis.\n\n\n\n\n\n\nNote\n\n\n\nTechnically, there’s a third kind of missing data: missing completely at random (MCAR). When values are missing completely at random, the cause for missingness is completely unrelated to the research question and features of interest. This kind of missing data is rare in practice and impossible to identify from the data alone. Values MCAR can be ignored without causing any bias.\n\n\nThe default way to handle missing values in R is to ignore them. This is just a default, not necessarily the best or even an appropriate way to deal with them. You can remove missing values from a data set by indexing:\n\nnonpred_eggs_no_na = terns[!is.na(terns$nonpred_eggs), ]\n\nhead(nonpred_eggs_no_na)\n\n  year                                    site_name\n1 2000                        PITTSBURG POWER PLANT\n3 2000                                ALAMEDA POINT\n5 2000 OCEANO DUNES STATE VEHICULAR RECREATION AREA\n6 2000              RANCHO GUADALUPE DUNES PRESERVE\n8 2000        SANTA CLARA RIVER MCGRATH STATE BEACH\n9 2000                                 ORMOND BEACH\n                           site_name_2013_2018  site_name_1988_2001\n1                        Pittsburg Power Plant NA_2013_2018 POLYGON\n3                                Alameda Point NA_2013_2018 POLYGON\n5 Oceano Dunes State Vehicular Recreation Area NA_2013_2018 POLYGON\n6              Rancho Guadalupe Dunes Preserve NA_2013_2018 POLYGON\n8                            Santa Clara River NA_2013_2018 POLYGON\n9                                 Ormond Beach NA_2013_2018 POLYGON\n     site_abbr region_3 region_4   event bp_min bp_max fl_min fl_max\n1   PITT_POWER S.F._BAY S.F._BAY LA_NINA     15     15     16     18\n3      ALAM_PT S.F._BAY S.F._BAY LA_NINA    282    301    200    230\n5 OCEANO_DUNES  CENTRAL  CENTRAL LA_NINA      4      5      4      4\n6         RGDP  CENTRAL  CENTRAL LA_NINA      9      9     17     17\n8   S_CLAR_MCG SOUTHERN  VENTURA LA_NINA     21     21      9      9\n9       ORMOND SOUTHERN  VENTURA LA_NINA     73     73     60     65\n  total_nests nonpred_eggs nonpred_chicks nonpred_fl nonpred_ad pred_control\n1          15            3              0          0          0             \n3         312          124             81          2          1             \n5           5            2              0          0          0             \n6           9            0              1          0          0             \n8          22            4              3         NA         NA             \n9          73            2              0          0          0             \n  pred_eggs pred_chicks pred_fl pred_ad pred_pefa pred_coy_fox pred_meso\n1         4           2       0       0         N            N         N\n3        17           0       0       0         N            N         N\n5         0           4       0       0         N            N         N\n6        NA          NA      NA      NA                                 \n8        NA          NA      NA      NA                                 \n9        NA          NA      NA      NA         N            N         Y\n  pred_owlspp pred_corvid pred_other_raptor pred_other_avian pred_misc\n1           N           Y                 Y                N         N\n3           N           N                 Y                Y         N\n5           N           N                 N                Y         N\n6                                                                     \n8                                                                     \n9           N           N                 Y                N         N\n  total_pefa total_coy_fox total_meso total_owlspp total_corvid\n1          0             0          0            0            4\n3          0             0          0            0            0\n5          0             0          0            0            0\n6         NA            NA         NA           NA           NA\n8         NA            NA         NA           NA           NA\n9         NA            NA         NA           NA           NA\n  total_other_raptor total_other_avian total_misc first_observed last_observed\n1                  2                 0          0     2000-05-11    2000-08-05\n3                  6                11          0     2000-05-01    2000-08-19\n5                  0                 4          0     2000-05-04    2000-08-30\n6                 NA                NA         NA     2000-05-07    2000-08-13\n8                 NA                NA         NA     2000-06-06    2000-09-05\n9                 NA                NA         NA                             \n  first_nest first_chick first_fledge\n1 2000-05-26  2000-06-18   2000-07-08\n3 2000-05-16  2000-06-07   2000-06-30\n5 2000-05-28  2000-06-20   2000-07-13\n6 2000-05-31  2000-06-22   2000-07-20\n8 2000-06-06  2000-06-28   2000-07-24\n9 2000-06-08  2000-06-26   2000-07-17\n\n\nThe na.omit function is less precise than indexing, because it removes rows that have a missing value in any column. This means lots of information gets lost.\nAnother way to handle missing values is to impute, or fill in, the values with estimates based on other data in the data set. We won’t get into the details of how to impute missing values here, since it is a fairly deep subject. Generally it is safe to impute MAR values, but not MNAR values.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Forensics</span>"
    ]
  },
  {
    "objectID": "chapters/week04/data-forensics.html#outliers",
    "href": "chapters/week04/data-forensics.html#outliers",
    "title": "16  Data Forensics",
    "section": "16.4 Outliers",
    "text": "16.4 Outliers\nAn outlier is an anomalous or extreme value in a data set. We can picture this as a value that’s far away from most of the other values. Sometimes outliers are a natural part of the data set. In other situations, outliers can indicate errors in how the data were measured, recorded, or cleaned.\nThere’s no specific definition for “extreme” or “far away”. A good starting point for detecting outliers is to make a plot that shows how the values are distributed. Box plots and density plots work especially well for this (you’ll learn about how to make plots in a later lesson):\n\nlibrary(\"ggplot2\")\n\nggplot(terns, aes(x = nonpred_eggs)) + geom_boxplot()\n\nWarning: Removed 164 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n# Some sites might have more eggs than others, so a high number of non-predator\n# egg mortalities does not necessarily mean a site is unusually dangerous. We\n# can find dangerous sites by looking at non-predator egg mortalities per nest:\n\nggplot(terns, aes(x = nonpred_eggs / total_nests)) + geom_boxplot()\n\nWarning: Removed 206 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nStatisticians tend to use the rule of thumb that any value more than 3 standard deviations away from the mean is an outlier. You can use the scale function to compute how many standard deviations the elements in a column are from their mean:\n\nz = scale(terns$nonpred_eggs)\nhead(z)\n\n           [,1]\n[1,] -0.5403550\n[2,]         NA\n[3,]  0.6008708\n[4,]         NA\n[5,] -0.5497867\n[6,] -0.5686499\n\nwhich(z &lt;= -3 | 3 &lt;= z)\n\n [1]  49  81 108 144 150 188 226 306 339 347 359 389 401 473 678\n\n\nBe careful to think about what your specific data set measures, as this definition isn’t appropriate in every situation.\nHow can you handle outliers? First, try inspecting other features from the row to determine whether the outlier is a valid measurement or an error. When an outlier is valid, keep it.\nIf the outlier interferes with a plot you want to make, you can adjust the x and y limits on plots as needed to “ignore” the outlier. Make sure to mention this in the plot’s title or caption.\nWhen an outlier is not valid, first try to correct it. For example:\n\nCorrect with a different covariate from the same observation.\nEstimate with a mean or median of similar observations. This is another example of imputing values.\n\n\nIf other features don’t help with correction, try getting information from external sources. If you can’t correct the outlier but know it’s invalid, replace it with a missing value NA.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Forensics</span>"
    ]
  },
  {
    "objectID": "chapters/week04/aggregation-grouping.html",
    "href": "chapters/week04/aggregation-grouping.html",
    "title": "17  Aggregation & Grouping",
    "section": "",
    "text": "17.1 The map Function\nSection 13.1.3 introduced vectorization, a convenient and efficient way to compute multiple results. That section also mentioned that some of R’s functions—the ones that summarize or aggregate data—are not vectorized.\nThe class function is an example of a function that’s not vectorized. If we call the class function on the least terns data set, we get just one result for the data set as a whole:\nclass(terns)\n\n[1] \"data.frame\"\nWhat if we want to get the class of each column? We can get the class for a single column by selecting the column with $, the dollar sign operator:\nclass(terns$year)\n\n[1] \"integer\"\nBut what if we want the classes for all the columns? We could write a call to class for each column, but that would be tedious. When you’re working with a programming language, you should try to avoid tedium; there’s usually a better, more automated way.\nSection 13.2.1 pointed out that data frames are technically lists, where each column is one element. With that in mind, what we need here is a line of code that calls class on each element of the data frame. The idea is similar to vectorization, but since we have a list and a non-vectorized function, we have to do a bit more than just call class(terns).\nThe purrr package is a collection of functions to help you do things repeatedly or for each element of a data structure. Install and load the package in order to follow along:\n# install.packages(\"purrr\")\nlibrary(\"purrr\")\nThe package’s map function calls a function on each element of a vector or list. We sometimes also say it maps or applies a function over the elements. The syntax is:\nThe map function calls the function F once for each element of DATA. It passes the element to F as the first argument. It also passes the ... arguments to F, which are constant across all of the calls.\nLet’s try this out with the least terns data and the class function:\nmap(terns, class)\n\n$year\n[1] \"integer\"\n\n$site_name\n[1] \"character\"\n\n$site_name_2013_2018\n[1] \"character\"\n\n$site_name_1988_2001\n[1] \"character\"\n\n$site_abbr\n[1] \"character\"\n\n$region_3\n[1] \"character\"\n\n$region_4\n[1] \"character\"\n\n$event\n[1] \"character\"\n\n$bp_min\n[1] \"numeric\"\n\n$bp_max\n[1] \"numeric\"\n\n$fl_min\n[1] \"integer\"\n\n$fl_max\n[1] \"integer\"\n\n$total_nests\n[1] \"integer\"\n\n$nonpred_eggs\n[1] \"integer\"\n\n$nonpred_chicks\n[1] \"integer\"\n\n$nonpred_fl\n[1] \"integer\"\n\n$nonpred_ad\n[1] \"integer\"\n\n$pred_control\n[1] \"character\"\n\n$pred_eggs\n[1] \"integer\"\n\n$pred_chicks\n[1] \"integer\"\n\n$pred_fl\n[1] \"integer\"\n\n$pred_ad\n[1] \"integer\"\n\n$pred_pefa\n[1] \"character\"\n\n$pred_coy_fox\n[1] \"character\"\n\n$pred_meso\n[1] \"character\"\n\n$pred_owlspp\n[1] \"character\"\n\n$pred_corvid\n[1] \"character\"\n\n$pred_other_raptor\n[1] \"character\"\n\n$pred_other_avian\n[1] \"character\"\n\n$pred_misc\n[1] \"character\"\n\n$total_pefa\n[1] \"integer\"\n\n$total_coy_fox\n[1] \"integer\"\n\n$total_meso\n[1] \"integer\"\n\n$total_owlspp\n[1] \"integer\"\n\n$total_corvid\n[1] \"integer\"\n\n$total_other_raptor\n[1] \"integer\"\n\n$total_other_avian\n[1] \"integer\"\n\n$total_misc\n[1] \"integer\"\n\n$first_observed\n[1] \"character\"\n\n$last_observed\n[1] \"character\"\n\n$first_nest\n[1] \"character\"\n\n$first_chick\n[1] \"character\"\n\n$first_fledge\n[1] \"character\"\nThe result is similar to if the class function was vectorized. In fact, if we use a vector and a vectorized function with map, the result is nearly identical to the result from vectorization:\nx = c(1, 2, pi)\n\nsin(x)\n\n[1] 8.414710e-01 9.092974e-01 1.224647e-16\n\nmap(x, sin)\n\n[[1]]\n[1] 0.841471\n\n[[2]]\n[1] 0.9092974\n\n[[3]]\n[1] 1.224647e-16\nThe only difference is that the result from map is a list. In fact, the map function always returns a list with one element for each element of the input data.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregation & Grouping</span>"
    ]
  },
  {
    "objectID": "chapters/week04/aggregation-grouping.html#sec-apply-functions",
    "href": "chapters/week04/aggregation-grouping.html#sec-apply-functions",
    "title": "17  Aggregation & Grouping",
    "section": "",
    "text": "map(DATA, F, ...)",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregation & Grouping</span>"
    ]
  },
  {
    "objectID": "chapters/week04/aggregation-grouping.html#other-map-functions",
    "href": "chapters/week04/aggregation-grouping.html#other-map-functions",
    "title": "17  Aggregation & Grouping",
    "section": "17.2 Other Map Functions",
    "text": "17.2 Other Map Functions\nThe purrr package provides many different map functions, all of which have names that start with map. All of them call another function on each element of a data structure. They also all have the same syntax. Where they differ is in how they return results. A few of these are shown in Table 26.1.\n\n\n\nTable 17.1\n\n\n\n\n\nFunction\nReturn Type\n\n\n\n\nmap_lgl\nlogical\n\n\nmap_int\ninteger\n\n\nmap_dbl\nnumeric (double)\n\n\nmap_chr\ncharacter\n\n\nmap\nlist\n\n\n\n\n\n\n\n\n\n\n\n\nNoteApply Functions\n\n\n\nR’s apply functions are a built-in equivalent to map functions. The lapply, sapply, and tapply functions are the three most important functions in the family of apply functions, but there are many more. The lapply function is nearly identical to the map function.\nWe focus on and recommend the map functions rather than the apply functions because they are more consistent in their syntax and specific in their return types. You can learn more about R’s apply functions by reading this StackOverflow post.\n\n\nWhen you have a choice between using vectorization or a map function, you should always choose vectorization. Vectorization is clearer—compare the two lines of code above—and it’s also significantly more efficient. In fact, vectorization is the most efficient way to call a function repeatedly in R.\nAs we saw with the class function, there are some situations where vectorization is not possible. That’s when you should think about using a map function.\nLet’s look at some examples of the other map functions. If we use map_chr to find the classes of the columns in the least terns data, we get a character vector:\n\nmap_chr(terns, class)\n\n               year           site_name site_name_2013_2018 site_name_1988_2001 \n          \"integer\"         \"character\"         \"character\"         \"character\" \n          site_abbr            region_3            region_4               event \n        \"character\"         \"character\"         \"character\"         \"character\" \n             bp_min              bp_max              fl_min              fl_max \n          \"numeric\"           \"numeric\"           \"integer\"           \"integer\" \n        total_nests        nonpred_eggs      nonpred_chicks          nonpred_fl \n          \"integer\"           \"integer\"           \"integer\"           \"integer\" \n         nonpred_ad        pred_control           pred_eggs         pred_chicks \n          \"integer\"         \"character\"           \"integer\"           \"integer\" \n            pred_fl             pred_ad           pred_pefa        pred_coy_fox \n          \"integer\"           \"integer\"         \"character\"         \"character\" \n          pred_meso         pred_owlspp         pred_corvid   pred_other_raptor \n        \"character\"         \"character\"         \"character\"         \"character\" \n   pred_other_avian           pred_misc          total_pefa       total_coy_fox \n        \"character\"         \"character\"           \"integer\"           \"integer\" \n         total_meso        total_owlspp        total_corvid  total_other_raptor \n          \"integer\"           \"integer\"           \"integer\"           \"integer\" \n  total_other_avian          total_misc      first_observed       last_observed \n          \"integer\"           \"integer\"         \"character\"         \"character\" \n         first_nest         first_chick        first_fledge \n        \"character\"         \"character\"         \"character\" \n\n\nLikewise, if we use map_dbl to compute the sin values, we get a numeric vector, the same as from vectorization:\n\nmap_dbl(x, sin)\n\n[1] 8.414710e-01 9.092974e-01 1.224647e-16\n\n\nIn spite of that, vectorization is still more efficient than sapply, so use vectorization instead when possible.\nMap functions are incredibly useful for summarizing data. For example, suppose we want to compute the frequencies for all of the columns in the least terns data set that aren’t numeric.\nFirst, we need to identify the columns. One way to do this is with the is.numeric function. Despite the name, this function actually tests whether its argument is a real number, not whether it its argument is a numeric vector. In other words, it also returns true for integer values. We can use map_lgl to apply this function to all of the columns in the least terns data set:\n\nis_not_number = !map_lgl(terns, is.numeric)\nis_not_number\n\n               year           site_name site_name_2013_2018 site_name_1988_2001 \n              FALSE                TRUE                TRUE                TRUE \n          site_abbr            region_3            region_4               event \n               TRUE                TRUE                TRUE                TRUE \n             bp_min              bp_max              fl_min              fl_max \n              FALSE               FALSE               FALSE               FALSE \n        total_nests        nonpred_eggs      nonpred_chicks          nonpred_fl \n              FALSE               FALSE               FALSE               FALSE \n         nonpred_ad        pred_control           pred_eggs         pred_chicks \n              FALSE                TRUE               FALSE               FALSE \n            pred_fl             pred_ad           pred_pefa        pred_coy_fox \n              FALSE               FALSE                TRUE                TRUE \n          pred_meso         pred_owlspp         pred_corvid   pred_other_raptor \n               TRUE                TRUE                TRUE                TRUE \n   pred_other_avian           pred_misc          total_pefa       total_coy_fox \n               TRUE                TRUE               FALSE               FALSE \n         total_meso        total_owlspp        total_corvid  total_other_raptor \n              FALSE               FALSE               FALSE               FALSE \n  total_other_avian          total_misc      first_observed       last_observed \n              FALSE               FALSE                TRUE                TRUE \n         first_nest         first_chick        first_fledge \n               TRUE                TRUE                TRUE \n\n\nIs it worth using R code to identify the non-numeric columns? Since there are only 43 columns in the least terns data set, maybe not. But if the data set was larger, with say 100 columns, it definitely would be.\nIn general, it’s a good habit to use R to do things rather than do them manually. You’ll get more practice programming, and your code will be more flexible if you want to adapt it to other data sets.\nNow that we know which columns are non-numeric, we can use the table function to compute frequencies. We only want to compute frequencies for those columns, so we need to subset the data:\n\nmap(terns[, is_not_number], table)\n\n$site_name\n\n                               ALAMEDA POINT \n                                          21 \n                          ALBANY CENTRAL AVE \n                                           2 \n                                ANAHEIM LAKE \n                                           3 \n                            ARIZONA GLENDALE \n                                           1 \n        BATIQUITOS LAGOON ECOLOGICAL RESERVE \n                                          21 \n              BOLSA CHICA ECOLOGICAL RESERVE \n                                          21 \n                               BURRIS ISLAND \n                                          18 \n                CHULA VISTA WILDLIFE RESERVE \n                                          21 \n                      COAL OIL POINT RESERVE \n                                          14 \n           DSTREET FILL SWEETWATER MARSH NWR \n                                          21 \n             EDEN LANDING ECOLOGICAL RESERVE \n                                          17 \n                             FAIRBANKS RANCH \n                                           9 \n                  GUADALUPE NIPOMO DUNES NWR \n                                           1 \n                  HAYWARD REGIONAL SHORELINE \n                                          19 \n                             HOLLYWOOD BEACH \n                                          18 \n                      HUNTINGTON STATE BEACH \n                                          21 \n                              KETTLEMAN CITY \n                                          15 \n                                   LA HARBOR \n                                          21 \n                               MALIBU LAGOON \n                                           7 \n                          MCB CAMP PENDLETON \n                                          21 \n                      MISSION BAY FAA ISLAND \n                                          21 \n                  MISSION BAY MARINERS POINT \n                                          21 \n             MISSION BAY NORTH FIESTA ISLAND \n                                          21 \n           MISSION BAY SAN DIEGO RIVER MOUTH \n                                          14 \n                     MISSION BAY STONY POINT \n                                          18 \n                          MONTEZUMA WETLANDS \n                                          18 \n             NAPA SONOMA MARSH WILDLIFE AREA \n                                          17 \n                            NAS NORTH ISLAND \n                                          21 \n              NAVAL AMPHIBIOUS BASE CORONADO \n                                          21 \n                             NBVC POINT MUGU \n                                          21 \nOCEANO DUNES STATE VEHICULAR RECREATION AREA \n                                          21 \n                                ORMOND BEACH \n                                          21 \n                       PITTSBURG POWER PLANT \n                                          15 \n             RANCHO GUADALUPE DUNES PRESERVE \n                                          20 \n                      SACRAMENTO BUFFERLANDS \n                                          10 \n                                  SALTON SEA \n                                           7 \n      SAN DIEGUITO LAGOON ECOLOGICAL RESERVE \n                                          10 \n         SAN ELIJO LAGOON ECOLOGICAL RESERVE \n                                          21 \n                           SAN PABLO BAY NWR \n                                           3 \n       SANTA CLARA RIVER MCGRATH STATE BEACH \n                                          21 \n  SATICOY UNITED WATER CONSERVATION DISTRICT \n                                           9 \n                        SDIA LINDBERGH FIELD \n                                          21 \n                  SEAL BEACH NWR ANAHEIM BAY \n                                          21 \n                   SILVER STRAND STATE BEACH \n                                           1 \n    SOUTH SAN DIEGO BAY UNIT SDNWR SALTWORKS \n                                          21 \n                        TIJUANA ESTUARY NERR \n                                          21 \n        UPPER NEWPORT BAY ECOLOGICAL RESERVE \n                                          21 \n                              VANDENBERG SFB \n                                          21 \n                                VENICE BEACH \n                                          21 \n\n$site_name_2013_2018\n\n                                                          \n                                                        3 \n                                            Alameda Point \n                                                       21 \n                                             Anaheim Lake \n                                                        3 \n                                        Batiquitos Lagoon \n                                                       21 \n                                              Bolsa Chica \n                                                       21 \n                                              Bufferlands \n                                                       10 \n                                             Burris Basin \n                                                       18 \n                                           Camp Pendleton \n                                                       21 \n                              Chula Vista Wildlife Refuge \n                                                       21 \n                                   Coal Oil Point Reserve \n                                                       14 \n                                            D Street Fill \n                                                       21 \n                          Eden Landing Ecological Reserve \n                                                       17 \n                                               FAA Island \n                                                       21 \n                                          Fairbanks Ranch \n                                                        9 \n                               Hayward Regional Shoreline \n                                                       19 \n                                          Hollywood Beach \n                                                       18 \n                              Huntington Beach State Park \n                                                       21 \n                                                Kettleman \n                                                       15 \n             Lindbergh Field/Former Naval Training Center \n                                                       21 \n                                            Malibu Lagoon \n                                                        7 \n                                          Mariner's Point \n                                                       21 \n                                                Montezuma \n                                                       18 \n                                            NA_NO POLYGON \n                                                        5 \nNapa Sonoma Marsh Wildlife Area Huichica Unit (Pond 7/7A) \n                                                       17 \n                                      Naval Base Coronado \n                                                       42 \n                                          NBVC Point Mugu \n                                                       21 \n                                      North Fiesta Island \n                                                       21 \n             Oceano Dunes State Vehicular Recreation Area \n                                                       21 \n                                             Ormond Beach \n                                                       21 \n                                    Pittsburg Power Plant \n                                                       15 \n                                               Port of LA \n                                                       21 \n                          Rancho Guadalupe Dunes Preserve \n                                                       20 \n                                               Salton Sea \n                                                        7 \n                                                Saltworks \n                                                       21 \n                                    San Diego River Mouth \n                                                       14 \n                                      San Diequito Lagoon \n                                                       10 \n                                         San Elijo Lagoon \n                                                       21 \n                                        Santa Clara River \n                                                       21 \n               Saticoy United Water Conservation District \n                                                        9 \n                      Seal Beach National Wildlife Refuge \n                                                       21 \n                                              Stony Point \n                                                       18 \n                                          Tijuana Estuary \n                                                       21 \n                                        Upper Newport Bay \n                                                       21 \n                                           Vandenberg AFB \n                                                       21 \n                                             Venice Beach \n                                                       21 \n\n$site_name_1988_2001\n\n                      Albany Central Avenue  NA_2013_2018 POLYGON \n                    3                     2                   783 \n        NA_NO POLYGON \n                    3 \n\n$site_abbr\n\n  AL_CENTAVE      ALAM_PT       ANA_LK     ARZ_GLEN         BCER         BLER \n           2           21            3            1           21           21 \n     BUR_ISL     COAL_OIL           CV         D_ST         ELER     FAIR_RAN \n          18           14           21           21           17            9 \n     GND_NWR HAY_REG_SHOR           HB          HSB      KET_CTY      LA_HARB \n           1           19           18           21           15           21 \n     MAL_LAG       MB_FAA    MB_MAR_PT       MB_NFI   MB_SDRIV_S     MB_STONY \n           7           21           21           21           14           18 \n       MCBCP         MONT          NAB        NASNI        NSMWA OCEANO_DUNES \n          21           18           21           21           17           21 \n      ORMOND   PITT_POWER      PT_MUGU         RGDP   S_CLAR_MCG      SAC_BUF \n          21           15           21           20           21           10 \n        SALT     SALT_SEA    SAN_ELIJO SANDIEGU_LAG      SAT_WCD      SDIA_LF \n          21            7           21           10            9           21 \nSEAL_BCH_NWR     SLVRSTRD       SPBNWR       TJ_RIV        UNBER      VAN_SFB \n          21            1            3           21           21           21 \n     VEN_BCH \n          21 \n\n$region_3\n\n   ARIZONA    CENTRAL      KINGS   S.F._BAY SACRAMENTO   SOUTHERN \n         1         77         15        112         10        576 \n\n$region_4\n\n   ARIZONA    CENTRAL      KINGS   S.F._BAY SACRAMENTO   SOUTHERN    VENTURA \n         1         77         15        112         10        486         90 \n\n$event\n\nEL_NINO LA_NINA NEUTRAL \n    120     258     413 \n\n$pred_control\n\n      N   Y \n342 134 315 \n\n$pred_pefa\n\n      N   Y \n143 423 225 \n\n$pred_coy_fox\n\n      N   Y \n142 535 114 \n\n$pred_meso\n\n      N   Y \n142 552  97 \n\n$pred_owlspp\n\n      N   Y \n142 531 118 \n\n$pred_corvid\n\n      N   Y \n142 423 226 \n\n$pred_other_raptor\n\n      N  NN   Y \n145 437   1 208 \n\n$pred_other_avian\n\n      N   Y \n143 395 253 \n\n$pred_misc\n\n      N   Y \n159 415 217 \n\n$first_observed\n\n           2000-04-16 2000-04-19 2000-04-21 2000-04-22 2000-04-23 2000-04-26 \n       141          1          1          1          1          1          1 \n2000-04-28 2000-05-01 2000-05-04 2000-05-06 2000-05-07 2000-05-09 2000-05-11 \n         5          1          3          1          2          1          1 \n2000-05-21 2000-06-06 2000-06-10 2004-04-08 2004-04-11 2004-04-12 2004-04-21 \n         1          1          1          1          1          2          1 \n2004-04-22 2004-04-23 2004-04-27 2004-04-30 2004-05-01 2004-05-03 2004-05-07 \n         2          1          1          1          1          2          1 \n2004-05-09 2004-05-10 2004-05-14 2004-05-17 2004-05-31 2004-06-03 2005-04-14 \n         1          1          1          1          1          1          1 \n2005-04-15 2005-04-18 2005-04-19 2005-04-20 2005-04-21 2005-04-22 2005-04-23 \n         1          2          1          2          2          1          2 \n2005-04-24 2005-04-28 2005-04-29 2005-04-30 2005-05-01 2005-05-05 2005-05-07 \n         1          2          1          1          1          1          1 \n2005-05-08 2005-05-10 2005-05-13 2005-05-27 2005-05-28 2006-04-08 2006-04-10 \n         1          1          1          1          1          2          1 \n2006-04-13 2006-04-15 2006-04-16 2006-04-20 2006-04-22 2006-04-27 2006-04-28 \n         1          1          1          1          3          1          1 \n2006-05-01 2006-05-03 2006-05-08 2006-05-10 2006-05-14 2006-05-15 2006-05-20 \n         1          2          1          1          2          1          1 \n2006-06-23 2006-06-24 2007-04-16 2007-04-18 2007-04-22 2007-04-23 2007-04-24 \n         1          1          1          1          1          1          2 \n2007-04-25 2007-04-26 2007-04-28 2007-05-01 2007-05-02 2007-05-03 2007-05-11 \n         4          2          1          3          3          1          1 \n2007-05-13 2007-05-14 2007-05-18 2007-05-24 2007-06-02 2007-06-06 2007-06-08 \n         2          1          1          1          1          1          1 \n2008-04-11 2008-04-16 2008-04-23 2008-04-24 2008-04-25 2008-04-27 2008-04-28 \n         1          2          1          4          1          3          3 \n2008-05-01 2008-05-04 2008-05-05 2008-05-07 2008-05-08 2008-05-09 2008-05-12 \n         1          1          1          1          1          2          1 \n2008-05-14 2008-05-15 2008-05-21 2008-05-22 2008-05-24 2008-05-28 2008-05-31 \n         1          1          1          1          1          1          1 \n2009-04-16 2009-04-19 2009-04-22 2009-04-23 2009-04-24 2009-04-25 2009-04-26 \n         2          1          1          4          2          1          3 \n2009-04-29 2009-04-30 2009-05-02 2009-05-03 2009-05-04 2009-05-05 2009-05-06 \n         1          1          1          2          1          1          1 \n2009-05-08 2009-05-11 2009-05-13 2009-05-28 2010-04-14 2010-04-16 2010-04-17 \n         3          1          2          1          1          2          1 \n2010-04-18 2010-04-19 2010-04-21 2010-04-22 2010-04-25 2010-04-27 2010-04-28 \n         2          1          1          2          2          1          1 \n2010-04-29 2010-05-01 2010-05-02 2010-05-04 2010-05-06 2010-05-07 2010-05-13 \n         1          1          1          1          1          1          2 \n2010-05-14 2010-05-15 2010-05-16 2010-05-26 2010-05-29 2010-05-30 2010-06-09 \n         2          4          1          1          1          1          1 \n2011-04-09 2011-04-15 2011-04-18 2011-04-20 2011-04-21 2011-04-22 2011-04-24 \n         1          1          1          1          5          2          1 \n2011-04-25 2011-04-27 2011-04-28 2011-04-29 2011-05-01 2011-05-04 2011-05-05 \n         1          1          1          1          1          1          2 \n2011-05-07 2011-05-08 2011-05-11 2011-05-12 2011-05-14 2011-05-28 2011-06-11 \n         1          1          1          1          3          1          1 \n2011-06-14 2011-06-25 2012-04-14 2012-04-15 2012-04-17 2012-04-18 2012-04-19 \n         1          1          2          1          2          3          5 \n2012-04-20 2012-04-21 2012-04-22 2012-04-25 2012-04-29 2012-05-01 2012-05-03 \n         1          2          2          1          2          1          1 \n2012-05-06 2012-05-08 2012-05-10 2012-05-11 2012-05-13 2012-05-14 2012-05-23 \n         1          1          3          1          1          1          1 \n2012-05-30 2012-06-09 2013-04-14 2013-04-15 2013-04-18 2013-04-21 2013-04-24 \n         1          1          1          1          1          1          2 \n2013-04-25 2013-04-29 2013-04-30 2013-05-01 2013-05-02 2013-05-03 2013-05-07 \n         1          1          1          2          2          3          1 \n2013-05-10 2013-05-11 2013-05-12 2013-05-13 2013-05-15 2013-05-17 2013-05-19 \n         1          1          1          1          1          1          1 \n2013-05-27 2013-06-01 2013-06-09 2013-06-30 2013-07-19 2014-04-15 2014-04-17 \n         1          2          1          1          1          2          5 \n2014-04-19 2014-04-21 2014-04-24 2014-04-25 2014-04-26 2014-04-30 2014-05-01 \n         1          1          1          2          1          1          1 \n2014-05-02 2014-05-04 2014-05-08 2014-05-09 2014-05-10 2014-05-11 2014-05-16 \n         1          2          1          2          2          1          1 \n2014-05-22 2014-05-24 2014-05-25 2014-05-31 2015-04-06 2015-04-15 2015-04-17 \n         1          2          1          1          1          2          2 \n2015-04-18 2015-04-19 2015-04-22 2015-04-23 2015-04-27 2015-04-29 2015-04-30 \n         1          3          3          1          1          2          3 \n2015-05-01 2015-05-07 2015-05-08 2015-05-09 2015-05-14 2015-05-17 2015-05-20 \n         3          1          2          1          2          1          1 \n2015-05-30 2015-06-05 2015-07-05 2015-07-07 2015-07-22 2016-04-10 2016-04-11 \n         1          1          1          1          1          2          1 \n2016-04-12 2016-04-13 2016-04-14 2016-04-15 2016-04-18 2016-04-20 2016-04-21 \n         1          1          2          1          2          1          1 \n2016-04-23 2016-04-26 2016-04-30 2016-05-01 2016-05-02 2016-05-03 2016-05-04 \n         1          1          2          1          1          2          1 \n2016-05-06 2016-05-07 2016-05-11 2016-05-12 2016-05-14 2016-05-18 2016-05-25 \n         2          2          2          1          1          2          1 \n2016-05-31 2016-06-12 2016-06-16 2016-06-18 2016-07-15 2017-04-13 2017-04-14 \n         1          1          1          2          1          2          1 \n2017-04-15 2017-04-16 2017-04-17 2017-04-18 2017-04-19 2017-04-20 2017-04-21 \n         2          1          2          2          1          1          1 \n2017-04-22 2017-04-24 2017-04-26 2017-04-29 2017-05-04 2017-05-06 2017-05-07 \n         1          1          1          1          2          3          2 \n2017-05-10 2017-05-11 2017-05-12 2017-05-19 2017-05-25 2017-06-03 2017-07-02 \n         3          2          1          1          2          1          1 \n2017-07-04 2018-03-25 2018-03-30 2018-04-13 2018-04-18 2018-04-19 2018-04-20 \n         1          1          1          2          1          2          3 \n2018-04-21 2018-04-25 2018-04-26 2018-04-27 2018-04-28 2018-04-29 2018-05-04 \n         2          2          1          1          4          2          1 \n2018-05-05 2018-05-10 2018-05-11 2018-05-13 2018-05-16 2018-05-20 2018-05-21 \n         1          2          1          1          1          1          1 \n2019-04-06 2019-04-12 2019-04-14 2019-04-17 2019-04-18 2019-04-19 2019-04-20 \n         1          3          1          1          1          3          2 \n2019-04-22 2019-04-24 2019-04-26 2019-04-27 2019-05-01 2019-05-02 2019-05-03 \n         1          1          1          4          1          1          2 \n2019-05-06 2019-05-08 2019-05-09 2019-05-10 2019-05-11 2019-05-12 2020-04-06 \n         2          1          2          1          3          1          1 \n2020-04-10 2020-04-11 2020-04-17 2020-04-18 2020-04-20 2020-04-21 2020-04-23 \n         1          1          2          1          2          1          1 \n2020-04-24 2020-04-25 2020-04-26 2020-04-29 2020-04-30 2020-05-01 2020-05-02 \n         2          2          2          2          2          1          1 \n2020-05-06 2020-05-08 2020-05-09 2020-05-10 2020-05-14 2020-05-17 2020-06-24 \n         2          3          3          1          1          1          1 \n2021-04-10 2021-04-12 2021-04-13 2021-04-17 2021-04-18 2021-04-19 2021-04-21 \n         1          1          1          1          1          2          4 \n2021-04-22 2021-04-23 2021-04-24 2021-04-25 2021-04-27 2021-04-28 2021-04-29 \n         3          1          2          1          1          2          2 \n2021-05-01 2021-05-06 2021-05-11 2021-05-13 2021-05-16 2021-05-21 2021-05-26 \n         1          2          1          2          1          1          1 \n2021-05-27 2021-06-04 2021-06-16 2022-04-10 2022-04-13 2022-04-15 2022-04-17 \n         1          1          1          1          1          1          2 \n2022-04-18 2022-04-21 2022-04-22 2022-04-23 2022-04-24 2022-04-26 2022-04-28 \n         2          2          2          1          1          2          2 \n2022-04-30 2022-05-05 2022-05-06 2022-05-08 2022-05-11 2022-05-14 2022-05-15 \n         3          1          2          2          1          1          1 \n2022-05-17 2022-05-19 2022-05-25 2022-05-29 2023-04-15 2023-04-18 2023-04-20 \n         2          2          1          1          1          1          2 \n2023-04-22 2023-04-23 2023-04-24 2023-04-26 2023-04-27 2023-04-28 2023-04-29 \n         2          4          1          1          1          2          1 \n2023-04-30 2023-05-01 2023-05-03 2023-05-04 2023-05-06 2023-05-07 2023-05-10 \n         2          1          1          1          1          2          1 \n2023-05-11 2023-05-17 2023-05-19 2023-05-24 2023-05-25 2023-05-27 2023-05-28 \n         2          1          1          1          1          1          2 \n2023-07-23 2023-08-04 2028-04-19 \n         1          1          1 \n\n$last_observed\n\n           2000-07-21 2000-08-05 2000-08-12 2000-08-13 2000-08-14 2000-08-17 \n       149          1          2          1          2          1          1 \n2000-08-18 2000-08-19 2000-08-20 2000-08-24 2000-08-26 2000-08-30 2000-08-31 \n         1          1          3          1          1          1          1 \n2000-09-01 2000-09-05 2000-09-06 2000-09-07 2000-09-15 2000-09-24 2004-06-14 \n         1          1          1          1          1          1          1 \n2004-07-01 2004-07-18 2004-07-21 2004-07-22 2004-07-23 2004-08-02 2004-08-06 \n         2          1          1          1          1          1          1 \n2004-08-09 2004-08-10 2004-08-13 2004-08-16 2004-08-19 2004-08-22 2004-08-23 \n         1          1          1          1          1          1          1 \n2004-08-24 2004-09-03 2004-09-09 2004-09-10 2005-06-10 2005-06-17 2005-06-24 \n         1          1          1          1          1          1          1 \n2005-06-29 2005-07-07 2005-07-23 2005-07-26 2005-07-28 2005-07-29 2005-07-31 \n         1          1          1          1          1          1          1 \n2005-08-03 2005-08-05 2005-08-13 2005-08-15 2005-08-17 2005-08-18 2005-08-24 \n         1          1          1          1          1          1          1 \n2005-08-26 2005-08-27 2005-08-29 2005-09-08 2005-09-12 2005-09-15 2006-06-18 \n         2          1          1          2          1          1          1 \n2006-06-27 2006-07-13 2006-07-23 2006-07-29 2006-08-05 2006-08-07 2006-08-09 \n         1          1          1          1          1          2          1 \n2006-08-11 2006-08-16 2006-08-17 2006-08-18 2006-08-31 2006-09-02 2006-09-05 \n         1          2          1          1          1          1          1 \n2006-09-10 2006-09-11 2006-09-13 2006-09-17 2006-09-18 2007-06-24 2007-07-18 \n         1          1          1          1          1          1          1 \n2007-08-01 2007-08-06 2007-08-08 2007-08-10 2007-08-11 2007-08-15 2007-08-17 \n         1          1          1          1          1          2          1 \n2007-08-19 2007-08-20 2007-08-22 2007-08-24 2007-08-25 2007-09-02 2007-09-06 \n         3          3          1          2          3          1          1 \n2007-09-08 2007-09-09 2007-09-12 2007-09-15 2008-06-09 2008-06-10 2008-06-14 \n         1          1          1          1          1          1          1 \n2008-06-27 2008-07-18 2008-07-31 2008-08-05 2008-08-07 2008-08-10 2008-08-13 \n         1          2          1          1          1          1          1 \n2008-08-15 2008-08-17 2008-08-19 2008-08-21 2008-08-24 2008-08-26 2008-08-28 \n         1          3          1          1          1          1          1 \n2008-08-29 2008-08-31 2008-09-01 2008-09-04 2008-09-19 2008-09-20 2009-06-10 \n         3          2          1          1          1          1          1 \n2009-07-13 2009-07-17 2009-07-18 2009-07-24 2009-07-26 2009-07-30 2009-08-01 \n         1          2          1          1          1          1          1 \n2009-08-05 2009-08-06 2009-08-07 2009-08-10 2009-08-11 2009-08-12 2009-08-13 \n         1          1          1          1          1          1          2 \n2009-08-14 2009-08-15 2009-08-19 2009-08-20 2009-08-21 2009-08-22 2009-08-24 \n         1          1          1          1          1          1          1 \n2009-08-25 2009-08-27 2009-09-05 2009-09-26 2009-09-27 2010-07-01 2010-07-17 \n         1          1          1          1          1          2          1 \n2010-07-21 2010-07-22 2010-07-27 2010-07-29 2010-07-30 2010-08-01 2010-08-04 \n         1          1          1          3          2          1          1 \n2010-08-05 2010-08-07 2010-08-09 2010-08-11 2010-08-12 2010-08-13 2010-08-14 \n         1          1          1          2          4          1          1 \n2010-08-15 2010-08-17 2010-08-18 2010-08-21 2010-08-23 2010-08-26 2010-09-01 \n         1          1          1          1          1          1          1 \n2010-09-03 2011-06-25 2011-07-06 2011-07-19 2011-07-21 2011-07-26 2011-07-27 \n         1          1          1          1          1          1          1 \n2011-07-29 2011-07-31 2011-08-01 2011-08-02 2011-08-03 2011-08-04 2011-08-05 \n         3          1          1          1          1          1          1 \n2011-08-06 2011-08-10 2011-08-11 2011-08-13 2011-08-15 2011-08-17 2011-08-18 \n         2          1          1          1          1          1          1 \n2011-08-20 2011-08-22 2011-08-23 2011-08-24 2011-08-26 2011-08-27 2012-06-13 \n         1          1          1          1          1          3          1 \n2012-06-16 2012-06-19 2012-06-22 2012-07-01 2012-07-10 2012-07-12 2012-07-13 \n         1          1          1          1          1          1          2 \n2012-07-17 2012-07-19 2012-07-22 2012-07-24 2012-07-25 2012-07-27 2012-08-01 \n         1          1          1          1          1          1          1 \n2012-08-05 2012-08-06 2012-08-08 2012-08-09 2012-08-10 2012-08-11 2012-08-12 \n         1          1          1          1          2          1          1 \n2012-08-16 2012-08-25 2012-08-26 2012-08-31 2012-09-01 2012-09-15 2013-05-09 \n         1          1          2          1          1          2          1 \n2013-06-07 2013-06-09 2013-06-10 2013-06-23 2013-06-26 2013-06-28 2013-07-01 \n         1          1          1          1          1          1          1 \n2013-07-10 2013-07-11 2013-07-21 2013-07-26 2013-07-28 2013-08-01 2013-08-02 \n         1          1          1          1          1          1          1 \n2013-08-03 2013-08-08 2013-08-11 2013-08-12 2013-08-16 2013-08-17 2013-08-21 \n         1          1          2          1          1          1          3 \n2013-08-26 2013-08-30 2013-09-02 2014-04-21 2014-05-03 2014-06-13 2014-06-29 \n         2          1          1          1          1          1          1 \n2014-07-15 2014-07-16 2014-07-24 2014-07-25 2014-07-26 2014-07-27 2014-08-02 \n         1          3          1          1          1          1          1 \n2014-08-03 2014-08-06 2014-08-07 2014-08-09 2014-08-10 2014-08-13 2014-08-14 \n         2          2          1          1          1          1          2 \n2014-08-15 2014-08-16 2014-08-24 2014-08-30 2014-09-06 2014-09-11 2014-09-16 \n         1          1          1          1          1          1          1 \n2015-05-12 2015-06-19 2015-06-22 2015-07-05 2015-07-09 2015-07-16 2015-07-17 \n         1          1          1          1          1          2          1 \n2015-07-23 2015-07-24 2015-07-27 2015-07-28 2015-07-29 2015-07-30 2015-07-31 \n         1          2          1          1          3          1          1 \n2015-08-03 2015-08-04 2015-08-06 2015-08-07 2015-08-09 2015-08-14 2015-08-15 \n         1          1          1          1          1          1          1 \n2015-08-16 2015-08-19 2015-08-20 2015-08-21 2015-08-23 2015-08-30 2015-09-02 \n         1          1          2          1          1          1          1 \n2015-09-03 2015-09-06 2016-06-02 2016-07-07 2016-07-10 2016-07-13 2016-07-14 \n         1          1          1          1          1          2          2 \n2016-07-15 2016-07-17 2016-07-20 2016-07-27 2016-07-28 2016-07-30 2016-07-31 \n         3          1          2          1          2          1          1 \n2016-08-02 2016-08-03 2016-08-04 2016-08-06 2016-08-11 2016-08-12 2016-08-14 \n         1          2          1          3          1          1          1 \n2016-08-15 2016-08-18 2016-08-24 2016-08-25 2016-08-26 2016-08-28 2016-09-02 \n         1          3          2          1          1          1          1 \n2016-09-03 2017-05-17 2017-05-21 2017-06-07 2017-06-24 2017-07-12 2017-07-17 \n         1          1          1          1          1          2          1 \n2017-07-21 2017-07-22 2017-07-23 2017-07-24 2017-07-28 2017-07-29 2017-07-30 \n         1          1          1          1          1          1          1 \n2017-07-31 2017-08-03 2017-08-04 2017-08-05 2017-08-10 2017-08-12 2017-08-13 \n         1          2          1          1          2          1          3 \n2017-08-18 2017-08-19 2017-08-21 2017-08-23 2017-08-24 2017-08-26 2017-08-30 \n         1          1          1          1          1          2          1 \n2017-09-11 2018-06-21 2018-06-23 2018-06-25 2018-07-05 2018-07-13 2018-07-16 \n         2          1          1          1          1          1          1 \n2018-07-21 2018-07-22 2018-07-23 2018-07-25 2018-07-26 2018-07-28 2018-07-29 \n         1          1          1          2          1          1          1 \n2018-07-30 2018-08-01 2018-08-02 2018-08-04 2018-08-08 2018-08-12 2018-08-15 \n         1          1          2          2          1          1          1 \n2018-08-23 2018-08-26 2018-08-29 2018-09-03 2018-09-06 2018-09-10 2019-05-20 \n         1          1          3          1          1          1          1 \n2019-06-12 2019-07-03 2019-07-18 2019-07-21 2019-07-24 2019-07-25 2019-07-26 \n         2          1          1          1          1          1          1 \n2019-07-30 2019-07-31 2019-08-01 2019-08-08 2019-08-09 2019-08-10 2019-08-12 \n         1          1          1          1          2          1          1 \n2019-08-14 2019-08-17 2019-08-18 2019-08-19 2019-08-21 2019-08-24 2019-08-26 \n         2          1          1          2          2          2          1 \n2019-08-27 2019-08-31 2019-09-06 2019-09-08 2019-09-16 2020-06-21 2020-07-02 \n         1          1          1          1          1          1          1 \n2020-07-15 2020-07-23 2020-07-26 2020-08-03 2020-08-05 2020-08-09 2020-08-11 \n         1          3          1          1          1          1          2 \n2020-08-12 2020-08-13 2020-08-14 2020-08-16 2020-08-19 2020-08-20 2020-08-22 \n         2          3          1          1          2          1          1 \n2020-08-26 2020-08-27 2020-08-28 2020-08-30 2020-09-04 2020-09-09 2020-09-12 \n         2          2          1          1          1          1          2 \n2020-09-17 2021-05-12 2021-06-03 2021-06-04 2021-06-06 2021-06-08 2021-07-04 \n         1          1          1          1          1          1          1 \n2021-07-11 2021-07-14 2021-07-24 2021-07-29 2021-08-01 2021-08-02 2021-08-03 \n         1          1          1          1          1          1          1 \n2021-08-04 2021-08-05 2021-08-11 2021-08-12 2021-08-13 2021-08-15 2021-08-18 \n         1          1          2          1          2          1          2 \n2021-08-19 2021-08-20 2021-08-21 2021-08-22 2021-08-26 2021-08-29 2021-09-01 \n         1          2          1          1          1          1          1 \n2021-09-10 2021-09-18 2021-09-23 2022-05-15 2022-05-27 2022-06-15 2022-06-29 \n         1          1          2          1          1          1          1 \n2022-07-01 2022-07-09 2022-07-18 2022-07-19 2022-07-21 2022-07-22 2022-07-25 \n         1          1          1          1          1          1          1 \n2022-07-26 2022-07-27 2022-07-31 2022-08-01 2022-08-02 2022-08-05 2022-08-06 \n         1          1          2          1          1          1          2 \n2022-08-09 2022-08-10 2022-08-11 2022-08-12 2022-08-17 2022-08-18 2022-08-19 \n         1          1          1          1          1          1          1 \n2022-08-24 2022-08-25 2022-08-28 2022-09-01 2022-09-02 2022-09-13 2023-05-28 \n         1          1          2          1          1          1          1 \n2023-06-14 2023-07-15 2023-07-26 2023-07-28 2023-07-29 2023-07-30 2023-08-02 \n         1          1          2          1          1          1          1 \n2023-08-03 2023-08-04 2023-08-09 2023-08-16 2023-08-17 2023-08-19 2023-08-20 \n         1          2          2          2          1          4          1 \n2023-08-24 2023-08-25 2023-08-26 2023-08-27 2023-08-28 2023-08-31 2023-09-02 \n         1          1          1          1          1          2          2 \n2023-09-09 2023-09-11 2023-09-22 \n         2          1          2 \n\n$first_nest\n\n           2000-05-05 2000-05-09 2000-05-10 2000-05-11 2000-05-13 2000-05-16 \n       189          1          2          3          1          3          1 \n2000-05-18 2000-05-19 2000-05-26 2000-05-28 2000-05-29 2000-05-31 2000-06-01 \n         2          1          1          2          1          1          1 \n2000-06-06 2000-06-08 2000-06-17 2004-05-05 2004-05-08 2004-05-09 2004-05-11 \n         1          1          1          1          1          1          1 \n2004-05-13 2004-05-14 2004-05-15 2004-05-16 2004-05-19 2004-05-20 2004-05-21 \n         2          1          1          2          2          1          1 \n2004-05-22 2004-05-26 2004-05-27 2004-05-28 2004-06-03 2004-06-06 2004-06-14 \n         1          1          1          1          1          1          1 \n2004-06-17 2005-05-04 2005-05-06 2005-05-07 2005-05-08 2005-05-11 2005-05-12 \n         1          1          3          1          1          1          3 \n2005-05-13 2005-05-14 2005-05-17 2005-05-18 2005-05-19 2005-05-20 2005-05-21 \n         1          2          1          1          1          1          1 \n2005-05-26 2005-05-28 2005-06-03 2005-06-10 2005-06-15 2005-06-16 2006-05-10 \n         2          1          1          1          1          1          1 \n2006-05-12 2006-05-14 2006-05-15 2006-05-17 2006-05-18 2006-05-20 2006-05-22 \n         3          1          1          2          1          3          3 \n2006-05-23 2006-05-25 2006-05-27 2006-06-01 2006-06-09 2006-06-10 2006-06-21 \n         1          1          2          1          1          1          1 \n2007-05-12 2007-05-14 2007-05-16 2007-05-17 2007-05-18 2007-05-19 2007-05-22 \n         1          1          3          4          1          2          1 \n2007-05-23 2007-05-25 2007-05-26 2007-05-31 2007-06-02 2007-06-04 2007-06-05 \n         1          1          2          1          1          1          1 \n2007-06-06 2007-06-08 2007-06-17 2007-06-21 2007-06-27 2008-05-06 2008-05-07 \n         1          2          1          1          1          1          1 \n2008-05-09 2008-05-10 2008-05-12 2008-05-15 2008-05-16 2008-05-17 2008-05-18 \n         1          1          2          2          3          2          3 \n2008-05-21 2008-05-23 2008-05-26 2008-05-28 2008-05-31 2008-06-05 2008-06-11 \n         2          1          1          1          1          3          1 \n2008-06-19 2009-05-03 2009-05-04 2009-05-06 2009-05-07 2009-05-09 2009-05-10 \n         1          1          1          1          1          2          1 \n2009-05-12 2009-05-13 2009-05-14 2009-05-18 2009-05-20 2009-05-21 2009-05-24 \n         2          2          4          1          2          1          2 \n2009-06-03 2009-06-06 2009-06-08 2009-06-14 2009-07-29 2010-05-01 2010-05-05 \n         1          2          1          1          1          1          2 \n2010-05-06 2010-05-07 2010-05-08 2010-05-09 2010-05-13 2010-05-14 2010-05-16 \n         3          1          1          1          3          4          2 \n2010-05-21 2010-05-22 2010-05-25 2010-05-27 2010-05-28 2010-06-04 2010-06-05 \n         1          2          1          2          1          2          1 \n2010-06-06 2010-06-09 2010-06-12 2010-06-17 2010-06-18 2011-04-30 2011-05-02 \n         1          1          1          1          1          1          1 \n2011-05-04 2011-05-06 2011-05-08 2011-05-09 2011-05-12 2011-05-14 2011-05-15 \n         1          2          1          2          2          2          3 \n2011-05-17 2011-05-18 2011-05-20 2011-05-21 2011-05-28 2011-05-29 2011-06-02 \n         1          2          1          1          2          3          1 \n2011-06-07 2011-06-08 2011-06-11 2012-04-30 2012-05-02 2012-05-04 2012-05-07 \n         1          1          1          1          1          3          1 \n2012-05-08 2012-05-09 2012-05-10 2012-05-11 2012-05-12 2012-05-13 2012-05-14 \n         1          1          4          4          1          1          2 \n2012-05-15 2012-05-16 2012-05-20 2012-05-24 2012-05-26 2012-05-29 2012-05-30 \n         1          1          2          1          1          1          1 \n2012-06-01 2012-06-11 2013-05-02 2013-05-03 2013-05-06 2013-05-09 2013-05-10 \n         1          1          1          2          1          1          2 \n2013-05-11 2013-05-12 2013-05-13 2013-05-14 2013-05-15 2013-05-16 2013-05-17 \n         2          1          1          2          1          3          1 \n2013-05-26 2013-06-01 2013-06-02 2013-06-06 2013-06-07 2013-06-29 2013-08-15 \n         3          2          1          1          2          1          1 \n2014-04-29 2014-05-01 2014-05-03 2014-05-08 2014-05-09 2014-05-10 2014-05-11 \n         1          1          1          3          4          2          1 \n2014-05-12 2014-05-14 2014-05-16 2014-05-17 2014-05-21 2014-05-22 2014-05-24 \n         2          2          2          1          1          2          2 \n2014-05-28 2014-05-29 2014-05-31 2014-06-01 2014-07-05 2015-04-27 2015-05-03 \n         1          2          1          1          1          1          1 \n2015-05-06 2015-05-07 2015-05-08 2015-05-09 2015-05-11 2015-05-12 2015-05-14 \n         3          2          1          3          4          1          2 \n2015-05-15 2015-05-16 2015-05-19 2015-05-20 2015-05-21 2015-05-24 2015-05-28 \n         2          1          1          2          1          1          1 \n2015-05-30 2015-06-05 2015-06-13 2015-07-11 2016-04-27 2016-04-29 2016-05-01 \n         1          1          1          1          1          1          1 \n2016-05-04 2016-05-05 2016-05-06 2016-05-08 2016-05-11 2016-05-12 2016-05-13 \n         1          2          1          1          2          3          2 \n2016-05-14 2016-05-16 2016-05-18 2016-05-19 2016-05-22 2016-05-25 2016-05-26 \n         2          1          1          1          1          3          1 \n2016-05-28 2016-05-29 2016-06-02 2016-06-04 2016-06-16 2016-06-18 2017-04-28 \n         1          1          1          1          1          2          1 \n2017-04-30 2017-05-03 2017-05-04 2017-05-06 2017-05-07 2017-05-08 2017-05-10 \n         2          1          1          1          2          1          2 \n2017-05-11 2017-05-12 2017-05-13 2017-05-19 2017-05-21 2017-05-24 2017-05-26 \n         2          3          1          3          1          1          1 \n2017-05-28 2017-05-31 2017-06-02 2017-06-03 2017-06-04 2017-06-14 2018-05-03 \n         1          1          2          1          1          1          1 \n2018-05-04 2018-05-06 2018-05-10 2018-05-11 2018-05-12 2018-05-14 2018-05-16 \n         1          2          2          2          2          2          3 \n2018-05-17 2018-05-18 2018-05-19 2018-05-20 2018-05-23 2018-05-29 2018-05-30 \n         2          1          1          2          1          1          2 \n2018-05-31 2018-06-01 2018-06-02 2018-06-15 2019-05-03 2019-05-04 2019-05-05 \n         3          1          1          1          1          1          1 \n2019-05-09 2019-05-10 2019-05-11 2019-05-14 2019-05-16 2019-05-17 2019-05-22 \n         4          3          5          1          1          1          1 \n2019-05-23 2019-05-24 2019-05-25 2019-06-01 2019-06-02 2019-06-05 2019-06-06 \n         1          1          2          1          1          1          2 \n2019-06-12 2019-07-06 2020-05-07 2020-05-10 2020-05-11 2020-05-13 2020-05-14 \n         1          1          1          1          1          2          2 \n2020-05-15 2020-05-16 2020-05-17 2020-05-19 2020-05-20 2020-05-21 2020-05-22 \n         3          1          1          1          1          2          3 \n2020-05-23 2020-05-24 2020-05-28 2020-05-29 2020-05-30 2020-06-03 2020-06-11 \n         1          1          1          2          2          1          1 \n2020-06-12 2020-06-27 2020-07-03 2021-05-05 2021-05-06 2021-05-08 2021-05-09 \n         1          1          1          2          1          1          2 \n2021-05-10 2021-05-12 2021-05-13 2021-05-14 2021-05-17 2021-05-19 2021-05-20 \n         1          2          1          3          1          4          2 \n2021-05-21 2021-05-25 2021-05-27 2021-05-28 2021-05-30 2021-06-02 2021-06-04 \n         1          1          1          1          1          2          2 \n2021-06-17 2022-05-06 2022-05-07 2022-05-08 2022-05-09 2022-05-11 2022-05-14 \n         1          1          1          3          1          1          1 \n2022-05-15 2022-05-16 2022-05-17 2022-05-18 2022-05-19 2022-05-20 2022-05-21 \n         1          1          1          1          2          2          3 \n2022-05-25 2022-05-26 2022-05-27 2022-05-29 2022-06-03 2022-06-08 2022-06-11 \n         1          1          1          2          2          1          1 \n2022-06-25 2022-06-27 2022-07-01 2023-05-07 2023-05-10 2023-05-11 2023-05-12 \n         1          1          1          2          1          1          2 \n2023-05-13 2023-05-14 2023-05-16 2023-05-19 2023-05-21 2023-05-24 2023-05-26 \n         1          3          2          1          1          2          1 \n2023-05-28 2023-05-30 2023-06-01 2023-06-02 2023-06-03 2023-06-07 2023-06-09 \n         1          1          2          2          1          2          1 \n2023-06-10 2023-06-17 2023-07-23 \n         1          1          1 \n\n$first_chick\n\n           2000-05-28 2000-05-31 2000-06-01 2000-06-02 2000-06-03 2000-06-04 \n       262          1          1          1          2          3          1 \n2000-06-06 2000-06-07 2000-06-08 2000-06-09 2000-06-18 2000-06-20 2000-06-22 \n         1          1          1          1          1          2          1 \n2000-06-24 2000-06-26 2000-06-28 2000-07-22 2004-05-31 2004-06-04 2004-06-05 \n         1          1          1          1          2          2          1 \n2004-06-06 2004-06-09 2004-06-11 2004-06-14 2004-06-16 2004-06-18 2004-06-20 \n         1          1          2          1          1          1          1 \n2004-06-21 2004-06-22 2004-06-27 2004-07-04 2004-07-18 2005-05-27 2005-05-28 \n         1          2          1          1          1          1          1 \n2005-06-01 2005-06-02 2005-06-03 2005-06-05 2005-06-06 2005-06-09 2005-06-10 \n         2          3          1          1          1          1          4 \n2005-06-11 2005-06-12 2005-06-15 2005-06-26 2005-07-01 2005-07-21 2006-06-02 \n         1          1          1          2          1          1          1 \n2006-06-03 2006-06-05 2006-06-07 2006-06-09 2006-06-10 2006-06-11 2006-06-13 \n         1          1          1          1          1          1          1 \n2006-06-14 2006-06-16 2006-06-17 2006-06-20 2006-06-24 2006-06-30 2006-07-03 \n         2          2          2          1          2          1          1 \n2006-07-14 2007-06-06 2007-06-07 2007-06-09 2007-06-10 2007-06-11 2007-06-12 \n         1          2          2          1          3          1          2 \n2007-06-13 2007-06-14 2007-06-15 2007-06-20 2007-06-27 2007-06-28 2007-06-29 \n         1          2          3          3          1          1          1 \n2007-07-02 2007-07-15 2007-07-19 2008-05-30 2008-06-02 2008-06-04 2008-06-05 \n         1          1          1          1          1          2          4 \n2008-06-07 2008-06-08 2008-06-09 2008-06-12 2008-06-17 2008-06-18 2008-06-25 \n         1          2          1          2          1          1          1 \n2008-06-26 2008-06-27 2008-06-29 2008-07-06 2008-07-10 2009-05-27 2009-05-28 \n         2          1          1          1          1          1          2 \n2009-05-29 2009-05-30 2009-05-31 2009-06-03 2009-06-04 2009-06-05 2009-06-06 \n         1          1          2          1          4          1          3 \n2009-06-10 2009-06-11 2009-06-17 2009-06-20 2009-06-24 2009-06-25 2009-06-26 \n         1          1          1          1          1          1          1 \n2009-06-29 2009-07-15 2010-05-25 2010-05-26 2010-05-27 2010-05-30 2010-06-01 \n         1          1          1          2          3          3          1 \n2010-06-03 2010-06-04 2010-06-09 2010-06-10 2010-06-11 2010-06-16 2010-06-17 \n         3          1          1          2          1          2          1 \n2010-06-18 2010-06-25 2010-06-30 2010-07-01 2010-07-02 2010-07-07 2010-07-09 \n         1          1          2          1          1          1          1 \n2011-05-25 2011-05-26 2011-05-27 2011-05-29 2011-05-31 2011-06-02 2011-06-03 \n         1          1          1          1          1          2          1 \n2011-06-04 2011-06-05 2011-06-06 2011-06-08 2011-06-09 2011-06-10 2011-06-11 \n         1          1          1          2          1          1          1 \n2011-06-12 2011-06-14 2011-06-16 2011-06-17 2011-06-18 2011-06-30 2011-07-01 \n         1          1          1          1          1          1          1 \n2011-07-10 2011-07-13 2011-07-21 2012-05-20 2012-05-23 2012-05-24 2012-05-26 \n         1          1          1          1          1          1          1 \n2012-05-27 2012-05-28 2012-05-31 2012-06-01 2012-06-02 2012-06-04 2012-06-06 \n         1          2          1          2          1          1          3 \n2012-06-08 2012-06-10 2012-06-15 2012-06-20 2012-06-23 2012-06-24 2012-07-01 \n         1          1          2          1          3          1          1 \n2013-05-11 2013-05-26 2013-05-27 2013-05-29 2013-05-31 2013-06-01 2013-06-03 \n         1          3          1          1          1          1          1 \n2013-06-06 2013-06-07 2013-06-08 2013-06-09 2013-06-12 2013-06-13 2013-06-19 \n         2          3          1          2          1          1          1 \n2013-06-21 2013-06-22 2013-06-23 2013-06-26 2013-06-29 2014-05-22 2014-05-24 \n         1          1          1          1          1          1          1 \n2014-05-25 2014-05-29 2014-05-31 2014-06-01 2014-06-02 2014-06-03 2014-06-05 \n         1          2          2          2          1          1          3 \n2014-06-06 2014-06-09 2014-06-11 2014-06-13 2014-06-14 2014-06-15 2014-06-19 \n         2          1          1          1          2          1          1 \n2014-06-20 2014-06-21 2014-06-29 2014-07-02 2014-07-26 2014-08-01 2015-05-22 \n         1          1          1          1          1          1          1 \n2015-05-25 2015-05-26 2015-05-28 2015-05-29 2015-05-30 2015-05-31 2015-06-04 \n         1          1          2          1          1          2          6 \n2015-06-05 2015-06-06 2015-06-08 2015-06-10 2015-06-11 2015-06-12 2015-06-19 \n         1          1          1          1          1          1          1 \n2015-06-26 2015-06-27 2015-08-02 2016-05-22 2016-05-25 2016-05-26 2016-05-27 \n         1          1          1          1          1          2          1 \n2016-05-28 2016-05-29 2016-06-02 2016-06-04 2016-06-05 2016-06-06 2016-06-08 \n         2          3          1          1          1          1          5 \n2016-06-09 2016-06-10 2016-06-12 2016-06-16 2016-06-17 2016-06-18 2016-07-13 \n         1          1          2          2          1          1          1 \n2016-07-14 2016-07-15 2016-07-16 2017-05-22 2017-05-25 2017-05-26 2017-05-28 \n         1          1          1          1          3          1          3 \n2017-05-30 2017-06-01 2017-06-02 2017-06-03 2017-06-08 2017-06-11 2017-06-12 \n         1          3          3          1          1          1          1 \n2017-06-14 2017-06-16 2017-06-17 2017-06-21 2017-06-22 2017-06-24 2018-05-27 \n         2          1          1          2          1          1          1 \n2018-05-30 2018-05-31 2018-06-02 2018-06-03 2018-06-04 2018-06-05 2018-06-07 \n         1          3          1          1          1          1          4 \n2018-06-08 2018-06-10 2018-06-13 2018-06-14 2018-06-18 2018-06-20 2018-06-23 \n         2          2          1          1          1          1          1 \n2018-06-25 2018-06-28 2018-07-13 2019-05-30 2019-05-31 2019-06-02 2019-06-03 \n         1          1          1          2          2          1          2 \n2019-06-04 2019-06-05 2019-06-06 2019-06-07 2019-06-09 2019-06-16 2019-06-18 \n         1          1          1          1          1          3          1 \n2019-06-20 2019-06-23 2019-06-26 2019-06-27 2019-06-30 2019-07-02 2019-07-03 \n         1          1          2          2          1          1          2 \n2019-07-04 2020-05-31 2020-06-01 2020-06-03 2020-06-04 2020-06-05 2020-06-07 \n         1          1          1          1          3          1          1 \n2020-06-12 2020-06-13 2020-06-14 2020-06-19 2020-06-20 2020-06-21 2020-06-23 \n         2          1          2          2          1          2          1 \n2020-06-27 2020-07-02 2020-07-03 2020-07-05 2020-07-09 2020-07-16 2020-07-20 \n         1          1          2          2          1          1          1 \n2020-07-30 2021-05-28 2021-05-30 2021-05-31 2021-06-03 2021-06-04 2021-06-05 \n         1          1          1          2          3          1          1 \n2021-06-06 2021-06-07 2021-06-11 2021-06-12 2021-06-14 2021-06-16 2021-06-18 \n         1          1          2          2          1          1          1 \n2021-06-27 2021-06-30 2021-07-01 2021-07-02 2021-07-15 2022-05-27 2022-06-02 \n         2          1          2          1          1          1          5 \n2022-06-03 2022-06-04 2022-06-06 2022-06-08 2022-06-11 2022-06-12 2022-06-13 \n         1          1          1          1          1          2          1 \n2022-06-15 2022-06-16 2022-06-17 2022-06-19 2022-06-20 2022-06-22 2022-06-23 \n         1          1          3          1          1          1          2 \n2022-06-25 2023-05-28 2023-05-31 2023-06-01 2023-06-04 2023-06-05 2023-06-09 \n         1          1          1          1          1          4          1 \n2023-06-10 2023-06-14 2023-06-17 2023-06-18 2023-06-19 2023-06-23 2023-06-25 \n         2          2          1          1          1          1          1 \n2023-06-29 2023-07-05 2023-07-09 2023-07-27 2023-08-02 2023-08-04 3000-06-02 \n         1          1          1          1          1          1          1 \n\n$first_fledge\n\n           2000-06-20 2000-06-21 2000-06-22 2000-06-24 2000-06-25 2000-06-26 \n       387          1          1          2          3          2          2 \n2000-06-30 2000-07-04 2000-07-08 2000-07-13 2000-07-15 2000-07-16 2000-07-17 \n         2          1          1          1          1          1          1 \n2000-07-20 2000-07-24 2000-08-06 2004-06-23 2004-06-25 2004-06-26 2004-06-30 \n         1          1          1          1          1          1          1 \n2004-07-02 2004-07-04 2004-07-10 2004-07-13 2004-07-14 2004-07-17 2004-07-18 \n         1          2          1          1          2          1          1 \n2004-07-25 2005-06-22 2005-06-23 2005-06-24 2005-06-25 2005-06-30 2005-07-01 \n         1          3          1          2          3          1          1 \n2005-07-02 2005-07-07 2005-07-08 2005-07-13 2005-07-18 2005-07-20 2005-07-22 \n         1          1          1          1          1          1          1 \n2005-08-27 2006-06-23 2006-06-25 2006-06-29 2006-06-30 2006-07-01 2006-07-04 \n         1          1          1          1          1          1          2 \n2006-07-07 2006-07-12 2006-07-13 2006-07-15 2006-07-17 2006-07-24 2006-07-28 \n         3          1          1          2          1          1          1 \n2006-08-02 2006-08-11 2007-06-26 2007-06-27 2007-06-28 2007-06-29 2007-06-30 \n         1          1          1          1          2          1          1 \n2007-07-02 2007-07-04 2007-07-05 2007-07-06 2007-07-10 2007-07-11 2007-07-12 \n         2          3          1          1          1          2          2 \n2007-07-13 2007-07-18 2007-07-23 2007-08-08 2008-06-20 2008-06-22 2008-06-25 \n         2          2          1          2          1          1          2 \n2008-06-26 2008-06-27 2008-06-28 2008-06-29 2008-07-02 2008-07-15 2008-07-17 \n         3          1          2          1          1          1          1 \n2008-07-18 2008-07-20 2008-07-24 2008-07-30 2008-07-31 2008-08-17 2009-06-16 \n         1          2          1          1          1          1          1 \n2009-06-20 2009-06-21 2009-06-24 2009-06-25 2009-06-26 2009-06-29 2009-06-30 \n         2          1          3          3          1          1          1 \n2009-07-01 2009-07-03 2009-07-05 2009-07-10 2009-07-15 2009-07-16 2009-07-20 \n         1          1          1          1          2          1          1 \n2009-08-07 2010-06-09 2010-06-16 2010-06-17 2010-06-18 2010-06-20 2010-06-22 \n         1          1          1          2          3          2          1 \n2010-06-24 2010-06-25 2010-06-27 2010-06-30 2010-07-01 2010-07-09 2010-07-14 \n         1          2          1          1          1          2          2 \n2010-07-21 2010-07-23 2010-07-27 2010-07-29 2010-07-30 2010-08-05 2011-06-14 \n         1          1          1          1          1          1          1 \n2011-06-17 2011-06-19 2011-06-22 2011-06-23 2011-06-25 2011-06-27 2011-06-28 \n         1          1          1          4          2          1          1 \n2011-07-01 2011-07-03 2011-07-06 2011-07-09 2011-07-11 2011-07-21 2011-07-22 \n         3          1          1          1          1          1          1 \n2011-07-24 2012-06-15 2012-06-17 2012-06-19 2012-06-21 2012-06-22 2012-06-23 \n         1          1          2          1          1          1          1 \n2012-06-24 2012-06-26 2012-06-27 2012-06-28 2012-06-29 2012-07-01 2012-07-03 \n         1          1          1          1          1          1          1 \n2012-07-07 2012-07-14 2012-07-15 2012-07-19 2012-07-21 2012-08-17 2013-06-20 \n         1          1          1          1          1          1          1 \n2013-06-21 2013-06-22 2013-06-23 2013-06-26 2013-06-27 2013-06-29 2013-06-30 \n         1          2          1          1          2          2          1 \n2013-07-02 2013-07-04 2013-07-05 2013-07-10 2013-07-11 2013-07-17 2013-07-20 \n         1          1          1          2          1          1          1 \n2013-07-31 2013-08-02 2014-06-15 2014-06-16 2014-06-18 2014-06-19 2014-06-21 \n         1          1          1          2          1          3          1 \n2014-06-22 2014-06-23 2014-06-27 2014-06-28 2014-07-03 2014-07-04 2014-07-05 \n         2          1          3          1          1          2          2 \n2014-07-09 2014-07-12 2014-07-16 2014-08-09 2014-08-15 2015-06-13 2015-06-14 \n         1          1          1          1          1          1          3 \n2015-06-17 2015-06-18 2015-06-20 2015-06-22 2015-06-25 2015-06-26 2015-06-27 \n         1          1          1          1          2          3          1 \n2015-06-30 2015-07-02 2015-07-03 2015-07-04 2015-07-17 2015-08-19 2016-06-09 \n         1          1          2          1          1          1          1 \n2016-06-14 2016-06-15 2016-06-17 2016-06-19 2016-06-23 2016-06-24 2016-06-27 \n         1          1          1          3          4          1          1 \n2016-06-29 2016-07-02 2016-07-03 2016-07-06 2016-07-07 2016-07-08 2016-07-09 \n         2          3          3          2          2          2          1 \n2016-08-05 2017-06-11 2017-06-15 2017-06-17 2017-06-18 2017-06-20 2017-06-21 \n         1          2          1          1          1          1          1 \n2017-06-22 2017-06-23 2017-06-24 2017-06-27 2017-06-29 2017-07-02 2017-07-05 \n         3          1          1          2          1          1          1 \n2017-07-06 2017-07-07 2017-07-11 2017-07-12 2017-07-14 2017-07-25 2017-08-05 \n         1          1          1          1          1          1          1 \n2018-06-21 2018-06-22 2018-06-23 2018-06-24 2018-06-26 2018-06-27 2018-06-28 \n         2          1          2          2          1          2          3 \n2018-06-29 2018-07-04 2018-07-05 2018-07-06 2018-07-12 2018-07-14 2018-07-19 \n         1          2          1          1          1          1          1 \n2018-08-03 2019-06-19 2019-06-20 2019-06-21 2019-06-23 2019-06-26 2019-06-27 \n         1          1          1          1          1          1          2 \n2019-06-28 2019-06-29 2019-06-30 2019-07-01 2019-07-03 2019-07-04 2019-07-07 \n         1          1          1          1          1          1          1 \n2019-07-08 2019-07-10 2019-07-12 2019-07-13 2019-07-18 2019-07-19 2019-07-20 \n         1          1          2          1          1          1          1 \n2019-07-21 2019-08-01 2020-06-20 2020-06-24 2020-06-25 2020-07-02 2020-07-03 \n         3          1          1          2          3          1          1 \n2020-07-04 2020-07-08 2020-07-09 2020-07-13 2020-07-14 2020-07-15 2020-07-17 \n         1          1          3          1          1          2          2 \n2020-07-24 2020-07-26 2020-08-06 2020-08-13 2020-08-23 2022-06-25 \n         1          2          1          1          1          1 \n\n\nWe use map rather than one of the other map functions for this step because the table for each column will have a different length.\nThe purrr documentation provides more details about how to use the many functions in the package.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregation & Grouping</span>"
    ]
  },
  {
    "objectID": "chapters/week04/aggregation-grouping.html#the-split-apply-pattern",
    "href": "chapters/week04/aggregation-grouping.html#the-split-apply-pattern",
    "title": "17  Aggregation & Grouping",
    "section": "17.3 The Split-Apply Pattern",
    "text": "17.3 The Split-Apply Pattern\nIn a data set with categorical features, it’s often useful to compute something for each category. The map functions can compute something for each element of a data structure, but categories are not necessarily elements.\nFor example, the least terns data set has 6 different categories in the region_3 column. If we want all of the rows for one region, one way to get them is by filtering:\n\nlibrary(\"dplyr\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsouthern = filter(terns, region_3 == \"SOUTHERN\")\nhead(southern)\n\n  year                             site_name\n1 2000 SANTA CLARA RIVER MCGRATH STATE BEACH\n2 2000                          ORMOND BEACH\n3 2000                       NBVC POINT MUGU\n4 2000                          VENICE BEACH\n5 2000                             LA HARBOR\n6 2000            SEAL BEACH NWR ANAHEIM BAY\n                  site_name_2013_2018  site_name_1988_2001    site_abbr\n1                   Santa Clara River NA_2013_2018 POLYGON   S_CLAR_MCG\n2                        Ormond Beach NA_2013_2018 POLYGON       ORMOND\n3                     NBVC Point Mugu NA_2013_2018 POLYGON      PT_MUGU\n4                        Venice Beach NA_2013_2018 POLYGON      VEN_BCH\n5                          Port of LA NA_2013_2018 POLYGON      LA_HARB\n6 Seal Beach National Wildlife Refuge NA_2013_2018 POLYGON SEAL_BCH_NWR\n  region_3 region_4   event bp_min bp_max fl_min fl_max total_nests\n1 SOUTHERN  VENTURA LA_NINA     21     21      9      9          22\n2 SOUTHERN  VENTURA LA_NINA     73     73     60     65          73\n3 SOUTHERN  VENTURA LA_NINA    166    167     64     64         252\n4 SOUTHERN SOUTHERN LA_NINA    274    294    150    200         308\n5 SOUTHERN SOUTHERN LA_NINA    437    437    570    570         565\n6 SOUTHERN SOUTHERN LA_NINA    107    107    180    180         107\n  nonpred_eggs nonpred_chicks nonpred_fl nonpred_ad pred_control pred_eggs\n1            4              3         NA         NA                     NA\n2            2              0          0          0                     NA\n3           NA             NA         NA         NA                     NA\n4           26             NA         NA         NA                     32\n5           77             NA         NA         NA                     24\n6           10              3         NA         NA                     NA\n  pred_chicks pred_fl pred_ad pred_pefa pred_coy_fox pred_meso pred_owlspp\n1          NA      NA      NA                                             \n2          NA      NA      NA         N            N         Y           N\n3          NA      NA      NA                                             \n4          20      20       3         Y            N         Y           N\n5          NA      15      NA         Y            N         N           N\n6          NA      NA      NA                                             \n  pred_corvid pred_other_raptor pred_other_avian pred_misc total_pefa\n1                                                                  NA\n2           N                 Y                N         N         NA\n3                                                                  NA\n4           Y                 N                N         N          2\n5           N                 Y                Y         N         17\n6                                                                  NA\n  total_coy_fox total_meso total_owlspp total_corvid total_other_raptor\n1            NA         NA           NA           NA                 NA\n2            NA         NA           NA           NA                 NA\n3            NA         NA           NA           NA                 NA\n4             0         42            0           31                  0\n5             0          0            0            0                  4\n6            NA         NA           NA           NA                 NA\n  total_other_avian total_misc first_observed last_observed first_nest\n1                NA         NA     2000-06-06    2000-09-05 2000-06-06\n2                NA         NA                              2000-06-08\n3                NA         NA     2000-05-21    2000-08-12 2000-06-01\n4                 0          0     2000-04-19    2000-08-20 2000-05-29\n5                24          0     2000-04-28    2000-08-20 2000-05-10\n6                NA         NA                                        \n  first_chick first_fledge\n1  2000-06-28   2000-07-24\n2  2000-06-26   2000-07-17\n3  2000-06-24   2000-07-16\n4                         \n5  3000-06-02   2000-06-22\n6                         \n\n\nTo get all 6 regions separately, we’d have to do this 6 times. If we want to compute something for each region, say the median of the total_nests column, we also have to repeat that computation 6 times. Here’s what it would look like for just the SOUTHERN region:\n\nmedian(southern$total_nests, na.rm = TRUE)\n\n[1] 64\n\n\nIf the categories were elements, we could avoid writing code to index each category, and just use a map function to apply the median function to each.\nThe split function splits a vector or data frame into groups based on a vector of categories. The first argument to split is the data, and the second argument is a congruent vector of categories.\nWe can use split to elegantly compute medians of total_nests broken down by country. First, we split the data by region Since we only want to compute on the total_nests column, we only split that column:\n\nby_region = split(terns$total_nests, terns$region_3)\nclass(by_region)\n\n[1] \"list\"\n\nnames(by_region)\n\n[1] \"ARIZONA\"    \"CENTRAL\"    \"KINGS\"      \"S.F._BAY\"   \"SACRAMENTO\"\n[6] \"SOUTHERN\"  \n\n\nThe result from split is a list with one element for each category. The individual elements contain pieces of the original total_nests column:\n\nhead(by_region$SOUTHERN)\n\n[1]  22  73 252 308 565 107\n\n\nSince the categories are elements in the split data, now we can use map_dbl the same way we did in previous examples:\n\nmap_dbl(by_region, median, na.rm = TRUE)\n\n   ARIZONA    CENTRAL      KINGS   S.F._BAY SACRAMENTO   SOUTHERN \n         3         17          1         35          1         64 \n\n\nThis two-step process is an R idiom called the split-apply pattern. First you use split to convert categories into list elements, then you use a map (or apply) function to compute something on each category. Any time you want to compute results by category, you should think of this pattern.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregation & Grouping</span>"
    ]
  },
  {
    "objectID": "chapters/week04/aggregation-grouping.html#grouping",
    "href": "chapters/week04/aggregation-grouping.html#grouping",
    "title": "17  Aggregation & Grouping",
    "section": "17.4 Grouping",
    "text": "17.4 Grouping\nThe dplyr package’s group_by and summarize functions are analogous to split and purrr’s map functions, but return the results in a data frame with one row for each category. In many cases, this is more convenient than a list or vector result.\nAs an example, here’s how to use group_by and summarize to compute the total_nests medians:\n\nby_region = group_by(terns, region_3)\nsummarize(by_region, median(total_nests, na.rm = TRUE))\n\n# A tibble: 6 × 2\n  region_3   `median(total_nests, na.rm = TRUE)`\n  &lt;chr&gt;                                    &lt;dbl&gt;\n1 ARIZONA                                      3\n2 CENTRAL                                     17\n3 KINGS                                        1\n4 S.F._BAY                                    35\n5 SACRAMENTO                                   1\n6 SOUTHERN                                    64\n\n\nThe dplyr documentation provides more details about how to use the group_by and summarize functions.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aggregation & Grouping</span>"
    ]
  },
  {
    "objectID": "chapters/week05/data-visualization-principles.html",
    "href": "chapters/week05/data-visualization-principles.html",
    "title": "18  Data Visualization Principles & Perception",
    "section": "",
    "text": "18.1 A Brief History of Data Visualization\nData visualization is the graphical display of abstract information to help us make sense of phenomena and to communicate these findings. It is a powerful tool to help us uncover and share the stories of our data. Visualizations help us retain and analyze all the information in our data, uncover and share our insights, and describe our research in a useful way. If a picture is worth a thousand words, then a good data visualization is worth millions.\nBut how many of us have ever taken a course explicitly on data visualization? It’s typically not taught in standard data analysis courses, yet it is a mainstay for nearly every sector in today’s data-driven world. Today we’ll dive into the what, how, and why of data visualization and describe some best practices that you can immediately implement into your research workflows. Along the way we’ll also focus on building up our collective data literacy skills, and employ critical approaches to produce science that is more robust, transparent, and equitable.\nData visualization is not a modern invention. Quantitative information display has been traced back to prehistory with the locations of stars mapped on the Lasaux cave drawings. Clay tokens, quipu, and stick charts dating back as far as 5500 BC also illustrate our long history of creating shared representations of data. The oldest known data visualization dates to 1160 BC with the Turin Papyrus Map, which accurately illustrates the distribution of geological resources in a region in Egypt. These earliest forms of data visualization served purposes of navigating culture and living within society—from accounting to agriculture, transportation, religion, and medicine. They were used to help us explore and understand natural phenomena and the workings of the universe.\nThe French philosopher Rene Descartes is attributed as developing the precursor to today’s modern plot in the 17th century—a two-dimensional coordinate system for displaying values. Later in the 18th century William Playfair began creating left to right oriented plots, allowing the viewer to explore how values change over time. He’s also attributed to inventing the bar graph and, unfortunately, the pie chart (we’ll get to why that’s unfortunate, later). Into the 19th and 20th centuries we see an explosion of chart types\nUnsurprisingly, the invention of tools like paper and computers shaped our relationship with knowledge and information, playing a strong role in how we collect, analyze, store and visualize data. As we gather more and more complex data, we seek more ways to visualize its meaning and in the 19th and 20th centuries we see an explosion of chart types and techniques for communicating with statistical graphics.\nIn particular, check out the following famous data visualizations.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Principles & Perception</span>"
    ]
  },
  {
    "objectID": "chapters/week05/data-visualization-principles.html#a-brief-history-of-data-visualization",
    "href": "chapters/week05/data-visualization-principles.html#a-brief-history-of-data-visualization",
    "title": "18  Data Visualization Principles & Perception",
    "section": "",
    "text": "Figure 18.1: (source)\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSee also\n\n\n\nFor a more complete history of data visualization, check out A Brief History of Data Visualization by M. Friendly.\n\n\n\n\n18.1.1 Famous Data Visualizations\nFlorence Nightingale, the “mother of nursing,” produced in 1857 a rose diagram depicting seasonal sources of British soldier’s fatalities in the Crimean War. Out of the 18,000 soldiers who had died, 16,000 had died of disease in a hospital (blue shading) rather than from their wounds (black shading). This image is credited with helping to persuade the British government to improve conditions in military hospitals.\n\n\n\n\n\n\nFigure 18.2: The context surrounding this rose diagram is actually quite bit more complex. For additional background, check out this podcast based on the book The Data Detective by Tim Harford.\n\n\n\nJohn Snow, a London physician, traced the source of an 1854 cholera outbreak in Soho. By examining the locations of reported cholera deaths, Snow demonstrated that the disease was connected to a contaminated well on Broad Street, contributing to growing understanding that cholera was a waterborne disease and not caused by foul ‘miasmas’ in the air. He later used a map in his publication to show the concentration of the cholera cases around the contaminated pump. On this map, the height of the dark bars correspond to the number of deaths at a given location. While Snow didn’t invent the mapping technique of layering thematic data on top of topographic maps, nor actually compose the map himself (it was created by cartographer Charles Cheffins), this map was so effective that history often calls Snow the “father of epidemiology.” Learn more about the history of the map in this recent post by Kenneth Field.\n\n\n\n\n\n\nFigure 18.3\n\n\n\nLastly, no discussion of the history of data visualization is complete without a nod to Charles Minard’s acclaimed depiction of Napoleon Bonaparte’s ill-fated invasion of Russia. Edward Tufte declared in his popular 1983 book The Visual Display of Quantitative Information that Napoleon’s March “may well be the best statistical graphic ever produced.” The thick band denotes the size of the army at each position, beginning at the Polish-Russian border. The dark lower band is tied to temperature and time scales, and shows the path of Napoleon’s retreat from Moscow and shrinking army size during the bitterly cold winter.\n\n\n\n\n\n\nFigure 18.4\n\n\n\nThis image has also been recreated with modern plotting software, including the ggplot2 package in R, which you will learn about in the next lesson.\n\n\n\n\n\n\nFigure 18.5: (source: ggplot2: Grammar of Graphics in R by Hadley Wickam)",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Principles & Perception</span>"
    ]
  },
  {
    "objectID": "chapters/week05/data-visualization-principles.html#what-is-data-visualization",
    "href": "chapters/week05/data-visualization-principles.html#what-is-data-visualization",
    "title": "18  Data Visualization Principles & Perception",
    "section": "18.2 What is Data Visualization?",
    "text": "18.2 What is Data Visualization?\nAt their core, data visualizations are products that:\n\nRepresent data.\nHave a specific purpose.\nTell a data-driven story.\n\nThere are two main types of data visualizations:\n\nInformation visualizations (aka infographics or infoviz) tend to be visually striking, dramatizing a problem with unique and visually appealing imagery that draws the casual viewer in.\nStatistical graphics aim to make comparisons, to reveal patterns and discrepancies. We use statistical graphs to communicate our research results, often for viewers who are already immersed or interested in the problem.\n\nWhile many topics within this reader will apply to infoviz as well, our emphasis is on creating judicious and accurate statistical graphics.\n\n\n\n\n\n\nNoteSee also\n\n\n\nFor more info comparing infoviz and statistical graphs, see Gelman and Unwin 2013.\n\n\n\n18.2.1 Why Viz?\nThere are lots of ways to represent our data. In fact, tables are often the most common way to report data, and they are great at conveying exact values. But interpretation of data displayed in a table is largely up to the viewer. It’s hard to perceive the overall summary of the data from a table, unless it’s really simple and, in that case, you often don’t even need a table and can just report those statistics as text.\nData visualization, on the other hand, takes advantage of our ability to process information by shifting the balance between our natural perceptive and cognitive abilities to convey a specific message. Most of the information that’s sent to our brains is visual. In fact, it’s been found that the human brain processes visual imagery 60,000 times faster than text! Data visualizations allow us to move from a predominantly thinking perspective to a seeing perspective. The cerebral cortex, which primarily handles our cognition, is slow and less efficient than the visual cortex, which processes images. Thus, visual diagrams are often easier for us to process than pages of words describing our research. Absorbing information quickly allows us to make novel inferences, and make more productive and informed decisions. Not surprisingly, well composed data visualizations are the most effective type of scientific communication.\n\n\n\n\n\n\nNoteSee also\n\n\n\nFor guidance on how to convert a table into a plot, see this paper by Andrew Gelman.\n\n\nUltimately, the utility of a data visualization depends on how well it’s composed.\n\n\n18.2.2 Good Data Visualizations\n\nProvide rapid access to data.\nFaithfully represent the data and tell a story.\nAre expressive.\nAre effective.\n\nHelpful data visualizations intuitively, clearly, accurately, and efficiently explain complex ideas. The patterns and relationships presented must be valid, and the visual relevant to the data it presents. A data visualization cannot exist without a narrative, and good data visualizations always include context. Good plots grab our attention and create a positive visual impact. This aids our ability to make connections and recall the features of the data. They can be aesthetically pleasing but that’s not the end goal. Good plots are accessible (not everyone perceives the visual world the same way). They leverage aspects of human perception to allow for intuitive inference of relationships between abstract concepts (our data).\n\n\n\n\n\n\nNoteSee also\n\n\n\nWant to feel inspired? Check out Information is Beautiful and Flowing Data.\n\n\n\n\n\n\n\n\nFigure 18.6: (source)\n\n\n\n\n\n18.2.3 Bad Data Visualizations\n\nHave too much, or too little, information.\nAre inconsistent.\nIgnore limits of human perception.\nMisrepresent the data.\nUse inappropriate (or garbage) data.\n\nHave you ever seen a pie chart where the labeled slices add up to something other than 100%? That’s a poorly executed data visualization. Goal: don’t end up on WTF Viz.\n\n\n\n\n\n\nFigure 18.7: (source)",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Principles & Perception</span>"
    ]
  },
  {
    "objectID": "chapters/week05/data-visualization-principles.html#before-you-viz-make-a-plan",
    "href": "chapters/week05/data-visualization-principles.html#before-you-viz-make-a-plan",
    "title": "18  Data Visualization Principles & Perception",
    "section": "18.3 Before You Viz, Make a Plan",
    "text": "18.3 Before You Viz, Make a Plan\nModern software makes it easy to quickly create a plot. But before you fire up your computer and start plotting, stop and think. Write out your visualization plan. This will save you time in the long run, and result in a more robust data visualization.\nAsk yourself:\n\nWhy am I making this visualization? (purpose)\nWho am I making it for? (audience)\nHow will I use and share it? (medium)\nWhat can I use to make it? (tools)\nWhat story does it tell? (message)\nWho does it affect? Who is left out? (critical approach)\n\n\n\n\n\n\n\nTip\n\n\n\nHow many plots you need is always the wrong question. You need exactly as many as you need to tell your story.\n\n\n\n18.3.1 Purpose\nFirst, identify why you are making a visualization. We use data visualizations in different ways across the iterative steps of the research data pipeline:\n\n\n\n\n\n\nFigure 18.8\n\n\n\n\nCollection: plots can help us understand who, what, and where the data represent. It can help us track our progress, and help us project required effort to complete this phase of the project.\nCleaning: plotting is a quick and effective way to spot errors in our data. It allows us to grasp the extent of issues such as outliers and missing data.\nExploration: plots are a powerful tool for exploratory data analysis (EDA). Plots help us identify patterns, summarize variables and relationships. (see Tukey 1960)\nConfirmation: plots also help us conduct confirmatory data analysis (CDA). We can plot diagnostics like the model fit, residuals, and model comparisons that confirm whether a model is correct. CDA is an iterative process over the course of research, one reason why we advocate using scripting languages and other reproducible workflows for generating graphics.\nValidation: plots also help us to debug and validate our code. We can visually inspect the results at each step of the code we are writing and verify whether it satisfies our expectations.\nCommunication: sharing the insights from our data with others is probably the most commonly understood and emphasized purpose of data visualizations. This is also often the hardest type of data visualization to “get right,” because we don’t always remember to design the visual to speak specifically to who we are sharing it with.\n\n\n\n18.3.2 Audience\nWho are you making the data visualization for? There is no such thing as a “generic” data visualization. Are you making the figure for:\n\nYourself, to help you clean or explore your data?\nYour immediate colleagues or research team to update them on your research progress?\nExperts in your field reading your publication or listening to your presentation?\nA general audience as part of your public outreach?\nPolicy makers who might not know all the details but might be making big decisions based on your results?\n\nKnowing who you’re making the visualization for will help you think through the following steps to create something of value for your intended purpose. It will also help you determine how effort is needed to compose a plot to achieve your goal.\n\n\n18.3.3 Medium\nThere are always constraints when creating a data visualization. It’s best to discover these before you start, rather than after you’ve created a beautiful data visualization that’s completely inappropriate for your intended use.\nIf you’re creating the visual to accompany a journal article, you probably need to use a static figure and not an interactive or dynamic dashboard. Does your journal allow for color figures? When in doubt, start with greyscale—it’s a lot easier to add color, rather than take it away, as you revise your figures.\nIf you’re showing the figure during a presentation, you probably want to simplify it—you audience will have 5 seconds max to read, understand, and interpret your visualization. A really complex figure that requires minutes to comprehend will just distract your audience away from what you—and your data—are saying. It might be more effective to compose and display the same plot in different ways to best communicate your points.\nFor a poster presentation where your audience is expected to spend significant time pondering over your findings, you might want to have one very large, clear figure that disentangles the complexity of your project.\nIf you’re creating a visual for a website, you might be able to go nuts—bring on the interactivity, the dynamic data display—until you crash the server because it requires too much compute time.\n\n\n\n\n\n\nNoteChecklist\n\n\n\n\nStatic or dynamic/interactive?\nDashboard/apps?\nProjector, paper, website?\nResolution?\nColor?\n\n\n\n\n\n18.3.4 Tools\nAt the UC Davis DataLab, we advocate for the use of open-source software and scripting languages for data-driven research projects, including for generating data visualizations.\nUsing scripting languages makes it easy for you to reproduce your data visualizations. As you clean and update your data, you can re-create your visuals easily by re-running your code. You can also return to a figure later and know exactly what it represents and how you made it. You don’t have to worry about remembering which buttons you clicked, and in what order, like you would when using a GUI based software.\nUsing free, open-source software also means that you can easily and freely share your data, code, and output with your collaborators, reducing the equity and reproducibility barriers posed by the use of proprietary software. Open-source software that’s great for plotting—like R—also has amazing user communities and resources to help you learn the code and create your ideal visualization.\n\n\n\n\n\n\nTip\n\n\n\nBe practical with yourself: you probably aren’t going to learn a new package or other plotting software overnight. If your conference talk is tomorrow, using familiar software like Excel for plotting can be fine, especially if you know some tricks to clean up and customize the appearance of your plots.\n\n\n\n\n\n\n\n\nNoteSee also\n\n\n\n\n\nHere’s a non-exhaustive list of open source tools we recommend for data visualization:\n\nStatic visualizations:\n\nggplot for R, Python, and Julia\nSeaborn for Python\n\nGeospatial visualizations:\n\nQGIS\n\nDynamic and interactive visualizations:\n\nLeaflet for mapping\nD3\nPlotly for R, Python, and Julia\nBokeh for R and Python\n\n\n\n\n\n\n\n18.3.5 Message\nResearch is storytelling with data. Every data visualization is an important piece of that story. It may help you confirm (or reject) a hypothesis, discover new correlations, or predict the likelihood of a future event.\nCreating statistical graphics is like writing a novel—you get to decide who and what will be featured in your data story. And just like one page of a novel, your data visualization alone doesn’t tell the whole story. Every data visualization should contain the details required for explanation, and they require narratives.\nWrite out captions for each plot before you make it. What does the plot show? After creating the plot, go back and update the caption with the take home points for your viewer. How might others focus on a different message? If you can’t articulate what the plot is about then you probably should rethink what you are choosing to display and how you are showing it.\n\n\n18.3.6 Critical Approaches\n\n\n\n\n\n\nImportant\n\n\n\nDon’t skip this step. It’s last on this list but is the most important on your journey to making useful data visualizations. Data are information, and information is power. Use this power intentionally and mindfully throughout the process of creating and sharing your visualizations.\n\n\nAs you reflect on your answers to the planning prompts above, critically review the features of your data:\n\nWhat do the variables you’ve selected for your visualization mean? How are they defined? How did those definitions come to be? Why did you select them?\nWho will your data visualizations affect? What groups are left out? How does this affect the story your data tells? How might someone misrepresent or misunderstand your story? Bring back the bodies.\n\nConducting these connotative and denotative explorations of your data will not only result in a more robust visualization, but will make you a better researcher and support a more inclusive and equitable society.\n\n\n\n\n\n\nNoteSee also\n\n\n\nTo learn more and practice these steps on some case studies, check out our Critical Approach to Data Visualization workshop and Data Feminism research and learning cluster.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Principles & Perception</span>"
    ]
  },
  {
    "objectID": "chapters/week05/data-visualization-principles.html#graphical-elements-of-a-plot",
    "href": "chapters/week05/data-visualization-principles.html#graphical-elements-of-a-plot",
    "title": "18  Data Visualization Principles & Perception",
    "section": "18.4 Graphical Elements of a Plot",
    "text": "18.4 Graphical Elements of a Plot\nA data visualization is useful only if it encodes information in a way that our eyes can perceive and our brain can understand. Marks and channels are the building blocks of all data visualizations and are employed to accomplish this encoding.\nMarks are the the basic geometries, or graphical elements, in a plot that depict our data items or their linkages. Marks indicate “where” something is and include points (0d), lines (1d), areas (2d), and volumes (3d).\nChannels are the attributes of that control how the marks appear. Channels are used to encode (or indicate) the values or meaning of our data. Channels were first described in the mid-20th century by Jacques Bertin in his book Semilogie graphique (the Semiology of Graphics [1967]), which argues that visual perception operates according to rules that can be followed to express information visually in intuitive, accurate and efficient ways. He described seven main categories of visual variables (channels): location or position, size, shape, orientation, color, and texture. More recent publications list up to 12 channels useful for encoding meaning in data visualizations (Roth 2017).\nBy understanding the nature of our data in combination with the principles of visual perception, we can decide which marks and channels to use for a given data visualizations.\n\n\n\n\n\n\nFigure 18.9: (source: Visualization Analysis and Design by Tamara Munzner)",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Principles & Perception</span>"
    ]
  },
  {
    "objectID": "chapters/week05/data-visualization-principles.html#principles-of-visual-perception",
    "href": "chapters/week05/data-visualization-principles.html#principles-of-visual-perception",
    "title": "18  Data Visualization Principles & Perception",
    "section": "18.5 Principles of Visual Perception",
    "text": "18.5 Principles of Visual Perception\nLeveraging principles of visual perception (the ability to see and interpret surrounding visual information) will help us identify appropriate plot types and design better, more informative graphics. Humans are wired to look for structure, patterns, and logic. Our brains are amazing—they take ambiguous visual information and transform it into something organized, symmetrical, or familiar so we can understand it. But, we don’t process all visual information equally.\n\n18.5.1 Visual Magic Tricks\nTake a look at the following questions and images.\n\n\n\n\n\n\nFigure 18.10: Which line is bigger?\n\n\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nThey’re the same length, if you pay careful attention to the scales of the axes!\n\n\n\n\n\n\n\n\n\nFigure 18.11: Which inner circle is bigger?\n\n\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nThe circles are the same size.\n\n\n\n\n\n\n\n\n\nFigure 18.12: Do these lines connect?\n\n\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nThe lines do NOT connect. Hold up a ruler or straight edge and prove it for yourself.\n\n\n\n\n\n\n\n\n\nFigure 18.13: Is the center bar in this image by Dodek a gradient?\n\n\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nNope! It’s a solid color.\n\n\n\n\n\n\n\n\n\nFigure 18.14: What shape(s) do you see in this image?\n\n\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nDid you see a vase or two faces?\n\n\n\nThese visual “magic tricks” work because they capitalize on innate weaknesses in our visual perception.\n\n\n18.5.2 Steven’s Psychophysical Power Law\nResearch studies by Stanley Smith Stevens and others have shown that we exhibit innate biases in how we perceive magnitude changes in the intensity of various types of stimuli.\nFor example, we perceive the intensity of an electrical shock to a greater degree than its actual, physical intensity would seem to warrant. We’re also poor at accurately perceiving changes in brightness and estimate it to increasing less than it actually does. However, we have near perfect perception of length proportional to its actual increase. This is especially true if lengths are aligned and on the same scale. Knowing this can help us design more intuitively useful plots.\n\n\n\n\n\n\nFigure 18.15: (source: Figure 5.7 in Visualization Analysis and Design by Tamara Munzner)\n\n\n\n\n\n18.5.3 Perception and Encodings\nBased on psychophysics, we can rank encodings to help us identify which ones will more accurately allow us to judge differences in relative magnitudes, which is important when working with ordinal, interval or ratio data.\n\n\n\n\n\n\nFigure 18.16: (source: Jock Mackinlay, 1986, Computer Science ACM Trans. Graph.)\n\n\n\nFrom most to least accurate by magnitude perception:\n\nPosition along a common scale. Spatial position is the easiest feature for us to recognize and evaluate, and unsurprisingly is used in the most common plot types: bar charts, scatterplots.\nPositions along identical but nonaligned scales. Small multiples, grid, lattice, panel, and Rellis charts.\nLength. We can easily recognize proportions and evaluate lengths, especially when they are aligned, such as in bar charts.\nDirection. We recognize directionality fairly easily. Trend charts utilize this to demonstrate changes over time.\nAngle, slope. It’s harder to evaluate angles than length or position. Pie charts can be as efficient as stacked bar charts, unless there are more than 3 parts to the whole. But ask yourself—if there are fewer than 3 or fewer parts, do you really need a visualization?\nArea. Determining the relative magnitude of areas is much harder compared to lengths, and should be used (like in bubble charts) for indicating the relative importance, and not absolute magnitude changes.\nVolume. 3D objects as represented in 2-D space are hard to evaluate. Avoid them. I’m looking at you, exploding 3D pie chart.\nCurvature. Perceiving changes in the degree of a curve magnifies the difficulties in detecting direction, angle, and non-aligned lengths.\nDensity, color saturation and shading. Color is the least accurate way to convey patterns. Saturation is the intensity of a single hue, and increasing color intensity is intuitively perceived as correlating to an increasing value. But individual hues are hard to compare to one another. Heatmaps along the same color gradient can be a good way to convey an overall picture of change in values over a range. We’ll talk more about color later on.\nColor hue. For data visualizations, color hue is the most challenging encoding to detect changes in magnitude.\n\n\n\n18.5.4 Evaluating Graphics\n\nNo matter how clever the choice of the information, and no matter how technologically impressive the encoding, a visualization fails if the decoding fails. (Cleveland 1983)\n\nHow do we detect if our encodings have failed? Munzner uses the principles of expressiveness and effectiveness to help us evaluate our data visualizations.\nThe expressiveness of a visual encoding should “express all of, and only, the attributes of the data.” It is violated when we use encodings that do not match our data type or our visualization goals. When it fails, a chart is not only sub-optimal and confusing, it can be incorrect and misleading. Charts can fail the expressiveness test if their encodings imply ordering when there actually is none, or they mis-order a variable.\nThe effectiveness of a visual encoding addresses how accurately can the interpreter of the chart decode the encodings within it and derive accurate knowledge. According to Munzner, “the importance of the attribute should match the salience of the channel,” meaning we should use channels at the top of the list to encode the variables that are the most important to communicating our data story.\nWhen looking at a plot, can you accurately detect differences is the sizes of the bubbles? Can you discriminate between all of the colors, compare the shades? Can you separate the dimensions of the data?\nTaking these principles together, when we want to compare magnitudes of ordinal data (numeric, continuous, or ordered qualitative data—like height, weight, number of children in a family, or a rating), we should use encodings at the top of Mackinlay’s list above.\nConversely, for nominal data (categorical or un-ordered qualitative data—like gender), use these identity channels:\n\nShape: glyphs are effective at grouping categorical attributes together. But be mindful that the more shapes you use, the harder it will be for a viewer to remember what corresponds to which specific data attribute.\nColor: while color can be very effective in data visualizations (see the Gestalt principles below) typically less is more. Apply contrasting colors only to differences in meanings in the data, or to emphasize the main elements. Start with grey, and add color only as necessary. And, be mindful when defining your color palette. Color brewer and Viz Palette provide palettes that optimize our perceptive abilities and design for accessibility. Do a color check—how will a person with colorblindness perceive your graphics?\nTexture: similar to shape and color, texture can be useful for differentiating between categories or separate areas. Textures can be particularly effective at replacing colors, such as in black and white figures, and for increasing accessibility by reinforcing a color encoding.\n\nCombining channels can result in integral or separable coding pairs, respectively allowing attributes to be perceived holistically or with separate judgments regarding their graphical dimension.\n\n\n\n\n\n\nFigure 18.17: (source: Colin Ware (2019) “Information Visualization”)\n\n\n\n\n\n18.5.5 Gestalt Principles\nIn addition to decoding specific elements, our brains have an amazing ability to create and perceive structure along visual objects. This is commonly referred to as the Gestalt principles of visual perception. This framework, combined with Steven’s Law, can help us think through how to use marks and channels together to create expressive and effective data visualizations:\n\nSimilarity: objects with the same visual properties are assumed to be similar and are grouped together.\n\nExample: use design elements such as shape, color, and organization to indicate groupings of the data. In design theory these are called “preattentive features” because we actually see and perceive them before we really think about them. In some experiments it was found to take less than 0.5 seconds for the eye and brain to process a preattentive property of an image.\n\nProximity: objects that are close together are perceived as a group.\n\nExample: since physical distance connotes similarity, grouping bars on a chart can indicate similarities among their data. Instead of listing it in a legend, directly label data groupings by adding informative text directly onto the graph.\n\nContinuity: elements that are aligned (on the same line, curve, or plane) are perceived to be more closely related to each other than to other elements.\n\nExample: it is often easier for us to perceive the groupings if the shapes are curves, rather than lines with sharp edges.\n\nEnclosure: objects that appear to have a boundary around them (i.e., are found within the same common or enclosed region) are perceived as being related.\n\nExample: Add line boundaries or shades to group objects.\n\nConnection: objects that are connected, such as by a line, are perceived as a group.\n\nExample: connect different data together to indicate a relationship. This connectedness is highly effective as it often over-rules the other principles for group perception. Every line plot is an example of connectedness.\n\nClosure: complex arrangements of visual elements are perceived as a single, recognizable pattern.\n\nExample: open structures are often perceived as closed, complete and regular.\n\nFigure and Ground: objects are perceived as either standing out prominently in the foreground (or front figure) of an image, or recede into the background.\n\nExample: shading or color blocking can be employed to to distinguish between the more important figure and less important ground features of an image. Place elements of the most importance in the foreground figure.\n\nFocal Point: whatever stands out visually is perceived as the most important. It will grabs our attention first, and holds it for the longest.\n\nExample: use design elements selectively to draw attention to the most important features of the data.\n\n\n\n\n\n\n\n\nFigure 18.18: Gestalt principles for perpetual grouping and figure-ground segregation. (source: Gestalt Principles for Attention and Segmentation in Natural and Artificial Vision Systems by G. Kootstra, N. Bergstrom, D. Kragic (2011).)",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Principles & Perception</span>"
    ]
  },
  {
    "objectID": "chapters/week05/data-visualization-principles.html#sec-accessible-data-visualizations",
    "href": "chapters/week05/data-visualization-principles.html#sec-accessible-data-visualizations",
    "title": "18  Data Visualization Principles & Perception",
    "section": "18.6 Accessible Data Visualizations",
    "text": "18.6 Accessible Data Visualizations\n\n18.6.1 Color\nColor can be one of the most challenging—and important—attributes to apply to a plot. Special care must be taken when applying color to our data visualizations to ensure they are accessible to persons with color blindness. Color blindness prevents viewers from distinguishing between certain colors, their brightness, and/or shades of a color. Affecting approximately 1 in 12 men (8%) and 1 in 200 women (0.5%) around the world, it is likely that some viewers of your data visualization will perceive its colors differently.\nOverall we’re not doing a good job at using color mindfully in our science communication. If you want to use color, the following are some recommendations to keep in mind.\nRecommendation 1: Avoid problematic color combinations. The most common types of color blindness makes it hard to tell the difference between red and green (deuteranope and protanope color blindness). Blue-yellow color blindness (tritanope) is less common. Avoid using: red/green, green/brown, green/blue, blue/gray combinations. Many graphing software unfortunately use these combinations as a default and you will have to manually change this on your figures.\nTo demonstrate why these combinations are problematic, here is a color vision test:\n\n\n\n\n\n\nFigure 18.19: (source: Crameri, F., Shephard, G.E. & Heron, P.J. The misuse of colour in science communication. Nat Commun 11, 5444 (2020))\n\n\n\nRecommendation 2: Use an online tool to help you pick a colorblind friendly palette depending on your data and visualization needs. Examples include:\n\ncolorbrewer: palettes, color advice for mapping, and good general tips\ncoolors\n\n\n\n\n\n\n\nFigure 18.20\n\n\n\nRecommendation 3: Use a colorblindness simulator to check your visualization. Who won’t be able to see the differences you’re trying to display with color? Here are a few simulators:\n\nCoblis\nColor Oracle\n\nRecommendation 4: Add textures, symbols, or other channels to reinforce the grouping attributes on your plot.\n\n\n\n\n\n\nFigure 18.21: (source)\n\n\n\nRecommendation 5: Rethink your plot. You may not actually need color at all to effectively display your data.\n\n\n\n\n\n\nFigure 18.22: (source)\n\n\n\n\n\n\n\n\n\nNoteSee also\n\n\n\n\n\nHere are some more resources to help you use color effectively and mindfully in your data visualizations.\nColor and design:\n\nVisualization-Aware Color Design: Aesthetic, Perceptual & Functional Constraints\nModeling Color Difference for Visualization Design (n experiment showing how mark type influences color perception in data viz)\nTextures and patterns for colorblindness\n\n\nColor accessibility in R:\n\nRColorBrewer package\nviridis package\nggpattern package\nR color cheatsheet\n\n\n\n\n\n\n18.6.2 Alternative Text\nSo far we’ve taken for granted that visualization is an accessible mode of communication, but researchers and audiences alike are not all sighted. RStudio is behind on vision impairment accessibility, but some packages can provide text descriptions and sonification/audification of plots to improve accessibility for non-visual data interaction.\nFor example, the BrailleR package, has a VI function that wraps around ggplot objects and provides a text-description output. This description is a starting point but it does not summarize the data itself, so it is important to consider also informative figure captions or embedded alternative text so that all viewers are able to interpret the visualization.\nOther packages like the sonification package’s sonify function can be used to represent data in audio form. With the function, the x-axis can span sound across time, so that the length of time a sound plays follows the data long the x-axis from left to right; the y-axis can be expressed as pitch, so that the pitch of the sound matches to the values of the data (lower value means lower pitch).",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Principles & Perception</span>"
    ]
  },
  {
    "objectID": "chapters/week05/data-visualization-principles.html#designing-statistical-graphics",
    "href": "chapters/week05/data-visualization-principles.html#designing-statistical-graphics",
    "title": "18  Data Visualization Principles & Perception",
    "section": "18.7 Designing Statistical Graphics",
    "text": "18.7 Designing Statistical Graphics\nYou are now ready to make your plot! You can combine marks and channels to create nearly any plot type, and there are many established types of statistical graphics that you can choose from to showcase your data. Each type has its benefits, and drawbacks, based on how it encodes your data.\n\n\n\n\n\n\nTip\n\n\n\nMatch the chart type to your data—and what you want it to show—and not the other way around.\n\n\n\nStep 1: Identify Your Data Type\nData can be quantitative or qualitative:\n\nQuantitative data is either continuous (numerical data like height and weight), or discrete (constrained values, such as the number of children in a family).\nQualitative data can be ordered (categories that have a relationship but no meaningful distance between them, such as movie star ratings), or nominal (categories that have no meaningful order, such as gender).\n\n\n\nStep 2: Determine Your Functional Approach\nAsk ask yourself:\n\nWhat are the tasks you want the visual to support?\n\nShowing how values compare to each other? How the data are distributed? How they are composed? How values relate?\n\nWhat specific visual best supports those tasks?\nWhat do you expect people to naturally do in their “visual queries” as they explore the plot?\nHow can you modify the graphical marks and channels to support faster queries?\n\n\n\nStep 3: Select a Plot Type\nNow that you’ve identified your data types and what you need your visualization to show, explore your different chart type options! Start with this nifty From Data to Viz tool. Select your data type(s) and click through for the pros, cons, and alternate options for a bevy of charts.\n\n\n\n\n\n\nFigure 18.23\n\n\n\nFor example, if you want to enable accurate comparisons of individual quantitative values and their relationships, try a scatterplot or a chart with lines or bars sitting aligned on a single axis.\nThe following list contains an overview of some of the most common plot types you may encounter:\nSingle quantitative variables are plotted to show the frequency distribution of the data. While histograms are the most common density plots, a single quantitative variable can also be plotted using a rug plot/strip chart, boxplot, or violin plot (described below, where they’re most commonly employed).\nTwo quantitative variables can be plotted using a:\n\nScatterplot: each axis encodes the values of a different quantitative variable, and individual data are represented as points (or dots) on the chart.\nLine plot: data points are connected by straight lines. Line-scatter plots are are common for time series or trend data.\n\nOne quantitative and one qualitative variable are suitable for a:\n\nBar chart: bars represent the amount of data in different categories of a variable. One axis encodes the frequencies of the quantitative data, and the other axis the categories of the qualitative data.\nBoxplot: shows the median, quartiles, and bounds of your data.\nViolin plot: in essence, a boxplot that also shows the distribution of your quantitative variable.\nWord cloud: these eye-catching visualizations display a list of words with their font size corresponding to their importance. But, they require huge sample sizes and are not very useful as they often distort reality. For example, long words will look more prominent just because they have more letters and cover more area—our eyes don’t intuitively parse out word height from length.\nPie chart: uses relative frequencies to show how large each category is in relation to the whole.\n\n\n\n\n\n\n\nImportantWarning\n\n\n\nPie charts are grudgingly listed here because you will see them in the wild, but resist the temptation to use them!\nBased on our visual perception, pie charts are inherently problematic because they encode values as visual attributes. Pie charts encode data as the area of each slice, as well as the angle that it forms in the center of the pie, making it difficult to easily perceive and compare differences.\nOver 492 posts on WTF Visualizations are tagged as pie charts! Almost any other chart type is better than a pie chart.\n\n\nOther complex plot types you may encounter that layer additional marks and channels on the above chart types are:\n\nLollipop chart: a dot chart where the dots are connected by lines to an axis.\nMosaic plot: also called a treemap, these plots display hierarchical data as sets of nested rectangles sized proportionately to their values.\nBubble plots: scatterplots where the size of a dot corresponds to a third numerical or ordered categorical value.\nRadar plots / star chart: line plots where each variable has its own axis and all axes are joined at the center of the figure.\nNetwork diagrams: also called graphs, these plots show connections (edges) between entities (nodes).\n\n\n\n\n\n\n\nNoteSee also\n\n\n\nTo learn more about network diagrams, check out DataLab’s network toolkit and network analysis workshop.\n\n\n\n\n\n\n\n\nNoteSee also\n\n\n\nGoogle also has an interactive plot gallery. And this Stack Exchange post has even more chart type resources.\nBut remember, some encodings are more difficult to accurately decode. When in doubt, stick to simple figures with points and lines.\n\n\n\n\n\n\n\n\nNoteWhat About Maps?!\n\n\n\nGeospatial data visualization by nature is complex and encodes a lot of attributes. Interested in learning more? Check our DataLab’s Spatial Sciences research and learning cluster and workshops.\n\n\n\n\nStep 4: Iterate\nCreate your visual, and run through step 2 again keeping in mind the principles of visual perception, effectiveness and efficiency. Does it meet your needs? If not, try a different type. Graphing, like writing, requires continuous editing.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Principles & Perception</span>"
    ]
  },
  {
    "objectID": "chapters/week05/data-visualization-principles.html#tips-for-better-plots",
    "href": "chapters/week05/data-visualization-principles.html#tips-for-better-plots",
    "title": "18  Data Visualization Principles & Perception",
    "section": "18.8 Tips for Better Plots",
    "text": "18.8 Tips for Better Plots\nMaking effective data visualizations takes practice and experience. The more plots you look at, the more you will intuitively recognize what works—and what doesn’t—for data visual storytelling. One takeaway I hope you discover is the need to avoid unnecessary complexities.\n\n\n\n\n\n\nTip\n\n\n\nIf the “story” is simple, keep it simple. If the “story” is complex, make it look simple.\n\n\nBelow are some tips to help achieve those goals.\n\n18.8.1 Get Rid of Chartjunk\nAn easy way to instantly improve your plots is to eliminate superfluous material. Extra tick marks and grid lines; unnecessary text and arrows; decimal places beyond the measurement error of the level of difference; cute little butterfly clip art: this chartjunk has no meaning and it clutters up a chart, making it hard for your viewer to see what’s most important—your data. The amount of ink on your figure should directly correspond with the amount of data you present. If it doesn’t, you have a lot of chartjunk. (Evidence #10298 that pie charts are never a good choice.)\nTry these de-cluttering steps to improve your charts:\n\nShift from center to left-justified text\nRetain white space\nClear contrasts\nRemove chart borders\nRemove (or strongly mute) gridlines\nRemove data markers and point labels (unless they are important)\nRemove unnecessary polygon filling\nCleanup and rename axis labels to be intuitive\nReplace the title with something informative\nLabel the data directly using the principle of proximity\nLeverage consistent color and other aesthetics\n\nCreating visual order and reducing chartjunk will dramatically improve your graphic by helping your data stand out.\n\n\n18.8.2 Facilitate Comparisons\n\nAvoid having the graph elements interfere with the data\nJuxtapose or supepose plots (using the same scales)\nUse visually prominent symbols\nAvoid over-plotting; try jittering, or smoothing\nDon’t change a scale mid-axis\nUse only one scale on one axis\nUse color, judiciously\nAvoid jiggling the baseline\nDon’t distort the data; take care when selecting the encodings\n\nA common mistake is to use more encodings than there are dimensions of the data. If you data only has two dimensions (say number of students in STEM by gender identity), your figure could reasonably use points, rarely area, and never volume. (I’m looking at you, 3D pie chart.)\n\n\n18.8.3 Create Information-Rich Plots\nData visualizations cannot exist without text. They require context to infer meaning. Ask yourself:\n\nDoes the caption describe what has been graphed? Does it draw attention to the important features? Describe the conclusions drawn by the graph?\nAre the legends and labels clear and intuitive?\nAre important reference lines and points labeled?\n\n\n\n18.8.4 Don’t Distort the Data\nThere’s a bestselling book called [“How to Lie with Statistics”][]. Written by the journalist (and not a statistician) Darrell Huff in 1954, the book focuses on how decisions we make in selecting the data and analysis method, along with errors in interpretation, can generate incorrect conclusions. Similarly, visualization principles can be mis-applied when graphing such that the takeaway message from a graphic distorts reality. Review your plots to make sure they both tell, and show, the truth.\n\n\n18.8.5 Practice\nJust as an author edits before publishing the novel, and an artist sketches before making the masterpiece, plotting is an iterative process. Proofread for clarity and consistency. Check whether your plots pass the expressiveness and effectiveness tests. Does a viewer draw the same conclusions from the figure that you do?\nHere’s a cheat sheet and checklist to help you design and improve your data visualizations. Happy plotting!\n\n\n\n\n\n\nNoteReferences and Additional Resources\n\n\n\n\n\nWebsites:\n\nMilestones in the History of Thematic Cartography, Statistical Graphics, and Data Visualization\nFrom Data to Viz\nInformation is Beautiful\nFlowing Data\nWTF Visualizations\nPerception in Visualization, a computer scientist’s viewpoint\n\nArticles:\n\nMickinlay, Jock. 1986. Automating the design of graphical presentations of relational information. ACM Transactions on Graphics. https://doi.org/10.1145/22949.22950\nCleveland, William S. & Kleiner, Beat. 1975. A Graphical Technique for Enhancing Scatterplots with Moving Statistics. In Proceedings of the Annual Meeting. Atlanta, GA.\nFisher, Ronald Alymer. (1915). Theory of Statistical Estimation. Proceedings of the Cambridge Philosophical Society. 22. 700-725.\n\nContemporary books and chapters:\n\nCleveland, William. 1994. The elements of graphing data, 2nd edition. Hobart Press.\nDrucker, J. 2014. Graphesis: Visual Forms of Knowledge Production. Harvard UP. Cambridge, MA.\nFriendly, M. 2007. A Brief History of Data Visualization. In Handbook of Computational Statistics: Data Visualization. III. Springer-Verlag. Heidelberg. 1-34.\nMunzner, Tamara. 2014. Visualization analysis and design.\nHuff, Darrell. 1954. How to Lie with statistics. W. W. Norton & Company. New York.\nTufte, Edward R. 1983. The Visual Display of Quantitative Information. Graphics Press. Cheshire, CT.\nWainer, Howard. 2007. Graphic discovery: a trout in the milk and other visual adventures.\nWilkinson, Leland. 2005. The Grammar of Graphics, 2nd ed.. Springer. New York. Yau, Visualize this: The flowing data guide to design, visualization, and statistics\n\nHistorical books:\n\nBertin, Jacques. 1983. Semiology of Graphics. University of Wisconsin Press. Madison, WI. (trans. W. Berg) 1967\nDescartes, Réne. 1637. La Géométrie. In Discours de la Méthode. Essellier. Paris.\nMinard, Charles Joseph. 1861. Des Tableaux Graphiques et des Cartes Figuratives. E. Thunot et Cie. Paris.\nPlayfair, William. 1786. Commercial and Political Atlas: Representing, by Copper-Plate Charts, the Progress of the Commerce, Revenues, Expenditure, and Debts of England, during the Whole of the Eighteenth Century. Corry. London.\nSnow, John. 1855. On the Mode of Communication of Cholera. (n.p.). London.\nTukey, John Wilder. 1977. Exploratory Data Analysis. Addison-Wesley. Reading, MA.\nTukey, John Wilder. 1960. A survey of sampling from contaminated distributions. In Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling (I. Olkin et al., eds.) 448–485. Stanford Univ. Press.\n\nR graphics references:\n\nMurrell, Paul. 2019. R Graphics (3rd Edition). Chapman and Hall/CRC.\nSarkar, Deepayan. 2008. Lattice: Multivariate data visualization with R. Springer.\nggplot2: Elegant Graphics for Data Analysis (3e) by Wickham, Navarro, and Pedersen\nR Graphics Cookbook (2e) by Chang",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Visualization Principles & Perception</span>"
    ]
  },
  {
    "objectID": "chapters/week05/intro-ggplot2.html",
    "href": "chapters/week05/intro-ggplot2.html",
    "title": "19  Introduction to ggplot2",
    "section": "",
    "text": "19.1 Data Visualization\nIn this chapter, you’ll learn how to efficiently explore and summarize data with visualizations.\nThere are three popular systems for creating visualizations in R:\nThese three systems are not interoperable! Consequently, it’s best to choose one to use exclusively. Compared to base R, both lattice and ggplot2 are better at handling grouped data and generally require less code to create a nice-looking visualization.\nThe ggplot2 package is so popular that there are now knockoff packages for other data-science-oriented programming languages like Python and Julia. The package is also part of the Tidyverse, a popular collection of R packages designed to work well together. Because of these advantages, we’ll use ggplot2 for visualizations in this and all future lessons.\nggplot2 has detailed documentation and also a cheatsheet.\nThe “gg” in ggplot2 stands for grammar of graphics. The idea of a grammar of graphics is that visualizations can be built up in layers. In ggplot2, the three layers every plot must have are:\nThere are also several optional layers. Here are a few:\nLet’s visualize the California least terns data set from Section 11.4 to see how the grammar of graphics works in practice. But what kind of plot should we make? It depends on what we want to know about the data set!\nSuppose we want to understand the relationship between the number of breeding pairs and the total number of nests at each site, and whether this relationship is affected by climate events. One way to show the relationship between two numerical features like these is to make a scatter plot.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/week05/intro-ggplot2.html#sec-data-visualization",
    "href": "chapters/week05/intro-ggplot2.html#sec-data-visualization",
    "title": "19  Introduction to ggplot2",
    "section": "",
    "text": "The base R functions (primarily the plot function)\nThe lattice package\nThe ggplot2 package\n\n\n\n\n\n\nData\nGeometry\nAesthetics\n\n\n\n\n\nLayer\nDescription\n\n\n\n\nscales\nTitle, label, and axis value settings\n\n\nfacets\nSide-by-side plots\n\n\nguides\nAxis and legend position settings\n\n\nannotations\nShapes that are not mapped to data\n\n\ncoordinates\nCoordinate systems (Cartesian, logarithmic, polar)\n\n\n\n\n\n\n19.1.1 Loading ggplot2\nBefore we can make the plot, we need to load ggplot2. As always, if this is your first time using the package, you’ll have to install it. Then you can load the package:\n\n# install.packages(\"ggplot2\")\nlibrary(\"ggplot2\")\n\n\n\n19.1.2 Layer 1: Data\nThe data layer determines the data set(s) used to make the plot.\nggplot2 and most other Tidyverse packages are designed to work with tidy data, which means:\n\nEach feature has its own column.\nEach observation has its own row.\nEach value has its own cell.\n\nThese rules ensure data are easy to read visually and access with indexing. The least terns data set satisfies all of these rules.\n\n\n\n\n\n\nNoteSee also\n\n\n\nAll of the data sets we use in this reader are tidy. To learn how to tidy an untidy data set, see the Untidy & Relational Data chapter of DataLab’s Intermediate R workshop reader.\n\n\nTo set up the data layer, call the ggplot function on a data frame:\n\nggplot(terns)\n\n\n\n\n\n\n\n\nThis returns a blank plot. We still need to add a few more layers.\n\n\n19.1.3 Layer 2: Geometry\nThe geometry layer determines the shape or appearance of the visual elements of the plot. In other words, the geometry layer determines what kind of plot to make: one with points, lines, boxes, or something else.\nThere are many different geometries available in ggplot2. The package provides a function for each geometry, always prefixed with geom_.\nTo add a geometry layer to the plot, choose the geom_ function you want and add it to the plot with the + operator. We’ll use geom_point, which makes a scatter plot (a plot with points):\n\nggplot(terns) + geom_point()\n\nError in `geom_point()`:\n! Problem while setting up geom.\nℹ Error occurred in the 1st layer.\nCaused by error in `compute_geom_1()`:\n! `geom_point()` requires the following missing aesthetics: x and y.\n\n\nThis returns an error message that we’re missing aesthetics x and y. We’ll learn more about aesthetics in the next section, but this error message is especially helpful: it tells us exactly what we’re missing. When you use a geometry you’re unfamiliar with, it can be helpful to run the code for just the data and geometry layer like this, to see exactly which aesthetics need to be set.\nAs we’ll see later, it’s possible to add multiple geometries to a plot.\n\n\n19.1.4 Layer 3: Aesthetics\nThe aesthetic layer determines the relationship between the data and the geometry. Use the aesthetic layer to map features in the data to aesthetics (visual elements) of the geometry.\nThe aes function creates an aesthetic layer. The syntax is:\n\naes(AESTHETIC = FEATURE, ...)\n\nThe names of the aesthetics depend on the geometry, but some common ones are x, y, color, fill, shape, and size. There is more information about and examples of aesthetic names in the documentation.\nFor the scatter plot of breeding pairs against total nests, we’ll put bp_min on the x-axis and total_nests on the y-axis. Below, we set both of these aesthetics. We also enclose all of the code for the plot in parentheses () so that we can put the code for each layer on a separate line, which makes the layers easier to distinguish:\n\nggplot(terns) +\n  aes(x = bp_min, y = total_nests) +\n  geom_point()\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the aes function, column names are never quoted.\n\n\n\n\n\n\n\n\nNoteNote: The Old Aesthetic Layer Syntax\n\n\n\n\n\nIn older versions of ggplot2, you must pass the aesthetic layer as the second argument of the ggplot function rather than using + to add it to the plot. This syntax is still widely used:\n\nggplot(terns, aes(x = bp_min, y = total_nests)) +\n  geom_point()\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nAt this point, we’ve supplied all three layers necessary to make a plot: data, geometry, and aesthetics. The plot shows what looks like a linear relationship between number of breeding pairs and total nests. To refine the plot, you can add more layers and/or set parameters on the layers you have.\nLet’s add another aesthetic to the plot: we’ll make the color and shape of each point correspond to event, the climate event for each observation:\n\nggplot(terns) +\n  aes(x = bp_min, y = total_nests, color = event, shape = event) +\n  geom_point()\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nUsing color and shape for the same feature is redundant, but ensures that the plot is accessible to colorblind people.\n\nAdditional Geometries\nEach observation in the least terns data corresponds to a specific year and site. What if we label the points with their years? You can add text labels to a plot with geom_text. The required aesthetic for this geometry is label:\n\nggplot(terns) +\n  aes(\n    x = bp_min, y = total_nests,\n    color = event, shape = event,\n    label = year\n  ) +\n  geom_point() +\n  geom_text()\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nThe labels make the plot more difficult to read and probably would even if we made them smaller, because there are so many points on the plot. Making a high-quality visualization is typically a process of drafting and revising, similar to writing a high-quality essay. In this example, adding year labels to the plot doesn’t work well, so we’ll backtrack and leave them off of the plot. If accounting for year was critical to our research question, we could do it in other ways, such as by making separate plots for each year.\n\n\nPer-geometry Aesthetics\nBefore we remove the labels, let’s use them to demonstrate an important point about using multiple geometry and aesthetic layers: when you add an aesthetic layer to a plot, it applies to the entire plot. You can also set an aesthetic layer for an individual geometry by passing the layer as the first argument in the geom_ function. Here’s the same plot as above, but with the color aesthetic only set for the labels:\n\nggplot(terns) +\n  aes(\n    x = bp_min, y = total_nests,\n    shape = event,\n    label = year\n  ) +\n  geom_point() +\n  geom_text(aes(color = event))\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nNotice that the points are no longer color-coded. Where you put aesthetic layers matters.\n\n\nConstant Aesthetics\nIf you want to set an aesthetic to a constant value, rather than one that’s data dependent, do so in the geometry layer rather than the aesthetic layer.\nFor instance, suppose we want to make all of the points blue and use only point shape to indicate climate events:\n\nggplot(terns) +\n  aes(\n    x = bp_min, y = total_nests,\n    shape = event\n  ) +\n  geom_point(color = \"blue\")\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIf you set an aesthetic to a constant value inside of the aesthetic layer, the results you get might not be what you expect:\n\nggplot(terns) +\n  aes(\n    x = bp_min, y = total_nests,\n    color = \"blue\", shape = event,\n    label = year\n  ) +\n  geom_point() +\n  geom_text()\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n\n\n19.1.5 Layer 4: Scales\nThe scales layer controls the title, axis labels, and axis scales of the plot. Most of the functions in the scales layer are prefixed with scale_, but not all of them.\nThe labs function is especially important, because it’s used to set the title and axis labels. Visualizations should generally have a title and axis labels, to aid the viewer:\n\nggplot(terns) +\n  aes(\n    x = bp_min, y = total_nests,\n    color = event, shape = event\n  ) +\n  geom_point() +\n  labs(\n    x = \"Minimum Reported Breeding Pairs\",\n    y = \"Total Nests\",\n    color = \"Climate Event\", shape = \"Climate Event\",\n    title = \"California Least Terns: Breeding Pairs vs. Nests\"\n  )\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNotice that to set the title for a legend with labs, you can set the parameters of the same names as the corresponding aesthetics. While our plot is still far from perfect—some of the points are hard to see because of how many there are—it’s now good enough to provide some insight into the relationship between number of breeding pairs and nests.\n\n\n19.1.6 Saving Plots\nYou can use the ggsave function to save a plot you’ve assigned to a variable or the most recent plot you created (with no argument to ggsave):\n\nggsave(\"myplot.png\")\n\nThe file format is selected automatically based on the extension. Common formats include PNG, TIFF, SVG, and PDF.\n\nPNG and SVG are good choices for sharing visualizations online, while TIFF and PDF are good choices for print. Many journals require that visualizations be in TIFF format.\n\n\n\n\n\n\n\nNoteNote: R Plot Devices\n\n\n\n\n\nYou can also save a plot with one of R’s “plot device” functions. The steps are:\n\nCall a plot device function: png, jpeg, pdf, bmp, tiff, or svg.\nRun your code to make the plot.\nCall dev.off to indicate that you’re done plotting.\n\nThis strategy works with any of R’s graphics systems (not just ggplot2).\nHere’s an example:\n\n# Run these lines in the console, not the notebook!\njpeg(\"myplot.jpeg\")\nggplot(terns) +\n  aes(\n    x = bp_min, y = total_nests,\n    color = event, shape = event\n  ) +\n  geom_point() +\n  labs(\n    x = \"Minimum Reported Breeding Pairs\",\n    y = \"Total Nests\",\n    color = \"Climate Event\", shape = \"Climate Event\",\n    title = \"California Least Terns: Breeding Pairs vs. Nests\"\n  )\ndev.off()\n\n\n\n\n\n\n19.1.7 Example: Bar Plot\nSuppose we want to visualize how many fledglings there are each year, further broken down by region. A bar plot is one appropriate way to represent this visually.\nThe geometry for a bar plot is geom_bar. Since bar plots are mainly used to display frequencies, by default the geom_bar function counts the number of observations in each category on the x-axis and displays these counts on the y-axis. You can make geom_bar display values from a column on the y-axis by setting the weight aesthetic:\n\nggplot(terns) +\n  aes(x = year, weight = fl_min, fill = region_3) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteNote: Setting the Statistics Layer\n\n\n\n\n\nEvery geometry layer has a corresponding statistics layer, which transforms feature values into quantities to plot. For many geometries, the default statistics layer is the only one that makes sense.\nBar plots are an exception. The default statistics layer is stat_count, which counts observations. If you already have counts (or just want to display some quantities as bars), you need stat_identity (or the weight aesthetic described above). Here’s one way to change the statistics layer:\n\nggplot(terns) +\n  aes(x = year, y = fl_min, fill = region_3) +\n  geom_bar(stat = \"identity\")\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nThis produces the same plot as setting weight and using the default statistics layer stat_count.\n\n\n\nThe plot reveals that there are a few extraneous categories in the region_3 column: ARIZONA, KINGS, and SACRAMENTO. These might or might not be erroneous—and it would be good to investigate—but they don’t add anything to this plot, so let’s exclude them.\nLet’s also change the color map, the palette of colors used for the categories. These are both properties of the scale layer for the fill aesthetic, so we’ll use a scale_fill_ function. In particular, we’ll use the “viridis” color map, and since the fill color corresponds to categorical (discrete) data, we’ll use scale_fill_viridis_d. We’ll also add labels:\n\nterms_to_keep = c(\"S.F._BAY\", \"CENTRAL\", \"SOUTHERN\")\nterns_filtered = terns[terns$region_3 %in% terms_to_keep, ]\n\nggplot(terns_filtered) +\n  aes(x = year, weight = fl_min, fill = region_3) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"California Least Terns: Fledglings\",\n    x = \"Year\",\n    y = \"Minimum Reported Fledglings\",\n    fill = \"Region\"\n  )\n\n\n\n\n\n\n\n\nYou can read more about the viridis color map in ggplot2’s documentation for this function. The plot reveals that the data set is missing 2001-2003 and that overall, fledgling counts seem to be declining in recent years.\n\n\n\n\n\n\nTip\n\n\n\nThe setting position = \"dodge\" instructs geom_bar to put the bars side-by-side rather than stacking them.\n\n\n\n\n19.1.8 Visualization Design\nDesigning high-quality visualizations goes beyond just mastering which R functions to call. You also need to think carefully about what kind of data you have and what message you want to convey. This section provides a few guidelines.\nThe first step in data visualization is choosing an appropriate kind of plot. Here are some suggestions (not rules):\n\n\n\nFeature 1\nFeature 2\nPlot\n\n\n\n\ncategorical\n\nbar, dot\n\n\ncategorical\ncategorical\nbar, dot, mosaic\n\n\nnumerical\n\nbox, density, histogram\n\n\nnumerical\ncategorical\nbox, density, ridge\n\n\nnumerical\nnumerical\nline, scatter, smooth scatter\n\n\n\nIf you want to add a:\n\n3rd numerical feature, use it to change point/line sizes.\n3rd categorical feature, use it to change point/line styles.\n4th categorical feature, use side-by-side plots.\n\nOnce you’ve selected a plot, here are some rules you should almost always follow:\n\nAlways add a title and axis labels. These should be in plain English, not variable names!\nSpecify units after the axis label if the axis has units. For instance, “Height (ft)”.\nDon’t forget that many people are colorblind! Also, plots are often printed in black and white. Use point and line styles to distinguish groups; color is optional.\nAdd a legend whenever you’ve used more than one point or line style.\nAlways write a few sentences explaining what the plot reveals. Don’t describe the plot, because the reader can just look at it. Instead, explain what they can learn from the plot and point out important details that are easily overlooked.\nSometimes points get plotted on top of each other. This is called overplotting. Plots with a lot of overplotting can be hard to read and can even misrepresent the data by hiding how many points are present. Use a two-dimensional density plot or jitter the points to deal with overplotting.\nFor side-by-side plots, use the same axis scales for both plots so that comparing them is not deceptive.\n\n\n\n\n\n\n\nNoteSee also\n\n\n\nVisualization design is a deep topic, and whole books have been written about it. One resource where you can learn more is DataLab’s Principles of Data Visualization Workshop Reader.",
    "crumbs": [
      "Week 5",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/week06/date-processing.html",
    "href": "chapters/week06/date-processing.html",
    "title": "20  Dates & Times",
    "section": "",
    "text": "20.1 The lubridate Package\nGiven a dataset with dates or times, you might want to:\nYou can do all of these things and more with R, but only if your dates and times are represented by appropriate data types.\nAs explained in Section 14.3, we recommend the Tidyverse packages for working with dates and times over other packages or R’s built-in functions. There are two:\nThis chapter only covers lubridate, since it’s more useful in most situations. The package has detailed documentation and a cheatsheet.\nYou’ll have to install the package if you haven’t already, and then load it:\n# install.packages(\"lubridate\")\nlibrary(\"lubridate\")\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dates & Times</span>"
    ]
  },
  {
    "objectID": "chapters/week06/date-processing.html#the-lubridate-package",
    "href": "chapters/week06/date-processing.html#the-lubridate-package",
    "title": "20  Dates & Times",
    "section": "",
    "text": "lubridate, the primary package for working with dates and times\nhms, a package specifically for working with time durations\n\n\n\n\n\n\n\n\nNote\n\n\n\nA relatively new package, clock, tries to solve some problems with the Date class people have identified over the years. The package is in the r-lib collection of packages, which provide low-level functionality complementary to the Tidyverse. Eventually, it may be preferable to use the classes in clock rather than the Date class, but for now, the Date class is still suitable for most tasks.",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dates & Times</span>"
    ]
  },
  {
    "objectID": "chapters/week06/date-processing.html#parsing-dates",
    "href": "chapters/week06/date-processing.html#parsing-dates",
    "title": "20  Dates & Times",
    "section": "20.2 Parsing Dates",
    "text": "20.2 Parsing Dates\nMany popular file formats for data, such as CSV, do not store metadata about which values are dates and times, so these values get typed as strings. Dates and times extracted from text are also naturally strings.\nThe first step to working with dates and times typed as strings is to parse them: break them down into their components and cast them to more suitable data types. To demonstrate parsing dates, consider a vector of date strings:\n\ndate_strings = c(\"Jan 10, 2021\", \"Sep 3, 2018\", \"Feb 28, 1982\")\ndate_strings\n\n[1] \"Jan 10, 2021\" \"Sep 3, 2018\"  \"Feb 28, 1982\"\n\n\nYou can tell that these are dates, but as far as R is concerned, they’re text.\nThe lubridate package provides a variety of functions to automatically parse strings into date or time objects that R understands. These functions are named with one letter per component of the date or time. The order of the letters must match the order of the components in the string you want to parse.\nThe example strings have the month (m), then the day (d), and then the year (y), so you can use the mdy function to parse them automatically:\n\ndates = mdy(date_strings)\ndates\n\n[1] \"2021-01-10\" \"2018-09-03\" \"1982-02-28\"\n\nclass(dates)\n\n[1] \"Date\"\n\n\nNotice that R prints the dates differently now. Thanks to the mdy function, they now have class Date, one of R’s built-in classes for representing dates. R recognizes that the dates are in fact dates, so they’re ready to use in an analysis.\nThere is a complete list of the automatic parsing functions in the lubridate documentation.\nOccasionally, a date or time string may have a format that lubridate can’t parse automatically. In that case, you can use the fast_strptime function to describe the format in detail.\nAt a minimum, the function requires two arguments: a vector of strings to parse and a format string. The format string describes the format of the dates. In a format string, a percent sign % followed by a character is called a specifier and represents a component of a data or time. Here are some useful specifiers:\n\n\n\nSpecification\nDescription\n2015-01-29 21:32:55\n\n\n\n\n%Y\n4-digit year\n2015\n\n\n%m\n2-digit month\n01\n\n\n%d\n2-digit day\n29\n\n\n%H\n2-digit hour\n21\n\n\n%M\n2-digit minute\n32\n\n\n%S\n2-digit second\n55\n\n\n%%\nliteral %\n%\n\n\n%y\n2-digit year\n15\n\n\n%B\nfull month name\nJanuary\n\n\n%b\nshort month name\nJan\n\n\n\nOther characters in the format string don’t have any special meaning. Write the format string so that it matches the format of the dates you want to parse.\n\n\n\n\n\n\nNote\n\n\n\nMost programming languages use format strings to parse and format dates and times. There isn’t a standard, but the idea seems to have originated with the strptime and strftime functions in the C programming language.\nYou can find a complete list of specifiers for lubridate in ?fast_strptime.\n\n\nTo demonstrate fast_strptime, let’s try parsing an unusual time format:\n\ntime_string = \"6 minutes, 32 seconds after 10 o'clock\"\ntime = fast_strptime(time_string, \"%M minutes, %S seconds after %H o'clock\")\ntime\n\n[1] \"0-01-01 10:06:32 UTC\"\n\nclass(time)\n\n[1] \"POSIXlt\" \"POSIXt\" \n\n\nR represents datetimes (which combine a date and a time) with the classes POSIXlt and POSIXct. There’s no built-in class to represent times alone, which is why the result in the example above includes a date.\n\n\n\n\n\n\nNote\n\n\n\nInternally, a POSIXlt object is a list with elements to store different date and time components. On the other hand, a POSIXct object is a single floating point number (type double).\nIf you want to store your time data in a data frame, use POSIXct objects, since data frames don’t work well with columns of lists.\nYou can control whether fast_strptime returns a POSIXlt or POSIXct object by setting the lt parameter to TRUE or FALSE:\n\ntime_ct = fast_strptime(time_string, \"%M minutes, %S seconds after %H o'clock\",\n  lt = FALSE)\n\nclass(time_ct)\n\n[1] \"POSIXct\" \"POSIXt\"",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dates & Times</span>"
    ]
  },
  {
    "objectID": "chapters/week06/date-processing.html#creating-dates",
    "href": "chapters/week06/date-processing.html#creating-dates",
    "title": "20  Dates & Times",
    "section": "20.3 Creating Dates",
    "text": "20.3 Creating Dates\nAnother common task is combining the numeric components of a date or time into a single object. You can use the make_date and make_datetime functions to do this. The parameters are named for the different components. For example:\n\nevents = make_datetime(\n  year = c(2023, 2002),\n  month = c(1, 8),\n  day = c(10, 16),\n  hour = c(8, 14),\n  min = c(3, 59)\n)\nevents\n\n[1] \"2023-01-10 08:03:00 UTC\" \"2002-08-16 14:59:00 UTC\"\n\n\nThese functions are vectorized, so you can use them to combine the components of many dates or times at once. They’re especially useful for reconstructing dates and times from tabular datasets where each component is stored in a separate column.",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dates & Times</span>"
    ]
  },
  {
    "objectID": "chapters/week06/date-processing.html#extracting-components",
    "href": "chapters/week06/date-processing.html#extracting-components",
    "title": "20  Dates & Times",
    "section": "20.4 Extracting Components",
    "text": "20.4 Extracting Components\nOnce you have dates or times, you might want to extract components from them. For instance, suppose we want to get the years from the dates we parsed earlier. You can use lubridate functions to get or set the components. These functions usually have the same name as the component. For instance, the year function gets the year:\n\nyear(dates)\n\n[1] 2021 2018 1982\n\n\nSimilarly, the month function gets the month:\n\nmonth(dates)\n\n[1] 1 9 2\n\n\nSee the lubridate documentation for even more details about what you can do.",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dates & Times</span>"
    ]
  },
  {
    "objectID": "chapters/week06/date-processing.html#durations-periods",
    "href": "chapters/week06/date-processing.html#durations-periods",
    "title": "20  Dates & Times",
    "section": "20.5 Durations & Periods",
    "text": "20.5 Durations & Periods\nOccasionally, you might need to adjust dates or times by adding an offset. For example, suppose we want to add 30 days to the dates from earlier. The duration function creates a duration (class Duration), which represents a fixed amount of time. So to add 30 days:\n\ndates + duration(30, \"days\")\n\n[1] \"2021-02-09\" \"2018-10-03\" \"1982-03-30\"\n\n\nWhat if we want to add 1 month to the dates instead of 30 days? The length of a month varies. The duration function gives the average amount of time in a month, but that won’t be correct for some months. The solution is to use a period (class Period) rather than a duration. The period function creates a period. So to add 1 month (of variable length):\n\ndates + period(1, \"month\")\n\n[1] \"2021-02-10\" \"2018-10-03\" \"1982-03-28\"\n\n\nThis increments each date by exactly 1 month, regardless of how many days that is.\n\n\n\n\n\n\nTip\n\n\n\nThere are helper functions for common periods named after the associated unit of time. For example, another way add a 1-month period to the dates is with the months function:\n\ndates + months(1)\n\n[1] \"2021-02-10\" \"2018-10-03\" \"1982-03-28\"\n\n\nSimilarly, there are helper functions for common durations. These are also named after the associated unit of time, but always begin with the prefix d. So to add a 30-day duration to the dates:\n\ndates + ddays(30)\n\n[1] \"2021-02-09\" \"2018-10-03\" \"1982-03-30\"",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dates & Times</span>"
    ]
  },
  {
    "objectID": "chapters/week06/date-processing.html#case-study-ca-parks-recreation-fleet",
    "href": "chapters/week06/date-processing.html#case-study-ca-parks-recreation-fleet",
    "title": "20  Dates & Times",
    "section": "20.6 Case Study: CA Parks & Recreation Fleet",
    "text": "20.6 Case Study: CA Parks & Recreation Fleet\nThe government of California publishes data about its fleet of vehicles on the California Open Data portal. As of March 2025, the dataset includes all non-confidential vehicles owned by agencies from 2015-2023. We’ll use a subset of this data to compute how many vehicles the CA Department of Parks and Recreation purchased each year from 2019 to 2023. The dataset is published as a messy CSV, so we’ll need to do some cleaning and parse the dates in order to use it.\n\n\n\n\n\n\nImportant\n\n\n\nClick here to download the CA Parks & Recreation Fleet dataset.\nIf you haven’t already, we recommend you create a directory for this workshop. In your workshop directory, create a data/ subdirectory. Download and save the dataset in the data/ subdirectory.\n\n\n\n\n\n\n\n\nNoteDocumentation for the CA Parks & Recreation Fleet Dataset\n\n\n\n\n\nEach row in the dataset contains measurements from one vehicle-year combination.\nClick here to download the documentation for the columns.\nThis dataset is a subset of the much larger CA State Fleet dataset.\n\n\n\nTo get started, read the data set from wherever you saved it:\n\nfleet = read.csv(\"data/2015-2023_ca_parks_fleet.csv\")\nhead(fleet)\n\n                               agency report_year disposed equipment_number\n1 Parks and Recreation, Department of        2015       No            93462\n2 Parks and Recreation, Department of        2015       No           100380\n3 Parks and Recreation, Department of        2015      Yes           117844\n4 Parks and Recreation, Department of        2015       No           115503\n5 Parks and Recreation, Department of        2015       No             3554\n6 Parks and Recreation, Department of        2015       No           D12628\n  asset_category model_year               make_model postal_code\n1         Ground       1976              TUCKER 1643       96142\n2         Ground       1979        FLEETWOOD HOMETTE       94954\n3         Ground       2001          MACHETE CHIPPER       91302\n4         Ground       1987       KIT OFFICE TRAILER       95430\n5         Ground       1960 MAVERICK MFG CORP WELDER       95430\n6         Ground       1991          GEARMORE GE200B       93449\n              asset_type weight_class passenger_vehicle payload_rating\n1 Construction Equipment                             No             NA\n2     Non-Self Propelled                            Yes             NA\n3     Non-Self Propelled                            Yes             NA\n4     Non-Self Propelled                            Yes             NA\n5     Non-Self Propelled   Light Duty               Yes             NA\n6     Non-Self Propelled                            Yes             NA\n  shipping_weight wheel_type tire_size fuel_type engine_configuration\n1              NA                         Diesel            Dedicated\n2              NA                                                    \n3              NA                       Gasoline            Dedicated\n4              NA                                                    \n5              NA                                                    \n6              NA                                                    \n  emissions_type_code              primary_application secondary_application\n1                  NA Maintenance of public facilities                      \n2                  NA Maintenance of public facilities                      \n3                  NA Maintenance of public facilities                      \n4                  NA Maintenance of public facilities                      \n5                  NA Maintenance of public facilities                      \n6                  NA Maintenance of public facilities                      \n  acquisition_delivery_date suv_justification pickup_4_by_4_justification\n1                  1/1/2001                                              \n2                  1/1/1979                                              \n3                 1/31/2001                                              \n4                  1/1/2001                                              \n5                  1/1/2001                                              \n6                  1/1/1991                                              \n  acquisition_method purchase_price annual_lease_rate acquisition_mileage\n1           Purchase           2000                NA               Null \n2           Purchase          11483                NA               Null \n3           Purchase          26870                NA               Null \n4           Purchase          10197                NA               Null \n5           Purchase            500                NA               Null \n6           Purchase           2500                NA               Null \n  disposition_date transferred_to disposition_method disposition_reason\n1                                                                      \n2                                                                      \n3         6/7/2018                                                     \n4                                                                      \n5                                                                      \n6                                                                      \n  disposition_mileage disposition_sold_amount total_miles\n1                  NA                      NA          NA\n2                  NA                      NA          NA\n3                  NA                      NA          NA\n4                  NA                      NA          NA\n5                  NA                      NA          NA\n6                  NA                      NA          NA\n\n\nSince we want to compute the number of acquisitions each year, we’ll focus on two columns: acquisition_method and acquisition_delivery_date. The acquisition_method column indicates whether the vehicle was purchased, donated, transferred from a different agency, or something else. We’re only interested in purchases, so we can filter the others out:\n\nlibrary(\"dplyr\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nfleet = filter(fleet, acquisition_method == \"Purchase\")\n\nIn order to determine how many vehicles were purchased each year, we need to extract the year from acquisition_delivery_date. Let’s take a look at this column:\n\nhead(fleet$acquisition_delivery_date)\n\n[1] \"1/1/2001\"  \"1/1/1979\"  \"1/31/2001\" \"1/1/2001\"  \"1/1/2001\"  \"1/1/1991\" \n\n\nIt looks like the dates are in month-day-year format, so we can use lubridate’s mdy function:\n\ndates = mdy(fleet$acquisition_delivery_date)\n\nWarning: 4149 failed to parse.\n\n\nUh-oh. According to the warning, lubridate couldn’t parse some of the dates. This means they probably don’t follow the month-day-year format. When lubridate can’t parse a date, it returns a missing value. We can use this to figure out what the dates look like:\n\nis_bad_date = is.na(dates)\nbad_dates = fleet$acquisition_delivery_date[is_bad_date]\nhead(bad_dates, 20)\n\n [1] \"\"      \"\"      \"\"      \"\"      \"\"      \"\"      \"\"      \"\"      \"44832\"\n[10] \"44462\" \"42270\" \"44454\" \"42262\" \"44088\" \"43719\" \"43719\" \"43719\" \"43718\"\n[19] \"41892\" \"44078\"\n\n\nInstead of month-day-year dates, they’re numbers like 44832, 44462, and 43451. These numbers probably seem inscrutable, but there is an explanation for them: Microsoft Excel, perhaps the most popular tool for working with tabular data, stores dates by counting days from 31 December 1899. So 1 is 1 January 1900, 32 is 1 February 1900, and so on. Unfortunately, the Excel developers incorrectly assumed 1900 was a leap year, so all of the counts were off by 1 after 28 February 1900. Most modern Excel-compatible spreadsheet programs fix this by counting from 30 December 1899 rather than 31 December, so that only dates before 28 February have different numbers.\nWe can convert the numbers in the bad_dates variable into dates by treating them as offsets to 30 December 1899:\n\nstart_date = make_date(1899, 12, 30)\n\nday_offsets = as.numeric(bad_dates)\nfixed_dates = start_date + days(day_offsets)\nhead(fixed_dates, 20)\n\n [1] NA           NA           NA           NA           NA          \n [6] NA           NA           NA           \"2022-09-28\" \"2021-09-23\"\n[11] \"2015-09-23\" \"2021-09-15\" \"2015-09-15\" \"2020-09-14\" \"2019-09-11\"\n[16] \"2019-09-11\" \"2019-09-11\" \"2019-09-10\" \"2014-09-10\" \"2020-09-04\"\n\n\nWe can insert these back into the dates vector at their original positions. Then we can replace the acquisition_delivery_date column with dates:\n\ndates[is_bad_date] = fixed_dates\n\nfleet$acquisition_delivery_date = dates\n\nWith the dates in hand, we can compute the number of purchases from 2019 to 2023. Since each row in the dataset represents a vehicle-year, and we just want to count vehicles, we must first get a subset of unique vehicles. We can do this by grouping on the equipment_number column, which is a unique identifier for each vehicle, and getting the first row in each group:\n\nby_vehicle = group_by(fleet, equipment_number)\nvehicles = filter(by_vehicle, row_number() == 1)\n\nWe can compute the number of purchases by extracting the years with lubridate’s year function and then counting them with the table function:\n\ntable(year(vehicles$acquisition_delivery_date))\n\n\n1950 1958 1961 1964 1965 1966 1967 1968 1970 1971 1972 1973 1974 1975 1976 1977 \n   1    2    1    2    3    2    3    3    1    1    3    5    3    6    9    3 \n1978 1979 1980 1981 1982 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 \n  10   10    4    7    5    6   12   25   21   28   54   39   17   11   46   78 \n1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 \n  79   74  119  111  164  235  741  140  220   88  233  118  398  342  388  122 \n2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 \n  97  219  142  139  263  237  219  206  345  254  189  227  192 \n\n\nIt’s unclear whether the dataset contains complete records for 2023, so the count for that year might be too small. It’s also likely that the agency purchases vehicles months or years in advance of their delivery, so these counts are more accurately described as deliveries rather than purchases.",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dates & Times</span>"
    ]
  },
  {
    "objectID": "chapters/week06/date-processing.html#case-study-ocean-temperatures",
    "href": "chapters/week06/date-processing.html#case-study-ocean-temperatures",
    "title": "20  Dates & Times",
    "section": "20.7 Case Study: Ocean Temperatures",
    "text": "20.7 Case Study: Ocean Temperatures\nThe U.S. National Oceanic and Atmospheric Administration (NOAA) publishes ocean temperature data collected by sensor buoys off the coast on the National Data Buoy Center (NDBC) website. California also has many sensors collecting ocean temperature data that are not administered by the federal government. Data from these is published on the California Ocean Observing Systems (CALOOS) Data portal.\nSuppose you’re a researcher who wants to combine ocean temperature data from both sources to use in R. Both publish the data in comma-separated value (CSV) format, but record dates, times, and temperatures differently. Thus you need to be careful that the dates and times are parsed correctly.\n\n\n\n\n\n\nImportant\n\n\n\nDownload these two 2021 datasets:\n\n2021_noaa-ndbc_46013.txt, from NOAA buoy 46013, off the coast of Bodega Bay (DOWNLOAD)(source)\n2021_ucdavis_bml_wts.csv, from the UC Davis Bodega Bay Marine Laboratory’s sensors (DOWNLOAD)(source)\n\n\n\nThe NOAA data has a fixed-width format, which means each column has a fixed width in characters over all rows. The readr package provides a function read_fwf that can automatically guess the column widths and read the data into a data frame. The column names appear in the first row and column units appear in the second row, so read those rows separately:\n\n# install.packages(\"readr\")\nlibrary(\"readr\")\n\nnoaa_path = \"data/ocean_data/2021_noaa-ndbc_46013.txt\"\nnoaa_headers = read_fwf(noaa_path, n_max = 2, guess_max = 1)\n\nRows: 2 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\n\nchr (18): X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnoaa = read_fwf(noaa_path, skip = 2)\n\nRows: 3323 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\n\nchr  (4): X2, X3, X4, X5\ndbl (14): X1, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(noaa) = as.character(noaa_headers[1, ])\nnames(noaa)[1] = \"YY\"\n\nThe dates and times for the observations are separated into component columns, and the read_fwf function does not convert some of these to numbers automatically. You can use as.numeric to convert them to numbers:\n\ncols = 2:5\nnoaa[cols] = lapply(noaa[cols], as.numeric)\n\nFinally, use the make_datetime function to combine the components into date-time objects:\n\nnoaa_dt = make_datetime(year = noaa$YY, month = noaa$MM, day = noaa$DD,\n  hour = noaa$hh, min = noaa$mm)\nnoaa$date = noaa_dt\nhead(noaa_dt)\n\n[1] \"2021-01-01 00:00:00 UTC\" \"2021-01-01 00:10:00 UTC\"\n[3] \"2021-01-01 00:20:00 UTC\" \"2021-01-01 00:30:00 UTC\"\n[5] \"2021-01-01 00:40:00 UTC\" \"2021-01-01 00:50:00 UTC\"\n\n\nThat takes care of the dates in the NOAA data.\nThe Bodega Marine Lab data is CSV format, which you can read with read.csv or the readr package’s read_csv function. The latter is faster and usually better at guessing column types. The column names appear in the first row and the column units appear in the second row. The read_csv function handles the names automatically, but you’ll have to remove the unit row as a separate step:\n\nbml = read_csv(\"data/ocean_data/2021_ucdavis_bml_wts.csv\")\n\nRows: 87283 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): time, sea_water_temperature, z\ndbl (1): sea_water_temperature_qc_agg\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbml = bml[-1, ]\n\nThe dates and times of the observations were loaded as strings. You can use lubridate’s ymd_hms function to automatically parse them:\n\nbml_dt = ymd_hms(bml$time)\nbml$date = bml_dt\nhead(bml_dt)\n\n[1] \"2020-12-31 09:06:00 UTC\" \"2020-12-31 09:12:00 UTC\"\n[3] \"2020-12-31 09:18:00 UTC\" \"2020-12-31 09:24:00 UTC\"\n[5] \"2020-12-31 09:30:00 UTC\" \"2020-12-31 09:36:00 UTC\"\n\n\nNow you have date and time objects for both datasets, so you can combine the two. For example, you could extract the date and water temperature columns from each, create a new column identifying the data source, and then row-bind the datasets together.",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Dates & Times</span>"
    ]
  },
  {
    "objectID": "chapters/week06/reshaping-data.html",
    "href": "chapters/week06/reshaping-data.html",
    "title": "21  Reshaping Data",
    "section": "",
    "text": "21.1 The tidyr Package\nThe structure of a dataset—its shape and organization—has enormous influence on how difficult it will be to analyze, so making structural changes is an important part of the cleaning process. This chapter explains how to reshape untidy data into tidy data (Section 11.2). While reshaping can seem tricky at first, making sure your dataset has the right structure before you begin analysis saves time and frustration in the long run.\nThe tidyr package provides functions to reshape tabular datasets. It also provides examples of tidy and untidy datasets. Like most Tidyverse packages, it comes with detailed documentation and a cheatsheet.\nAs usual, install the package if you haven’t already, and then load it:\n# install.packages(\"tidyr\")\nlibrary(\"tidyr\")",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "chapters/week06/reshaping-data.html#an-untidy-dataset",
    "href": "chapters/week06/reshaping-data.html#an-untidy-dataset",
    "title": "21  Reshaping Data",
    "section": "21.2 An Untidy Dataset",
    "text": "21.2 An Untidy Dataset\nThe City of Davis has two bike counters: one is at the intersection of 3rd Street and University Avenue (the 3rd Street bike obelisk) and the other is at the intersection of Loyola Drive and Pole Line Road. The City publishes data from the bike counters online. DataLab combined the City’s 2020 bike counts, aggregated to the day level, with precipitation and wind data from the U.S. National Oceanic and Atmospheric Administration’s weather station at Sacramento Metropolitan Airport (this was the nearest weather station with complete records for 2020). We’ll use this dataset to demonstrate how to transform untidy data.\n\n\n\n\n\n\nImportant\n\n\n\nClick here to download the 2020 Davis bike counts dataset.\nIf you haven’t already, we recommend you create a directory for this workshop. In your workshop directory, create a data/ subdirectory. Download and save the dataset in the data/ subdirectory.\n\n\n\n\n\n\n\n\nNoteDocumentation for the 2020 Davis Bike Counts Dataset\n\n\n\n\n\nEach row in the dataset contains measurements from one date-variable combination.\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ndate\nThe date of measurement\n\n\nvariable\nWhat was measured: third and loyola are bike counts, prcp is total precipitation in millimeters, awnd is average daily wind speed in meters per second\n\n\nvalue\nThe measured value\n\n\n\nThe source for the bike counts is the City of Davis’ Bike and Pedestrian Statistics web page. The source for the total precipitation and average wind speed is NOAA’s weather station at Sacramento Metropolitan Airport.\n\n\n\nThe dataset is saved in an RDS file, which you can use the built-in readRDS function to read:\n\nbikes = readRDS(\"data/2020_davis_bikes.rds\")\nhead(bikes)\n\n        date variable value\n1 2020-01-01    third 332.0\n2 2020-01-01   loyola 181.0\n3 2020-01-01     awnd   1.7\n4 2020-01-01     prcp   0.0\n5 2020-01-02    third 357.0\n6 2020-01-02   loyola 401.0\n\n\nThis data is not tidy, because it breaks rule 1. The value column contains many different features—they even have different units! Soon we’ll reshape the dataset to make it tidy.\nBefore you reshape a dataset, you should also think about what role each column serves:\n\nIdentifiers (or indexes) are labels that distinguish observations from one another. They’re often but not always categorical. Examples include names or identification numbers, treatment groups, and dates or times. In the bike counts dataset, the date column is an identifier.\nMeasurements are the values collected for each observation and typically the values of research interest. In the bike counts dataset, the value column is a measurement.\n\nA clear understanding of which columns are identifiers and which are measurements makes it easier to write the code to reshape.",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "chapters/week06/reshaping-data.html#sec-rows-into-columns",
    "href": "chapters/week06/reshaping-data.html#sec-rows-into-columns",
    "title": "21  Reshaping Data",
    "section": "21.3 Rows into Columns",
    "text": "21.3 Rows into Columns\nIn order to make the Davis bike counts data tidy, the measurements in the value column need to be moved into two separate columns, one for each of the categories in the variable column.\nYou can use the pivot_wider function to pivot a data frame, creating new columns from values in the rows. This makes the data frame wider (and shorter). Let’s pivot the bikes data frame on the variable column to create four new columns filled with values from the values column.\nThe pivot_wider function’s most important parameters are:\n\nvalues_from – The column(s) that contains values for the new columns.\nnames_from – The column that contains names for the new columns.\nid_cols – The identifier columns, which are not pivoted. This defaults to all columns except those in values_from and names_from.\n\nHere’s how to use the function to make bikes tidy:\n\nbikes2 = pivot_wider(bikes, values_from = value, names_from = variable)\nhead(bikes2)\n\n# A tibble: 6 × 5\n  date       third loyola  awnd  prcp\n  &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2020-01-01   332    181   1.7     0\n2 2020-01-02   357    401   3       0\n3 2020-01-03   426    504   1       0\n4 2020-01-04   337    475   3.2     2\n5 2020-01-05   370    563   2.5     0\n6 2020-01-06   852   1676   1.7     0\n\n\nThe function automatically removes values from the date column as needed to maintain the original correspondence with the pivoted values.\nThe new bikes2 data frame contains all of the data from bikes, but now the measurements for each date share a row. In other words, the observational units for bikes2 are dates, which is convenient for investigating how individual features change over time, as well as making same-time comparisons between features. To illustrate this, we can use ggplot2 to make a scatter plot of the bike counts at 3rd Street against the counts at Loyola Drive:\n\nlibrary(\"ggplot2\")\n\nggplot(bikes2) +\n  aes(x = third, y = loyola) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe plot shows that Loyola Drive occasionally has days with much higher traffic than 3rd Street, but it’s difficult to tell whether 3rd or Loyola is typically busier (that is, whether there are more points above or below the \\(y = x\\) line).",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "chapters/week06/reshaping-data.html#sec-columns-into-rows",
    "href": "chapters/week06/reshaping-data.html#sec-columns-into-rows",
    "title": "21  Reshaping Data",
    "section": "21.4 Columns into Rows",
    "text": "21.4 Columns into Rows\nSuppose we want to try to get a better answer to whether Loyola or 3rd tends to be busier. One way we can do it is by making a line plot with the counts for each site over time. This way we’re treating each site as a group within the data and making a comparison between groups.\nComparing groups in a data frame is generally easier when each row corresponds to an observation from one group. Let’s reshape the bikes2 data frame so that the observational units are date-site combinations. To do this, the third and loyola columns need to be transformed into two new columns: one for measurements (the counts) and one for identifiers (the sites). It might help to visualize this as stacking the two separate columns third and loyola together, one on top of the other, and then adding a second column with the corresponding site names.\nYou can use the pivot_longer function to unpivot a data frame, creating new rows from values in the columns. This is the inverse of a pivot. It makes the data frame longer (and narrower). We’ll unpivot the bikes2 data frame on the third and loyola columns.\nThe pivot_longer function’s parameters are:\n\ncols – The columns to stack into a new column; the names of these columns will also go into a new column.\nvalues_to – Name(s) for the new measurement column(s)\nnames_to – Name(s) for the new identifier column(s)\n\nThe code to unpivot bikes2 is:\n\nbikes3 = pivot_longer(\n    bikes2,\n    cols = c(third, loyola),\n    values_to = \"count\",\n    names_to = \"site\"\n)\nhead(bikes3)\n\n# A tibble: 6 × 5\n  date        awnd  prcp site   count\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 2020-01-01   1.7     0 third    332\n2 2020-01-01   1.7     0 loyola   181\n3 2020-01-02   3       0 third    357\n4 2020-01-02   3       0 loyola   401\n5 2020-01-03   1       0 third    426\n6 2020-01-03   1       0 loyola   504\n\n\nFor bikes3, the observational units are date-site combinations, as planned. This is convenient for comparing the two sites to each other with statistics and visualizations. We can use ggplot2 to make a line plot of the counts for the two sites:\n\nggplot(bikes3) +\n  aes(x = date, y = count, color = site) +\n  geom_line()\n\n\n\n\n\n\n\n\nFrom this plot, we can see that Loyola Drive was generally busier for the first 3 months of 2020. Traffic dropped at both sites in mid-March, probably due to the COVID-19 pandemic. The drop was sharper at Loyola Drive than 3rd Street, so 3rd Street was generally busier for the remaining 9 months of 2020.\n\n\n\n\n\n\nNote\n\n\n\nWe didn’t use prcp and awnd (and they don’t differ between sites anyway). If we dropped them, the resulting data frame would have the same observational units as bikes3, but would also be a subset of the original bikes data frame (albeit with different column names).",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "chapters/week06/reshaping-data.html#case-study-smart-ridership",
    "href": "chapters/week06/reshaping-data.html#case-study-smart-ridership",
    "title": "21  Reshaping Data",
    "section": "21.5 Case Study: SMART Ridership",
    "text": "21.5 Case Study: SMART Ridership\nSonoma-Marin Area Rail Transit (SMART) is a relatively new single-line passenger rail service between the San Francisco Bay and Santa Rosa. They publish data about monthly ridership online, but the format is slightly messy. Let’s clean and reshape the data in order to make a plot of ridership over time.\n\n\n\n\n\n\nImportant\n\n\n\nClick here to download the SMART Ridership dataset (version 2026-01).\nIf you haven’t already, we recommend you create a directory for this workshop. In your workshop directory, create a data/ subdirectory. Download and save the dataset in the data/ subdirectory.\n\n\n\n\n\n\n\n\nNoteDocumentation for the SMART Ridership Dataset\n\n\n\n\n\nThe source for the dataset is the SMART Ridership Reports web page.\n\n\n\nThe dataset is saved as a Microsoft Excel file. Before reading an Excel file, it’s a good idea to manually inspect it with spreadsheet software to figure out how the data are organized. The SMART dataset contains two tables on the left side of the first sheet: one for total monthly ridership and one for average weekday ridership (by month).\nThe readxl package provides functions to read data from Excel files. Install the package if you don’t already have it installed, and then load it:\n\n# install.packages(\"readxl\")\nlibrary(\"readxl\")\n\nWe can use the package’s read_excel function to read sheets from an Excel file. It has a parameter range to control which cells it reads.\nLet’s focus on the total monthly ridership table, which occupies cells B4 to K16:\n\nsmart = read_excel(\"data/2026-01_smart_ridership.xlsx\", range = \"B4:K16\")\nhead(smart)\n\n# A tibble: 6 × 10\n  Month FY18   FY19   FY20  FY21   FY22  FY23  FY24  FY25   FY26\n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Jul   -     63864 62851   9427 24627  43752 65779 88022 132805\n2 Aug   54484 74384 65352   8703 25020  48278 72171 91894 125407\n3 Sep   65019 62314 62974   8910 27967  49134 68506 92834 121621\n4 Oct   57453 65492 57222   9851 26998. 59322 70807 96599 120127\n5 Nov   56125 52774 64966   8145 26575  51383 65445 78550  99471\n6 Dec   56425 51670 58199.  7414 24050  47606 66684 76624 106975\n\n\nThe dataset needs to be cleaned. The FY18 column uses a hyphen - to indicate a missing value and has the wrong data type. We can use indexing to replace the hyphen with a missing value and then convert the column to an appropriate type with the as.numeric function:\n\nsmart$FY18[smart$FY18 == \"-\"] = NA\nsmart$FY18 = as.numeric(smart$FY18)\nhead(smart)\n\n# A tibble: 6 × 10\n  Month  FY18  FY19   FY20  FY21   FY22  FY23  FY24  FY25   FY26\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Jul      NA 63864 62851   9427 24627  43752 65779 88022 132805\n2 Aug   54484 74384 65352   8703 25020  48278 72171 91894 125407\n3 Sep   65019 62314 62974   8910 27967  49134 68506 92834 121621\n4 Oct   57453 65492 57222   9851 26998. 59322 70807 96599 120127\n5 Nov   56125 52774 64966   8145 26575  51383 65445 78550  99471\n6 Dec   56425 51670 58199.  7414 24050  47606 66684 76624 106975\n\n\nThere’s still a lot of cleaning to do. The identifiers in this dataset are the months and years, and they’re split between the row and column names. Each row contains data from several different years, so the dataset is not tidy. In addition, the years are indicated in fiscal years (FY), which begin in July rather than January, so some of the years need to be adjusted.\nTo make the dataset tidy, it needs to be reshaped so that the values in the various fiscal year columns are all in one column. In other words, the dataset needs to be unpivoted (Section 21.4) on all of the FY columns. The result of the pivot will be easier to understand if we rename the columns as their years first. Here’s one way to do that:\n\nnames(smart)[-1] = 2018:2026\nhead(smart)\n\n# A tibble: 6 × 10\n  Month `2018` `2019` `2020` `2021` `2022` `2023` `2024` `2025` `2026`\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Jul       NA  63864 62851    9427 24627   43752  65779  88022 132805\n2 Aug    54484  74384 65352    8703 25020   48278  72171  91894 125407\n3 Sep    65019  62314 62974    8910 27967   49134  68506  92834 121621\n4 Oct    57453  65492 57222    9851 26998.  59322  70807  96599 120127\n5 Nov    56125  52774 64966    8145 26575   51383  65445  78550  99471\n6 Dec    56425  51670 58199.   7414 24050   47606  66684  76624 106975\n\n\nNext, we use pivot_longer to pivot the dataset:\n\nsmart = pivot_longer(\n  smart,\n  cols = -Month,\n  values_to = \"count\",\n  names_to = \"fiscal_year\"\n)\nhead(smart)\n\n# A tibble: 6 × 3\n  Month fiscal_year count\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 Jul   2018           NA\n2 Jul   2019        63864\n3 Jul   2020        62851\n4 Jul   2021         9427\n5 Jul   2022        24627\n6 Jul   2023        43752\n\n\nIn order to use the months and years in the data, we need to convert them to dates. As a first step towards this, we’ll cast the values in the new fiscal_year column to integers:\n\nsmart$fiscal_year = as.numeric(smart$fiscal_year)\nhead(smart)\n\n# A tibble: 6 × 3\n  Month fiscal_year count\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Jul          2018    NA\n2 Jul          2019 63864\n3 Jul          2020 62851\n4 Jul          2021  9427\n5 Jul          2022 24627\n6 Jul          2023 43752\n\n\nNext, we can use the lubridate package’s fast_strptime and month functions to create a new of column month numbers:\n\nlibrary(\"lubridate\")\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nmonth_num = month(fast_strptime(smart$Month, \"%m\"))\n\nNow we need to transform the fiscal years in the fiscal_year column into calendar years. A SMART fiscal year extends from July to the following June and is named after the calendar year at the end of the fiscal year. So from July to December, the calendar year is the fiscal year minus 1. We can use the built-in ifelse function to subtract either 1 or 0 depending on whether the condition month_num &gt;= 7 is true or false:\n\ncal_year = smart$fiscal_year - ifelse(month_num &gt;= 7, 1, 0)\n\n\n\n\n\n\n\nNote\n\n\n\nAlternatively, we can compute the calendar year by taking advantage of implicit coercion. The logical value FALSE corresponds to 0 and TRUE corresponds to 1, so we can just subtract the condition to get the calendar year:\n\ncal_year = smart$fiscal_year - (month_num &gt;= 7)\n\n\n\nFinally, we can use the lubridate package’s make_date function to construct dates from the cal_year and month_num variables:\n\nsmart$date = make_date(year = cal_year, month = month_num)\n\nWith the dates in the date column and the counts in the count column, we have everything we need to make a plot of SMART ridership over time. We can use ggplot2 to make the plot:\n\nggplot(smart) +\n  aes(x = date, y = count) +\n  geom_line()\n\nWarning: Removed 7 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nNotice the huge drop (more than 90%) in April 2020 due to the COVID-19 pandemic!",
    "crumbs": [
      "Week 6",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "chapters/week07/data-structures.html",
    "href": "chapters/week07/data-structures.html",
    "title": "22  Data Structures",
    "section": "",
    "text": "22.1 What Are Data?\nThis lesson provides a conceptual introduction to different ways of structuring data.\nMerriam-Webster’s Dictionary defines data as:\nSeveral key principals are introduced in the above definition:\nData Scientists (as differentiated from statisticians or computer scientists, for example) are expert in understanding the nature of data itself and the steps necessary to assess the suitability of a given data set for answering specific research questions and the work required to properly prepare data for successful analysis. In the broadest terms, we call this process data forensics.\nThe first step in the data forensics process is understanding the format(s) through which data are stored and transferred.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "chapters/week07/data-structures.html#what-are-data",
    "href": "chapters/week07/data-structures.html#what-are-data",
    "title": "22  Data Structures",
    "section": "",
    "text": "factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation\ninformation in digital form that can be transmitted or processed\ninformation output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful\n\n\n\ndata is an intermediary step leading towards some form of analysis or or presentation, not typically an end in itself\ndata comes in multiple formats, both digital and analogue\ndata can be collected by both humans and machines\nnot all data in a given dataset is necessarily meaningful, correct nor useful",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "chapters/week07/data-structures.html#tabular-data",
    "href": "chapters/week07/data-structures.html#tabular-data",
    "title": "22  Data Structures",
    "section": "22.2 Tabular Data",
    "text": "22.2 Tabular Data\nTabular data is the most ubiquitous form of data storage and the one most familiar to most users. Tabular data consists of organizing data in a table of rows and columns. Traditionally, each column in the table represents a field or variable and each row represents an observation or entity. For example, the table below shows a tabular organization of a subset of the mtcars dataset:\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\n\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n\n\nDatsun 710\n22.8\n4\n108.0\n93\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n\n\nHornet Sportabout\n18.7\n8\n360.0\n175\n\n\nValiant\n18.1\n6\n225.0\n105\n\n\nDuster 360\n14.3\n8\n360.0\n245\n\n\nMerc 240D\n24.4\n4\n146.7\n62\n\n\nMerc 230\n22.8\n4\n140.8\n95\n\n\nMerc 280\n19.2\n6\n167.6\n123",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "chapters/week07/data-structures.html#tree-document-data-structures",
    "href": "chapters/week07/data-structures.html#tree-document-data-structures",
    "title": "22  Data Structures",
    "section": "22.3 Tree / Document Data Structures",
    "text": "22.3 Tree / Document Data Structures\nAnother popular form of data structure is the tree structure, sometimes referred to as a document-based data structure. Tree data structures present data in a hierarchical tree-like structure in which all items related back to a single, root node. A “Family Tree” is a good example of tree structured data:\n\nThe mtcars data from the above table can also be represented using a tree structure:\n\nThe above image visually depicts the mtcars data as a tree, which works well for a human reader but is not parsable by the computer. There are a variety of ways to represent tree data as a computer file (or data stream) so that it can be read and parsed by the computer. In this class, we will cover two of the most popular formats: XML and JSON.\n\n22.3.1 Structuring Data as XML\nXML stands for extensible markup language. Markup languages have been around since the 1960s and were originally developed as a means of adding structured information to an existing unstructured text. In the days of analog text preparation, professional editors typically used a blue or red pencil to make notes on typed manuscripts. The use of a specially colored pen or pencil for “marking up” documents, as the procedure was known in the industry, easily allowed subsequent readers to distinguish between editorial comments and formatting notes on typed manuscripts from the text itself. Computerized markup languages were developed as a means of allowing data specialists to markup a text in a manner that would allow the computer to distinguish between textual content and meta-information (information about the text) when both types of information appear in the same file.\nXML is the most widely used form of markup today. In fact, nearly every webpage that you have ever viewed is actually an XML document that contains both content to be displayed and instructions for the computer on how to display that content embedded in the file using XML tags, which are simply instructions contained with the special characters &lt; and &gt;. For example, consider the following short email text:\nTo: Tavi\nFrom: Jonna\nSubject: Meeting\nDate: Thursday, February 4, 2021 at 2:46 PM\n\nDon't forget about meeting with Sarah next week, 2pm in room 242.\n\nThanks,\n\nJonna\nThis email contains quite a bit of structured email (sender, receiver, date/time, etc.), but there is no easy way for the computer easily extract this structure. We can solve this problem by using XML to embed information about the structure directly in the document as follows:\n&lt;head&gt;\n   &lt;to&gt;Tavi&lt;/to&gt;\n   &lt;from&gt;Jonna&lt;/from&gt;\n   &lt;subject&gt;Meeting&lt;/subject&gt;\n   &lt;datetime&gt;\n      &lt;dayofweek&gt;Thursday&lt;/dayofweek&gt;\n      &lt;month&gt;February&lt;/month&gt;\n      &lt;day&gt;4&lt;/day&gt;\n      &lt;year&gt;2021&lt;/year&gt;\n      &lt;time&gt;2:46 PM&lt;/time&gt;\n   &lt;/datetime&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n   Don't forget about meeting with Sarah next week, 2pm in room 242.\n\n   Thanks,\n\n   &lt;signature&gt;Jonna&lt;/signuature&gt;\n&lt;/body&gt;\nBy using XML, we are able to identify specific information in the email in a way that the computer is a capable of parsing. This allows us to use computational methods to easily extract information in bulk from many emails and it also allows us to program a computer program, such as an email client, to organize and properly display all of the parts of the email.\nThe above XML example illustrates several important aspects of XML:\n\nAll XML tags are enclosed in &lt; and &gt; symbols.\nThere are 2 primary types of tags, opening tags, which designate the beginning character that is defined by the tag, and closing tags, which designate the end of the portion of the text to be associated with the opening tag.\nClosing tags are always indicated by slash character, as in &lt;/TAG&gt;, where TAG is the name of the opening tag that is being closed.\nTags be be embedded within each other in a tree-like structure. However, any tags opened within a tag must be closed before that tag can be closed. For example, &lt;name&gt;&lt;first&gt;John&lt;/first&gt; &lt;last&gt;Doe&lt;/last&gt;&lt;/name&gt; is valid, but &lt;name&gt;&lt;first&gt;John&lt;/first&gt; &lt;last&gt;Doe&lt;/name&gt;&lt;/last&gt; is not valid.\n\nWhile XML was originally developed as a means of embedding meta information about a text directly in a text, it also quickly evolved into a stand-alone means of representing tree-structured data for exchange between computer systems. To this end, many computer applications use XML to store, share, and retrieve data. For example, we can represent the data in our truncated mtcars dataset as XML as follows:\n&lt;cars&gt;\n   &lt;make id=\"mazda\"&gt;\n      &lt;model id=\"RX4\"&gt;\n         &lt;mpg&gt;21.0&lt;/mpg&gt;\n         &lt;cyl&gt;6&lt;/cyl&gt;\n         &lt;disp&gt;160.0&lt;/disp&gt;\n         &lt;hp&gt;110&lt;/hp&gt;\n      &lt;/model&gt;\n      &lt;model id=\"RX4 Wag\"&gt;\n         &lt;mpg&gt;21.0&lt;/mpg&gt;\n         &lt;cyl&gt;6&lt;/cyl&gt;\n         &lt;disp&gt;160.0&lt;/disp&gt;\n         &lt;hp&gt;110&lt;/hp&gt;\n      &lt;/model&gt;\n   &lt;/make&gt;\n   &lt;make id=\"Datsun\"&gt;\n      &lt;model id=\"710\"&gt;\n         &lt;mpg&gt;22.8&lt;/mpg&gt;\n         &lt;cyl&gt;4&lt;/cyl&gt;\n         &lt;disp&gt;108.0&lt;/disp&gt;\n         &lt;hp&gt;93&lt;/hp&gt;\n      &lt;/model&gt;\n   &lt;/make&gt;\n   &lt;make id=\"Hornet\"&gt;\n      &lt;model id=\"4 Drive\"&gt;\n         &lt;mpg&gt;21.4&lt;/mpg&gt;\n         &lt;cyl&gt;6&lt;/cyl&gt;\n         &lt;disp&gt;258.0&lt;/disp&gt;\n         &lt;hp&gt;110&lt;/hp&gt;\n      &lt;/model&gt;\n      &lt;model id=\"Sportabout\"&gt;\n         &lt;mpg&gt;18.7&lt;/mpg&gt;\n         &lt;cyl&gt;8&lt;/cyl&gt;\n         &lt;disp&gt;360.0&lt;/disp&gt;\n         &lt;hp&gt;175&lt;/hp&gt;\n      &lt;/model&gt;\n   &lt;/make&gt;\n   &lt;make id=\"Valiant\"&gt;\n      &lt;model id=\"valiant\"&gt;\n         &lt;mpg&gt;18.1&lt;/mpg&gt;\n         &lt;cyl&gt;6&lt;/cyl&gt;\n         &lt;disp&gt;225.0&lt;/disp&gt;\n         &lt;hp&gt;105&lt;/hp&gt;\n      &lt;/model&gt;\n   &lt;/make&gt;\n   &lt;make id=\"Duster\"&gt;\n      &lt;model id=\"360\"&gt;\n         &lt;mpg&gt;14.3&lt;/mpg&gt;\n         &lt;cyl&gt;8&lt;/cyl&gt;\n         &lt;disp&gt;360.0&lt;/disp&gt;\n         &lt;hp&gt;245&lt;/hp&gt;\n      &lt;/model&gt;\n   &lt;/make&gt;\n   &lt;make id=\"Merc\"&gt;\n      &lt;model id=\"240D\"&gt;\n         &lt;mpg&gt;24.4&lt;/mpg&gt;\n         &lt;cyl&gt;4&lt;/cyl&gt;\n         &lt;disp&gt;146.7&lt;/disp&gt;\n         &lt;hp&gt;62&lt;/hp&gt;\n      &lt;/model&gt;\n      &lt;model id=\"230\"&gt;\n         &lt;mpg&gt;22.8&lt;/mpg&gt;\n         &lt;cyl&gt;4&lt;/cyl&gt;\n         &lt;disp&gt;140.8&lt;/disp&gt;\n         &lt;hp&gt;95&lt;/hp&gt;\n      &lt;/model&gt;\n      &lt;model id=\"280\"&gt;\n         &lt;mpg&gt;19.2&lt;/mpg&gt;\n         &lt;cyl&gt;6&lt;/cyl&gt;\n         &lt;disp&gt;167.6&lt;/disp&gt;\n         &lt;hp&gt;123&lt;/hp&gt;\n      &lt;/model&gt;\n   &lt;/make&gt;\n&lt;/cars&gt;\nFor an XML dataset to be technically valid, the tags used to markup the dataset must themselves be defined according to a schema, another XML document that defines all tags that can be used in marking up a dataset and the allowable tree structure of the markup (for example, which tags can be parents of which other tags, etc.). You do not need to understand, or even know, the schema being used to present data in order to read and parse an XML document. However, schemas are extremely useful (and often necessary) for building applications that perform advanced processing of XML documents, such as web browsers, email clients, etc.\n\n\n\n\n\n\nNoteSee also\n\n\n\nFor more information on XML and XML schemas, see the W3Schools XML Tutorial.\n\n\n\n\n22.3.2 Structuring Data as JSON\nXML provides an excellent framework for encoding, saving, and transferring all kinds of data, and it was the dominant mode of transferring data across the internet for many years. However, XML has an Achilles’ Heel from the data transfer perspective: a lack of sparsity. If you look closely at the XML mtcars data set example above, you will note that the markup accounts for more of the total characters in the document than the data itself. In a world where data is regularly being exchanged in real time across networks, the use of XML can result in the necessity to exchange a lot more data to accomplish the same task. This adds both time and cost to every data transaction.\nJavaScript object notation (JSON) was developed as a standard to address this problem and provides a sparse framework for representing data that introduces minimal, non-data elements into the overall data structure. JSON uses a key/value pair structure to represent data elements:\n\"model\": \"RX4\"\nIndividual data elements are then grouped to reflect more complex data structures:\n{\"model\": {\"id\": \"2\", \"hp\": \"120\"}}\nThe example below shows the subsetted mtcars dataset represented as JSON. Note the use of the [ character to indicated repeated elements in the data:\n{\n    \"cars\": [{\n            \"make\": \"Mazda\",\n            \"model\": [{\n                    \"id\": \"RX4\",\n                    \"mpg\": \"21.0\",\n                    \"cyl\": \"6\",\n                    \"disp\": \"160.0\",\n                    \"hp\": \"110\"\n                },\n                {\n                    \"id\": \"RX4 Wag\",\n                    \"mpg\": \"21.0\",\n                    \"cyl\": \"6\",\n                    \"disp\": \"160.0\",\n                    \"hp\": \"110\"\n                }\n            ]\n        },\n        {\n            \"make\": \"Datsun\",\n            \"model\": {\n                \"id\": \"710\",\n                \"mpg\": \"22.8\",\n                \"cyl\": \"4\",\n                \"disp\": \"108.0\",\n                \"hp\": \"93\"\n            }\n        },\n        {\n            \"make\": \"Hornet\",\n            \"model\": [{\n                    \"id\": \"4 Drive\",\n                    \"mpg\": \"21.4\",\n                    \"cyl\": \"6\",\n                    \"disp\": \"258.0\",\n                    \"hp\": \"110\"\n                },\n                {\n                    \"id\": \"Sportabout\",\n                    \"mpg\": \"18.7\",\n                    \"cyl\": \"8\",\n                    \"disp\": \"360.0\",\n                    \"hp\": \"175\"\n                }\n            ]\n        },\n        {\n            \"make\": \"Valiant\",\n            \"model\": {\n                \"id\": \"valiant\",\n                \"mpg\": \"18.1\",\n                \"cyl\": \"6\",\n                \"disp\": \"225.0\",\n                \"hp\": \"105\"\n            }\n        },\n        {\n            \"make\": \"Duster\",\n            \"model\": {\n                \"id\": \"360\",\n                \"mpg\": \"14.3\",\n                \"cyl\": \"8\",\n                \"disp\": \"360.0\",\n                \"hp\": \"245\"\n            }\n        },\n        {\n            \"make\": \"Merc\",\n            \"model\": [{\n                    \"id\": \"240D\",\n                    \"mpg\": \"24.4\",\n                    \"cyl\": \"4\",\n                    \"disp\": \"146.7\",\n                    \"hp\": \"62\"\n                },\n                {\n                    \"id\": \"230\",\n                    \"mpg\": \"22.8\",\n                    \"cyl\": \"4\",\n                    \"disp\": \"140.8\",\n                    \"hp\": \"95\"\n                },\n                {\n                    \"id\": \"280\",\n                    \"mpg\": \"19.2\",\n                    \"cyl\": \"6\",\n                    \"disp\": \"167.6\",\n                    \"hp\": \"123\"\n                }\n            ]\n        }\n    ]\n}\n\n\n\n\n\n\nNoteSee also\n\n\n\nFor information on the JSON format, see the Tutorials Point JSON Tutorial.\nYou can also use the JSONLint JSON Validator to check the syntax of any JSON representation.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "chapters/week07/data-structures.html#relational-databases",
    "href": "chapters/week07/data-structures.html#relational-databases",
    "title": "22  Data Structures",
    "section": "22.4 Relational Databases",
    "text": "22.4 Relational Databases\n*Relational databases, frequently referred to as relational database management systems** (RDBMS), provide another way of structuring data. Unlike tabular, XML, and JSON data representations, RDBMS data is not easily human readable, and specialized software is usually required to interact with data stored as relational data. Most programming environments (including R) provide specialized drivers for communicating with RDBMS in order to facilitate working with data stored in these systems.\nRDBMS have three primary purposes as a data storage format:\n\nTo reduce duplication of data;\nTo speed-up access and insertion of new data;\nTo insure data integrity.\n\nItems 2 and 3 above are accomplished at the software level, by deploying strict checks on data input, complex data indexing systems, and implementing redundant, automated, backup systems, to name just a few of the functionalities offered by RDBMS. Item 1 above, reducing duplication of data, is accomplished by using a specific, relational data structure that encourages the use of controlled lists of data mapped to individual observations. Looking at our mtcars subset data, for example, we see that while there are ten observations, there are only 6 makes of cars. To represent this in RDBMS, we first create a table, a named collection of data, that contains a unique list of car makes:\n\n\n\n\n\n\n\n\n\nid\nMake\n\n\n\n\n1\nMazda\n\n\n2\nDatsun\n\n\n3\nHornet\n\n\n4\nValiant\n\n\n5\nDuster\n\n\n6\nMerc\n\n\n\n\n\n\n\n\nFigure 22.1: MAKE_TABLE\n\n\n\nOnce we have a table of unique lists, we then create and populate a table of our cars, associating each car with its appropriate make from the MAKE_TABLE table:\n\n\n\n\n\n\n\n\n\nMake\nModel\nmpg\ncyl\ndisp\nhp\n\n\n\n\n1\nRX4\n21.0\n6\n160.0\n110\n\n\n1\nRX4 Wag\n21.0\n6\n160.0\n110\n\n\n2\n710\n22.8\n4\n108.0\n93\n\n\n3\n4 Drive\n21.4\n6\n258.0\n110\n\n\n3\nSportabout\n18.7\n8\n360.0\n175\n\n\n4\nValiant\n18.1\n6\n225.0\n105\n\n\n5\n360\n14.3\n8\n360.0\n245\n\n\n6\n240D\n24.4\n4\n146.7\n62\n\n\n6\n230\n22.8\n4\n140.8\n95\n\n\n6\n280\n19.2\n6\n167.6\n123\n\n\n\n\n\n\n\n\nFigure 22.2: CARS_TABLE\n\n\n\nIn the above table, we only normalized the car Make field. In a fully normalized RDBMS data structure, we would also create a control table for the Model field in anticipation of the fact that we could have more than one observation for a given model. Fully normalized RBDMS data structures use control tables for all fields that contain string data.\nThe image below shows a sample entry relationship diagram (ERD) for a more complex dataset relating to course offerings and enrollments. Each line connecting two tables marks a field in a “join” table that uses the id field in a control table (known as a foreign key) to associate information in the control table with the records in the join table.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "chapters/week07/data-structures.html#non-hierarchical-relational-data",
    "href": "chapters/week07/data-structures.html#non-hierarchical-relational-data",
    "title": "22  Data Structures",
    "section": "22.5 Non-Hierarchical Relational Data",
    "text": "22.5 Non-Hierarchical Relational Data\nIn the era of the social network, it is becoming increasingly necessary to represent relationships between entities that are not hierarchical. Unlike a family tree, the fact that you are connected to someone on Facebook or Instagram does not imply any type of hierarchical relationship. Such networks are typically represented using the graph data structure:\n\nGraphs consist of collections of vertices or nodes, the entities being graphed, and edges, the relationships between nodes.\n\nAnother important aspect of graph data is the concept of directionality. A directed graph indicates the direction of the relationship identified by the edge. We might, for example, wish to draw edges that indicate that one node was influenced by another node, in which case we could identify an “influence” edge and use directionality to indicate who influenced whom:\n\nGraph data can be stored and or transferred using any of the data formats discussed above or using specialized graph databases management software.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "chapters/week07/data-structures.html#geospatial-data",
    "href": "chapters/week07/data-structures.html#geospatial-data",
    "title": "22  Data Structures",
    "section": "22.6 Geospatial Data",
    "text": "22.6 Geospatial Data\nGeospatial data represents a final type of data with its own unique data structure. Geospatial data is unique because it always relates directly to the physical world, and because it relies on world-wide standards which have been in development and communally accepted for hundreds of years. Because of its uniqueness as a data type, geospatial data is covered as a stand-alone topic in Chapter 37.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "chapters/week07/string-processing.html",
    "href": "chapters/week07/string-processing.html",
    "title": "23  Strings & Regular Expressions",
    "section": "",
    "text": "23.1 String Fundamentals\nStrings represent text, but even if your datasets are composed entirely of numbers, you’ll need to know how to work with strings. Text formats for data are widespread: comma-separated values (CSV), tab-separated values (TSV), JavaScript object notation (JSON), various markup languages (HTML, XML, YAML, TOML), and more. When you read data in these formats into R, sometimes R will correctly convert the values to appropriate non-string types. The rest of the time, you need to know how to work with strings so that you can fix whatever went wrong and convert the data yourself.\nString processing encompasses a variety of tasks such as searching for patterns within strings, extracting data from within strings, splitting strings into component parts, and removing or replacing unwanted characters (excess whitespace, punctuation, and so on).\nThis section introduces several fundamental concepts related to working with strings.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Strings & Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/week07/string-processing.html#string-fundamentals",
    "href": "chapters/week07/string-processing.html#string-fundamentals",
    "title": "23  Strings & Regular Expressions",
    "section": "",
    "text": "23.1.1 Escape Sequences\nIn a string, an escape sequence or escape code consists of a backslash followed by one or more characters. Escape sequences make it possible to:\n\nWrite quotes or backslashes within a string\nWrite characters that don’t appear on your keyboard (for example, characters in a foreign language)\n\nFor example, the escape sequence \\n corresponds to the newline character. Notice that the message function prints \\n as a literal new line:\n\nx = \"Hello\\nNick\"\n\nmessage(x)\n\nHello\nNick\n\n\nOn the other hand, when you let R print a string or other object automatically (without explicitly calling message), it prints a programmer-friendly representation. For strings, the representation shows escape codes:\n\nx\n\n[1] \"Hello\\nNick\"\n\n\nThis makes it easy to identify characters that are not normally visible, such as whitespace, and also makes it easy to copy the string. You can use the built-in print function to get a representation for any object.\n\n\n\n\n\n\nNote\n\n\n\nTo print the actual characters in a string, use the message function.\nTo print a representation of the characters in a string, use the print function. The representation is useful to identify characters that are not normally visible, such as tabs and the characters that mark the end of a line.\nThere are at least two more functions in R that can print things: cat and show. The cat function is similar to message, but bypasses R’s system for keeping track of messages. The show function is similar to print, but specifically designed for R’s S4 objects, which we won’t cover. You generally shouldn’t use these two functions directly.\n\n\nAs another example, suppose we want to put literal quotation marks in a string. We can either enclose the string in the other kind of quotation marks (single versus double), or escape the quotation marks in the string:\n\nx = 'She said, \"Hi\"'\nmessage(x)\n\nShe said, \"Hi\"\n\ny = \"She said, \\\"Hi\\\"\"\nmessage(y)\n\nShe said, \"Hi\"\n\n\nSince escape sequences begin with backslash, we also need to use an escape sequence to write a literal backslash. The escape sequence for a literal backslash is two backslashes:\n\nx = \"\\\\\"\nmessage(x)\n\n\\\n\n\nThere’s a complete list of escape sequences for R in the ?Quotes help file. Other programming languages also use escape sequences, and many of them are the same as in R.\n\n\n\n\n\n\nTip 23.1\n\n\n\nA raw string is a string where escape sequences are turned off. Raw strings are especially useful for writing regular expressions (covered in Section 23.3).\nRaw strings begin with r\" and an opening delimiter (, [, or {. Raw strings end with a matching closing delimiter and quote. For example:\n\nx = r\"(quotes \" and backslashes \\)\"\n\nmessage(x)\n\nquotes \" and backslashes \\\n\n\nRaw strings were added to R in version 4.0 (April 2020), and won’t work correctly in older versions.\n\n\n\n\n23.1.2 Character Encodings\nComputers store data as numbers. In order to store text on a computer, people have to agree on a character encoding, a system for mapping characters to numbers. For example, in ASCII, one of the most popular encodings in the United States, the character a maps to the number 97.\nMany different character encodings exist, and sharing text used to be an inconvenient process of asking or trying to guess the correct encoding. This was so inconvenient that in the 1980s, software engineers around the world united to create the Unicode standard. Unicode includes symbols for nearly all languages in use today, as well as emoji and many ancient languages (such as Egyptian hieroglyphs).\nUnicode maps characters to numbers, but unlike a character encoding, it doesn’t dictate how those numbers should be mapped to bytes (sequences of ones and zeroes). As a result, there are several different character encodings that support and are synonymous with Unicode. The most popular of these is UTF-8.\nIn R, you can write Unicode characters with the escape sequence \\u (or \\U) followed by a 4-digit (or 8-digit) number for the character in base 16. For instance, the number for a in Unicode is 97 (the same as in ASCII). In base 16, 97 is 61. So you can write an a as:\n\nx = \"\\u61\"\nx\n\n[1] \"a\"\n\n\nUnicode escape sequences are usually only used for characters that are not easy to type. For example, the cat emoji is number 1f408 (in base 16) in Unicode. So the string \"\\U1f408\" is the cat emoji:\n\n\"\\U1f408\"\n\n[1] \"🐈\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhether you can see a Unicode character depends on whether the current font has a glyph (image representation) for that character. Most fonts only have glyphs for a few languages.\nMake sure to use a font with good Unicode coverage if you love emoji or expect to work with many different languages. The Noto Fonts project aims to create a collection of fonts with a common style and complete language coverage. The NerdFonts project patches fonts commonly used for programming so that they have better coverage of symbols.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMost of the time, R will handle character encodings for you automatically. However, if you ever read or write a text file (including CSV and other formats) and the text looks like gibberish, it might be an encoding problem. This is especially true on Windows, the only modern operating system that does not (yet) use UTF-8 as the default encoding.\nEncoding problems when reading a file can usually be fixed by passing the encoding to the function doing the reading. For instance, the code to read a UTF-8 encoded CSV file on Windows is:\n\nread.csv(\"my_data.csv\", fileEncoding = \"UTF-8\")\n\nOther reader functions may use a different parameter to set the encoding, so always check the documentation.\nOn computers where the native language is not set to English, it can also help to set R’s native language to English with Sys.setlocale(locale = \"English\").\nEncoding problems when writing a file are slightly more complicated to fix. See this blog post for thorough explanation.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Strings & Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/week07/string-processing.html#sec-the-stringr-package",
    "href": "chapters/week07/string-processing.html#sec-the-stringr-package",
    "title": "23  Strings & Regular Expressions",
    "section": "23.2 The stringr Package",
    "text": "23.2 The stringr Package\nAlthough R has built-in functions for string processing, we recommend using the stringr package for all of your string processing needs. The package is part of the Tidyverse, a collection of packages introduced in Section 14.3. Major advantages of stringr over other packages and R’s built-in functions include:\n\nCorrectness: the package builds on International Components for Unicode (ICU), the Unicode Consortium’s own library for handling text encodings\nDiscoverability: every function’s name begins with str_ so they’re easy to discover, remember, and identify in code\nInterface consistency: the first argument is always the string to process, the second argument is always the pattern to match (if applicable)\nVectorization: most of the functions are vectorized in the first and second argument\n\nstringr has detailed documentation and also a cheatsheet.\nThe first time you use stringr, you’ll have to install it with install.packages (the same as any other package). Then you can load the package with the library function:\n\n# install.packages(\"stringr\")\nlibrary(stringr)\n\nThe typical syntax of a stringr function is:\n\nstr_name(string, pattern, ...)\n\nWhere:\n\nname describes what the function does\nstring is a string to search within or transform\npattern is a pattern to search for, if applicable\n... is additional, function-specific arguments\n\nFor example, the str_detect function detects whether a pattern appears within a string. The function returns TRUE if the pattern is found and FALSE if it isn’t:\n\nstr_detect(\"hello\", \"el\")\n\n[1] TRUE\n\nstr_detect(\"hello\", \"ol\")\n\n[1] FALSE\n\n\nMost of the stringr functions are vectorized in the string parameter:\n\nstr_detect(c(\"hello\", \"goodbye\", \"lo\"), \"lo\")\n\n[1]  TRUE FALSE  TRUE\n\n\nAs another example, the str_sub function extracts a substring from a string, given the substring’s position. The first argument is the string, the second is the position of the substring’s first character, and the third is the position of the substring’s last character:\n\nstr_sub(\"You speak of destiny as if it was fixed.\", 5, 9)\n\n[1] \"speak\"\n\n\nThe str_sub function is especially useful for extracting data from strings that have a fixed width (although the readr package’s read_fwf is usually a better choice if you have a fixed-width file).\nThere are a lot of stringr functions. Five that are especially important and are explained in this reader are:\n\nstr_detect, to test whether a string contains a pattern\nstr_sub, to extract a substring at a given position from a string\nstr_replace, to replace or remove parts of a string\nstr_split_fixed, to split a string into parts\nstr_match, to extract data from a string\n\nYou can find a complete list of functions with examples on the stringr documentation’s reference page and the cheatsheet.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Strings & Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/week07/string-processing.html#sec-regular-expressions",
    "href": "chapters/week07/string-processing.html#sec-regular-expressions",
    "title": "23  Strings & Regular Expressions",
    "section": "23.3 Regular Expressions",
    "text": "23.3 Regular Expressions\nThe stringr functions use a special language called regular expressions or regex to describe patterns in strings. Many other programming languages also have string processing tools that use regular expressions, so fluency with regular expressions is a valuable, transferrable skill.\nYou can use a regular expression to describe a complicated pattern in just a few characters because some characters, called metacharacters, have special meanings. Metacharacters are usually punctation characters. They are never letters or numbers, which always have their literal meaning.\nThis table lists some of the most useful metacharacters:\n\n\n\n\n\n\n\nMetacharacter\nMeaning\n\n\n\n\n.\nany one character (wildcard)\n\n\n\\\nescape character (in both R and regex), see Section 23.8.2\n\n\n^\nthe beginning of string (not a character)\n\n\n$\nthe end of string (not a character)\n\n\n[ab]\none character, either 'a' or 'b'\n\n\n[^ab]\none character, anything except 'a' or 'b'\n\n\n?\nthe previous character appears 0 or 1 times\n\n\n*\nthe previous character appears 0 or more times\n\n\n+\nthe previous character appears 1 or more times\n\n\n()\nmake a group\n\n\n|\nmatch left OR right side (not a character)\n\n\n\nSection 23.8 provides examples of how most of the metacharacters work.\n\n\n\n\n\n\nNoteSee Also\n\n\n\nEven more examples are presented in the stringr package’s regular expressions vignette. You can find a complete listing of regex metacharacters in ?regex or on the stringr cheatsheet.\n\n\nYou can disable regular expressions in a stringr function by calling the fixed function on the pattern. For example, to test whether a string contains a literal dot .:\n\nx = c(\"No dot\", \"Lotsa dots...\")\nstr_detect(x, fixed(\".\"))\n\n[1] FALSE  TRUE\n\n\n\n\n\n\n\n\nTip\n\n\n\nMake it a habit to call fixed anytime you use a pattern that doesn’t contain regex metacharacters. Doing so communicates to the reader that you’re not using regex, prevents bugs, and makes your code more efficient.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Strings & Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/week07/string-processing.html#replacing-parts-of-strings",
    "href": "chapters/week07/string-processing.html#replacing-parts-of-strings",
    "title": "23  Strings & Regular Expressions",
    "section": "23.4 Replacing Parts of Strings",
    "text": "23.4 Replacing Parts of Strings\nReplacing part of a string is a common string processing task. For instance, quantitative data often contain non-numeric characters such as commas, currency symbols, and percent signs. These must be removed before converting to numeric data types. Replacement and removal go hand-in-hand, since removal is equivalent to replacing part of a string with the empty string \"\".\nThe str_replace function replaces the first part of a string that matches a pattern (from left to right), while the related str_replace_all function replaces every part of a string that matches a pattern. Most stringr functions that do pattern matching come in a pair like this: one to process only the first match and one to process every match.\nAs an example, suppose you want to remove commas from a vector of numbers (in strings) so that you can cast them to a numeric data type with as.numeric. You want to remove all of the commas, so str_replace_all is the function to use. The first argument is the string and the second is the pattern. The third argument is the replacement. In this case, the pattern is \",\" and the replacement is the empty string \"\". So the code to remove the commas and cast the numbers is:\n\nx = c(\"1,000,000\", \"525,600\", \"42\")\nas.numeric(str_replace_all(x, fixed(\",\"), \"\"))\n\n[1] 1000000  525600      42\n\n\nThe str_replace function wouldn’t work as well for this task, since it only replaces the first match to the pattern:\n\nstr_replace(x, fixed(\",\"), \"\")\n\n[1] \"1000,000\" \"525600\"   \"42\"      \n\n\nYou can also use these functions to replace or remove longer patterns within words. For instance, suppose you want to change the word \"dog\" to \"cat\":\n\nx = c(\"dogs are great, dogs are fun\", \"dogs are fluffy\")\nstr_replace(x, fixed(\"dog\"), \"cat\")\n\n[1] \"cats are great, dogs are fun\" \"cats are fluffy\"             \n\nstr_replace_all(x, fixed(\"dog\"), \"cat\")\n\n[1] \"cats are great, cats are fun\" \"cats are fluffy\"             \n\n\nAs a final example, you can use the replacement functions and a regex pattern to replace repeated spaces with a single space. This is a good standardization step if you’re working with text. The key is to use the regex quantifier +, which means a character “repeats one or more times” in the pattern, and to use a single space \" \" as the replacement:\n\nx = \"This    sentence  has  extra      space.\"\nstr_replace_all(x, \" +\", \" \")\n\n[1] \"This sentence has extra space.\"\n\n\nThe str_squish function does this as well as removing all whitespace from the beginning and end of a string.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Strings & Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/week07/string-processing.html#splitting-strings",
    "href": "chapters/week07/string-processing.html#splitting-strings",
    "title": "23  Strings & Regular Expressions",
    "section": "23.5 Splitting Strings",
    "text": "23.5 Splitting Strings\nDistinct data in a text are generally separated by a character like a space or a comma, to make them easy for people to read. Often these separators also make the data easy for R to parse. The idea is to split the string into a separate value at each separator.\nThe str_split function splits a string at each match to a pattern. The matching characters—that is, the separators—are discarded.\nFor example, suppose you want to split several numbers separated by commas and spaces:\n\nx = \"21, 32.3, 5, 64\"\nresult = str_split(x, \", \")\nresult\n\n[[1]]\n[1] \"21\"   \"32.3\" \"5\"    \"64\"  \n\n\nThe str_split function always returns a list with one element for each input string. Here the list only has one element because x only has one element. You can get the first element with:\n\nresult[[1]]\n\n[1] \"21\"   \"32.3\" \"5\"    \"64\"  \n\n\nYou then convert the values with as.numeric.\nTo see why the str_split function always returns a list, consider what happens if you try to split two different strings at once:\n\nx = c(x, \"10, 15, 1.3\")\nresult = str_split(x, \", \")\nresult\n\n[[1]]\n[1] \"21\"   \"32.3\" \"5\"    \"64\"  \n\n[[2]]\n[1] \"10\"  \"15\"  \"1.3\"\n\n\nEach string has a different number of parts, so the vectors in the result have different lengths. So a list is the only way to store them.\nYou can also use the str_split function to split a sentence into words. Use spaces for the split:\n\nsentences = c(\n  \"The wonderful, wonderful cat!\",\n  \"Who is this duke of zill anyway?\"\n)\n\nwords = str_split(sentences, \" \")\nwords\n\n[[1]]\n[1] \"The\"        \"wonderful,\" \"wonderful\"  \"cat!\"      \n\n[[2]]\n[1] \"Who\"     \"is\"      \"this\"    \"duke\"    \"of\"      \"zill\"    \"anyway?\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you know exactly how many parts you expect a string to have, use the str_split_fixed function instead of str_split. It requires a third argument for the maximum number of splits. Because the number of splits is fixed, the function returns the results in a matrix rather than a list. For example:\n\nx = c(\"1, 2, 3\", \"10, 20, 30\")\nstr_split_fixed(x, \", \", 3)\n\n     [,1] [,2] [,3]\n[1,] \"1\"  \"2\"  \"3\" \n[2,] \"10\" \"20\" \"30\"\n\n\nThe str_split_fixed function is often more convenient than str_split because the nth piece of each input string is just the nth column of the result.\nFor example, suppose you want to get the area codes from some phone numbers:\n\nphones = c(\"717-555-3421\", \"629-555-8902\", \"903-555-6781\")\nresult = str_split_fixed(phones, \"-\", 3)\n\nresult[, 1]\n\n[1] \"717\" \"629\" \"903\"",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Strings & Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/week07/string-processing.html#sec-extracting-matches",
    "href": "chapters/week07/string-processing.html#sec-extracting-matches",
    "title": "23  Strings & Regular Expressions",
    "section": "23.6 Extracting Matches",
    "text": "23.6 Extracting Matches\nOccasionally, you might need to extract parts of a string in a more complicated way than string splitting allows. One solution is to write a regular expression that will match all of the data you want to capture, with parentheses ( ), the regex metacharacter for a group, around each distinct value. Then you can use the str_match function to extract the groups. Section 23.8.6 presents some examples of regex groups.\nFor example, suppose you want to split an email address into three parts: the user name, the domain name, and the [top-level domain][tld]. To create a regular expression that matches email addresses, you can use the @ and . in the address as anchors. The surrounding characters are generally alphanumeric, which you can represent with the “word” metacharacter \\w:\n\\w+@\\w+[.]\\w+\nNext, put parentheses ( ) around each part that you want to extract:\n(\\w+)@(\\w+)[.](\\w+)\nFinally, use this pattern in str_match, adding extra backslashes so that everything is escaped correctly:\n\nx = \"datalab@ucdavis.edu\"\nregex = \"(\\\\w+)@(\\\\w+)[.](\\\\w+)\"\nstr_match(x, regex)\n\n     [,1]                  [,2]      [,3]      [,4] \n[1,] \"datalab@ucdavis.edu\" \"datalab\" \"ucdavis\" \"edu\"\n\n\nThe function extracts the overall match to the pattern, as well as the match to each group.\nThe pattern in this example doesn’t work for all possible email addresses, since user names can contain dots and other characters that are not alphanumeric. You could generalize the pattern if necessary. The point is that the str_match function and groups provide an extremely flexible way to extract data from strings.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Strings & Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/week07/string-processing.html#case-study-u.s.-cold-storage",
    "href": "chapters/week07/string-processing.html#case-study-u.s.-cold-storage",
    "title": "23  Strings & Regular Expressions",
    "section": "23.7 Case Study: U.S. Cold Storage",
    "text": "23.7 Case Study: U.S. Cold Storage\nThe U.S. Department of Agriculture (USDA) publishes a variety of datasets online, particularly through its National Agricultural Statistics Service (NASS). Unfortunately, most of are published in PDF or semi-structured text format, which makes reading the data into R or other statistical software a challenge.\nThe USDA NASS posts monthly reports about stocks of agricultural products in refrigerated warehouses. In this case study, you’ll use string processing functions to extract a table of data from the December 2022 report.\n\n\n\n\n\n\nImportant\n\n\n\nClick here to download the December 2022 Cold Storage report.\nIf you haven’t already, we recommend you create a directory for this workshop. In your workshop directory, create a data/ subdirectory. Download and save the dataset in the data/ subdirectory.\n\n\nThe goal is to extract the first table, about “Nuts, Dairy Products, Frozen Eggs, and Frozen Poultry,” from the report.\nThe report is a semi-structured mix of natural language text and fixed-width tables. As a consequence, most functions for reading tabular data will not work well on the entire report. You could try to use a function for reading fixed-width data, such as read.fwf or the readr package’s read_fwf on only the lines containing a table. Another approach, which is shown here, is to use string processing functions to find and extract the table.\nThe readLines function reads a text file into a character vector with one element for each line. This makes the function useful for reading unstructured or semi-structured text. Use the function to read the report:\n\nreport = readLines(\"data/2022-12_cold-storage.txt\")\nhead(report)\n\n[1] \"\"                                                                            \n[2] \"Cold Storage\"                                                                \n[3] \"\"                                                                            \n[4] \"ISSN: 1948-903X\"                                                             \n[5] \"\"                                                                            \n[6] \"Released December 22, 2022, by the National Agricultural Statistics Service \"\n\n\nIn the report, tables always begin and end with lines that contain only dashes -. By locating these all-dash lines, you can locate the tables. Like str_detect, the str_which function tests whether strings in a vector match a pattern. The only difference is that str_which returns the indexes of the strings that matched (as if you had called which) rather than a logical vector. Use str_which to find the all-dash lines:\n\n# The regex means:\n#   ^  begining of string\n#   -+ one or more dashes\n#   $  end of string\n\ndashes = str_which(report, \"^-+$\")\nhead(report[dashes], 2)\n\n[1] \"--------------------------------------------------------------------------------------------------------------------------\"\n[2] \"--------------------------------------------------------------------------------------------------------------------------\"\n\n\nEach table contains three dash lines—one separates the header and body. The header and body of the first table are:\n\nreport[dashes[1]:dashes[2]]\n\n[1] \"--------------------------------------------------------------------------------------------------------------------------\"\n[2] \"                                      :             :                           :     November 30, 2022     :   Public    \"\n[3] \"                                      :        Stocks in all warehouses         :      as a percent of      :  warehouse  \"\n[4] \"                                      :             :                           :                           :   stocks    \"\n[5] \"               Commodity              :-----------------------------------------------------------------------------------\"\n[6] \"                                      :November 30, : October 31, :November 30, :November 30, : October 31, :November 30, \"\n[7] \"                                      :    2021     :    2022     :    2022     :    2021     :    2022     :    2022     \"\n[8] \"--------------------------------------------------------------------------------------------------------------------------\"\n\nbod = report[dashes[2]:dashes[3]]\nhead(bod)\n\n[1] \"--------------------------------------------------------------------------------------------------------------------------\"\n[2] \"                                      :  ------------ 1,000 pounds -----------        ---- percent ----      1,000 pounds \"\n[3] \"                                      :                                                                                   \"\n[4] \"Nuts                                  :                                                                                   \"\n[5] \"Shelled                               :                                                                                   \"\n[6] \"  Pecans .............................:     30,906        38,577        34,489        112            89                   \"\n\n\nThe columns have fixed widths, so extracting the columns is relatively easy with str_sub if you can get the offsets. In the last line of the header, the columns are separated by colons :. Thus you can use the str_locate_all function, which returns the locations of a pattern in a string, to get the offsets:\n\n# The regex means:\n#   [^:]+  one or more characters, excluding colons\n#   (:|$)  a colon or the end of the line\n\ncols = str_locate_all(report[dashes[2] - 1], \"[^:]+(:|$)\")\n# Like str_split, str_locate_all returns a list\ncols = cols[[1]]\ncols\n\n     start end\n[1,]     1  39\n[2,]    40  53\n[3,]    54  67\n[4,]    68  81\n[5,]    82  95\n[6,]    96 109\n[7,]   110 122\n\n\nYou can use these offsets with str_sub to break a line in the body of the table into columns:\n\nstr_sub(bod[6], cols)\n\n[1] \"  Pecans .............................:\"\n[2] \"     30,906   \"                         \n[3] \"     38,577   \"                         \n[4] \"     34,489   \"                         \n[5] \"     112      \"                         \n[6] \"      89      \"                         \n[7] \"             \"                          \n\n\nIn order to process every line in the body of the table in one vectorized call, we have to use str_sub_all rather than str_sub. The result is a list with one element for each line (that is, a list of rows), but we want a list of columns, so we also need to transpose the list with the purrr package’s list_transpose function:\n\nlibrary(\"purrr\")\ntab = str_sub_all(bod, cols)\ntab = list_transpose(tab)\n\nThe columns still contain undesirable punctuation and whitespace, but we can remove these with str_replace_all and str_squish. We can use purrr’s map function to call these on each column:\n\n# The regex means:\n#   ,     a comma\n#   |     OR\n#   [.]*  zero or more literal dots\n#   :     a colon\n#   $     the end of the line\n\ntab = map(tab, \\(col) {\n  col = str_replace_all(col, \",|[.]*:$\", \"\")\n  str_squish(col)\n})\ntab[[1]]\n\n [1] \"---------------------------------------\"\n [2] \"\"                                       \n [3] \"\"                                       \n [4] \"Nuts\"                                   \n [5] \"Shelled\"                                \n [6] \"Pecans\"                                 \n [7] \"In-Shell\"                               \n [8] \"Pecans\"                                 \n [9] \"\"                                       \n[10] \"Dairy products\"                         \n[11] \"Butter\"                                 \n[12] \"Natural cheese\"                         \n[13] \"American\"                               \n[14] \"Swiss\"                                  \n[15] \"Other\"                                  \n[16] \"Total natural cheese\"                   \n[17] \"\"                                       \n[18] \"Frozen eggs\"                            \n[19] \"Whites\"                                 \n[20] \"Yolks\"                                  \n[21] \"Whole and mixed\"                        \n[22] \"Unclassified\"                           \n[23] \"Total frozen eggs\"                      \n[24] \"\"                                       \n[25] \"Frozen poultry\"                         \n[26] \"Chicken\"                                \n[27] \"Broilers fryers and roasters\"           \n[28] \"Hens mature chickens\"                   \n[29] \"Breasts and breast meat\"                \n[30] \"Drumsticks\"                             \n[31] \"Leg quarters\"                           \n[32] \"Legs\"                                   \n[33] \"Thigh and thigh quarters\"               \n[34] \"Thigh Meat\"                             \n[35] \"Wings\"                                  \n[36] \"Paws and feet\"                          \n[37] \"Other\"                                  \n[38] \"Total chicken\"                          \n[39] \"\"                                       \n[40] \"Turkey\"                                 \n[41] \"Whole turkeys\"                          \n[42] \"Toms\"                                   \n[43] \"Hens\"                                   \n[44] \"Total whole turkeys\"                    \n[45] \"Breasts\"                                \n[46] \"Legs\"                                   \n[47] \"Mechanically deboned meat\"              \n[48] \"Other\"                                  \n[49] \"Unclassified\"                           \n[50] \"Total turkey\"                           \n[51] \"\"                                       \n[52] \"Ducks\"                                  \n[53] \"\"                                       \n[54] \"Total frozen poultry\"                   \n[55] \"---------------------------------------\"\n\n\nTo turn the list of columns into an actual data frame, we can use the dplyr package’s bind_cols function:\n\nlibrary(\"dplyr\")\ntab = bind_cols(tab)\nhead(tab, 10)\n\n# A tibble: 10 × 7\n   ...1                                      ...2  ...3  ...4  ...5  ...6  ...7 \n   &lt;chr&gt;                                     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 \"---------------------------------------\" \"---… \"---… \"---… \"---… \"---… \"---…\n 2 \"\"                                        \"---… \"100… \"---… \"---… \"ent… \"100…\n 3 \"\"                                        \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 4 \"Nuts\"                                    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 5 \"Shelled\"                                 \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 6 \"Pecans\"                                  \"309… \"385… \"344… \"112\" \"89\"  \"\"   \n 7 \"In-Shell\"                                \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 8 \"Pecans\"                                  \"637… \"443… \"476… \"75\"  \"107\" \"\"   \n 9 \"\"                                        \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n10 \"Dairy products\"                          \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n\n\nThe first few rows and the last row can be removed, since they don’t contain data. Then we can convert the individual columns to appropriate data types:\n\ntab = tab[-c(1:3, nrow(tab)), ]\ntab[2:7] = map(tab[2:7], as.numeric)\nhead(tab, 10)\n\n# A tibble: 10 × 7\n   ...1               ...2   ...3   ...4  ...5  ...6   ...7\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 \"Nuts\"               NA     NA     NA    NA    NA     NA\n 2 \"Shelled\"            NA     NA     NA    NA    NA     NA\n 3 \"Pecans\"          30906  38577  34489   112    89     NA\n 4 \"In-Shell\"           NA     NA     NA    NA    NA     NA\n 5 \"Pecans\"          63788  44339  47638    75   107     NA\n 6 \"\"                   NA     NA     NA    NA    NA     NA\n 7 \"Dairy products\"     NA     NA     NA    NA    NA     NA\n 8 \"Butter\"         210473 239658 199695    95    83 188566\n 9 \"Natural cheese\"     NA     NA     NA    NA    NA     NA\n10 \"American\"       834775 831213 815655    98    98     NA\n\n\nThe data frame is now sufficiently clean that we could use it for a simple analysis. Of course, there are many things we could do to improve the extracted data frame, such as identifying categories and subcategories in the first column, removing rows that are completely empty, and adding column names. These entail more string processing and data frame manipulation—if you want to practice your R skills, try doing them on your own.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Strings & Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/week07/string-processing.html#sec-regular-expression-examples",
    "href": "chapters/week07/string-processing.html#sec-regular-expression-examples",
    "title": "23  Strings & Regular Expressions",
    "section": "23.8 Regular Expression Examples",
    "text": "23.8 Regular Expression Examples\n\n\n\n\n\n\nImportant\n\n\n\nThis section is intended as a reference and is not taught in the live session.\n\n\nThis section provides examples of several different regular expression metacharacters and other features. Most of the examples use the str_view function, which is especially helpful for testing regular expressions. The function displays an HTML-rendered version of the string with the first match highlighted.\nThe RegExr website is also helpful for testing regular expressions; it provides an interactive interface where you can write regular expressions and see where they match a string.\n\n23.8.1 The Wildcard\nThe regex wildcard character is . and matches any single character. For example:\n\nx = \"dog\"\nstr_view(x, \"d.g\")\n\n\n\n\n\nBy default, regex searches from left to right:\n\nstr_view(x, \".\")\n\n\n\n\n\n\n\n23.8.2 Escape Sequences\nLike R, regular expressions can contain escape sequences that begin with a backslash. These are computed separately and after R escape sequences. The main use for escape sequences in regex is to turn a metacharacter into a literal character.\nFor example, suppose you want to match a literal dot .. The regex for a literal dot is \\.. Since backslashes in R strings have to be escaped, the R string for this regex is \"\\\\.. For example:\n\nstr_view(\"this.string\", \"\\\\.\")\n\n\n\n\n\nThe double backslash can be confusing, and it gets worse if you want to match a literal backslash. You have to escape the backslash in the regex (because backslash is the regex escape character) and then also have to escape the backslashes in R (because backslash is also the R escape character). So to match a single literal backslash in R, the code is:\n\nstr_view(\"this\\\\that\", \"\\\\\\\\\")\n\n\n\n\n\nRaw strings (see Tip 23.1) make regular expressions easier to read, because they make backslashes literal (but they still mark the beginning of an escape sequence in regex). You can use a raw string to write the above as:\n\nstr_view(r\"(this\\that)\", r\"(\\\\)\")\n\n\n\n\n\n\n\n23.8.3 Anchors\nBy default, a regex will match anywhere in the string. If you want to force a match at specific place, use an anchor.\nThe beginning of string anchor is ^. It marks the beginning of the string, but doesn’t count as a character in the pattern.\nFor example, suppose you want to match an a at the beginning of the string:\n\nx = c(\"abc\", \"cab\")\n\nstr_view(x, \"a\")\n\n\n\n\nstr_view(x, \"^a\")\n\n\n\n\n\nIt doesn’t make sense to put characters before ^, since no characters can come before the beginning of the string.\nLikewise, the end of string anchor is $. It marks the end of the string, but doesn’t count as a character in the pattern.\n\n\n23.8.4 Character Classes\nIn regex, square brackets [ ] denote a character class. A character class matches exactly one character, but that character can be any of the characters inside of the square brackets. The square brackets themselves don’t count as characters in the pattern.\nFor example, suppose you want to match c followed by either a or t:\n\nx = c(\"ca\", \"ct\", \"cat\", \"cta\")\n\nstr_view(x, \"c[ta]\")\n\n\n\n\n\nYou can use a dash - in a character class to create a range. For example, to match letters p through z:\n\nstr_view(x, \"c[p-z]\")\n\n\n\n\n\nRanges also work with numbers and capital letters. To match a literal dash, place the dash at the end of the character class (instead of between two other characters), as in [abc-].\nMost metacharacters are literal when inside a character class. For example, [.] matches a literal dot.\nA hat ^ at the beginning of the character class negates the class. So for example, [^abc] matches any one character except for a, b, or c:\n\nstr_view(\"abcdef\", \"[^abc]\")\n\n\n\n\n\n\n\n23.8.5 Quantifiers\nQuantifiers are metacharacters that affect how many times the preceding character must appear in a match. The quantifier itself doesn’t count as a character in the match.\nFor example, the question mark ? quantifier means the preceding character can appear 0 or 1 times. In other words, ? makes the preceding character optional. For example:\n\nx = c(\"abc\", \"ab\", \"ac\", \"abbc\")\n\nstr_view(x, \"ab?c\")\n\n\n\n\n\nThe star * quantifier means the preceding character can appear 0 or more times. In other words, * means the preceding character can appear any number of times or not at all. For instance:\n\nstr_view(x, \"ab*c\")\n\n\n\n\n\nThe plus + quantifier means the preceding character must appear 1 or more times.\nQuantifiers are greedy, meaning they always match as many characters as possible. In this example, notice that the pattern matches the entire string, even though it could also match just abba:\n\nstr_view(\"abbabbba\", \".+a\")\n\n\n\n\n\nYou can add a question mark ? after another quantifier to make it non-greedy:\n\nstr_view(\"abbabbba\", \".+?a\")\n\n\n\n\n\n\n\n23.8.6 Groups\nIn regex, parentheses ( ) denote a group. The parentheses themselves don’t count as characters in the pattern. Groups are useful for repeating or extracting specific parts of a pattern (see Section 23.6).\nQuantifiers can act on groups in addition to individual characters. For example, suppose you want to make the entire substring \", dogs,\" optional in a pattern, so that both of the test strings in this example match:\n\nx = c(\"cats, dogs, and frogs\", \"cats and frogs\")\n\nstr_view(x, \"cats(, dogs,)? and frogs\")",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Strings & Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/week07/getting-data-from-web.html",
    "href": "chapters/week07/getting-data-from-web.html",
    "title": "24  Getting Data from the Web",
    "section": "",
    "text": "24.1 How the Web Works\nThe internet in general and the World Wide Web specifically serves as a vast source of data suitable for answering a wide range of research questions. In this lesson we cover several common methods for acquiring this data.\nThe discipline of Data Science was, in large part, ushered into being by the increasing availability of information available on the World Wide Web or through other internet sources. Prior to the popularization of the internet as a publishing and communications platform, the majority of scientific research involved controlled studies in which researchers would collect their own data through various direct means (surveys, medical testing, etc.) in order to test a stated hypothesis.\nThe vast amount of information available on the internet disrupted this centuries-long dominance. Today, the dominant form of scientific research involves using data collected or produced by others for reasons having little or nothing to do with the research question being investigated by scholar. Users who post items about their favorite political candidate are not, for example, doing this so that sociologists can better under how politics function in America. However, their Tweets are being used in that and many other unforeseen capacities.\nBecause the internet provides such a rich trove of information for study, understanding how to effectively get, process, and prepare information from the internet for scientific research is a crucial skill for any data scientist. And in order to understand these workflows, the data scientist must first understand how the internet itself functions.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Getting Data from the Web</span>"
    ]
  },
  {
    "objectID": "chapters/week07/getting-data-from-web.html#how-the-web-works",
    "href": "chapters/week07/getting-data-from-web.html#how-the-web-works",
    "title": "24  Getting Data from the Web",
    "section": "",
    "text": "24.1.1 Client-Server Architecture\nThe base architecture and functioning of the internet is quite simple:\n\n\n\n\n\n\nFigure 24.1\n\n\n\n\nA content producer puts information on a computer called the server for others to retrieve;\nA user uses their local computer, called the client, to request the information from the sever;\nThe server delivers the information to the client.\n\nEach of the above detailed steps is accomplished using a technically complex but conceptually simple set of computer protocols. The technical details are beyond the scope of this course. We are here concerned with their conceptual architecture.\n\n24.1.1.1 Communication Between Clients and Servers\nAnytime a computer connects to any network, that computer is assigned a unique identifier known as an internet protocol (IP) address that uniquely identifies that computer on the network. IP addresses have the form x.x.x.x, where each x can be any integer from 0 to 255. For example, 169.237.102.141 is the current IP address of the computer that hosts the DataLab website. IP addresses are sometimes pre-designated for particular computers. A pre-designated IP address is known as static IP address. In other cases IP addresses are dynamically assigned from a range of available IP Address using a system known as the Dynamic Host Configuration Protocol (DHCP). Servers are typically assigned static IP addresses and clients are typically assigned dynamic IP addresses.\n\n\n\n\n\n\nFigure 24.2\n\n\n\nAs humans, we are used to accessing websites via a domain name (which we’ll discuss shortly), but you can also contact any server on the internet by simply typing the IP address into your browser address bar where you would normally enter the URL. For example, you can simply click on https://169.237.102.144 to access the DataLab website.\n\n\n\n\n\n\nNote\n\n\n\nYour browser may give you a security warning if you try to access a server directly using an IP address. For the link above, it is safe to proceed to the site.\n\n\n\n\n24.1.1.2 Domain Name Resolution\nIP addresses are the unique identifiers that make the internet work, but they are not very human friendly. To solve this problem, a system of domain name resolution was created. Under this system, internet service providers access a universal domain registry database that associates human readable domain names with machine readable IP addresses, and a secondary set of of internet connected servers known as domain name servers (DNS) provide a lookup service that translates domain names into IP addresses in the background. As the end-user, you enter and see only domain names, but the actual request process is a multi-step process in which domain names are translated to IP address in the background:\n\n\n\n\n\n\nFigure 24.3\n\n\n\n\nA content producer puts information on a computer called the server for others to retrieve;\nA user uses their local computer, called the client, to request the information from the sever using a domain name using request software such as a web browser;\nThe user’s client software first sends a request to a DNS server to retrieve the IP address of the server on the network associated with the entered domain name;\nThe DNS server returns the associated IP address to the client;\nThe client then makes the information request to the server using its retrieved IP address;\nThe server delivers the information to the client.\n\n\n\n24.1.1.3 Request Routing\nOur simple diagram of the client server process shows only two computers. But when you connect to the internet you are not, of course, creating a direct connection to a single computer. Rather, you are connecting to vase network of literally millions of computers, what we have come to refer to as the cloud.\nIn order to solve this problem, the internet backbone also deploys a routing system that directs requests and responses across the network to the appropriate servers and clients.\nWhen you connect to the WiFi network in your home, office, or the local coffee house, you are connecting to a router. That router receives all of your requests and, provided you are not requesting something directly from another computer that is connected to the same router, passes that request on to a larger routing network at your internet service provider (ISP). When the ISP routers receive your request, they check to see if you’re requesting something from a computer that is connected to their network. If it is, they deliver the request. If it is not, they pass the request on to another, regional routing network. And this routing process is repeated until your request if finally routed to the correct server.\n\n\n\n\n\n\nFigure 24.4\n\n\n\n\n\n24.1.1.4 The Server Response\nWhen a request is sent to a server across the internet, the request includes both the specific URL of the resource being request and also an hidden request header. The request header provides information to the server such as the IP address and the operating system of the client, the transfer protocol being used, and the software on the client that is making the request. The server uses this information to properly format its response and to route it back to the requesting client using the same IP routing process as described above.\n\n\n24.1.1.5 Internet Transfer Protocols\nAll of the information transferred between computers over the network is transferred as streams of binary data. In order to ensure data integrity, these streams are usually broken up into smaller packets of data which are transmitted independent of each other and then reassembled by the receiving computer once it has received all of the packets in the stream. The first packet returned (a header packet) typically delivers information about how many packets the client should expect to receive and about how they should be reassembled to recreate the original data stream.\nThere are many different standards for how data streams are divided into packets. One standard might, for example, break the stream into a collection of 50-byte packets, while another might use 100-byte packages. These standards are called protocols. The two protocols that are familiar to most users are HTTP and HTTPS, which define the hypertext transfer protocol and its sibling the hypertext transfer secure protocol. When you type a URL like https://datalab.ucdavis.edu into your browser, you are instructing the browser to use the HTTPS protocol to exchange information. Because HTTP and HTTPS are so common, most modern browsers do not require you to type the protocol name. They will simply insert the protocol for you in the background.\n\n\n\n24.1.2 Understanding URLs\nURL is an acronym for uniform resource locator. “Uniform” is a key term in this context. URLs are not arbitrary pointers to information. They are machine parsable, human readable, and can contain a lot of information.\nAll URLs are constructed using a standardized format. Consider the following URL:\nhttps://sfbaywildlife.info/species/common_birds.htm\nThere are actually several distinct components to the above URL:\n\n\n\n\n\n\nprotocol\nserver\npath to file\n\n\n\n\nhttps://\nsfbaywildlife.info\n/species/common_birds.htm\n\n\n\n\n\n\nWe’ve already discussed internet protocols and domain names. The file path portion of the URL can also provide valuable information about the server. It reads exactly like a Unix file path on the command line. The path /species/common_birds.htm indicates that the file common_birds.htm is in the species directory on the server.\n\n24.1.2.1 Dynamic Files\nIn the above example, when you enter the URL https://sfbaywildlife.info/species/common_birds.htm, your browser requests the file at /species/common_birds.html on the server. The server simply finds the file and delivers it to your web browser. We call this a static web server because the server itself does not do any processing of files prior to delivery. It simply receives requests for files living on the server and then sends them to the client, whose browser renders the file for viewing.\nMany websites, however, use dynamic processing. Pages with file extensions such as .php or .jsp, for example, include computer code in them. When these pages are requested by the server, the server executes the code in the designated file and sends the output of that execution to the requesting client rather than the actual file. Many sites, such as online stores and blogs, use this functionality to connect their web pages to active databases that track inventory and orders, for example.\n\n\n24.1.2.2 Query Strings\nDynamic websites, such as e-commerce sites that are connected to databases, require a mechanism for users to submit information to the server for processing. This is accomplished through one of two HTTP requests: GET or POST.\nPOST requests send submitted information to the server via a hidden HTTP header that is invisible to the end user. Scraping sites that require POST transactions is possible but can require significant sleuthing to determine the correct parameters and is beyond the scope of this course.\nGET requests, which are, happily for web scrapers more ubiquitous than POST requests, are much easier to understand. They are submitted via a query string that is simply appended to the request URL as in the following example:\nhttps://ebba.english.ucsb.edu/search_combined/?ft=dragon&numkw=52\nHere we see a query string appended to the end of the actual URL:\n\n\n\n\n\n\n\n\n\n\n\n\nprotocol\nserver\npath to file\nquery string\n\n\n\n\nhttps://\nebba.english.ucsb.edu\n/search_combined/index.php\n?ft=dragon&numkw=52\n\n\n\n\n\n\nQuery strings always appear at the end of the URL and begin with the ? character followed by a series of key/value pairs separated by the & character. In the above example we see that two parameters are submitted to the server via the query string as follows:\n\nft=dragon\nnumkw=52\n\nThe server will use these parameter values as input to perform a dynamic operation, in this case searching a database.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Getting Data from the Web</span>"
    ]
  },
  {
    "objectID": "chapters/week07/getting-data-from-web.html#accessing-data-online",
    "href": "chapters/week07/getting-data-from-web.html#accessing-data-online",
    "title": "24  Getting Data from the Web",
    "section": "24.2 Accessing Data Online",
    "text": "24.2 Accessing Data Online\nWhile there are many methods (and R packages) for acquiring data from the web, all fall into one of three general categories of data acquisition:\n\nDirect download describes the case where a data provider has provided a specific URL or web link from which you can download the data. For example, when you download data from the “Files” section of Canvas, you are using the direct download method of data acquisition.\nWeb application programming interfaces (web APIs) are web-accessible endpoints that you access via a URL, just as you would any website, but that are designed specifically to interact with computers (as opposed to humans). APIs receive requests and return data to the requester in machine, as opposed to human, readable formats, such as JSON or XML. We will learn more about working with APIs in this unit.\nScraping a web page means extracting information from human readable internet sources so that it can be used programmatically (for instance, in R). We will also learn more about Scraping in this unit.\n\nEach of the above general methods can be accomplished by applying any number of sub-methods and packages. And each brings with it its own degree of complexity and difficulty. As a general rule, the various ways you can get data from the web can be ranked according to difficulty from most to least convenient as follows:\n\nDirect download or “data dump”\nR or Python package (there are packages for many popular web APIs)\nDocumented web API\nUndocumented web API\nScraping",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Getting Data from the Web</span>"
    ]
  },
  {
    "objectID": "chapters/week07/getting-data-from-web.html#web-apis",
    "href": "chapters/week07/getting-data-from-web.html#web-apis",
    "title": "24  Getting Data from the Web",
    "section": "24.3 Web APIs",
    "text": "24.3 Web APIs\nAs noted earlier, a web application programming interface (API) provides a machine readable gateway for accessing data from the web. Most APIs provide programmatic access to the data that lives behind a human readable website. For example, most social media platforms such as Twitter, Facebook, and Instagram provide web APIs that allow computers to programmatically access the same data that you, as a human, see when you interact with these platforms via a web browser of mobile app. Some web APIs, however, are stand-alone, in that they provide machine access to data sources that have no human readable interface.\nOne of the challenges with working with web APIs is that, while there are some conventions for behavior, you need to know what and how to query a specific API in order to interact with it. Some APIs are well documented, while others are not. And some, for example the Twitter API, have extensive documentation that is frequently erroneous, incomplete, or out of date. As a result, working with web APIs can sometimes be challenging.\nFor this unit, we will work with the RESTCountries API which stores data on countries all over the world: their currencies, flags, population, etc. There are a large number of public web APIs out there, such as those listed by the public-api project.\n\n24.3.1 Querying a Web API\nThe R community has developed packages to facilitate interaction with many popular web APIs (Twitter, Facebook, etc.). Because web APIs are web-accessible, however, all can be accessed programmatically using basic internet request protocols, just as if you were going to a human readable webpage, provided you know how to formulate your request as a URL. The RESTcountries webpage publishes guidelines for accessing its API.\nWe can see from the documentation that the API will allow us to query a list of country features that appear in the database using a URL with the following construction:\n\"https://restcountries.com/v3.1/all?fields=name\"\nYou can actually go to this URL in your web browser and see the response, a portion of which is reproduced here:\n[{\"name\":{\"common\":\"American Samoa\",\"official\":\"American Samoa\",\"nativeName\":{\"eng\":{\"official\":\"American Samoa\",\"common\":\"American Samoa\"},\"smo\":{\"official\":\"Sāmoa Amelika\",\"common\":\"Sāmoa Amelika\"}}}},{\"name\":{\"common\":\"Peru\",\"official\":\"Republic of Peru\",\"nativeName\":{\"aym\":{\"official\":\"Piruw Suyu\",\"common\":\"Piruw\"},\"que\":{\"official\":\"Piruw Ripuwlika\",\"common\":\"Piruw\"},\"spa\":{\"official\":\"República del Perú\",\"common\":\"Perú\"}}}},{\"name\":{\"common\":\"Tonga\",\"official\":\"Kingdom of Tonga\",\"nativeName\":{\"eng\":{\"official\":\"Kingdom of Tonga\",\"common\":\"Tonga\"},\"ton\":{\"official\":\"Kingdom of Tonga\",\"common\":\"Tonga\"}}}}\nIf you look closely at this extract form the response, you will see that it contains HTML but is not HTML. Remember that APIs exist to deliver machine readable information. In this case, the API is delivering data in the JSON format, and some of the fields in the JSON object contain information provided as HTML.\nBecause the response is machine readable, we can make better use of the query if we run it in R rather than our web browser. Before we can do so, we need to setup our R environment to execute HTTP queries against the API and to process JSON. We’ll use the httr package to process our http transactions and the jsonlite package to process the JSON that we receive\ninstall.packages(\"httr\")\ninstall.packages(\"jsonlite\")\nWith our packages installed, we execute our query in R with one simple command:\n\nlibrary(\"httr\")\nlibrary(\"jsonlite\")\n\nresponse &lt;- GET(\"https://restcountries.com/v3.1/all?fields=name\")\n\nThe above executes an HTTP GET request (just like your web browser) to the identified query URL and loads it into an httr “response” object. If you take some time to examine the response object, you will see that it is a container object that contains a lot of useful information in addition to the actual JSON response that you see when you load the URL in your browser. For example, we can check the status of the response to see if it was successful by look at the response “status_code” which should be 200 if the query executed successfully.\n\nmessage(response$status_code)\n\n200\n\n\nThe actual content of the response can be found in the response object’s “content” element. Note, however, that we have to do some formatting on the object in order to access it. Go ahead and look at the actual response$content object:\n\nresponse$content\n\n    [1] 5b 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 56 61 74 69\n   [25] 63 61 6e 20 43 69 74 79 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 56 61\n   [49] 74 69 63 61 6e 20 43 69 74 79 20 53 74 61 74 65 22 2c 22 6e 61 74 69 76\n   [73] 65 4e 61 6d 65 22 3a 7b 22 69 74 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n   [97] 22 3a 22 53 74 61 74 6f 20 64 65 6c 6c 61 20 43 69 74 74 c3 a0 20 64 65\n  [121] 6c 20 56 61 74 69 63 61 6e 6f 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 56 61\n  [145] 74 69 63 61 6e 6f 22 7d 2c 22 6c 61 74 22 3a 7b 22 6f 66 66 69 63 69 61\n  [169] 6c 22 3a 22 53 74 61 74 75 73 20 43 69 76 69 74 61 74 69 73 20 56 61 74\n  [193] 69 63 61 6e c3 a6 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 56 61 74 69 63 61\n  [217] 6e c3 a6 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f\n  [241] 6e 22 3a 22 42 65 6c 67 69 75 6d 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a\n  [265] 22 4b 69 6e 67 64 6f 6d 20 6f 66 20 42 65 6c 67 69 75 6d 22 2c 22 6e 61\n  [289] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 64 65 75 22 3a 7b 22 6f 66 66 69 63\n  [313] 69 61 6c 22 3a 22 4b c3 b6 6e 69 67 72 65 69 63 68 20 42 65 6c 67 69 65\n  [337] 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 65 6c 67 69 65 6e 22 7d 2c 22\n  [361] 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 6f 79 61 75 6d\n  [385] 65 20 64 65 20 42 65 6c 67 69 71 75 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n  [409] 22 42 65 6c 67 69 71 75 65 22 7d 2c 22 6e 6c 64 22 3a 7b 22 6f 66 66 69\n  [433] 63 69 61 6c 22 3a 22 4b 6f 6e 69 6e 6b 72 69 6a 6b 20 42 65 6c 67 69 c3\n  [457] ab 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 65 6c 67 69 c3 ab 22 7d 7d 7d\n  [481] 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 6f 6d\n  [505] 6f 72 6f 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 55 6e 69 6f 6e 20\n  [529] 6f 66 20 74 68 65 20 43 6f 6d 6f 72 6f 73 22 2c 22 6e 61 74 69 76 65 4e\n  [553] 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n  [577] 22 d8 a7 d9 84 d8 a7 d8 aa d8 ad d8 a7 d8 af 20 d8 a7 d9 84 d9 82 d9 85\n  [601] d8 b1 d9 8a 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8 a7 d9 84 d9 82 d9 85\n  [625] d8 b1 e2 80 8e 22 7d 2c 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n  [649] 22 3a 22 55 6e 69 6f 6e 20 64 65 73 20 43 6f 6d 6f 72 65 73 22 2c 22 63\n  [673] 6f 6d 6d 6f 6e 22 3a 22 43 6f 6d 6f 72 65 73 22 7d 2c 22 7a 64 6a 22 3a\n  [697] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 55 64 7a 69 6d 61 20 77 61 20 4b\n  [721] 6f 6d 6f 72 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4b 6f 6d 6f 72 69 22\n  [745] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n  [769] 50 75 65 72 74 6f 20 52 69 63 6f 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a\n  [793] 22 43 6f 6d 6d 6f 6e 77 65 61 6c 74 68 20 6f 66 20 50 75 65 72 74 6f 20\n  [817] 52 69 63 6f 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67\n  [841] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6d 6d 6f 6e 77 65 61\n  [865] 6c 74 68 20 6f 66 20 50 75 65 72 74 6f 20 52 69 63 6f 22 2c 22 63 6f 6d\n  [889] 6d 6f 6e 22 3a 22 50 75 65 72 74 6f 20 52 69 63 6f 22 7d 2c 22 73 70 61\n  [913] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 45 73 74 61 64 6f 20 4c 69\n  [937] 62 72 65 20 41 73 6f 63 69 61 64 6f 20 64 65 20 50 75 65 72 74 6f 20 52\n  [961] 69 63 6f 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 50 75 65 72 74 6f 20 52 69\n  [985] 63 6f 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n [1009] 22 3a 22 55 6e 69 74 65 64 20 53 74 61 74 65 73 20 4d 69 6e 6f 72 20 4f\n [1033] 75 74 6c 79 69 6e 67 20 49 73 6c 61 6e 64 73 22 2c 22 6f 66 66 69 63 69\n [1057] 61 6c 22 3a 22 55 6e 69 74 65 64 20 53 74 61 74 65 73 20 4d 69 6e 6f 72\n [1081] 20 4f 75 74 6c 79 69 6e 67 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74 69\n [1105] 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61\n [1129] 6c 22 3a 22 55 6e 69 74 65 64 20 53 74 61 74 65 73 20 4d 69 6e 6f 72 20\n [1153] 4f 75 74 6c 79 69 6e 67 20 49 73 6c 61 6e 64 73 22 2c 22 63 6f 6d 6d 6f\n [1177] 6e 22 3a 22 55 6e 69 74 65 64 20 53 74 61 74 65 73 20 4d 69 6e 6f 72 20\n [1201] 4f 75 74 6c 79 69 6e 67 20 49 73 6c 61 6e 64 73 22 7d 7d 7d 7d 2c 7b 22\n [1225] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42 6f 75 76 65 74 20\n [1249] 49 73 6c 61 6e 64 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 6f 75 76\n [1273] 65 74 20 49 73 6c 61 6e 64 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a\n [1297] 7b 22 6e 6f 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 6f 75 76\n [1321] 65 74 c3 b8 79 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 6f 75 76 65 74\n [1345] c3 b8 79 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d\n [1369] 6f 6e 22 3a 22 54 61 6e 7a 61 6e 69 61 22 2c 22 6f 66 66 69 63 69 61 6c\n [1393] 22 3a 22 55 6e 69 74 65 64 20 52 65 70 75 62 6c 69 63 20 6f 66 20 54 61\n [1417] 6e 7a 61 6e 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65\n [1441] 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 55 6e 69 74 65 64 20\n [1465] 52 65 70 75 62 6c 69 63 20 6f 66 20 54 61 6e 7a 61 6e 69 61 22 2c 22 63\n [1489] 6f 6d 6d 6f 6e 22 3a 22 54 61 6e 7a 61 6e 69 61 22 7d 2c 22 73 77 61 22\n [1513] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4a 61 6d 68 75 72 69 20 79 61\n [1537] 20 4d 75 75 6e 67 61 6e 6f 20 77 61 20 54 61 6e 7a 61 6e 69 61 22 2c 22\n [1561] 63 6f 6d 6d 6f 6e 22 3a 22 54 61 6e 7a 61 6e 69 61 22 7d 7d 7d 7d 2c 7b\n [1585] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 6f 72 74 68 20\n [1609] 4d 61 63 65 64 6f 6e 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n [1633] 65 70 75 62 6c 69 63 20 6f 66 20 4e 6f 72 74 68 20 4d 61 63 65 64 6f 6e\n [1657] 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6d 6b 64 22 3a\n [1681] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d0 a0 d0 b5 d0 bf d1 83 d0 b1 d0\n [1705] bb d0 b8 d0 ba d0 b0 20 d0 a1 d0 b5 d0 b2 d0 b5 d1 80 d0 bd d0 b0 20 d0\n [1729] 9c d0 b0 d0 ba d0 b5 d0 b4 d0 be d0 bd d0 b8 d1 98 d0 b0 22 2c 22 63 6f\n [1753] 6d 6d 6f 6e 22 3a 22 d0 9c d0 b0 d0 ba d0 b5 d0 b4 d0 be d0 bd d0 b8 d1\n [1777] 98 d0 b0 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f\n [1801] 6e 22 3a 22 53 61 75 64 69 20 41 72 61 62 69 61 22 2c 22 6f 66 66 69 63\n [1825] 69 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20 53 61 75 64 69 20 41\n [1849] 72 61 62 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72\n [1873] 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 a7 d9 84 d9 85 d9 85\n [1897] d9 84 d9 83 d8 a9 20 d8 a7 d9 84 d8 b9 d8 b1 d8 a8 d9 8a d8 a9 20 d8 a7\n [1921] d9 84 d8 b3 d8 b9 d9 88 d8 af d9 8a d8 a9 22 2c 22 63 6f 6d 6d 6f 6e 22\n [1945] 3a 22 d8 a7 d9 84 d8 b9 d8 b1 d8 a8 d9 8a d8 a9 20 d8 a7 d9 84 d8 b3 d8\n [1969] b9 d9 88 d8 af d9 8a d8 a9 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b\n [1993] 22 63 6f 6d 6d 6f 6e 22 3a 22 43 68 72 69 73 74 6d 61 73 20 49 73 6c 61\n [2017] 6e 64 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 65 72 72 69 74 6f 72\n [2041] 79 20 6f 66 20 43 68 72 69 73 74 6d 61 73 20 49 73 6c 61 6e 64 22 2c 22\n [2065] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66\n [2089] 69 63 69 61 6c 22 3a 22 54 65 72 72 69 74 6f 72 79 20 6f 66 20 43 68 72\n [2113] 69 73 74 6d 61 73 20 49 73 6c 61 6e 64 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n [2137] 22 43 68 72 69 73 74 6d 61 73 20 49 73 6c 61 6e 64 22 7d 7d 7d 7d 2c 7b\n [2161] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 6c 6f 76 61 6b\n [2185] 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 6c 6f 76 61 6b 20 52\n [2209] 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n [2233] 73 6c 6b 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 6c 6f 76 65 6e\n [2257] 73 6b c3 a1 20 72 65 70 75 62 6c 69 6b 61 22 2c 22 63 6f 6d 6d 6f 6e 22\n [2281] 3a 22 53 6c 6f 76 65 6e 73 6b 6f 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n [2305] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 6f 72 74 68 20 4b 6f 72 65 61 22\n [2329] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 44 65 6d 6f 63 72 61 74 69 63 20\n [2353] 50 65 6f 70 6c 65 27 73 20 52 65 70 75 62 6c 69 63 20 6f 66 20 4b 6f 72\n [2377] 65 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6b 6f 72 22 3a\n [2401] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 ec a1 b0 ec 84 a0 eb af bc ec a3\n [2425] bc ec a3 bc ec 9d 98 ec 9d b8 eb af bc ea b3 b5 ed 99 94 ea b5 ad 22 2c\n [2449] 22 63 6f 6d 6d 6f 6e 22 3a 22 ec a1 b0 ec 84 a0 22 7d 7d 7d 7d 2c 7b 22\n [2473] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 76 61 6c 62 61 72\n [2497] 64 20 61 6e 64 20 4a 61 6e 20 4d 61 79 65 6e 22 2c 22 6f 66 66 69 63 69\n [2521] 61 6c 22 3a 22 53 76 61 6c 62 61 72 64 20 6f 67 20 4a 61 6e 20 4d 61 79\n [2545] 65 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6e 6f 72 22 3a\n [2569] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 76 61 6c 62 61 72 64 20 6f 67\n [2593] 20 4a 61 6e 20 4d 61 79 65 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 76\n [2617] 61 6c 62 61 72 64 20 6f 67 20 4a 61 6e 20 4d 61 79 65 6e 22 7d 7d 7d 7d\n [2641] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 65 6e 74\n [2665] 72 61 6c 20 41 66 72 69 63 61 6e 20 52 65 70 75 62 6c 69 63 22 2c 22 6f\n [2689] 66 66 69 63 69 61 6c 22 3a 22 43 65 6e 74 72 61 6c 20 41 66 72 69 63 61\n [2713] 6e 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n [2737] 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9\n [2761] 70 75 62 6c 69 71 75 65 20 63 65 6e 74 72 61 66 72 69 63 61 69 6e 65 22\n [2785] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 63\n [2809] 65 6e 74 72 61 66 72 69 63 61 69 6e 65 22 7d 2c 22 73 61 67 22 3a 7b 22\n [2833] 6f 66 66 69 63 69 61 6c 22 3a 22 4b c3 b6 64 c3 b6 72 c3 b6 73 c3 aa 73\n [2857] 65 20 74 c3 ae 20 42 c3 aa 61 66 72 c3 ae 6b 61 22 2c 22 63 6f 6d 6d 6f\n [2881] 6e 22 3a 22 42 c3 aa 61 66 72 c3 ae 6b 61 22 7d 7d 7d 7d 2c 7b 22 6e 61\n [2905] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 61 6d 62 69 61 22 2c 22\n [2929] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 5a\n [2953] 61 6d 62 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e\n [2977] 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n [3001] 20 6f 66 20 5a 61 6d 62 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 61\n [3025] 6d 62 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d\n [3049] 6f 6e 22 3a 22 4a 61 6d 61 69 63 61 22 2c 22 6f 66 66 69 63 69 61 6c 22\n [3073] 3a 22 4a 61 6d 61 69 63 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a\n [3097] 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4a 61 6d 61\n [3121] 69 63 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4a 61 6d 61 69 63 61 22 7d\n [3145] 2c 22 6a 61 6d 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4a 61 6d 61\n [3169] 69 63 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4a 61 6d 61 69 63 61 22 7d\n [3193] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 50\n [3217] 68 69 6c 69 70 70 69 6e 65 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22\n [3241] 52 65 70 75 62 6c 69 63 20 6f 66 20 74 68 65 20 50 68 69 6c 69 70 70 69\n [3265] 6e 65 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22\n [3289] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f\n [3313] 66 20 74 68 65 20 50 68 69 6c 69 70 70 69 6e 65 73 22 2c 22 63 6f 6d 6d\n [3337] 6f 6e 22 3a 22 50 68 69 6c 69 70 70 69 6e 65 73 22 7d 2c 22 66 69 6c 22\n [3361] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f\n [3385] 66 20 74 68 65 20 50 68 69 6c 69 70 70 69 6e 65 73 22 2c 22 63 6f 6d 6d\n [3409] 6f 6e 22 3a 22 50 69 6c 69 70 69 6e 61 73 22 7d 7d 7d 7d 2c 7b 22 6e 61\n [3433] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 6f 6e 74 65 6e 65 67 72\n [3457] 6f 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4d 6f 6e 74 65 6e 65 67 72\n [3481] 6f 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 63 6e 72 22 3a 7b\n [3505] 22 6f 66 66 69 63 69 61 6c 22 3a 22 d0 a6 d1 80 d0 bd d0 b0 20 d0 93 d0\n [3529] be d1 80 d0 b0 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 a6 d1 80 d0 bd d0\n [3553] b0 20 d0 93 d0 be d1 80 d0 b0 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a\n [3577] 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 68 69 6c 65 22 2c 22 6f 66 66 69 63\n [3601] 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 43 68 69 6c 65 22\n [3625] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f\n [3649] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 20\n [3673] 43 68 69 6c 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 68 69 6c 65 22 7d\n [3697] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4f\n [3721] 6d 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 75 6c 74 61 6e 61\n [3745] 74 65 20 6f 66 20 4f 6d 61 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n [3769] 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 b3 d9\n [3793] 84 d8 b7 d9 86 d8 a9 20 d8 b9 d9 85 d8 a7 d9 86 22 2c 22 63 6f 6d 6d 6f\n [3817] 6e 22 3a 22 d8 b9 d9 85 d8 a7 d9 86 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n [3841] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 41 72 6d 65 6e 69 61 22 2c 22 6f\n [3865] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 41 72\n [3889] 6d 65 6e 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 68 79\n [3913] 65 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d5 80 d5 a1 d5 b5 d5 a1\n [3937] d5 bd d5 bf d5 a1 d5 b6 d5 ab 20 d5 80 d5 a1 d5 b6 d6 80 d5 a1 d5 ba d5\n [3961] a5 d5 bf d5 b8 d6 82 d5 a9 d5 b5 d5 b8 d6 82 d5 b6 22 2c 22 63 6f 6d 6d\n [3985] 6f 6e 22 3a 22 d5 80 d5 a1 d5 b5 d5 a1 d5 bd d5 bf d5 a1 d5 b6 22 7d 7d\n [4009] 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 50 61\n [4033] 6b 69 73 74 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 49 73 6c 61\n [4057] 6d 69 63 20 52 65 70 75 62 6c 69 63 20 6f 66 20 50 61 6b 69 73 74 61 6e\n [4081] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22\n [4105] 6f 66 66 69 63 69 61 6c 22 3a 22 49 73 6c 61 6d 69 63 20 52 65 70 75 62\n [4129] 6c 69 63 20 6f 66 20 50 61 6b 69 73 74 61 6e 22 2c 22 63 6f 6d 6d 6f 6e\n [4153] 22 3a 22 50 61 6b 69 73 74 61 6e 22 7d 2c 22 75 72 64 22 3a 7b 22 6f 66\n [4177] 66 69 63 69 61 6c 22 3a 22 d8 a7 d8 b3 d9 84 d8 a7 d9 85 db 8c 20 d8 ac\n [4201] d9 85 db 81 d9 88 d8 b1 db 8c db 82 20 d9 be d8 a7 d9 83 d8 b3 d8 aa d8\n [4225] a7 d9 86 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d9 be d8 a7 d9 83 d8 b3 d8\n [4249] aa d8 a7 d9 86 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d\n [4273] 6d 6f 6e 22 3a 22 54 6f 6b 65 6c 61 75 22 2c 22 6f 66 66 69 63 69 61 6c\n [4297] 22 3a 22 54 6f 6b 65 6c 61 75 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n [4321] 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 6f 6b\n [4345] 65 6c 61 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54 6f 6b 65 6c 61 75 22\n [4369] 7d 2c 22 73 6d 6f 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 6f 6b\n [4393] 65 6c 61 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54 6f 6b 65 6c 61 75 22\n [4417] 7d 2c 22 74 6b 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 6f 6b\n [4441] 65 6c 61 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54 6f 6b 65 6c 61 75 22\n [4465] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n [4489] 42 65 6e 69 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62\n [4513] 6c 69 63 20 6f 66 20 42 65 6e 69 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n [4537] 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n [4561] c3 a9 70 75 62 6c 69 71 75 65 20 64 75 20 42 c3 a9 6e 69 6e 22 2c 22 63\n [4585] 6f 6d 6d 6f 6e 22 3a 22 42 c3 a9 6e 69 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61\n [4609] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61 69 6e 74 20 50 69 65\n [4633] 72 72 65 20 61 6e 64 20 4d 69 71 75 65 6c 6f 6e 22 2c 22 6f 66 66 69 63\n [4657] 69 61 6c 22 3a 22 53 61 69 6e 74 20 50 69 65 72 72 65 20 61 6e 64 20 4d\n [4681] 69 71 75 65 6c 6f 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n [4705] 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6c 6c 65 63\n [4729] 74 69 76 69 74 c3 a9 20 74 65 72 72 69 74 6f 72 69 61 6c 65 20 64 65 20\n [4753] 53 61 69 6e 74 2d 50 69 65 72 72 65 2d 65 74 2d 4d 69 71 75 65 6c 6f 6e\n [4777] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61 69 6e 74 2d 50 69 65 72 72 65\n [4801] 2d 65 74 2d 4d 69 71 75 65 6c 6f 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n [4825] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 6f 73 74 61 20 52 69 63 61 22\n [4849] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66\n [4873] 20 43 6f 73 74 61 20 52 69 63 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n [4897] 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65\n [4921] 70 c3 ba 62 6c 69 63 61 20 64 65 20 43 6f 73 74 61 20 52 69 63 61 22 2c\n [4945] 22 63 6f 6d 6d 6f 6e 22 3a 22 43 6f 73 74 61 20 52 69 63 61 22 7d 7d 7d\n [4969] 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 65\n [4993] 72 6e 73 65 79 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 61 69 6c 69\n [5017] 77 69 63 6b 20 6f 66 20 47 75 65 72 6e 73 65 79 22 2c 22 6e 61 74 69 76\n [5041] 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n [5065] 22 3a 22 42 61 69 6c 69 77 69 63 6b 20 6f 66 20 47 75 65 72 6e 73 65 79\n [5089] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 65 72 6e 73 65 79 22 7d 2c 22\n [5113] 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 61 69 6c 6c 69\n [5137] 61 67 65 20 64 65 20 47 75 65 72 6e 65 73 65 79 22 2c 22 63 6f 6d 6d 6f\n [5161] 6e 22 3a 22 47 75 65 72 6e 65 73 65 79 22 7d 2c 22 6e 66 72 22 3a 7b 22\n [5185] 6f 66 66 69 63 69 61 6c 22 3a 22 44 67 c3 a8 72 6e c3 a9 73 69 61 69 73\n [5209] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 44 67 c3 a8 72 6e c3 a9 73 69 61 69\n [5233] 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n [5257] 3a 22 42 72 69 74 69 73 68 20 56 69 72 67 69 6e 20 49 73 6c 61 6e 64 73\n [5281] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 56 69 72 67 69 6e 20 49 73 6c\n [5305] 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67\n [5329] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 56 69 72 67 69 6e 20 49 73\n [5353] 6c 61 6e 64 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 72 69 74 69 73 68\n [5377] 20 56 69 72 67 69 6e 20 49 73 6c 61 6e 64 73 22 7d 7d 7d 7d 2c 7b 22 6e\n [5401] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 75 72 69 6e 61 6d 65\n [5425] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f\n [5449] 66 20 53 75 72 69 6e 61 6d 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n [5473] 3a 7b 22 6e 6c 64 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n [5497] 75 62 6c 69 65 6b 20 53 75 72 69 6e 61 6d 65 22 2c 22 63 6f 6d 6d 6f 6e\n [5521] 22 3a 22 53 75 72 69 6e 61 6d 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n [5545] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 48 75 6e 67 61 72 79 22 2c 22 6f 66\n [5569] 66 69 63 69 61 6c 22 3a 22 48 75 6e 67 61 72 79 22 2c 22 6e 61 74 69 76\n [5593] 65 4e 61 6d 65 22 3a 7b 22 68 75 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n [5617] 22 3a 22 4d 61 67 79 61 72 6f 72 73 7a c3 a1 67 22 2c 22 63 6f 6d 6d 6f\n [5641] 6e 22 3a 22 4d 61 67 79 61 72 6f 72 73 7a c3 a1 67 22 7d 7d 7d 7d 2c 7b\n [5665] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 65 74 68 65 72\n [5689] 6c 61 6e 64 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67 64\n [5713] 6f 6d 20 6f 66 20 74 68 65 20 4e 65 74 68 65 72 6c 61 6e 64 73 22 2c 22\n [5737] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6e 6c 64 22 3a 7b 22 6f 66 66\n [5761] 69 63 69 61 6c 22 3a 22 4b 6f 6e 69 6e 6b 72 69 6a 6b 20 64 65 72 20 4e\n [5785] 65 64 65 72 6c 61 6e 64 65 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 65\n [5809] 64 65 72 6c 61 6e 64 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n [5833] 6f 6d 6d 6f 6e 22 3a 22 55 7a 62 65 6b 69 73 74 61 6e 22 2c 22 6f 66 66\n [5857] 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 55 7a 62 65\n [5881] 6b 69 73 74 61 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 72\n [5905] 75 73 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d0 a0 d0 b5 d1 81 d0\n [5929] bf d1 83 d0 b1 d0 bb d0 b8 d0 ba d0 b0 20 d0 a3 d0 b7 d0 b1 d0 b5 d0 ba\n [5953] d0 b8 d1 81 d1 82 d0 b0 d0 bd 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 a3\n [5977] d0 b7 d0 b1 d0 b5 d0 ba d0 b8 d1 81 d1 82 d0 b0 d0 bd 22 7d 2c 22 75 7a\n [6001] 62 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4f 27 7a 62 65 6b 69 73\n [6025] 74 6f 6e 20 52 65 73 70 75 62 6c 69 6b 61 73 69 22 2c 22 63 6f 6d 6d 6f\n [6049] 6e 22 3a 22 4f e2 80 98 7a 62 65 6b 69 73 74 6f 6e 22 7d 7d 7d 7d 2c 7b\n [6073] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 45 72 69 74 72 65\n [6097] 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 74 61 74 65 20 6f 66 20\n [6121] 45 72 69 74 72 65 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n [6145] 61 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 af d9 88 d9 84\n [6169] d8 a9 20 d8 a5 d8 b1 d8 aa d8 b1 d9 8a d8 a7 22 2c 22 63 6f 6d 6d 6f 6e\n [6193] 22 3a 22 d8 a5 d8 b1 d8 aa d8 b1 d9 8a d8 a7 e2 80 8e 22 7d 2c 22 65 6e\n [6217] 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 74 61 74 65 20 6f 66\n [6241] 20 45 72 69 74 72 65 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 45 72 69 74\n [6265] 72 65 61 22 7d 2c 22 74 69 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n [6289] 22 e1 88 83 e1 8c 88 e1 88 a8 20 e1 8a a4 e1 88 ad e1 89 b5 e1 88 ab 22\n [6313] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e1 8a a4 e1 88 ad e1 89 b5 e1 88 ab 22\n [6337] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n [6361] 53 69 65 72 72 61 20 4c 65 6f 6e 65 22 2c 22 6f 66 66 69 63 69 61 6c 22\n [6385] 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 53 69 65 72 72 61 20 4c 65 6f\n [6409] 6e 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a\n [6433] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66\n [6457] 20 53 69 65 72 72 61 20 4c 65 6f 6e 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n [6481] 22 53 69 65 72 72 61 20 4c 65 6f 6e 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n [6505] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4b 75 77 61 69 74 22 2c 22 6f\n [6529] 66 66 69 63 69 61 6c 22 3a 22 53 74 61 74 65 20 6f 66 20 4b 75 77 61 69\n [6553] 74 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b\n [6577] 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 af d9 88 d9 84 d8 a9 20 d8 a7 d9\n [6601] 84 d9 83 d9 88 d9 8a d8 aa 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8 a7 d9\n [6625] 84 d9 83 d9 88 d9 8a d8 aa 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b\n [6649] 22 63 6f 6d 6d 6f 6e 22 3a 22 42 68 75 74 61 6e 22 2c 22 6f 66 66 69 63\n [6673] 69 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20 42 68 75 74 61 6e 22\n [6697] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 64 7a 6f 22 3a 7b 22 6f\n [6721] 66 66 69 63 69 61 6c 22 3a 22 e0 bd a0 e0 bd 96 e0 be b2 e0 bd b4 e0 bd\n [6745] 82 e0 bc 8b e0 bd a2 e0 be 92 e0 be b1 e0 bd a3 e0 bc 8b e0 bd 81 e0 bd\n [6769] 96 e0 bc 8b 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e0 bd a0 e0 bd 96 e0 be\n [6793] b2 e0 bd b4 e0 bd 82 e0 bc 8b e0 bd a1 e0 bd b4 e0 bd a3 e0 bc 8b 22 7d\n [6817] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42\n [6841] 65 72 6d 75 64 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 65 72 6d\n [6865] 75 64 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22\n [6889] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 65 72 6d 75 64 61 22 2c 22\n [6913] 63 6f 6d 6d 6f 6e 22 3a 22 42 65 72 6d 75 64 61 22 7d 7d 7d 7d 2c 7b 22\n [6937] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 79 70 72 75 73 22\n [6961] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66\n [6985] 20 43 79 70 72 75 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n [7009] 65 6c 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 ce 94 ce b7 ce bc\n [7033] ce bf ce ba cf 81 ce b1 cf 84 ce af ce b1 20 cf 84 ce b7 cf 82 20 ce 9a\n [7057] cf 8d cf 80 cf 81 ce bf cf 82 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 ce 9a\n [7081] cf 8d cf 80 cf 81 ce bf cf 82 22 7d 2c 22 74 75 72 22 3a 7b 22 6f 66 66\n [7105] 69 63 69 61 6c 22 3a 22 4b c4 b1 62 72 c4 b1 73 20 43 75 6d 68 75 72 69\n [7129] 79 65 74 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4b c4 b1 62 72 c4 b1 73\n [7153] 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a\n [7177] 22 46 72 61 6e 63 65 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 72 65\n [7201] 6e 63 68 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n [7225] 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n [7249] c3 a9 70 75 62 6c 69 71 75 65 20 66 72 61 6e c3 a7 61 69 73 65 22 2c 22\n [7273] 63 6f 6d 6d 6f 6e 22 3a 22 46 72 61 6e 63 65 22 7d 7d 7d 7d 2c 7b 22 6e\n [7297] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 50 6f 6c 61 6e 64 22 2c\n [7321] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20\n [7345] 50 6f 6c 61 6e 64 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 70\n [7369] 6f 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 7a 65 63 7a 70 6f\n [7393] 73 70 6f 6c 69 74 61 20 50 6f 6c 73 6b 61 22 2c 22 63 6f 6d 6d 6f 6e 22\n [7417] 3a 22 50 6f 6c 73 6b 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22\n [7441] 63 6f 6d 6d 6f 6e 22 3a 22 41 75 73 74 72 69 61 22 2c 22 6f 66 66 69 63\n [7465] 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 41 75 73 74 72 69\n [7489] 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 62 61 72 22 3a 7b\n [7513] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 6b 20 c3 96 73\n [7537] 74 65 72 72 65 69 63 68 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 c3 96 73 74\n [7561] 65 72 72 65 69 63 68 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n [7585] 6f 6d 6d 6f 6e 22 3a 22 4c 69 65 63 68 74 65 6e 73 74 65 69 6e 22 2c 22\n [7609] 6f 66 66 69 63 69 61 6c 22 3a 22 50 72 69 6e 63 69 70 61 6c 69 74 79 20\n [7633] 6f 66 20 4c 69 65 63 68 74 65 6e 73 74 65 69 6e 22 2c 22 6e 61 74 69 76\n [7657] 65 4e 61 6d 65 22 3a 7b 22 64 65 75 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n [7681] 22 3a 22 46 c3 bc 72 73 74 65 6e 74 75 6d 20 4c 69 65 63 68 74 65 6e 73\n [7705] 74 65 69 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4c 69 65 63 68 74 65 6e\n [7729] 73 74 65 69 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d\n [7753] 6d 6f 6e 22 3a 22 43 61 6d 62 6f 64 69 61 22 2c 22 6f 66 66 69 63 69 61\n [7777] 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20 43 61 6d 62 6f 64 69 61 22\n [7801] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6b 68 6d 22 3a 7b 22 6f\n [7825] 66 66 69 63 69 61 6c 22 3a 22 e1 9e 96 e1 9f 92 e1 9e 9a e1 9f 87 e1 9e\n [7849] 9a e1 9e b6 e1 9e 87 e1 9e b6 e1 9e 8e e1 9e b6 e1 9e 85 e1 9e 80 e1 9f\n [7873] 92 e1 9e 9a e1 9e 80 e1 9e 98 e1 9f 92 e1 9e 96 e1 9e bb e1 9e 87 e1 9e\n [7897] b6 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4b c3 a2 6d 70 c5 ad 63 68 c3 a9\n [7921] 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n [7945] 3a 22 53 61 69 6e 74 20 4b 69 74 74 73 20 61 6e 64 20 4e 65 76 69 73 22\n [7969] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 65 64 65 72 61 74 69 6f 6e 20\n [7993] 6f 66 20 53 61 69 6e 74 20 43 68 72 69 73 74 6f 70 68 65 72 20 61 6e 64\n [8017] 20 4e 65 76 69 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65\n [8041] 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 65 64 65 72 61 74\n [8065] 69 6f 6e 20 6f 66 20 53 61 69 6e 74 20 43 68 72 69 73 74 6f 70 68 65 72\n [8089] 20 61 6e 64 20 4e 65 76 69 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61\n [8113] 69 6e 74 20 4b 69 74 74 73 20 61 6e 64 20 4e 65 76 69 73 22 7d 7d 7d 7d\n [8137] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61 69 6e\n [8161] 74 20 4c 75 63 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 61 69\n [8185] 6e 74 20 4c 75 63 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b\n [8209] 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 61 69 6e 74\n [8233] 20 4c 75 63 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61 69 6e 74 20\n [8257] 4c 75 63 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d\n [8281] 6d 6f 6e 22 3a 22 47 72 65 6e 61 64 61 22 2c 22 6f 66 66 69 63 69 61 6c\n [8305] 22 3a 22 47 72 65 6e 61 64 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n [8329] 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 47 72 65\n [8353] 6e 61 64 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 47 72 65 6e 61 64 61 22\n [8377] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n [8401] 47 65 72 6d 61 6e 79 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 65 64\n [8425] 65 72 61 6c 20 52 65 70 75 62 6c 69 63 20 6f 66 20 47 65 72 6d 61 6e 79\n [8449] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 64 65 75 22 3a 7b 22\n [8473] 6f 66 66 69 63 69 61 6c 22 3a 22 42 75 6e 64 65 73 72 65 70 75 62 6c 69\n [8497] 6b 20 44 65 75 74 73 63 68 6c 61 6e 64 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n [8521] 22 44 65 75 74 73 63 68 6c 61 6e 64 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n [8545] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 69 6e 65 61 22 2c 22 6f 66\n [8569] 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 47 75 69\n [8593] 6e 65 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22\n [8617] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75\n [8641] 65 20 64 65 20 47 75 69 6e c3 a9 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n [8665] 47 75 69 6e c3 a9 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n [8689] 6f 6d 6d 6f 6e 22 3a 22 49 6e 64 69 61 22 2c 22 6f 66 66 69 63 69 61 6c\n [8713] 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 49 6e 64 69 61 22 2c 22 6e\n [8737] 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69\n [8761] 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 49 6e 64 69 61\n [8785] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 49 6e 64 69 61 22 7d 2c 22 68 69 6e\n [8809] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 e0 a4 ad e0 a4 be e0 a4 b0\n [8833] e0 a4 a4 20 e0 a4 97 e0 a4 a3 e0 a4 b0 e0 a4 be e0 a4 9c e0 a5 8d e0 a4\n [8857] af 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e0 a4 ad e0 a4 be e0 a4 b0 e0 a4\n [8881] a4 22 7d 2c 22 74 61 6d 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 e0\n [8905] ae 87 e0 ae a8 e0 af 8d e0 ae a4 e0 ae bf e0 ae af e0 ae 95 e0 af 8d 20\n [8929] e0 ae 95 e0 af 81 e0 ae 9f e0 ae bf e0 ae af e0 ae b0 e0 ae 9a e0 af 81\n [8953] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e0 ae 87 e0 ae a8 e0 af 8d e0 ae a4\n [8977] e0 ae bf e0 ae af e0 ae be 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b\n [9001] 22 63 6f 6d 6d 6f 6e 22 3a 22 45 71 75 61 74 6f 72 69 61 6c 20 47 75 69\n [9025] 6e 65 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69\n [9049] 63 20 6f 66 20 45 71 75 61 74 6f 72 69 61 6c 20 47 75 69 6e 65 61 22 2c\n [9073] 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66\n [9097] 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 65 20\n [9121] 6c 61 20 47 75 69 6e c3 a9 65 20 c3 89 71 75 61 74 6f 72 69 61 6c 65 22\n [9145] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 69 6e c3 a9 65 20 c3 a9 71 75 61\n [9169] 74 6f 72 69 61 6c 65 22 7d 2c 22 70 6f 72 22 3a 7b 22 6f 66 66 69 63 69\n [9193] 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 64 61 20 47 75 69 6e c3\n [9217] a9 20 45 71 75 61 74 6f 72 69 61 6c 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n [9241] 47 75 69 6e c3 a9 20 45 71 75 61 74 6f 72 69 61 6c 22 7d 2c 22 73 70 61\n [9265] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63\n [9289] 61 20 64 65 20 47 75 69 6e 65 61 20 45 63 75 61 74 6f 72 69 61 6c 22 2c\n [9313] 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 69 6e 65 61 20 45 63 75 61 74 6f 72\n [9337] 69 61 6c 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f\n [9361] 6e 22 3a 22 53 75 64 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n [9385] 65 70 75 62 6c 69 63 20 6f 66 20 74 68 65 20 53 75 64 61 6e 22 2c 22 6e\n [9409] 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69\n [9433] 63 69 61 6c 22 3a 22 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8 a9 20 d8 a7\n [9457] d9 84 d8 b3 d9 88 d8 af d8 a7 d9 86 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n [9481] d8 a7 d9 84 d8 b3 d9 88 d8 af d8 a7 d9 86 22 7d 2c 22 65 6e 67 22 3a 7b\n [9505] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20\n [9529] 74 68 65 20 53 75 64 61 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 75 64\n [9553] 61 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n [9577] 22 3a 22 44 6a 69 62 6f 75 74 69 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a\n [9601] 22 52 65 70 75 62 6c 69 63 20 6f 66 20 44 6a 69 62 6f 75 74 69 22 2c 22\n [9625] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66\n [9649] 69 63 69 61 6c 22 3a 22 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8 a9 20 d8\n [9673] ac d9 8a d8 a8 d9 88 d8 aa d9 8a 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8\n [9697] ac d9 8a d8 a8 d9 88 d8 aa d9 8a e2 80 8e 22 7d 2c 22 66 72 61 22 3a 7b\n [9721] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20\n [9745] 64 65 20 44 6a 69 62 6f 75 74 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 44\n [9769] 6a 69 62 6f 75 74 69 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n [9793] 6f 6d 6d 6f 6e 22 3a 22 44 65 6e 6d 61 72 6b 22 2c 22 6f 66 66 69 63 69\n [9817] 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20 44 65 6e 6d 61 72 6b 22\n [9841] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 64 61 6e 22 3a 7b 22 6f\n [9865] 66 66 69 63 69 61 6c 22 3a 22 4b 6f 6e 67 65 72 69 67 65 74 20 44 61 6e\n [9889] 6d 61 72 6b 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 44 61 6e 6d 61 72 6b 22\n [9913] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n [9937] 53 77 65 64 65 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67\n [9961] 64 6f 6d 20 6f 66 20 53 77 65 64 65 6e 22 2c 22 6e 61 74 69 76 65 4e 61\n [9985] 6d 65 22 3a 7b 22 73 77 65 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[10009] 4b 6f 6e 75 6e 67 61 72 69 6b 65 74 20 53 76 65 72 69 67 65 22 2c 22 63\n[10033] 6f 6d 6d 6f 6e 22 3a 22 53 76 65 72 69 67 65 22 7d 7d 7d 7d 2c 7b 22 6e\n[10057] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 56 65 6e 65 7a 75 65 6c\n[10081] 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 6f 6c 69 76 61 72 69 61\n[10105] 6e 20 52 65 70 75 62 6c 69 63 20 6f 66 20 56 65 6e 65 7a 75 65 6c 61 22\n[10129] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f\n[10153] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 42 6f 6c\n[10177] 69 76 61 72 69 61 6e 61 20 64 65 20 56 65 6e 65 7a 75 65 6c 61 22 2c 22\n[10201] 63 6f 6d 6d 6f 6e 22 3a 22 56 65 6e 65 7a 75 65 6c 61 22 7d 7d 7d 7d 2c\n[10225] 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4c 69 62 79 61\n[10249] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 74 61 74 65 20 6f 66 20 4c\n[10273] 69 62 79 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61\n[10297] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 a7 d9 84 d8 af d9 88 d9\n[10321] 84 d8 a9 20 d9 84 d9 8a d8 a8 d9 8a d8 a7 22 2c 22 63 6f 6d 6d 6f 6e 22\n[10345] 3a 22 e2 80 8f d9 84 d9 8a d8 a8 d9 8a d8 a7 22 7d 7d 7d 7d 2c 7b 22 6e\n[10369] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 61 72 69 62 62 65 61\n[10393] 6e 20 4e 65 74 68 65 72 6c 61 6e 64 73 22 2c 22 6f 66 66 69 63 69 61 6c\n[10417] 22 3a 22 42 6f 6e 61 69 72 65 2c 20 53 69 6e 74 20 45 75 73 74 61 74 69\n[10441] 75 73 20 61 6e 64 20 53 61 62 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[10465] 22 3a 7b 22 6e 6c 64 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 6f\n[10489] 6e 61 69 72 65 2c 20 53 69 6e 74 20 45 75 73 74 61 74 69 75 73 20 65 6e\n[10513] 20 53 61 62 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 61 72 69 62 69 73\n[10537] 63 68 20 4e 65 64 65 72 6c 61 6e 64 22 7d 2c 22 70 61 70 22 3a 7b 22 6f\n[10561] 66 66 69 63 69 61 6c 22 3a 22 42 6f 6e 65 69 72 75 2c 20 53 69 6e 74 20\n[10585] 45 75 73 74 61 74 69 75 73 20 79 20 53 61 62 61 22 2c 22 63 6f 6d 6d 6f\n[10609] 6e 22 3a 22 42 6f 6e 65 69 72 75 2c 20 53 69 6e 74 20 45 75 73 74 61 74\n[10633] 69 75 73 20 79 20 53 61 62 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a\n[10657] 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 61 74 65 6d 61 6c 61 22 2c 22 6f\n[10681] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 47 75\n[10705] 61 74 65 6d 61 6c 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n[10729] 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62\n[10753] 6c 69 63 61 20 64 65 20 47 75 61 74 65 6d 61 6c 61 22 2c 22 63 6f 6d 6d\n[10777] 6f 6e 22 3a 22 47 75 61 74 65 6d 61 6c 61 22 7d 7d 7d 7d 2c 7b 22 6e 61\n[10801] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 55 6e 69 74 65 64 20 41 72\n[10825] 61 62 20 45 6d 69 72 61 74 65 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a\n[10849] 22 55 6e 69 74 65 64 20 41 72 61 62 20 45 6d 69 72 61 74 65 73 22 2c 22\n[10873] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66\n[10897] 69 63 69 61 6c 22 3a 22 d8 a7 d9 84 d8 a5 d9 85 d8 a7 d8 b1 d8 a7 d8 aa\n[10921] 20 d8 a7 d9 84 d8 b9 d8 b1 d8 a8 d9 8a d8 a9 20 d8 a7 d9 84 d9 85 d8 aa\n[10945] d8 ad d8 af d8 a9 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8 af d9 88 d9 84\n[10969] d8 a9 20 d8 a7 d9 84 d8 a5 d9 85 d8 a7 d8 b1 d8 a7 d8 aa 20 d8 a7 d9 84\n[10993] d8 b9 d8 b1 d8 a8 d9 8a d8 a9 20 d8 a7 d9 84 d9 85 d8 aa d8 ad d8 af d8\n[11017] a9 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[11041] 3a 22 4c 61 6f 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4c 61 6f 20\n[11065] 50 65 6f 70 6c 65 27 73 20 44 65 6d 6f 63 72 61 74 69 63 20 52 65 70 75\n[11089] 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6c 61 6f\n[11113] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 e0 ba aa e0 ba b2 e0 ba 97\n[11137] e0 ba b2 e0 ba a5 e0 ba b0 e0 ba 99 e0 ba b0 20 e0 ba 8a e0 ba b2 e0 ba\n[11161] 97 e0 ba b4 e0 ba 9b e0 ba b0 e0 bb 84 e0 ba 95 20 e0 ba 84 e0 ba bb e0\n[11185] ba 99 e0 ba a5 e0 ba b2 e0 ba a7 20 e0 ba 82 e0 ba ad e0 ba 87 22 2c 22\n[11209] 63 6f 6d 6d 6f 6e 22 3a 22 e0 ba aa e0 ba 9b e0 ba 9b e0 ba a5 e0 ba b2\n[11233] e0 ba a7 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f\n[11257] 6e 22 3a 22 42 61 6e 67 6c 61 64 65 73 68 22 2c 22 6f 66 66 69 63 69 61\n[11281] 6c 22 3a 22 50 65 6f 70 6c 65 27 73 20 52 65 70 75 62 6c 69 63 20 6f 66\n[11305] 20 42 61 6e 67 6c 61 64 65 73 68 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[11329] 22 3a 7b 22 62 65 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 e0 a6\n[11353] ac e0 a6 be e0 a6 82 e0 a6 b2 e0 a6 be e0 a6 a6 e0 a7 87 e0 a6 b6 20 e0\n[11377] a6 97 e0 a6 a3 e0 a6 aa e0 a7 8d e0 a6 b0 e0 a6 9c e0 a6 be e0 a6 a4 e0\n[11401] a6 a8 e0 a7 8d e0 a6 a4 e0 a7 8d e0 a6 b0 e0 a7 80 22 2c 22 63 6f 6d 6d\n[11425] 6f 6e 22 3a 22 e0 a6 ac e0 a6 be e0 a6 82 e0 a6 b2 e0 a6 be e0 a6 a6 e0\n[11449] a7 87 e0 a6 b6 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d\n[11473] 6d 6f 6e 22 3a 22 54 75 76 61 6c 75 22 2c 22 6f 66 66 69 63 69 61 6c 22\n[11497] 3a 22 54 75 76 61 6c 75 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b\n[11521] 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 75 76 61 6c\n[11545] 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54 75 76 61 6c 75 22 7d 2c 22 74\n[11569] 76 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 75 76 61 6c 75 22\n[11593] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54 75 76 61 6c 75 22 7d 7d 7d 7d 2c 7b\n[11617] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 41 6c 62 61 6e 69\n[11641] 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20\n[11665] 6f 66 20 41 6c 62 61 6e 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[11689] 3a 7b 22 73 71 69 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n[11713] 75 62 6c 69 6b 61 20 65 20 53 68 71 69 70 c3 ab 72 69 73 c3 ab 22 2c 22\n[11737] 63 6f 6d 6d 6f 6e 22 3a 22 53 68 71 69 70 c3 ab 72 69 61 22 7d 7d 7d 7d\n[11761] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 45 73 77 61\n[11785] 74 69 6e 69 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67 64 6f\n[11809] 6d 20 6f 66 20 45 73 77 61 74 69 6e 69 22 2c 22 6e 61 74 69 76 65 4e 61\n[11833] 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[11857] 4b 69 6e 67 64 6f 6d 20 6f 66 20 45 73 77 61 74 69 6e 69 22 2c 22 63 6f\n[11881] 6d 6d 6f 6e 22 3a 22 45 73 77 61 74 69 6e 69 22 7d 2c 22 73 73 77 22 3a\n[11905] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 55 6d 62 75 73 6f 20 77 65 53 77\n[11929] 61 74 69 6e 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 65 53 77 61 74 69 6e\n[11953] 69 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[11977] 3a 22 59 65 6d 65 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n[12001] 75 62 6c 69 63 20 6f 66 20 59 65 6d 65 6e 22 2c 22 6e 61 74 69 76 65 4e\n[12025] 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[12049] 22 d8 a7 d9 84 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8 a9 20 d8 a7 d9 84\n[12073] d9 8a d9 85 d9 86 d9 8a d8 a9 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8 a7\n[12097] d9 84 d9 8a d9 8e d9 85 d9 8e d9 86 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n[12121] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 6f 63 6f 73 20 28 4b 65 65 6c\n[12145] 69 6e 67 29 20 49 73 6c 61 6e 64 73 22 2c 22 6f 66 66 69 63 69 61 6c 22\n[12169] 3a 22 54 65 72 72 69 74 6f 72 79 20 6f 66 20 74 68 65 20 43 6f 63 6f 73\n[12193] 20 28 4b 65 65 6c 69 6e 67 29 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74\n[12217] 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69\n[12241] 61 6c 22 3a 22 54 65 72 72 69 74 6f 72 79 20 6f 66 20 74 68 65 20 43 6f\n[12265] 63 6f 73 20 28 4b 65 65 6c 69 6e 67 29 20 49 73 6c 61 6e 64 73 22 2c 22\n[12289] 63 6f 6d 6d 6f 6e 22 3a 22 43 6f 63 6f 73 20 28 4b 65 65 6c 69 6e 67 29\n[12313] 20 49 73 6c 61 6e 64 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22\n[12337] 63 6f 6d 6d 6f 6e 22 3a 22 4d 6f 6e 74 73 65 72 72 61 74 22 2c 22 6f 66\n[12361] 66 69 63 69 61 6c 22 3a 22 4d 6f 6e 74 73 65 72 72 61 74 22 2c 22 6e 61\n[12385] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63\n[12409] 69 61 6c 22 3a 22 4d 6f 6e 74 73 65 72 72 61 74 22 2c 22 63 6f 6d 6d 6f\n[12433] 6e 22 3a 22 4d 6f 6e 74 73 65 72 72 61 74 22 7d 7d 7d 7d 2c 7b 22 6e 61\n[12457] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 41 6e 67 6f 6c 61 22 2c 22\n[12481] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 41\n[12505] 6e 67 6f 6c 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 70 6f\n[12529] 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69\n[12553] 63 61 20 64 65 20 41 6e 67 6f 6c 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[12577] 41 6e 67 6f 6c 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f\n[12601] 6d 6d 6f 6e 22 3a 22 52 77 61 6e 64 61 22 2c 22 6f 66 66 69 63 69 61 6c\n[12625] 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 52 77 61 6e 64 61 22 2c 22\n[12649] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66\n[12673] 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 52 77 61 6e\n[12697] 64 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 52 77 61 6e 64 61 22 7d 2c 22\n[12721] 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62\n[12745] 6c 69 71 75 65 20 72 77 61 6e 64 61 69 73 65 22 2c 22 63 6f 6d 6d 6f 6e\n[12769] 22 3a 22 52 77 61 6e 64 61 22 7d 2c 22 6b 69 6e 22 3a 7b 22 6f 66 66 69\n[12793] 63 69 61 6c 22 3a 22 52 65 70 75 62 75 6c 69 6b 61 20 79 27 75 20 52 77\n[12817] 61 6e 64 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 52 77 61 6e 64 61 22 7d\n[12841] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 44\n[12865] 52 20 43 6f 6e 67 6f 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 44 65 6d\n[12889] 6f 63 72 61 74 69 63 20 52 65 70 75 62 6c 69 63 20 6f 66 20 74 68 65 20\n[12913] 43 6f 6e 67 6f 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72\n[12937] 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69\n[12961] 71 75 65 20 64 c3 a9 6d 6f 63 72 61 74 69 71 75 65 20 64 75 20 43 6f 6e\n[12985] 67 6f 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 52 44 20 43 6f 6e 67 6f 22 7d\n[13009] 2c 22 6b 6f 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75\n[13033] 62 69 6c 69 6b 61 20 79 61 20 4b 6f 6e 67 6f 20 44 65 6d 6f 6b 72 61 74\n[13057] 69 6b 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 52 65 70 75 62 69 6c 69 6b\n[13081] 61 20 79 61 20 4b 6f 6e 67 6f 20 44 65 6d 6f 6b 72 61 74 69 6b 69 22 7d\n[13105] 2c 22 6c 69 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75\n[13129] 62 6c 69 6b 69 20 79 61 20 4b 6f 6e 67 c3 b3 20 44 65 6d 6f 6b 72 61 74\n[13153] 69 6b 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 52 65 70 75 62 6c 69 6b 69\n[13177] 20 79 61 20 4b 6f 6e 67 c3 b3 20 44 65 6d 6f 6b 72 61 74 69 6b 69 22 7d\n[13201] 2c 22 6c 75 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 44 69 74 75\n[13225] 6e 67 61 20 64 69 61 20 4b 6f 6e 67 75 20 77 61 20 4d 75 6e 67 61 6c 61\n[13249] 61 74 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 44 69 74 75 6e 67 61 20 64\n[13273] 69 61 20 4b 6f 6e 67 75 20 77 61 20 4d 75 6e 67 61 6c 61 61 74 61 22 7d\n[13297] 2c 22 73 77 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4a 61 6d 68\n[13321] 75 72 69 20 79 61 20 4b 69 64 65 6d 6f 6b 72 61 73 69 61 20 79 61 20 4b\n[13345] 6f 6e 67 6f 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4a 61 6d 68 75 72 69 20\n[13369] 79 61 20 4b 69 64 65 6d 6f 6b 72 61 73 69 61 20 79 61 20 4b 6f 6e 67 6f\n[13393] 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a\n[13417] 22 54 72 69 6e 69 64 61 64 20 61 6e 64 20 54 6f 62 61 67 6f 22 2c 22 6f\n[13441] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 54 72\n[13465] 69 6e 69 64 61 64 20 61 6e 64 20 54 6f 62 61 67 6f 22 2c 22 6e 61 74 69\n[13489] 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61\n[13513] 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 54 72 69 6e 69 64 61 64\n[13537] 20 61 6e 64 20 54 6f 62 61 67 6f 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54\n[13561] 72 69 6e 69 64 61 64 20 61 6e 64 20 54 6f 62 61 67 6f 22 7d 7d 7d 7d 2c\n[13585] 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 55 6b 72 61 69\n[13609] 6e 65 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 55 6b 72 61 69 6e 65 22\n[13633] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 75 6b 72 22 3a 7b 22 6f\n[13657] 66 66 69 63 69 61 6c 22 3a 22 d0 a3 d0 ba d1 80 d0 b0 d1 97 d0 bd d0 b0\n[13681] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 a3 d0 ba d1 80 d0 b0 d1 97 d0 bd\n[13705] d0 b0 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n[13729] 22 3a 22 54 6f 67 6f 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 6f 67\n[13753] 6f 6c 65 73 65 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e\n[13777] 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[13801] 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 74 6f 67 6f 6c 61 69 73 65 22 2c\n[13825] 22 63 6f 6d 6d 6f 6e 22 3a 22 54 6f 67 6f 22 7d 7d 7d 7d 2c 7b 22 6e 61\n[13849] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4c 69 74 68 75 61 6e 69 61\n[13873] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f\n[13897] 66 20 4c 69 74 68 75 61 6e 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[13921] 22 3a 7b 22 6c 69 74 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4c 69\n[13945] 65 74 75 76 6f 73 20 52 65 73 70 75 62 6c 69 6b 6f 73 22 2c 22 63 6f 6d\n[13969] 6d 6f 6e 22 3a 22 4c 69 65 74 75 76 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[13993] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42 61 68 72 61 69 6e 22 2c 22\n[14017] 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20 42 61\n[14041] 68 72 61 69 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72\n[14065] 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d9 85 d9 85 d9 84 d9 83\n[14089] d8 a9 20 d8 a7 d9 84 d8 a8 d8 ad d8 b1 d9 8a d9 86 22 2c 22 63 6f 6d 6d\n[14113] 6f 6e 22 3a 22 e2 80 8f d8 a7 d9 84 d8 a8 d8 ad d8 b1 d9 8a d9 86 22 7d\n[14137] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 48\n[14161] 6f 6e 67 20 4b 6f 6e 67 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 48 6f\n[14185] 6e 67 20 4b 6f 6e 67 20 53 70 65 63 69 61 6c 20 41 64 6d 69 6e 69 73 74\n[14209] 72 61 74 69 76 65 20 52 65 67 69 6f 6e 20 6f 66 20 74 68 65 20 50 65 6f\n[14233] 70 6c 65 27 73 20 52 65 70 75 62 6c 69 63 20 6f 66 20 43 68 69 6e 61 22\n[14257] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f\n[14281] 66 66 69 63 69 61 6c 22 3a 22 48 6f 6e 67 20 4b 6f 6e 67 20 53 70 65 63\n[14305] 69 61 6c 20 41 64 6d 69 6e 69 73 74 72 61 74 69 76 65 20 52 65 67 69 6f\n[14329] 6e 20 6f 66 20 74 68 65 20 50 65 6f 70 6c 65 27 73 20 52 65 70 75 62 6c\n[14353] 69 63 20 6f 66 20 43 68 69 6e 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 48\n[14377] 6f 6e 67 20 4b 6f 6e 67 22 7d 2c 22 7a 68 6f 22 3a 7b 22 6f 66 66 69 63\n[14401] 69 61 6c 22 3a 22 e4 b8 ad e5 8d 8e e4 ba ba e6 b0 91 e5 85 b1 e5 92 8c\n[14425] e5 9b bd e9 a6 99 e6 b8 af e7 89 b9 e5 88 ab e8 a1 8c e6 94 bf e5 8c ba\n[14449] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e9 a6 99 e6 b8 af 22 7d 7d 7d 7d 2c\n[14473] 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42 61 72 62 61\n[14497] 64 6f 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 61 72 62 61 64 6f\n[14521] 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b\n[14545] 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 61 72 62 61 64 6f 73 22 2c 22 63\n[14569] 6f 6d 6d 6f 6e 22 3a 22 42 61 72 62 61 64 6f 73 22 7d 7d 7d 7d 2c 7b 22\n[14593] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 50 6f 72 74 75 67 61\n[14617] 6c 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 6f 72 74 75 67 75 65 73\n[14641] 65 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[14665] 3a 7b 22 70 6f 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n[14689] c3 ba 62 6c 69 63 61 20 50 6f 72 74 75 67 75 65 73 61 22 2c 22 63 6f 6d\n[14713] 6d 6f 6e 22 3a 22 50 6f 72 74 75 67 61 6c 22 7d 7d 7d 7d 2c 7b 22 6e 61\n[14737] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 49 73 72 61 65 6c 22 2c 22\n[14761] 6f 66 66 69 63 69 61 6c 22 3a 22 53 74 61 74 65 20 6f 66 20 49 73 72 61\n[14785] 65 6c 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a\n[14809] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 af d9 88 d9 84 d8 a9 20 d8 a5\n[14833] d8 b3 d8 b1 d8 a7 d8 a6 d9 8a d9 84 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[14857] d8 a5 d8 b3 d8 b1 d8 a7 d8 a6 d9 8a d9 84 22 7d 2c 22 68 65 62 22 3a 7b\n[14881] 22 6f 66 66 69 63 69 61 6c 22 3a 22 d7 9e d7 93 d7 99 d7 a0 d7 aa 20 d7\n[14905] 99 d7 a9 d7 a8 d7 90 d7 9c 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d7 99 d7\n[14929] a9 d7 a8 d7 90 d7 9c 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n[14953] 6f 6d 6d 6f 6e 22 3a 22 53 c3 a3 6f 20 54 6f 6d c3 a9 20 61 6e 64 20 50\n[14977] 72 c3 ad 6e 63 69 70 65 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 44 65\n[15001] 6d 6f 63 72 61 74 69 63 20 52 65 70 75 62 6c 69 63 20 6f 66 20 53 c3 a3\n[15025] 6f 20 54 6f 6d c3 a9 20 61 6e 64 20 50 72 c3 ad 6e 63 69 70 65 22 2c 22\n[15049] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 70 6f 72 22 3a 7b 22 6f 66 66\n[15073] 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 44 65 6d 6f 63\n[15097] 72 c3 a1 74 69 63 61 20 64 6f 20 53 c3 a3 6f 20 54 6f 6d c3 a9 20 65 20\n[15121] 50 72 c3 ad 6e 63 69 70 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 c3 a3\n[15145] 6f 20 54 6f 6d c3 a9 20 65 20 50 72 c3 ad 6e 63 69 70 65 22 7d 7d 7d 7d\n[15169] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 52 6f 6d 61\n[15193] 6e 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 6f 6d 61 6e 69 61\n[15217] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 72 6f 6e 22 3a 7b 22\n[15241] 6f 66 66 69 63 69 61 6c 22 3a 22 52 6f 6d c3 a2 6e 69 61 22 2c 22 63 6f\n[15265] 6d 6d 6f 6e 22 3a 22 52 6f 6d c3 a2 6e 69 61 22 7d 7d 7d 7d 2c 7b 22 6e\n[15289] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 61 6d 22 2c 22 6f\n[15313] 66 66 69 63 69 61 6c 22 3a 22 47 75 61 6d 22 2c 22 6e 61 74 69 76 65 4e\n[15337] 61 6d 65 22 3a 7b 22 63 68 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[15361] 22 47 75 c3 a5 68 c3 a5 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 c3\n[15385] a5 68 c3 a5 6e 22 7d 2c 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[15409] 22 3a 22 47 75 61 6d 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 61 6d 22\n[15433] 7d 2c 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 47 75 61\n[15457] 6d 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 61 6d 22 7d 7d 7d 7d 2c 7b\n[15481] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 54 6f 6e 67 61 22\n[15505] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20\n[15529] 54 6f 6e 67 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e\n[15553] 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20\n[15577] 6f 66 20 54 6f 6e 67 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54 6f 6e 67\n[15601] 61 22 7d 2c 22 74 6f 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b\n[15625] 69 6e 67 64 6f 6d 20 6f 66 20 54 6f 6e 67 61 22 2c 22 63 6f 6d 6d 6f 6e\n[15649] 22 3a 22 54 6f 6e 67 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22\n[15673] 63 6f 6d 6d 6f 6e 22 3a 22 47 61 6d 62 69 61 22 2c 22 6f 66 66 69 63 69\n[15697] 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 74 68 65 20 47 61 6d\n[15721] 62 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22\n[15745] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f\n[15769] 66 20 74 68 65 20 47 61 6d 62 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[15793] 47 61 6d 62 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f\n[15817] 6d 6d 6f 6e 22 3a 22 4e 69 67 65 72 22 2c 22 6f 66 66 69 63 69 61 6c 22\n[15841] 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4e 69 67 65 72 22 2c 22 6e 61\n[15865] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63\n[15889] 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 75 20 4e 69 67\n[15913] 65 72 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 69 67 65 72 22 7d 7d 7d 7d\n[15937] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 54 61 69 77\n[15961] 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n[15985] 20 6f 66 20 43 68 69 6e 61 20 28 54 61 69 77 61 6e 29 22 2c 22 6e 61 74\n[16009] 69 76 65 4e 61 6d 65 22 3a 7b 22 7a 68 6f 22 3a 7b 22 6f 66 66 69 63 69\n[16033] 61 6c 22 3a 22 e4 b8 ad e8 8f af e6 b0 91 e5 9c 8b 22 2c 22 63 6f 6d 6d\n[16057] 6f 6e 22 3a 22 e5 8f b0 e7 81 a3 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n[16081] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 6f 72 66 6f 6c 6b 20 49 73 6c 61\n[16105] 6e 64 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 65 72 72 69 74 6f 72\n[16129] 79 20 6f 66 20 4e 6f 72 66 6f 6c 6b 20 49 73 6c 61 6e 64 22 2c 22 6e 61\n[16153] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63\n[16177] 69 61 6c 22 3a 22 54 65 72 72 69 74 6f 72 79 20 6f 66 20 4e 6f 72 66 6f\n[16201] 6c 6b 20 49 73 6c 61 6e 64 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 6f 72\n[16225] 66 6f 6c 6b 20 49 73 6c 61 6e 64 22 7d 2c 22 70 69 68 22 3a 7b 22 6f 66\n[16249] 66 69 63 69 61 6c 22 3a 22 54 65 72 61 74 72 69 20 6f 66 20 4e 6f 72 66\n[16273] 27 6b 20 41 69 6c 65 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 6f 72 66\n[16297] 27 6b 20 41 69 6c 65 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22\n[16321] 63 6f 6d 6d 6f 6e 22 3a 22 49 72 61 71 22 2c 22 6f 66 66 69 63 69 61 6c\n[16345] 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 49 72 61 71 22 2c 22 6e 61\n[16369] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63\n[16393] 69 61 6c 22 3a 22 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8 a9 20 d8 a7 d9\n[16417] 84 d8 b9 d8 b1 d8 a7 d9 82 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8 a7 d9\n[16441] 84 d8 b9 d8 b1 d8 a7 d9 82 22 7d 2c 22 61 72 63 22 3a 7b 22 6f 66 66 69\n[16465] 63 69 61 6c 22 3a 22 dc a9 dc 98 dc bc dc 9b dc a2 dc b5 dc 90 20 dc 90\n[16489] dc 9d dc bc dc aa dc b2 dc a9 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 dc a9\n[16513] dc 98 dc bc dc 9b dc a2 dc b5 dc 90 22 7d 2c 22 63 6b 62 22 3a 7b 22 6f\n[16537] 66 66 69 63 69 61 6c 22 3a 22 da a9 db 86 d9 85 d8 a7 d8 b1 db 8c 20 d8\n[16561] b9 db 8e d8 b1 d8 a7 d9 82 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 da a9 db\n[16585] 86 d9 85 d8 a7 d8 b1 db 8c 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b\n[16609] 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 65 77 20 5a 65 61 6c 61 6e 64 22 2c 22\n[16633] 6f 66 66 69 63 69 61 6c 22 3a 22 4e 65 77 20 5a 65 61 6c 61 6e 64 22 2c\n[16657] 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66\n[16681] 66 69 63 69 61 6c 22 3a 22 4e 65 77 20 5a 65 61 6c 61 6e 64 22 2c 22 63\n[16705] 6f 6d 6d 6f 6e 22 3a 22 4e 65 77 20 5a 65 61 6c 61 6e 64 22 7d 2c 22 6d\n[16729] 72 69 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 41 6f 74 65 61 72 6f\n[16753] 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 41 6f 74 65 61 72 6f 61 22 7d 2c\n[16777] 22 6e 7a 73 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4e 65 77 20 5a\n[16801] 65 61 6c 61 6e 64 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 65 77 20 5a 65\n[16825] 61 6c 61 6e 64 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d\n[16849] 6d 6f 6e 22 3a 22 43 75 62 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[16873] 52 65 70 75 62 6c 69 63 20 6f 66 20 43 75 62 61 22 2c 22 6e 61 74 69 76\n[16897] 65 4e 61 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[16921] 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 20 43 75 62 61 22 2c 22\n[16945] 63 6f 6d 6d 6f 6e 22 3a 22 43 75 62 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[16969] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 45 74 68 69 6f 70 69 61 22 2c\n[16993] 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 65 64 65 72 61 6c 20 44 65 6d 6f\n[17017] 63 72 61 74 69 63 20 52 65 70 75 62 6c 69 63 20 6f 66 20 45 74 68 69 6f\n[17041] 70 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 6d 68 22\n[17065] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 e1 8b a8 e1 8a a2 e1 89 b5 e1\n[17089] 8b ae e1 8c b5 e1 8b ab 20 e1 8d 8c e1 8b b4 e1 88 ab e1 88 8b e1 8b 8a\n[17113] 20 e1 8b b2 e1 88 9e e1 8a ad e1 88 ab e1 88 b2 e1 8b ab e1 8b 8a 20 e1\n[17137] 88 aa e1 8d 90 e1 89 a5 e1 88 8a e1 8a ad 22 2c 22 63 6f 6d 6d 6f 6e 22\n[17161] 3a 22 e1 8a a2 e1 89 b5 e1 8b ae e1 8c b5 e1 8b ab 22 7d 7d 7d 7d 2c 7b\n[17185] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 6c 74 61 22\n[17209] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66\n[17233] 20 4d 61 6c 74 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65\n[17257] 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69\n[17281] 63 20 6f 66 20 4d 61 6c 74 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61\n[17305] 6c 74 61 22 7d 2c 22 6d 6c 74 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[17329] 22 52 65 70 75 62 62 6c 69 6b 61 20 74 61 20 27 20 4d 61 6c 74 61 22 2c\n[17353] 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 6c 74 61 22 7d 7d 7d 7d 2c 7b 22 6e\n[17377] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 64 61 67 61 73 63\n[17401] 61 72 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n[17425] 20 6f 66 20 4d 61 64 61 67 61 73 63 61 72 22 2c 22 6e 61 74 69 76 65 4e\n[17449] 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[17473] 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 65 20 4d 61 64 61 67 61 73 63\n[17497] 61 72 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 64 61 67 61 73 63 61 72\n[17521] 22 7d 2c 22 6d 6c 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65\n[17545] 70 6f 62 6c 69 6b 61 6e 27 69 20 4d 61 64 61 67 61 73 69 6b 61 72 61 22\n[17569] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 64 61 67 61 73 69 6b 61 72 61 22\n[17593] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[17617] 42 6f 73 6e 69 61 20 61 6e 64 20 48 65 72 7a 65 67 6f 76 69 6e 61 22 2c\n[17641] 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 6f 73 6e 69 61 20 61 6e 64 20 48\n[17665] 65 72 7a 65 67 6f 76 69 6e 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[17689] 3a 7b 22 62 6f 73 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 6f 73\n[17713] 6e 61 20 69 20 48 65 72 63 65 67 6f 76 69 6e 61 22 2c 22 63 6f 6d 6d 6f\n[17737] 6e 22 3a 22 42 6f 73 6e 61 20 69 20 48 65 72 63 65 67 6f 76 69 6e 61 22\n[17761] 7d 2c 22 68 72 76 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 6f 73\n[17785] 6e 61 20 69 20 48 65 72 63 65 67 6f 76 69 6e 61 22 2c 22 63 6f 6d 6d 6f\n[17809] 6e 22 3a 22 42 6f 73 6e 61 20 69 20 48 65 72 63 65 67 6f 76 69 6e 61 22\n[17833] 7d 2c 22 73 72 70 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d0 91 d0\n[17857] be d1 81 d0 bd d0 b0 20 d0 b8 20 d0 a5 d0 b5 d1 80 d1 86 d0 b5 d0 b3 d0\n[17881] be d0 b2 d0 b8 d0 bd d0 b0 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 91 d0\n[17905] be d1 81 d0 bd d0 b0 20 d0 b8 20 d0 a5 d0 b5 d1 80 d1 86 d0 b5 d0 b3 d0\n[17929] be d0 b2 d0 b8 d0 bd d0 b0 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b\n[17953] 22 63 6f 6d 6d 6f 6e 22 3a 22 47 65 6f 72 67 69 61 22 2c 22 6f 66 66 69\n[17977] 63 69 61 6c 22 3a 22 47 65 6f 72 67 69 61 22 2c 22 6e 61 74 69 76 65 4e\n[18001] 61 6d 65 22 3a 7b 22 6b 61 74 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[18025] 22 e1 83 a1 e1 83 90 e1 83 a5 e1 83 90 e1 83 a0 e1 83 97 e1 83 95 e1 83\n[18049] 94 e1 83 9a e1 83 9d 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e1 83 a1 e1 83\n[18073] 90 e1 83 a5 e1 83 90 e1 83 a0 e1 83 97 e1 83 95 e1 83 94 e1 83 9a e1 83\n[18097] 9d 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[18121] 3a 22 4e 65 77 20 43 61 6c 65 64 6f 6e 69 61 22 2c 22 6f 66 66 69 63 69\n[18145] 61 6c 22 3a 22 4e 65 77 20 43 61 6c 65 64 6f 6e 69 61 22 2c 22 6e 61 74\n[18169] 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69\n[18193] 61 6c 22 3a 22 4e 6f 75 76 65 6c 6c 65 2d 43 61 6c c3 a9 64 6f 6e 69 65\n[18217] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 6f 75 76 65 6c 6c 65 2d 43 61 6c\n[18241] c3 a9 64 6f 6e 69 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n[18265] 6f 6d 6d 6f 6e 22 3a 22 45 73 74 6f 6e 69 61 22 2c 22 6f 66 66 69 63 69\n[18289] 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 45 73 74 6f 6e 69 61\n[18313] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 73 74 22 3a 7b 22\n[18337] 6f 66 66 69 63 69 61 6c 22 3a 22 45 65 73 74 69 20 56 61 62 61 72 69 69\n[18361] 6b 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 45 65 73 74 69 22 7d 7d 7d 7d 2c\n[18385] 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 48 6f 6e 64 75\n[18409] 72 61 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69\n[18433] 63 20 6f 66 20 48 6f 6e 64 75 72 61 73 22 2c 22 6e 61 74 69 76 65 4e 61\n[18457] 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[18481] 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 20 48 6f 6e 64 75 72 61 73 22 2c\n[18505] 22 63 6f 6d 6d 6f 6e 22 3a 22 48 6f 6e 64 75 72 61 73 22 7d 7d 7d 7d 2c\n[18529] 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 41 66 67 68 61\n[18553] 6e 69 73 74 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 49 73 6c 61\n[18577] 6d 69 63 20 52 65 70 75 62 6c 69 63 20 6f 66 20 41 66 67 68 61 6e 69 73\n[18601] 74 61 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 70 72 73 22\n[18625] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 ac d9 85 d9 87 d9 88 d8 b1\n[18649] db 8c 20 d8 a7 d8 b3 d9 84 d8 a7 d9 85 db 8c 20 d8 a7 d9 81 d8 ba d8 a7\n[18673] d9 86 d8 b3 d8 aa d8 a7 d9 86 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8 a7\n[18697] d9 81 d8 ba d8 a7 d9 86 d8 b3 d8 aa d8 a7 d9 86 22 7d 2c 22 70 75 73 22\n[18721] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 af 20 d8 a7 d9 81 d8 ba d8\n[18745] a7 d9 86 d8 b3 d8 aa d8 a7 d9 86 20 d8 a7 d8 b3 d9 84 d8 a7 d9 85 d9 8a\n[18769] 20 d8 ac d9 85 d9 87 d9 88 d8 b1 db 8c d8 aa 22 2c 22 63 6f 6d 6d 6f 6e\n[18793] 22 3a 22 d8 a7 d9 81 d8 ba d8 a7 d9 86 d8 b3 d8 aa d8 a7 d9 86 22 7d 2c\n[18817] 22 74 75 6b 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4f 77 67 61 6e\n[18841] 79 73 74 61 6e 20 59 73 6c 61 6d 20 52 65 73 70 75 62 6c 69 6b 61 73 79\n[18865] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4f 77 67 61 6e 79 73 74 61 6e 22 7d\n[18889] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43\n[18913] 75 72 61 c3 a7 61 6f 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 75\n[18937] 6e 74 72 79 20 6f 66 20 43 75 72 61 c3 a7 61 6f 22 2c 22 6e 61 74 69 76\n[18961] 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[18985] 22 3a 22 43 6f 75 6e 74 72 79 20 6f 66 20 43 75 72 61 c3 a7 61 6f 22 2c\n[19009] 22 63 6f 6d 6d 6f 6e 22 3a 22 43 75 72 61 c3 a7 61 6f 22 7d 2c 22 6e 6c\n[19033] 64 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4c 61 6e 64 20 43 75 72\n[19057] 61 c3 a7 61 6f 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 75 72 61 c3 a7 61\n[19081] 6f 22 7d 2c 22 70 61 70 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 50\n[19105] 61 69 73 20 4b c3 b2 72 73 6f 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 50\n[19129] 61 69 73 20 4b c3 b2 72 73 6f 75 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n[19153] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 48 61 69 74 69 22 2c 22 6f 66 66 69\n[19177] 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 48 61 69 74 69\n[19201] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22\n[19225] 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64\n[19249] 27 48 61 c3 af 74 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 48 61 c3 af 74\n[19273] 69 22 7d 2c 22 68 61 74 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[19297] 65 70 69 62 6c 69 6b 20 41 79 69 74 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[19321] 22 41 79 69 74 69 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f\n[19345] 6d 6d 6f 6e 22 3a 22 4e 61 6d 69 62 69 61 22 2c 22 6f 66 66 69 63 69 61\n[19369] 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4e 61 6d 69 62 69 61 22\n[19393] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 66 72 22 3a 7b 22 6f\n[19417] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 65 6b 20 76 61 6e 20\n[19441] 4e 61 6d 69 62 69 c3 ab 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 61 6d 69\n[19465] 62 69 c3 ab 22 7d 2c 22 64 65 75 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22\n[19489] 3a 22 52 65 70 75 62 6c 69 6b 20 4e 61 6d 69 62 69 61 22 2c 22 63 6f 6d\n[19513] 6d 6f 6e 22 3a 22 4e 61 6d 69 62 69 61 22 7d 2c 22 65 6e 67 22 3a 7b 22\n[19537] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4e\n[19561] 61 6d 69 62 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 61 6d 69 62 69\n[19585] 61 22 7d 2c 22 68 65 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[19609] 65 70 75 62 6c 69 63 20 6f 66 20 4e 61 6d 69 62 69 61 22 2c 22 63 6f 6d\n[19633] 6d 6f 6e 22 3a 22 4e 61 6d 69 62 69 61 22 7d 2c 22 68 67 6d 22 3a 7b 22\n[19657] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4e\n[19681] 61 6d 69 62 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 61 6d 69 62 69\n[19705] 61 22 7d 2c 22 6b 77 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[19729] 65 70 75 62 6c 69 63 20 6f 66 20 4e 61 6d 69 62 69 61 22 2c 22 63 6f 6d\n[19753] 6d 6f 6e 22 3a 22 4e 61 6d 69 62 69 61 22 7d 2c 22 6c 6f 7a 22 3a 7b 22\n[19777] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4e\n[19801] 61 6d 69 62 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 61 6d 69 62 69\n[19825] 61 22 7d 2c 22 6e 64 6f 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[19849] 65 70 75 62 6c 69 63 20 6f 66 20 4e 61 6d 69 62 69 61 22 2c 22 63 6f 6d\n[19873] 6d 6f 6e 22 3a 22 4e 61 6d 69 62 69 61 22 7d 2c 22 74 73 6e 22 3a 7b 22\n[19897] 6f 66 66 69 63 69 61 6c 22 3a 22 4c 65 66 61 74 73 68 65 20 6c 61 20 4e\n[19921] 61 6d 69 62 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 61 6d 69 62 69\n[19945] 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[19969] 3a 22 50 69 74 63 61 69 72 6e 20 49 73 6c 61 6e 64 73 22 2c 22 6f 66 66\n[19993] 69 63 69 61 6c 22 3a 22 50 69 74 63 61 69 72 6e 20 47 72 6f 75 70 20 6f\n[20017] 66 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a\n[20041] 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 69 74 63\n[20065] 61 69 72 6e 20 47 72 6f 75 70 20 6f 66 20 49 73 6c 61 6e 64 73 22 2c 22\n[20089] 63 6f 6d 6d 6f 6e 22 3a 22 50 69 74 63 61 69 72 6e 20 49 73 6c 61 6e 64\n[20113] 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[20137] 3a 22 53 61 69 6e 74 20 56 69 6e 63 65 6e 74 20 61 6e 64 20 74 68 65 20\n[20161] 47 72 65 6e 61 64 69 6e 65 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[20185] 53 61 69 6e 74 20 56 69 6e 63 65 6e 74 20 61 6e 64 20 74 68 65 20 47 72\n[20209] 65 6e 61 64 69 6e 65 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b\n[20233] 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 61 69 6e 74\n[20257] 20 56 69 6e 63 65 6e 74 20 61 6e 64 20 74 68 65 20 47 72 65 6e 61 64 69\n[20281] 6e 65 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61 69 6e 74 20 56 69 6e\n[20305] 63 65 6e 74 20 61 6e 64 20 74 68 65 20 47 72 65 6e 61 64 69 6e 65 73 22\n[20329] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[20353] 47 75 69 6e 65 61 2d 42 69 73 73 61 75 22 2c 22 6f 66 66 69 63 69 61 6c\n[20377] 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 47 75 69 6e 65 61 2d 42 69\n[20401] 73 73 61 75 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 70 6f 72\n[20425] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63\n[20449] 61 20 64 61 20 47 75 69 6e c3 a9 2d 42 69 73 73 61 75 22 2c 22 63 6f 6d\n[20473] 6d 6f 6e 22 3a 22 47 75 69 6e c3 a9 2d 42 69 73 73 61 75 22 7d 2c 22 70\n[20497] 6f 76 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c\n[20521] 69 63 61 20 64 61 20 47 75 69 6e c3 a9 2d 42 69 73 73 61 75 22 2c 22 63\n[20545] 6f 6d 6d 6f 6e 22 3a 22 47 75 69 6e c3 a9 2d 42 69 73 73 61 75 22 7d 7d\n[20569] 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 54 68\n[20593] 61 69 6c 61 6e 64 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67\n[20617] 64 6f 6d 20 6f 66 20 54 68 61 69 6c 61 6e 64 22 2c 22 6e 61 74 69 76 65\n[20641] 4e 61 6d 65 22 3a 7b 22 74 68 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22\n[20665] 3a 22 e0 b8 a3 e0 b8 b2 e0 b8 8a e0 b8 ad e0 b8 b2 e0 b8 93 e0 b8 b2 e0\n[20689] b8 88 e0 b8 b1 e0 b8 81 e0 b8 a3 e0 b9 84 e0 b8 97 e0 b8 a2 22 2c 22 63\n[20713] 6f 6d 6d 6f 6e 22 3a 22 e0 b8 9b e0 b8 a3 e0 b8 b0 e0 b9 80 e0 b8 97 e0\n[20737] b8 a8 e0 b9 84 e0 b8 97 e0 b8 a2 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n[20761] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4b 61 7a 61 6b 68 73 74 61 6e 22 2c\n[20785] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20\n[20809] 4b 61 7a 61 6b 68 73 74 61 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[20833] 3a 7b 22 6b 61 7a 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d2 9a d0\n[20857] b0 d0 b7 d0 b0 d2 9b d1 81 d1 82 d0 b0 d0 bd 20 d0 a0 d0 b5 d1 81 d0 bf\n[20881] d1 83 d0 b1 d0 bb d0 b8 d0 ba d0 b0 d1 81 d1 8b 22 2c 22 63 6f 6d 6d 6f\n[20905] 6e 22 3a 22 d2 9a d0 b0 d0 b7 d0 b0 d2 9b d1 81 d1 82 d0 b0 d0 bd 22 7d\n[20929] 2c 22 72 75 73 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d0 a0 d0 b5\n[20953] d1 81 d0 bf d1 83 d0 b1 d0 bb d0 b8 d0 ba d0 b0 20 d0 9a d0 b0 d0 b7 d0\n[20977] b0 d1 85 d1 81 d1 82 d0 b0 d0 bd 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0\n[21001] 9a d0 b0 d0 b7 d0 b0 d1 85 d1 81 d1 82 d0 b0 d0 bd 22 7d 7d 7d 7d 2c 7b\n[21025] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61 69 6e 74 20\n[21049] 42 61 72 74 68 c3 a9 6c 65 6d 79 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a\n[21073] 22 43 6f 6c 6c 65 63 74 69 76 69 74 79 20 6f 66 20 53 61 69 6e 74 20 42\n[21097] 61 72 74 68 c3 a9 6c 65 6d 79 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[21121] 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6c\n[21145] 6c 65 63 74 69 76 69 74 c3 a9 20 64 65 20 53 61 69 6e 74 2d 42 61 72 74\n[21169] 68 c3 a9 6c 65 6d 79 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61 69 6e 74\n[21193] 2d 42 61 72 74 68 c3 a9 6c 65 6d 79 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n[21217] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 55 72 75 67 75 61 79 22 2c 22 6f\n[21241] 66 66 69 63 69 61 6c 22 3a 22 4f 72 69 65 6e 74 61 6c 20 52 65 70 75 62\n[21265] 6c 69 63 20 6f 66 20 55 72 75 67 75 61 79 22 2c 22 6e 61 74 69 76 65 4e\n[21289] 61 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[21313] 22 52 65 70 c3 ba 62 6c 69 63 61 20 4f 72 69 65 6e 74 61 6c 20 64 65 6c\n[21337] 20 55 72 75 67 75 61 79 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 55 72 75 67\n[21361] 75 61 79 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f\n[21385] 6e 22 3a 22 49 76 6f 72 79 20 43 6f 61 73 74 22 2c 22 6f 66 66 69 63 69\n[21409] 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 43 c3 b4 74 65 20 64\n[21433] 27 49 76 6f 69 72 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n[21457] 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62\n[21481] 6c 69 71 75 65 20 64 65 20 43 c3 b4 74 65 20 64 27 49 76 6f 69 72 65 22\n[21505] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 c3 b4 74 65 20 64 27 49 76 6f 69 72\n[21529] 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[21553] 3a 22 42 72 61 7a 69 6c 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 65\n[21577] 64 65 72 61 74 69 76 65 20 52 65 70 75 62 6c 69 63 20 6f 66 20 42 72 61\n[21601] 7a 69 6c 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 70 6f 72 22\n[21625] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61\n[21649] 20 46 65 64 65 72 61 74 69 76 61 20 64 6f 20 42 72 61 73 69 6c 22 2c 22\n[21673] 63 6f 6d 6d 6f 6e 22 3a 22 42 72 61 73 69 6c 22 7d 7d 7d 7d 2c 7b 22 6e\n[21697] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 6f 6e 67 6f 6c 69 61\n[21721] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4d 6f 6e 67 6f 6c 69 61 22 2c\n[21745] 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6d 6f 6e 22 3a 7b 22 6f 66\n[21769] 66 69 63 69 61 6c 22 3a 22 d0 9c d0 be d0 bd d0 b3 d0 be d0 bb 20 d1 83\n[21793] d0 bb d1 81 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 9c d0 be d0 bd d0 b3\n[21817] d0 be d0 bb 20 d1 83 d0 bb d1 81 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n[21841] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42 72 69 74 69 73 68 20 49 6e 64 69\n[21865] 61 6e 20 4f 63 65 61 6e 20 54 65 72 72 69 74 6f 72 79 22 2c 22 6f 66 66\n[21889] 69 63 69 61 6c 22 3a 22 42 72 69 74 69 73 68 20 49 6e 64 69 61 6e 20 4f\n[21913] 63 65 61 6e 20 54 65 72 72 69 74 6f 72 79 22 2c 22 6e 61 74 69 76 65 4e\n[21937] 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[21961] 22 42 72 69 74 69 73 68 20 49 6e 64 69 61 6e 20 4f 63 65 61 6e 20 54 65\n[21985] 72 72 69 74 6f 72 79 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 72 69 74 69\n[22009] 73 68 20 49 6e 64 69 61 6e 20 4f 63 65 61 6e 20 54 65 72 72 69 74 6f 72\n[22033] 79 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[22057] 3a 22 4c 61 74 76 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65\n[22081] 70 75 62 6c 69 63 20 6f 66 20 4c 61 74 76 69 61 22 2c 22 6e 61 74 69 76\n[22105] 65 4e 61 6d 65 22 3a 7b 22 6c 61 76 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[22129] 22 3a 22 4c 61 74 76 69 6a 61 73 20 52 65 70 75 62 6c 69 6b 61 73 22 2c\n[22153] 22 63 6f 6d 6d 6f 6e 22 3a 22 4c 61 74 76 69 6a 61 22 7d 7d 7d 7d 2c 7b\n[22177] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 79 61 6e 61\n[22201] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 2d 6f 70 65 72 61 74 69\n[22225] 76 65 20 52 65 70 75 62 6c 69 63 20 6f 66 20 47 75 79 61 6e 61 22 2c 22\n[22249] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66\n[22273] 69 63 69 61 6c 22 3a 22 43 6f 2d 6f 70 65 72 61 74 69 76 65 20 52 65 70\n[22297] 75 62 6c 69 63 20 6f 66 20 47 75 79 61 6e 61 22 2c 22 63 6f 6d 6d 6f 6e\n[22321] 22 3a 22 47 75 79 61 6e 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b\n[22345] 22 63 6f 6d 6d 6f 6e 22 3a 22 46 72 65 6e 63 68 20 50 6f 6c 79 6e 65 73\n[22369] 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 72 65 6e 63 68 20 50\n[22393] 6f 6c 79 6e 65 73 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b\n[22417] 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 6f 6c 79 6e\n[22441] c3 a9 73 69 65 20 66 72 61 6e c3 a7 61 69 73 65 22 2c 22 63 6f 6d 6d 6f\n[22465] 6e 22 3a 22 50 6f 6c 79 6e c3 a9 73 69 65 20 66 72 61 6e c3 a7 61 69 73\n[22489] 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[22513] 3a 22 50 61 6e 61 6d 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65\n[22537] 70 75 62 6c 69 63 20 6f 66 20 50 61 6e 61 6d 61 22 2c 22 6e 61 74 69 76\n[22561] 65 4e 61 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[22585] 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 20 50 61 6e 61 6d c3 a1\n[22609] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 50 61 6e 61 6d c3 a1 22 7d 7d 7d 7d\n[22633] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4a 65 72 73\n[22657] 65 79 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 61 69 6c 69 77 69 63\n[22681] 6b 20 6f 66 20 4a 65 72 73 65 79 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[22705] 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 61\n[22729] 69 6c 69 77 69 63 6b 20 6f 66 20 4a 65 72 73 65 79 22 2c 22 63 6f 6d 6d\n[22753] 6f 6e 22 3a 22 4a 65 72 73 65 79 22 7d 2c 22 66 72 61 22 3a 7b 22 6f 66\n[22777] 66 69 63 69 61 6c 22 3a 22 42 61 69 6c 6c 69 61 67 65 20 64 65 20 4a 65\n[22801] 72 73 65 79 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4a 65 72 73 65 79 22 7d\n[22825] 2c 22 6e 72 66 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 61 69 6c\n[22849] 6c 69 61 67 65 20 64 c3 a9 20 4a c3 a8 72 72 69 22 2c 22 63 6f 6d 6d 6f\n[22873] 6e 22 3a 22 4a c3 a8 72 72 69 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a\n[22897] 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 6c 69 22 2c 22 6f 66 66 69 63 69\n[22921] 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4d 61 6c 69 22 2c 22\n[22945] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66\n[22969] 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 75 20 4d\n[22993] 61 6c 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 6c 69 22 7d 7d 7d 7d\n[23017] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 50 65 72 75\n[23041] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f\n[23065] 66 20 50 65 72 75 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61\n[23089] 79 6d 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 69 72 75 77 20 53\n[23113] 75 79 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 50 69 72 75 77 22 7d 2c 22\n[23137] 71 75 65 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 69 72 75 77 20\n[23161] 52 69 70 75 77 6c 69 6b 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 50 69 72\n[23185] 75 77 22 7d 2c 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[23209] 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 6c 20 50 65 72 c3 ba 22 2c 22 63\n[23233] 6f 6d 6d 6f 6e 22 3a 22 50 65 72 c3 ba 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[23257] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 49 6e 64 6f 6e 65 73 69 61 22\n[23281] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66\n[23305] 20 49 6e 64 6f 6e 65 73 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[23329] 3a 7b 22 69 6e 64 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n[23353] 75 62 6c 69 6b 20 49 6e 64 6f 6e 65 73 69 61 22 2c 22 63 6f 6d 6d 6f 6e\n[23377] 22 3a 22 49 6e 64 6f 6e 65 73 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n[23401] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 79 6f 74 74 65 22 2c 22 6f\n[23425] 66 66 69 63 69 61 6c 22 3a 22 44 65 70 61 72 74 6d 65 6e 74 20 6f 66 20\n[23449] 4d 61 79 6f 74 74 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n[23473] 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 44 c3 a9 70 61 72\n[23497] 74 65 6d 65 6e 74 20 64 65 20 4d 61 79 6f 74 74 65 22 2c 22 63 6f 6d 6d\n[23521] 6f 6e 22 3a 22 4d 61 79 6f 74 74 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n[23545] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 6f 6c 64 6f 76 61 22 2c 22 6f\n[23569] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4d 6f\n[23593] 6c 64 6f 76 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 72 6f\n[23617] 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n[23641] 61 20 4d 6f 6c 64 6f 76 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 6f 6c\n[23665] 64 6f 76 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d\n[23689] 6f 6e 22 3a 22 53 79 72 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[23713] 53 79 72 69 61 6e 20 41 72 61 62 20 52 65 70 75 62 6c 69 63 22 2c 22 6e\n[23737] 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69\n[23761] 63 69 61 6c 22 3a 22 d8 a7 d9 84 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8\n[23785] a9 20 d8 a7 d9 84 d8 b9 d8 b1 d8 a8 d9 8a d8 a9 20 d8 a7 d9 84 d8 b3 d9\n[23809] 88 d8 b1 d9 8a d8 a9 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8 b3 d9 88 d8\n[23833] b1 d9 8a d8 a7 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d\n[23857] 6d 6f 6e 22 3a 22 53 69 6e 67 61 70 6f 72 65 22 2c 22 6f 66 66 69 63 69\n[23881] 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 53 69 6e 67 61 70 6f\n[23905] 72 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a\n[23929] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66\n[23953] 20 53 69 6e 67 61 70 6f 72 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 69\n[23977] 6e 67 61 70 6f 72 65 22 7d 2c 22 7a 68 6f 22 3a 7b 22 6f 66 66 69 63 69\n[24001] 61 6c 22 3a 22 e6 96 b0 e5 8a a0 e5 9d a1 e5 85 b1 e5 92 8c e5 9b bd 22\n[24025] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e6 96 b0 e5 8a a0 e5 9d a1 22 7d 2c 22\n[24049] 6d 73 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c\n[24073] 69 6b 20 53 69 6e 67 61 70 75 72 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[24097] 53 69 6e 67 61 70 75 72 61 22 7d 2c 22 74 61 6d 22 3a 7b 22 6f 66 66 69\n[24121] 63 69 61 6c 22 3a 22 e0 ae 9a e0 ae bf e0 ae 99 e0 af 8d e0 ae 95 e0 ae\n[24145] aa e0 af 8d e0 ae aa e0 af 82 e0 ae b0 e0 af 8d 20 e0 ae 95 e0 af 81 e0\n[24169] ae 9f e0 ae bf e0 ae af e0 ae b0 e0 ae 9a e0 af 81 22 2c 22 63 6f 6d 6d\n[24193] 6f 6e 22 3a 22 e0 ae 9a e0 ae bf e0 ae 99 e0 af 8d e0 ae 95 e0 ae aa e0\n[24217] af 8d e0 ae aa e0 af 82 e0 ae b0 e0 af 8d 22 7d 7d 7d 7d 2c 7b 22 6e 61\n[24241] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 61 79 6d 61 6e 20 49 73\n[24265] 6c 61 6e 64 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 61 79 6d 61\n[24289] 6e 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a\n[24313] 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 61 79 6d\n[24337] 61 6e 20 49 73 6c 61 6e 64 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 61\n[24361] 79 6d 61 6e 20 49 73 6c 61 6e 64 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n[24385] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 45 67 79 70 74 22 2c 22 6f 66 66\n[24409] 69 63 69 61 6c 22 3a 22 41 72 61 62 20 52 65 70 75 62 6c 69 63 20 6f 66\n[24433] 20 45 67 79 70 74 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61\n[24457] 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 ac d9 85 d9 87 d9\n[24481] 88 d8 b1 d9 8a d8 a9 20 d9 85 d8 b5 d8 b1 20 d8 a7 d9 84 d8 b9 d8 b1 d8\n[24505] a8 d9 8a d8 a9 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d9 85 d8 b5 d8 b1 22\n[24529] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[24553] 48 65 61 72 64 20 49 73 6c 61 6e 64 20 61 6e 64 20 4d 63 44 6f 6e 61 6c\n[24577] 64 20 49 73 6c 61 6e 64 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 48\n[24601] 65 61 72 64 20 49 73 6c 61 6e 64 20 61 6e 64 20 4d 63 44 6f 6e 61 6c 64\n[24625] 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b\n[24649] 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 48 65 61 72 64\n[24673] 20 49 73 6c 61 6e 64 20 61 6e 64 20 4d 63 44 6f 6e 61 6c 64 20 49 73 6c\n[24697] 61 6e 64 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 48 65 61 72 64 20 49 73\n[24721] 6c 61 6e 64 20 61 6e 64 20 4d 63 44 6f 6e 61 6c 64 20 49 73 6c 61 6e 64\n[24745] 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[24769] 3a 22 4d 61 6c 64 69 76 65 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[24793] 52 65 70 75 62 6c 69 63 20 6f 66 20 74 68 65 20 4d 61 6c 64 69 76 65 73\n[24817] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 64 69 76 22 3a 7b 22\n[24841] 6f 66 66 69 63 69 61 6c 22 3a 22 de 8b de a8 de 88 de ac de 80 de a8 de\n[24865] 83 de a7 de 87 de b0 de 96 de ad de 8e de ac 20 de 96 de aa de 89 de b0\n[24889] de 80 de ab de 83 de a8 de 87 de b0 de 94 de a7 22 2c 22 63 6f 6d 6d 6f\n[24913] 6e 22 3a 22 de 8b de a8 de 88 de ac de 80 de a8 de 83 de a7 de 87 de b0\n[24937] de 96 de ad de 8e de ac 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22\n[24961] 63 6f 6d 6d 6f 6e 22 3a 22 54 69 6d 6f 72 2d 4c 65 73 74 65 22 2c 22 6f\n[24985] 66 66 69 63 69 61 6c 22 3a 22 44 65 6d 6f 63 72 61 74 69 63 20 52 65 70\n[25009] 75 62 6c 69 63 20 6f 66 20 54 69 6d 6f 72 2d 4c 65 73 74 65 22 2c 22 6e\n[25033] 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 70 6f 72 22 3a 7b 22 6f 66 66 69\n[25057] 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 44 65 6d 6f 63 72\n[25081] c3 a1 74 69 63 61 20 64 65 20 54 69 6d 6f 72 2d 4c 65 73 74 65 22 2c 22\n[25105] 63 6f 6d 6d 6f 6e 22 3a 22 54 69 6d 6f 72 2d 4c 65 73 74 65 22 7d 2c 22\n[25129] 74 65 74 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62\n[25153] 6c 69 6b 61 20 44 65 6d 6f 6b 72 c3 a1 74 69 6b 61 20 54 69 6d c3 b3 72\n[25177] 2d 4c 65 73 74 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54 69 6d c3 b3 72\n[25201] 2d 4c 65 73 74 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f\n[25225] 6d 6d 6f 6e 22 3a 22 4a 6f 72 64 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c\n[25249] 22 3a 22 48 61 73 68 65 6d 69 74 65 20 4b 69 6e 67 64 6f 6d 20 6f 66 20\n[25273] 4a 6f 72 64 61 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61\n[25297] 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 a7 d9 84 d9 85 d9\n[25321] 85 d9 84 d9 83 d8 a9 20 d8 a7 d9 84 d8 a3 d8 b1 d8 af d9 86 d9 8a d8 a9\n[25345] 20 d8 a7 d9 84 d9 87 d8 a7 d8 b4 d9 85 d9 8a d8 a9 22 2c 22 63 6f 6d 6d\n[25369] 6f 6e 22 3a 22 d8 a7 d9 84 d8 a3 d8 b1 d8 af d9 86 22 7d 7d 7d 7d 2c 7b\n[25393] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 6f 6d 61 6c 69\n[25417] 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 65 64 65 72 61 6c 20 52\n[25441] 65 70 75 62 6c 69 63 20 6f 66 20 53 6f 6d 61 6c 69 61 22 2c 22 6e 61 74\n[25465] 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63 69\n[25489] 61 6c 22 3a 22 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8 a9 20 d8 a7 d9 84\n[25513] d8 b5 d9 88 d9 85 d8 a7 d9 84 e2 80 8e e2 80 8e 22 2c 22 63 6f 6d 6d 6f\n[25537] 6e 22 3a 22 d8 a7 d9 84 d8 b5 d9 88 d9 85 d8 a7 d9 84 e2 80 8e e2 80 8e\n[25561] 22 7d 2c 22 73 6f 6d 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4a 61\n[25585] 6d 68 75 75 72 69 79 61 64 64 61 20 46 65 64 65 72 61 61 6c 6b 61 20 53\n[25609] 6f 6f 6d 61 61 6c 69 79 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 6f 6f\n[25633] 6d 61 61 6c 69 79 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n[25657] 6f 6d 6d 6f 6e 22 3a 22 41 7a 65 72 62 61 69 6a 61 6e 22 2c 22 6f 66 66\n[25681] 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 41 7a 65 72\n[25705] 62 61 69 6a 61 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61\n[25729] 7a 65 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 41 7a c9 99 72 62 61\n[25753] 79 63 61 6e 20 52 65 73 70 75 62 6c 69 6b 61 73 c4 b1 22 2c 22 63 6f 6d\n[25777] 6d 6f 6e 22 3a 22 41 7a c9 99 72 62 61 79 63 61 6e 22 7d 7d 7d 7d 2c 7b\n[25801] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 6f 7a 61 6d 62\n[25825] 69 71 75 65 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c\n[25849] 69 63 20 6f 66 20 4d 6f 7a 61 6d 62 69 71 75 65 22 2c 22 6e 61 74 69 76\n[25873] 65 4e 61 6d 65 22 3a 7b 22 70 6f 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[25897] 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 20 4d 6f c3 a7 61 6d 62\n[25921] 69 71 75 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 6f c3 a7 61 6d 62 69\n[25945] 71 75 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f\n[25969] 6e 22 3a 22 43 61 70 65 20 56 65 72 64 65 22 2c 22 6f 66 66 69 63 69 61\n[25993] 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 43 61 62 6f 20 56 65 72\n[26017] 64 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 70 6f 72 22 3a\n[26041] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20\n[26065] 64 65 20 43 61 62 6f 20 56 65 72 64 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[26089] 22 43 61 62 6f 20 56 65 72 64 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n[26113] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 61 6e 61 64 61 22 2c 22 6f 66 66\n[26137] 69 63 69 61 6c 22 3a 22 43 61 6e 61 64 61 22 2c 22 6e 61 74 69 76 65 4e\n[26161] 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[26185] 22 43 61 6e 61 64 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 61 6e 61 64\n[26209] 61 22 7d 2c 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43\n[26233] 61 6e 61 64 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 61 6e 61 64 61 22\n[26257] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[26281] 49 73 6c 65 20 6f 66 20 4d 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a\n[26305] 22 49 73 6c 65 20 6f 66 20 4d 61 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[26329] 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 49\n[26353] 73 6c 65 20 6f 66 20 4d 61 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 49 73\n[26377] 6c 65 20 6f 66 20 4d 61 6e 22 7d 2c 22 67 6c 76 22 3a 7b 22 6f 66 66 69\n[26401] 63 69 61 6c 22 3a 22 45 6c 6c 61 6e 20 56 61 6e 6e 69 6e 20 6f 72 20 4d\n[26425] 61 6e 6e 69 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 6e 6e 69 6e 22\n[26449] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[26473] 47 69 62 72 61 6c 74 61 72 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 47\n[26497] 69 62 72 61 6c 74 61 72 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b\n[26521] 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 47 69 62 72 61\n[26545] 6c 74 61 72 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 47 69 62 72 61 6c 74 61\n[26569] 72 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[26593] 3a 22 53 6f 75 74 68 20 4b 6f 72 65 61 22 2c 22 6f 66 66 69 63 69 61 6c\n[26617] 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4b 6f 72 65 61 22 2c 22 6e\n[26641] 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6b 6f 72 22 3a 7b 22 6f 66 66 69\n[26665] 63 69 61 6c 22 3a 22 eb 8c 80 ed 95 9c eb af bc ea b5 ad 22 2c 22 63 6f\n[26689] 6d 6d 6f 6e 22 3a 22 ed 95 9c ea b5 ad 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[26713] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 69 63 61 72 61 67 75 61 22\n[26737] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66\n[26761] 20 4e 69 63 61 72 61 67 75 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[26785] 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n[26809] c3 ba 62 6c 69 63 61 20 64 65 20 4e 69 63 61 72 61 67 75 61 22 2c 22 63\n[26833] 6f 6d 6d 6f 6e 22 3a 22 4e 69 63 61 72 61 67 75 61 22 7d 7d 7d 7d 2c 7b\n[26857] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42 75 6c 67 61 72\n[26881] 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n[26905] 20 6f 66 20 42 75 6c 67 61 72 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[26929] 65 22 3a 7b 22 62 75 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d0\n[26953] a0 d0 b5 d0 bf d1 83 d0 b1 d0 bb d0 b8 d0 ba d0 b0 20 d0 91 d1 8a d0 bb\n[26977] d0 b3 d0 b0 d1 80 d0 b8 d1 8f 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 91\n[27001] d1 8a d0 bb d0 b3 d0 b0 d1 80 d0 b8 d1 8f 22 7d 7d 7d 7d 2c 7b 22 6e 61\n[27025] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 45 6c 20 53 61 6c 76 61 64\n[27049] 6f 72 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n[27073] 20 6f 66 20 45 6c 20 53 61 6c 76 61 64 6f 72 22 2c 22 6e 61 74 69 76 65\n[27097] 4e 61 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22\n[27121] 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 20 45 6c 20 53 61 6c 76 61\n[27145] 64 6f 72 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 45 6c 20 53 61 6c 76 61 64\n[27169] 6f 72 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n[27193] 22 3a 22 4c 75 78 65 6d 62 6f 75 72 67 22 2c 22 6f 66 66 69 63 69 61 6c\n[27217] 22 3a 22 47 72 61 6e 64 20 44 75 63 68 79 20 6f 66 20 4c 75 78 65 6d 62\n[27241] 6f 75 72 67 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 64 65 75\n[27265] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 47 72 6f c3 9f 68 65 72 7a\n[27289] 6f 67 74 75 6d 20 4c 75 78 65 6d 62 75 72 67 22 2c 22 63 6f 6d 6d 6f 6e\n[27313] 22 3a 22 4c 75 78 65 6d 62 75 72 67 22 7d 2c 22 66 72 61 22 3a 7b 22 6f\n[27337] 66 66 69 63 69 61 6c 22 3a 22 47 72 61 6e 64 2d 44 75 63 68 c3 a9 20 64\n[27361] 65 20 4c 75 78 65 6d 62 6f 75 72 67 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[27385] 4c 75 78 65 6d 62 6f 75 72 67 22 7d 2c 22 6c 74 7a 22 3a 7b 22 6f 66 66\n[27409] 69 63 69 61 6c 22 3a 22 47 72 6f 75 73 73 68 65 72 7a 6f 67 74 75 6d 20\n[27433] 4c c3 ab 74 7a 65 62 75 65 72 67 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4c\n[27457] c3 ab 74 7a 65 62 75 65 72 67 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a\n[27481] 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 55 6e 69 74 65 64 20 53 74 61 74 65 73\n[27505] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 55 6e 69 74 65 64 20 53 74 61\n[27529] 74 65 73 20 6f 66 20 41 6d 65 72 69 63 61 22 2c 22 6e 61 74 69 76 65 4e\n[27553] 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[27577] 22 55 6e 69 74 65 64 20 53 74 61 74 65 73 20 6f 66 20 41 6d 65 72 69 63\n[27601] 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 55 6e 69 74 65 64 20 53 74 61 74\n[27625] 65 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n[27649] 22 3a 22 43 6f 6c 6f 6d 62 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a\n[27673] 22 52 65 70 75 62 6c 69 63 20 6f 66 20 43 6f 6c 6f 6d 62 69 61 22 2c 22\n[27697] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66\n[27721] 69 63 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 20 43 6f\n[27745] 6c 6f 6d 62 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 6f 6c 6f 6d 62\n[27769] 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n[27793] 22 3a 22 57 65 73 74 65 72 6e 20 53 61 68 61 72 61 22 2c 22 6f 66 66 69\n[27817] 63 69 61 6c 22 3a 22 53 61 68 72 61 77 69 20 41 72 61 62 20 44 65 6d 6f\n[27841] 63 72 61 74 69 63 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65\n[27865] 4e 61 6d 65 22 3a 7b 22 62 65 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22\n[27889] 3a 22 53 61 68 72 61 77 69 20 41 72 61 62 20 44 65 6d 6f 63 72 61 74 69\n[27913] 63 20 52 65 70 75 62 6c 69 63 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 57 65\n[27937] 73 74 65 72 6e 20 53 61 68 61 72 61 22 7d 2c 22 6d 65 79 22 3a 7b 22 6f\n[27961] 66 66 69 63 69 61 6c 22 3a 22 d8 a7 d9 84 d8 ac d9 85 d9 87 d9 88 d8 b1\n[27985] d9 8a d8 a9 20 d8 a7 d9 84 d8 b9 d8 b1 d8 a8 d9 8a d8 a9 20 d8 a7 d9 84\n[28009] d8 b5 d8 ad d8 b1 d8 a7 d9 88 d9 8a d8 a9 20 d8 a7 d9 84 d8 af d9 8a d9\n[28033] 85 d9 82 d8 b1 d8 a7 d8 b7 d9 8a d8 a9 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[28057] 22 d8 a7 d9 84 d8 b5 d8 ad d8 b1 d8 a7 d8 a1 20 d8 a7 d9 84 d8 ba d8 b1\n[28081] d8 a8 d9 8a d8 a9 22 7d 2c 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61\n[28105] 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 c3 81 72 61 62 65 20 53 61\n[28129] 68 61 72 61 75 69 20 44 65 6d 6f 63 72 c3 a1 74 69 63 61 22 2c 22 63 6f\n[28153] 6d 6d 6f 6e 22 3a 22 53 61 68 61 72 61 20 4f 63 63 69 64 65 6e 74 61 6c\n[28177] 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a\n[28201] 22 41 75 73 74 72 61 6c 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[28225] 43 6f 6d 6d 6f 6e 77 65 61 6c 74 68 20 6f 66 20 41 75 73 74 72 61 6c 69\n[28249] 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b\n[28273] 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6d 6d 6f 6e 77 65 61 6c 74 68\n[28297] 20 6f 66 20 41 75 73 74 72 61 6c 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[28321] 22 41 75 73 74 72 61 6c 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a\n[28345] 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 68 61 64 22 2c 22 6f 66 66 69 63 69\n[28369] 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 43 68 61 64 22 2c 22\n[28393] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66\n[28417] 69 63 69 61 6c 22 3a 22 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8 a9 20 d8\n[28441] aa d8 b4 d8 a7 d8 af 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8 aa d8 b4 d8\n[28465] a7 d8 af e2 80 8e 22 7d 2c 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61\n[28489] 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 75 20 54 63 68 61 64\n[28513] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54 63 68 61 64 22 7d 7d 7d 7d 2c 7b\n[28537] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61 69 6e 74 20\n[28561] 48 65 6c 65 6e 61 2c 20 41 73 63 65 6e 73 69 6f 6e 20 61 6e 64 20 54 72\n[28585] 69 73 74 61 6e 20 64 61 20 43 75 6e 68 61 22 2c 22 6f 66 66 69 63 69 61\n[28609] 6c 22 3a 22 53 61 69 6e 74 20 48 65 6c 65 6e 61 2c 20 41 73 63 65 6e 73\n[28633] 69 6f 6e 20 61 6e 64 20 54 72 69 73 74 61 6e 20 64 61 20 43 75 6e 68 61\n[28657] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22\n[28681] 6f 66 66 69 63 69 61 6c 22 3a 22 53 61 69 6e 74 20 48 65 6c 65 6e 61 2c\n[28705] 20 41 73 63 65 6e 73 69 6f 6e 20 61 6e 64 20 54 72 69 73 74 61 6e 20 64\n[28729] 61 20 43 75 6e 68 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61 69 6e 74\n[28753] 20 48 65 6c 65 6e 61 2c 20 41 73 63 65 6e 73 69 6f 6e 20 61 6e 64 20 54\n[28777] 72 69 73 74 61 6e 20 64 61 20 43 75 6e 68 61 22 7d 7d 7d 7d 2c 7b 22 6e\n[28801] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 54 75 72 6b 65 79 22 2c\n[28825] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20\n[28849] 54 75 72 6b 65 79 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 74\n[28873] 75 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 c3 bc 72 6b 69 79\n[28897] 65 20 43 75 6d 68 75 72 69 79 65 74 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[28921] 22 54 c3 bc 72 6b 69 79 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b\n[28945] 22 63 6f 6d 6d 6f 6e 22 3a 22 42 65 6c 69 7a 65 22 2c 22 6f 66 66 69 63\n[28969] 69 61 6c 22 3a 22 42 65 6c 69 7a 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[28993] 65 22 3a 7b 22 62 6a 7a 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42\n[29017] 65 6c 69 7a 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 65 6c 69 7a 65 22\n[29041] 7d 2c 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 65 6c\n[29065] 69 7a 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 65 6c 69 7a 65 22 7d 2c\n[29089] 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 65 6c 69 63\n[29113] 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 65 6c 69 63 65 22 7d 7d 7d 7d\n[29137] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 47 68 61 6e\n[29161] 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20\n[29185] 6f 66 20 47 68 61 6e 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b\n[29209] 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62\n[29233] 6c 69 63 20 6f 66 20 47 68 61 6e 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[29257] 47 68 61 6e 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d\n[29281] 6d 6f 6e 22 3a 22 53 61 69 6e 74 20 4d 61 72 74 69 6e 22 2c 22 6f 66 66\n[29305] 69 63 69 61 6c 22 3a 22 53 61 69 6e 74 20 4d 61 72 74 69 6e 22 2c 22 6e\n[29329] 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69\n[29353] 63 69 61 6c 22 3a 22 53 61 69 6e 74 2d 4d 61 72 74 69 6e 22 2c 22 63 6f\n[29377] 6d 6d 6f 6e 22 3a 22 53 61 69 6e 74 2d 4d 61 72 74 69 6e 22 7d 7d 7d 7d\n[29401] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 55 67 61 6e\n[29425] 64 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n[29449] 20 6f 66 20 55 67 61 6e 64 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[29473] 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n[29497] 75 62 6c 69 63 20 6f 66 20 55 67 61 6e 64 61 22 2c 22 63 6f 6d 6d 6f 6e\n[29521] 22 3a 22 55 67 61 6e 64 61 22 7d 2c 22 73 77 61 22 3a 7b 22 6f 66 66 69\n[29545] 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 55 67 61 6e 64\n[29569] 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 55 67 61 6e 64 61 22 7d 7d 7d 7d\n[29593] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 72 74\n[29617] 69 6e 69 71 75 65 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4d 61 72 74\n[29641] 69 6e 69 71 75 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66\n[29665] 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4d 61 72 74 69 6e 69\n[29689] 71 75 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 72 74 69 6e 69 71 75\n[29713] 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[29737] 3a 22 53 65 72 62 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65\n[29761] 70 75 62 6c 69 63 20 6f 66 20 53 65 72 62 69 61 22 2c 22 6e 61 74 69 76\n[29785] 65 4e 61 6d 65 22 3a 7b 22 73 72 70 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[29809] 22 3a 22 d0 a0 d0 b5 d0 bf d1 83 d0 b1 d0 bb d0 b8 d0 ba d0 b0 20 d0 a1\n[29833] d1 80 d0 b1 d0 b8 d1 98 d0 b0 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 a1\n[29857] d1 80 d0 b1 d0 b8 d1 98 d0 b0 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a\n[29881] 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 6c 61 79 73 69 61 22 2c 22 6f 66\n[29905] 66 69 63 69 61 6c 22 3a 22 4d 61 6c 61 79 73 69 61 22 2c 22 6e 61 74 69\n[29929] 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61\n[29953] 6c 22 3a 22 4d 61 6c 61 79 73 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[29977] 4d 61 6c 61 79 73 69 61 22 7d 2c 22 6d 73 61 22 3a 7b 22 6f 66 66 69 63\n[30001] 69 61 6c 22 3a 22 d9 85 d9 84 d9 8a d8 b3 d9 8a d8 a7 22 2c 22 63 6f 6d\n[30025] 6d 6f 6e 22 3a 22 d9 85 d9 84 d9 8a d8 b3 d9 8a d8 a7 22 7d 7d 7d 7d 2c\n[30049] 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 c3 85 6c 61 6e\n[30073] 64 20 49 73 6c 61 6e 64 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 c3\n[30097] 85 6c 61 6e 64 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61\n[30121] 6d 65 22 3a 7b 22 73 77 65 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[30145] 4c 61 6e 64 73 6b 61 70 65 74 20 c3 85 6c 61 6e 64 22 2c 22 63 6f 6d 6d\n[30169] 6f 6e 22 3a 22 c3 85 6c 61 6e 64 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n[30193] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 69 75 65 22 2c 22 6f 66 66 69 63\n[30217] 69 61 6c 22 3a 22 4e 69 75 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[30241] 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4e 69 75\n[30265] 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 69 75 65 22 7d 2c 22 6e 69 75\n[30289] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4e 69 75 c4 93 22 2c 22 63\n[30313] 6f 6d 6d 6f 6e 22 3a 22 4e 69 75 c4 93 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[30337] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 44 6f 6d 69 6e 69 63 61 22 2c\n[30361] 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6d 6d 6f 6e 77 65 61 6c 74 68\n[30385] 20 6f 66 20 44 6f 6d 69 6e 69 63 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[30409] 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43\n[30433] 6f 6d 6d 6f 6e 77 65 61 6c 74 68 20 6f 66 20 44 6f 6d 69 6e 69 63 61 22\n[30457] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 44 6f 6d 69 6e 69 63 61 22 7d 7d 7d 7d\n[30481] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 49 72 65 6c\n[30505] 61 6e 64 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69\n[30529] 63 20 6f 66 20 49 72 65 6c 61 6e 64 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[30553] 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[30577] 65 70 75 62 6c 69 63 20 6f 66 20 49 72 65 6c 61 6e 64 22 2c 22 63 6f 6d\n[30601] 6d 6f 6e 22 3a 22 49 72 65 6c 61 6e 64 22 7d 2c 22 67 6c 65 22 3a 7b 22\n[30625] 6f 66 66 69 63 69 61 6c 22 3a 22 50 6f 62 6c 61 63 68 74 20 6e 61 20 68\n[30649] c3 89 69 72 65 61 6e 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 c3 89 69 72\n[30673] 65 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[30697] 3a 22 46 61 6c 6b 6c 61 6e 64 20 49 73 6c 61 6e 64 73 22 2c 22 6f 66 66\n[30721] 69 63 69 61 6c 22 3a 22 46 61 6c 6b 6c 61 6e 64 20 49 73 6c 61 6e 64 73\n[30745] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22\n[30769] 6f 66 66 69 63 69 61 6c 22 3a 22 46 61 6c 6b 6c 61 6e 64 20 49 73 6c 61\n[30793] 6e 64 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 46 61 6c 6b 6c 61 6e 64 20\n[30817] 49 73 6c 61 6e 64 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n[30841] 6f 6d 6d 6f 6e 22 3a 22 55 6e 69 74 65 64 20 4b 69 6e 67 64 6f 6d 22 2c\n[30865] 22 6f 66 66 69 63 69 61 6c 22 3a 22 55 6e 69 74 65 64 20 4b 69 6e 67 64\n[30889] 6f 6d 20 6f 66 20 47 72 65 61 74 20 42 72 69 74 61 69 6e 20 61 6e 64 20\n[30913] 4e 6f 72 74 68 65 72 6e 20 49 72 65 6c 61 6e 64 22 2c 22 6e 61 74 69 76\n[30937] 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[30961] 22 3a 22 55 6e 69 74 65 64 20 4b 69 6e 67 64 6f 6d 20 6f 66 20 47 72 65\n[30985] 61 74 20 42 72 69 74 61 69 6e 20 61 6e 64 20 4e 6f 72 74 68 65 72 6e 20\n[31009] 49 72 65 6c 61 6e 64 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 55 6e 69 74 65\n[31033] 64 20 4b 69 6e 67 64 6f 6d 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b\n[31057] 22 63 6f 6d 6d 6f 6e 22 3a 22 44 6f 6d 69 6e 69 63 61 6e 20 52 65 70 75\n[31081] 62 6c 69 63 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 44 6f 6d 69 6e 69\n[31105] 63 61 6e 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[31129] 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[31153] 65 70 c3 ba 62 6c 69 63 61 20 44 6f 6d 69 6e 69 63 61 6e 61 22 2c 22 63\n[31177] 6f 6d 6d 6f 6e 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 44 6f 6d 69 6e\n[31201] 69 63 61 6e 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d\n[31225] 6d 6f 6e 22 3a 22 41 72 67 65 6e 74 69 6e 61 22 2c 22 6f 66 66 69 63 69\n[31249] 61 6c 22 3a 22 41 72 67 65 6e 74 69 6e 65 20 52 65 70 75 62 6c 69 63 22\n[31273] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 67 72 6e 22 3a 7b 22 6f\n[31297] 66 66 69 63 69 61 6c 22 3a 22 41 72 67 65 6e 74 69 6e 65 20 52 65 70 75\n[31321] 62 6c 69 63 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 41 72 67 65 6e 74 69 6e\n[31345] 61 22 7d 2c 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[31369] 65 70 c3 ba 62 6c 69 63 61 20 41 72 67 65 6e 74 69 6e 61 22 2c 22 63 6f\n[31393] 6d 6d 6f 6e 22 3a 22 41 72 67 65 6e 74 69 6e 61 22 7d 7d 7d 7d 2c 7b 22\n[31417] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 54 75 72 6b 73 20 61\n[31441] 6e 64 20 43 61 69 63 6f 73 20 49 73 6c 61 6e 64 73 22 2c 22 6f 66 66 69\n[31465] 63 69 61 6c 22 3a 22 54 75 72 6b 73 20 61 6e 64 20 43 61 69 63 6f 73 20\n[31489] 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n[31513] 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 75 72 6b 73 20\n[31537] 61 6e 64 20 43 61 69 63 6f 73 20 49 73 6c 61 6e 64 73 22 2c 22 63 6f 6d\n[31561] 6d 6f 6e 22 3a 22 54 75 72 6b 73 20 61 6e 64 20 43 61 69 63 6f 73 20 49\n[31585] 73 6c 61 6e 64 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f\n[31609] 6d 6d 6f 6e 22 3a 22 52 75 73 73 69 61 22 2c 22 6f 66 66 69 63 69 61 6c\n[31633] 22 3a 22 52 75 73 73 69 61 6e 20 46 65 64 65 72 61 74 69 6f 6e 22 2c 22\n[31657] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 72 75 73 22 3a 7b 22 6f 66 66\n[31681] 69 63 69 61 6c 22 3a 22 d0 a0 d0 be d1 81 d1 81 d0 b8 d0 b9 d1 81 d0 ba\n[31705] d0 b0 d1 8f 20 d0 a4 d0 b5 d0 b4 d0 b5 d1 80 d0 b0 d1 86 d0 b8 d1 8f 22\n[31729] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 a0 d0 be d1 81 d1 81 d0 b8 d1 8f 22\n[31753] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[31777] 56 61 6e 75 61 74 75 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n[31801] 75 62 6c 69 63 20 6f 66 20 56 61 6e 75 61 74 75 22 2c 22 6e 61 74 69 76\n[31825] 65 4e 61 6d 65 22 3a 7b 22 62 69 73 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[31849] 22 3a 22 52 69 70 61 62 6c 69 6b 20 62 6c 6f 6e 67 20 56 61 6e 75 61 74\n[31873] 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 56 61 6e 75 61 74 75 22 7d 2c 22\n[31897] 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c\n[31921] 69 63 20 6f 66 20 56 61 6e 75 61 74 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[31945] 22 56 61 6e 75 61 74 75 22 7d 2c 22 66 72 61 22 3a 7b 22 6f 66 66 69 63\n[31969] 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 65 20 56 61 6e\n[31993] 75 61 74 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 56 61 6e 75 61 74 75 22\n[32017] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[32041] 4d 65 78 69 63 6f 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 55 6e 69 74\n[32065] 65 64 20 4d 65 78 69 63 61 6e 20 53 74 61 74 65 73 22 2c 22 6e 61 74 69\n[32089] 76 65 4e 61 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61\n[32113] 6c 22 3a 22 45 73 74 61 64 6f 73 20 55 6e 69 64 6f 73 20 4d 65 78 69 63\n[32137] 61 6e 6f 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d c3 a9 78 69 63 6f 22\n[32161] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[32185] 42 72 75 6e 65 69 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4e 61 74 69\n[32209] 6f 6e 20 6f 66 20 42 72 75 6e 65 69 2c 20 41 62 6f 64 65 20 6f 66 20 50\n[32233] 65 61 63 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6d 73 61\n[32257] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4e 61 74 69 6f 6e 20 6f 66\n[32281] 20 42 72 75 6e 65 69 2c 20 41 62 6f 64 65 20 44 61 6d 61 69 22 2c 22 63\n[32305] 6f 6d 6d 6f 6e 22 3a 22 4e 65 67 61 72 61 20 42 72 75 6e 65 69 20 44 61\n[32329] 72 75 73 73 61 6c 61 6d 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22\n[32353] 63 6f 6d 6d 6f 6e 22 3a 22 46 72 65 6e 63 68 20 47 75 69 61 6e 61 22 2c\n[32377] 22 6f 66 66 69 63 69 61 6c 22 3a 22 47 75 69 61 6e 61 22 2c 22 6e 61 74\n[32401] 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69\n[32425] 61 6c 22 3a 22 47 75 79 61 6e 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 47\n[32449] 75 79 61 6e 65 20 66 72 61 6e c3 a7 61 69 73 65 22 7d 7d 7d 7d 2c 7b 22\n[32473] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 75 72 69 74 69\n[32497] 75 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n[32521] 20 6f 66 20 4d 61 75 72 69 74 69 75 73 22 2c 22 6e 61 74 69 76 65 4e 61\n[32545] 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[32569] 52 65 70 75 62 6c 69 63 20 6f 66 20 4d 61 75 72 69 74 69 75 73 22 2c 22\n[32593] 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 75 72 69 74 69 75 73 22 7d 2c 22 66 72\n[32617] 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69\n[32641] 71 75 65 20 64 65 20 4d 61 75 72 69 63 65 22 2c 22 63 6f 6d 6d 6f 6e 22\n[32665] 3a 22 4d 61 75 72 69 63 65 22 7d 2c 22 6d 66 65 22 3a 7b 22 6f 66 66 69\n[32689] 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 6b 20 4d 6f 72 69 73 22 2c 22\n[32713] 63 6f 6d 6d 6f 6e 22 3a 22 4d 6f 72 69 73 22 7d 7d 7d 7d 2c 7b 22 6e 61\n[32737] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42 65 6c 61 72 75 73 22 2c\n[32761] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20\n[32785] 42 65 6c 61 72 75 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n[32809] 62 65 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d0 a0 d1 8d d1 81\n[32833] d0 bf d1 83 d0 b1 d0 bb d1 96 d0 ba d0 b0 20 d0 91 d0 b5 d0 bb d0 b0 d1\n[32857] 80 d1 83 d1 81 d1 8c 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 91 d0 b5 d0\n[32881] bb d0 b0 d1 80 d1 83 cc 81 d1 81 d1 8c 22 7d 2c 22 72 75 73 22 3a 7b 22\n[32905] 6f 66 66 69 63 69 61 6c 22 3a 22 d0 a0 d0 b5 d1 81 d0 bf d1 83 d0 b1 d0\n[32929] bb d0 b8 d0 ba d0 b0 20 d0 91 d0 b5 d0 bb d0 b0 d1 80 d1 83 d1 81 d1 8c\n[32953] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 91 d0 b5 d0 bb d0 b0 d1 80 d1 83\n[32977] d1 81 d1 8c 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d\n[33001] 6f 6e 22 3a 22 4d 61 75 72 69 74 61 6e 69 61 22 2c 22 6f 66 66 69 63 69\n[33025] 61 6c 22 3a 22 49 73 6c 61 6d 69 63 20 52 65 70 75 62 6c 69 63 20 6f 66\n[33049] 20 4d 61 75 72 69 74 61 6e 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[33073] 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 a7\n[33097] d9 84 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8 a9 20 d8 a7 d9 84 d8 a5 d8\n[33121] b3 d9 84 d8 a7 d9 85 d9 8a d8 a9 20 d8 a7 d9 84 d9 85 d9 88 d8 b1 d9 8a\n[33145] d8 aa d8 a7 d9 86 d9 8a d8 a9 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d9 85\n[33169] d9 88 d8 b1 d9 8a d8 aa d8 a7 d9 86 d9 8a d8 a7 22 7d 7d 7d 7d 2c 7b 22\n[33193] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 54 75 72 6b 6d 65 6e\n[33217] 69 73 74 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 75 72 6b 6d\n[33241] 65 6e 69 73 74 61 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n[33265] 72 75 73 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d0 a2 d1 83 d1 80\n[33289] d0 ba d0 bc d0 b5 d0 bd d0 b8 d1 81 d1 82 d0 b0 d0 bd 22 2c 22 63 6f 6d\n[33313] 6d 6f 6e 22 3a 22 d0 a2 d1 83 d1 80 d0 ba d0 bc d0 b5 d0 bd d0 b8 d1 8f\n[33337] 22 7d 2c 22 74 75 6b 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 c3\n[33361] bc 72 6b 6d 65 6e 69 73 74 61 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 54\n[33385] c3 bc 72 6b 6d 65 6e 69 73 74 61 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n[33409] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 50 61 72 61 67 75 61 79 22 2c 22\n[33433] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 50\n[33457] 61 72 61 67 75 61 79 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n[33481] 67 72 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 65 74 c3 a3 20\n[33505] 50 61 72 61 67 75 c3 a1 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 50 61 72\n[33529] 61 67 75 c3 a1 69 22 7d 2c 22 73 70 61 22 3a 7b 22 6f 66 66 69 63 69 61\n[33553] 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 20 50 61 72 61 67 75\n[33577] 61 79 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 50 61 72 61 67 75 61 79 22 7d\n[33601] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43\n[33625] 7a 65 63 68 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 7a 65 63\n[33649] 68 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[33673] 3a 7b 22 63 65 73 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 c4 8c 65\n[33697] 73 6b c3 a1 20 72 65 70 75 62 6c 69 6b 61 22 2c 22 63 6f 6d 6d 6f 6e 22\n[33721] 3a 22 c4 8c 65 73 6b 6f 22 7d 2c 22 73 6c 6b 22 3a 7b 22 6f 66 66 69 63\n[33745] 69 61 6c 22 3a 22 c4 8c 65 73 6b c3 a1 20 72 65 70 75 62 6c 69 6b 61 22\n[33769] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 c4 8c 65 73 6b 6f 22 7d 7d 7d 7d 2c 7b\n[33793] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 72 69 20 4c 61\n[33817] 6e 6b 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 44 65 6d 6f 63 72 61\n[33841] 74 69 63 20 53 6f 63 69 61 6c 69 73 74 20 52 65 70 75 62 6c 69 63 20 6f\n[33865] 66 20 53 72 69 20 4c 61 6e 6b 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[33889] 22 3a 7b 22 73 69 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 e0 b7\n[33913] 81 e0 b7 8a e2 80 8d e0 b6 bb e0 b7 93 20 e0 b6 bd e0 b6 82 e0 b6 9a e0\n[33937] b7 8f 20 e0 b6 b4 e0 b7 8a e2 80 8d e0 b6 bb e0 b6 a2 e0 b7 8f e0 b6 ad\n[33961] e0 b7 8f e0 b6 b1 e0 b7 8a e0 b6 ad e0 b7 8a e2 80 8d e0 b6 bb e0 b7 92\n[33985] e0 b6 9a 20 e0 b7 83 e0 b6 b8 e0 b7 8f e0 b6 a2 e0 b7 80 e0 b7 8f e0 b6\n[34009] af e0 b7 93 20 e0 b6 a2 e0 b6 b1 e0 b6 bb e0 b6 a2 e0 b6 ba 22 2c 22 63\n[34033] 6f 6d 6d 6f 6e 22 3a 22 e0 b7 81 e0 b7 8a e2 80 8d e0 b6 bb e0 b7 93 20\n[34057] e0 b6 bd e0 b6 82 e0 b6 9a e0 b7 8f e0 b7 80 22 7d 2c 22 74 61 6d 22 3a\n[34081] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 e0 ae 87 e0 ae b2 e0 ae 99 e0 af\n[34105] 8d e0 ae 95 e0 af 88 20 e0 ae 9a e0 ae a9 e0 ae a8 e0 ae be e0 ae af e0\n[34129] ae 95 20 e0 ae 9a e0 af 8b e0 ae 9a e0 ae b2 e0 ae bf e0 ae 9a e0 ae 95\n[34153] e0 af 8d 20 e0 ae 95 e0 af 81 e0 ae 9f e0 ae bf e0 ae af e0 ae b0 e0 ae\n[34177] 9a e0 af 81 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e0 ae 87 e0 ae b2 e0 ae\n[34201] 99 e0 af 8d e0 ae 95 e0 af 88 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a\n[34225] 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 69 67 65 72 69 61 22 2c 22 6f 66 66\n[34249] 69 63 69 61 6c 22 3a 22 46 65 64 65 72 61 6c 20 52 65 70 75 62 6c 69 63\n[34273] 20 6f 66 20 4e 69 67 65 72 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[34297] 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 65\n[34321] 64 65 72 61 6c 20 52 65 70 75 62 6c 69 63 20 6f 66 20 4e 69 67 65 72 69\n[34345] 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 69 67 65 72 69 61 22 7d 7d 7d\n[34369] 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 50 61 6c\n[34393] 61 75 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n[34417] 20 6f 66 20 50 61 6c 61 75 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a\n[34441] 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75\n[34465] 62 6c 69 63 20 6f 66 20 50 61 6c 61 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[34489] 22 50 61 6c 61 75 22 7d 2c 22 70 61 75 22 3a 7b 22 6f 66 66 69 63 69 61\n[34513] 6c 22 3a 22 42 65 6c 75 75 20 65 72 20 61 20 42 65 6c 61 75 22 2c 22 63\n[34537] 6f 6d 6d 6f 6e 22 3a 22 42 65 6c 61 75 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[34561] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4b 79 72 67 79 7a 73 74 61 6e\n[34585] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 79 72 67 79 7a 20 52 65 70\n[34609] 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6b 69\n[34633] 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d0 9a d1 8b d1 80 d0 b3\n[34657] d1 8b d0 b7 20 d0 a0 d0 b5 d1 81 d0 bf d1 83 d0 b1 d0 bb d0 b8 d0 ba d0\n[34681] b0 d1 81 d1 8b 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 9a d1 8b d1 80 d0\n[34705] b3 d1 8b d0 b7 d1 81 d1 82 d0 b0 d0 bd 22 7d 2c 22 72 75 73 22 3a 7b 22\n[34729] 6f 66 66 69 63 69 61 6c 22 3a 22 d0 9a d1 8b d1 80 d0 b3 d1 8b d0 b7 d1\n[34753] 81 d0 ba d0 b0 d1 8f 20 d0 a0 d0 b5 d1 81 d0 bf d1 83 d0 b1 d0 bb d0 b8\n[34777] d0 ba d0 b0 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 9a d0 b8 d1 80 d0 b3\n[34801] d0 b8 d0 b7 d0 b8 d1 8f 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22\n[34825] 63 6f 6d 6d 6f 6e 22 3a 22 54 75 6e 69 73 69 61 22 2c 22 6f 66 66 69 63\n[34849] 69 61 6c 22 3a 22 54 75 6e 69 73 69 61 6e 20 52 65 70 75 62 6c 69 63 22\n[34873] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f\n[34897] 66 66 69 63 69 61 6c 22 3a 22 d8 a7 d9 84 d8 ac d9 85 d9 87 d9 88 d8 b1\n[34921] d9 8a d8 a9 20 d8 a7 d9 84 d8 aa d9 88 d9 86 d8 b3 d9 8a d8 a9 22 2c 22\n[34945] 63 6f 6d 6d 6f 6e 22 3a 22 d8 aa d9 88 d9 86 d8 b3 22 7d 7d 7d 7d 2c 7b\n[34969] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 61 64 65 6c\n[34993] 6f 75 70 65 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 47 75 61 64 65 6c\n[35017] 6f 75 70 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61\n[35041] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 47 75 61 64 65 6c 6f 75 70\n[35065] 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 47 75 61 64 65 6c 6f 75 70 65 22\n[35089] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[35113] 4b 69 72 69 62 61 74 69 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 49 6e\n[35137] 64 65 70 65 6e 64 65 6e 74 20 61 6e 64 20 53 6f 76 65 72 65 69 67 6e 20\n[35161] 52 65 70 75 62 6c 69 63 20 6f 66 20 4b 69 72 69 62 61 74 69 22 2c 22 6e\n[35185] 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69\n[35209] 63 69 61 6c 22 3a 22 49 6e 64 65 70 65 6e 64 65 6e 74 20 61 6e 64 20 53\n[35233] 6f 76 65 72 65 69 67 6e 20 52 65 70 75 62 6c 69 63 20 6f 66 20 4b 69 72\n[35257] 69 62 61 74 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4b 69 72 69 62 61 74\n[35281] 69 22 7d 2c 22 67 69 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[35305] 69 62 61 62 65 72 69 6b 69 20 4b 69 72 69 62 61 74 69 22 2c 22 63 6f 6d\n[35329] 6d 6f 6e 22 3a 22 4b 69 72 69 62 61 74 69 22 7d 7d 7d 7d 2c 7b 22 6e 61\n[35353] 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 46 69 6a 69 22 2c 22 6f 66\n[35377] 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 46 69 6a\n[35401] 69 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b\n[35425] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20\n[35449] 46 69 6a 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 46 69 6a 69 22 7d 2c 22\n[35473] 66 69 6a 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4d 61 74 61 6e 69\n[35497] 74 75 20 54 75 67 61 6c 61 6c 61 20 6f 20 56 69 74 69 22 2c 22 63 6f 6d\n[35521] 6d 6f 6e 22 3a 22 56 69 74 69 22 7d 2c 22 68 69 66 22 3a 7b 22 6f 66 66\n[35545] 69 63 69 61 6c 22 3a 22 e0 a4 b0 e0 a4 bf e0 a4 aa e0 a4 ac e0 a5 8d e0\n[35569] a4 b2 e0 a4 bf e0 a4 95 20 e0 a4 91 e0 a4 ab 20 e0 a4 ab e0 a5 80 e0 a4\n[35593] 9c e0 a5 80 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e0 a4 ab e0 a4 bf e0 a4\n[35617] 9c e0 a5 80 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d\n[35641] 6f 6e 22 3a 22 41 6e 64 6f 72 72 61 22 2c 22 6f 66 66 69 63 69 61 6c 22\n[35665] 3a 22 50 72 69 6e 63 69 70 61 6c 69 74 79 20 6f 66 20 41 6e 64 6f 72 72\n[35689] 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 63 61 74 22 3a 7b\n[35713] 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 72 69 6e 63 69 70 61 74 20 64 27\n[35737] 41 6e 64 6f 72 72 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 41 6e 64 6f 72\n[35761] 72 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n[35785] 22 3a 22 41 6e 74 61 72 63 74 69 63 61 22 2c 22 6f 66 66 69 63 69 61 6c\n[35809] 22 3a 22 41 6e 74 61 72 63 74 69 63 61 22 2c 22 6e 61 74 69 76 65 4e 61\n[35833] 6d 65 22 3a 7b 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f\n[35857] 6e 22 3a 22 4d 61 72 73 68 61 6c 6c 20 49 73 6c 61 6e 64 73 22 2c 22 6f\n[35881] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 74 68\n[35905] 65 20 4d 61 72 73 68 61 6c 6c 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74\n[35929] 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69\n[35953] 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 74 68 65 20 4d 61 72\n[35977] 73 68 61 6c 6c 20 49 73 6c 61 6e 64 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[36001] 22 4d 61 72 73 68 61 6c 6c 20 49 73 6c 61 6e 64 73 22 7d 2c 22 6d 61 68\n[36025] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20\n[36049] 6f 66 20 74 68 65 20 4d 61 72 73 68 61 6c 6c 20 49 73 6c 61 6e 64 73 22\n[36073] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d cc a7 61 6a 65 c4 bc 22 7d 7d 7d 7d\n[36097] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 68 69 6e\n[36121] 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 65 6f 70 6c 65 27 73 20\n[36145] 52 65 70 75 62 6c 69 63 20 6f 66 20 43 68 69 6e 61 22 2c 22 6e 61 74 69\n[36169] 76 65 4e 61 6d 65 22 3a 7b 22 7a 68 6f 22 3a 7b 22 6f 66 66 69 63 69 61\n[36193] 6c 22 3a 22 e4 b8 ad e5 8d 8e e4 ba ba e6 b0 91 e5 85 b1 e5 92 8c e5 9b\n[36217] bd 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e4 b8 ad e5 9b bd 22 7d 7d 7d 7d\n[36241] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 72 6f 61\n[36265] 74 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69\n[36289] 63 20 6f 66 20 43 72 6f 61 74 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[36313] 65 22 3a 7b 22 68 72 76 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[36337] 65 70 75 62 6c 69 6b 61 20 48 72 76 61 74 73 6b 61 22 2c 22 63 6f 6d 6d\n[36361] 6f 6e 22 3a 22 48 72 76 61 74 73 6b 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[36385] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 65 79 63 68 65 6c 6c 65 73\n[36409] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f\n[36433] 66 20 53 65 79 63 68 65 6c 6c 65 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[36457] 65 22 3a 7b 22 63 72 73 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[36481] 65 70 69 62 6c 69 6b 20 53 65 73 65 6c 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[36505] 22 53 65 73 65 6c 22 7d 2c 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61\n[36529] 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 53 65 79 63 68 65 6c 6c\n[36553] 65 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 65 79 63 68 65 6c 6c 65 73\n[36577] 22 7d 2c 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3\n[36601] a9 70 75 62 6c 69 71 75 65 20 64 65 73 20 53 65 79 63 68 65 6c 6c 65 73\n[36625] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 65 79 63 68 65 6c 6c 65 73 22 7d\n[36649] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53\n[36673] 6f 75 74 68 20 41 66 72 69 63 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a\n[36697] 22 52 65 70 75 62 6c 69 63 20 6f 66 20 53 6f 75 74 68 20 41 66 72 69 63\n[36721] 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 66 72 22 3a 7b\n[36745] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 65 6b 20 76 61\n[36769] 6e 20 53 75 69 64 2d 41 66 72 69 6b 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[36793] 22 53 6f 75 74 68 20 41 66 72 69 63 61 22 7d 2c 22 65 6e 67 22 3a 7b 22\n[36817] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 53\n[36841] 6f 75 74 68 20 41 66 72 69 63 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53\n[36865] 6f 75 74 68 20 41 66 72 69 63 61 22 7d 2c 22 6e 62 6c 22 3a 7b 22 6f 66\n[36889] 66 69 63 69 61 6c 22 3a 22 49 52 69 70 68 61 62 6c 69 6b 69 20 79 65 53\n[36913] 65 77 75 6c 61 20 41 66 72 69 6b 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[36937] 53 65 77 75 6c 61 20 41 66 72 69 6b 61 22 7d 2c 22 6e 73 6f 22 3a 7b 22\n[36961] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 68 61 62 6f 6c 69 6b 69 20 79\n[36985] 61 20 41 66 72 69 6b 61 2d 42 6f 72 77 61 20 22 2c 22 63 6f 6d 6d 6f 6e\n[37009] 22 3a 22 41 66 72 69 6b 61 2d 42 6f 72 77 61 22 7d 2c 22 73 6f 74 22 3a\n[37033] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 68 61 62 6f 6c 69 6b 69\n[37057] 20 79 61 20 41 66 72 69 6b 61 20 42 6f 72 77 61 22 2c 22 63 6f 6d 6d 6f\n[37081] 6e 22 3a 22 41 66 72 69 6b 61 20 42 6f 72 77 61 22 7d 2c 22 73 73 77 22\n[37105] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 49 52 69 70 68 61 62 68 75 6c\n[37129] 69 6b 68 69 20 79 65 4e 69 6e 67 69 7a 69 6d 75 20 41 66 72 69 6b 61 22\n[37153] 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 69 6e 67 69 7a 69 6d 75 20 41 66 72\n[37177] 69 6b 61 22 7d 2c 22 74 73 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[37201] 22 52 65 70 68 61 62 6f 6c 69 6b 69 20 79 61 20 41 66 6f 72 69 6b 61 20\n[37225] 42 6f 72 77 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 41 66 6f 72 69 6b 61\n[37249] 20 42 6f 72 77 61 22 7d 2c 22 74 73 6f 22 3a 7b 22 6f 66 66 69 63 69 61\n[37273] 6c 22 3a 22 52 69 70 68 61 62 6c 69 6b 69 20 72 61 20 41 66 72 69 6b 61\n[37297] 20 44 7a 6f 6e 67 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 41 66 72 69 6b\n[37321] 61 20 44 7a 6f 6e 67 61 22 7d 2c 22 76 65 6e 22 3a 7b 22 6f 66 66 69 63\n[37345] 69 61 6c 22 3a 22 52 69 70 68 61 62 75 e1 b8 bd 69 6b 69 20 79 61 20 41\n[37369] 66 75 72 69 6b 61 20 54 73 68 69 70 65 6d 62 65 22 2c 22 63 6f 6d 6d 6f\n[37393] 6e 22 3a 22 41 66 75 72 69 6b 61 20 54 73 68 69 70 65 6d 62 65 22 7d 2c\n[37417] 22 78 68 6f 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 49 52 69 70 68\n[37441] 61 62 6c 69 6b 69 20 79 61 73 65 4d 7a 61 6e 74 73 69 20 41 66 72 69 6b\n[37465] 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 7a 61 6e 74 73 69 20 41 66 72\n[37489] 69 6b 61 22 7d 2c 22 7a 75 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[37513] 22 49 52 69 70 68 61 62 6c 69 6b 69 20 79 61 73 65 4e 69 6e 67 69 7a 69\n[37537] 6d 75 20 41 66 72 69 6b 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 69 6e\n[37561] 67 69 7a 69 6d 75 20 41 66 72 69 6b 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[37585] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 6f 6f 6b 20 49 73 6c 61 6e\n[37609] 64 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6f 6b 20 49 73 6c\n[37633] 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67\n[37657] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6f 6b 20 49 73 6c 61\n[37681] 6e 64 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 6f 6f 6b 20 49 73 6c 61\n[37705] 6e 64 73 22 7d 2c 22 72 61 72 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[37729] 22 4b c5 ab 6b 69 20 27 c4 80 69 72 61 6e 69 22 2c 22 63 6f 6d 6d 6f 6e\n[37753] 22 3a 22 4b c5 ab 6b 69 20 27 c4 80 69 72 61 6e 69 22 7d 7d 7d 7d 2c 7b\n[37777] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 70 61 69 6e 22\n[37801] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20\n[37825] 53 70 61 69 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 73 70\n[37849] 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 69 6e 6f 20 64 65\n[37873] 20 45 73 70 61 c3 b1 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 45 73 70 61\n[37897] c3 b1 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f\n[37921] 6e 22 3a 22 47 72 65 65 6e 6c 61 6e 64 22 2c 22 6f 66 66 69 63 69 61 6c\n[37945] 22 3a 22 47 72 65 65 6e 6c 61 6e 64 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[37969] 65 22 3a 7b 22 6b 61 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b\n[37993] 61 6c 61 61 6c 6c 69 74 20 4e 75 6e 61 61 74 22 2c 22 63 6f 6d 6d 6f 6e\n[38017] 22 3a 22 4b 61 6c 61 61 6c 6c 69 74 20 4e 75 6e 61 61 74 22 7d 7d 7d 7d\n[38041] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 50 61 70 75\n[38065] 61 20 4e 65 77 20 47 75 69 6e 65 61 22 2c 22 6f 66 66 69 63 69 61 6c 22\n[38089] 3a 22 49 6e 64 65 70 65 6e 64 65 6e 74 20 53 74 61 74 65 20 6f 66 20 50\n[38113] 61 70 75 61 20 4e 65 77 20 47 75 69 6e 65 61 22 2c 22 6e 61 74 69 76 65\n[38137] 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22\n[38161] 3a 22 49 6e 64 65 70 65 6e 64 65 6e 74 20 53 74 61 74 65 20 6f 66 20 50\n[38185] 61 70 75 61 20 4e 65 77 20 47 75 69 6e 65 61 22 2c 22 63 6f 6d 6d 6f 6e\n[38209] 22 3a 22 50 61 70 75 61 20 4e 65 77 20 47 75 69 6e 65 61 22 7d 2c 22 68\n[38233] 6d 6f 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 49 6e 64 65 70 65 6e\n[38257] 64 65 6e 20 53 74 65 74 20 62 69 6c 6f 6e 67 20 50 61 70 75 61 20 4e 69\n[38281] 75 67 69 6e 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 50 61 70 75 61 20 4e\n[38305] 69 75 20 47 69 6e 69 22 7d 2c 22 74 70 69 22 3a 7b 22 6f 66 66 69 63 69\n[38329] 61 6c 22 3a 22 49 6e 64 65 70 65 6e 64 65 6e 20 53 74 65 74 20 62 69 6c\n[38353] 6f 6e 67 20 50 61 70 75 61 20 4e 69 75 67 69 6e 69 22 2c 22 63 6f 6d 6d\n[38377] 6f 6e 22 3a 22 50 61 70 75 61 20 4e 69 75 67 69 6e 69 22 7d 7d 7d 7d 2c\n[38401] 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 41 6e 74 69 67\n[38425] 75 61 20 61 6e 64 20 42 61 72 62 75 64 61 22 2c 22 6f 66 66 69 63 69 61\n[38449] 6c 22 3a 22 41 6e 74 69 67 75 61 20 61 6e 64 20 42 61 72 62 75 64 61 22\n[38473] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f\n[38497] 66 66 69 63 69 61 6c 22 3a 22 41 6e 74 69 67 75 61 20 61 6e 64 20 42 61\n[38521] 72 62 75 64 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 41 6e 74 69 67 75 61\n[38545] 20 61 6e 64 20 42 61 72 62 75 64 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n[38569] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 6f 72 77 61 79 22 2c 22 6f 66\n[38593] 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20 4e 6f 72 77\n[38617] 61 79 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6e 6e 6f 22 3a\n[38641] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 6f 6e 67 65 72 69 6b 65 74 20\n[38665] 4e 6f 72 65 67 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 6f 72 65 67 22 7d\n[38689] 2c 22 6e 6f 62 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 6f 6e 67\n[38713] 65 72 69 6b 65 74 20 4e 6f 72 67 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[38737] 4e 6f 72 67 65 22 7d 2c 22 73 6d 69 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[38761] 22 3a 22 4e 6f 72 67 67 61 20 67 6f 6e 61 67 61 73 72 69 69 6b 61 22 2c\n[38785] 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 6f 72 67 67 61 22 7d 7d 7d 7d 2c 7b 22\n[38809] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 77 69 74 7a 65 72\n[38833] 6c 61 6e 64 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 77 69 73 73 20\n[38857] 43 6f 6e 66 65 64 65 72 61 74 69 6f 6e 22 2c 22 6e 61 74 69 76 65 4e 61\n[38881] 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[38905] 43 6f 6e 66 c3 a9 64 c3 a9 72 61 74 69 6f 6e 20 73 75 69 73 73 65 22 2c\n[38929] 22 63 6f 6d 6d 6f 6e 22 3a 22 53 75 69 73 73 65 22 7d 2c 22 67 73 77 22\n[38953] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 63 68 77 65 69 7a 65 72 69\n[38977] 73 63 68 65 20 45 69 64 67 65 6e 6f 73 73 65 6e 73 63 68 61 66 74 22 2c\n[39001] 22 63 6f 6d 6d 6f 6e 22 3a 22 53 63 68 77 65 69 7a 22 7d 2c 22 69 74 61\n[39025] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6e 66 65 64 65 72 61\n[39049] 7a 69 6f 6e 65 20 53 76 69 7a 7a 65 72 61 22 2c 22 63 6f 6d 6d 6f 6e 22\n[39073] 3a 22 53 76 69 7a 7a 65 72 61 22 7d 2c 22 72 6f 68 22 3a 7b 22 6f 66 66\n[39097] 69 63 69 61 6c 22 3a 22 43 6f 6e 66 65 64 65 72 61 7a 69 75 6e 20 73 76\n[39121] 69 7a 72 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 76 69 7a 72 61 22 7d\n[39145] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 5a\n[39169] 69 6d 62 61 62 77 65 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n[39193] 75 62 6c 69 63 20 6f 66 20 5a 69 6d 62 61 62 77 65 22 2c 22 6e 61 74 69\n[39217] 76 65 4e 61 6d 65 22 3a 7b 22 62 77 67 22 3a 7b 22 6f 66 66 69 63 69 61\n[39241] 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 5a 69 6d 62 61 62 77 65\n[39265] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 69 6d 62 61 62 77 65 22 7d 2c 22\n[39289] 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c\n[39313] 69 63 20 6f 66 20 5a 69 6d 62 61 62 77 65 22 2c 22 63 6f 6d 6d 6f 6e 22\n[39337] 3a 22 5a 69 6d 62 61 62 77 65 22 7d 2c 22 6b 63 6b 22 3a 7b 22 6f 66 66\n[39361] 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 5a 69 6d 62\n[39385] 61 62 77 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 69 6d 62 61 62 77 65\n[39409] 22 7d 2c 22 6b 68 69 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65\n[39433] 70 75 62 6c 69 63 20 6f 66 20 5a 69 6d 62 61 62 77 65 22 2c 22 63 6f 6d\n[39457] 6d 6f 6e 22 3a 22 5a 69 6d 62 61 62 77 65 22 7d 2c 22 6e 64 63 22 3a 7b\n[39481] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20\n[39505] 5a 69 6d 62 61 62 77 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 69 6d 62\n[39529] 61 62 77 65 22 7d 2c 22 6e 64 65 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22\n[39553] 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 5a 69 6d 62 61 62 77 65 22 2c\n[39577] 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 69 6d 62 61 62 77 65 22 7d 2c 22 6e 79\n[39601] 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63\n[39625] 20 6f 66 20 5a 69 6d 62 61 62 77 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[39649] 5a 69 6d 62 61 62 77 65 22 7d 2c 22 73 6e 61 22 3a 7b 22 6f 66 66 69 63\n[39673] 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 5a 69 6d 62 61 62\n[39697] 77 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 69 6d 62 61 62 77 65 22 7d\n[39721] 2c 22 73 6f 74 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75\n[39745] 62 6c 69 63 20 6f 66 20 5a 69 6d 62 61 62 77 65 22 2c 22 63 6f 6d 6d 6f\n[39769] 6e 22 3a 22 5a 69 6d 62 61 62 77 65 22 7d 2c 22 74 6f 69 22 3a 7b 22 6f\n[39793] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 5a 69\n[39817] 6d 62 61 62 77 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 69 6d 62 61 62\n[39841] 77 65 22 7d 2c 22 74 73 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[39865] 52 65 70 75 62 6c 69 63 20 6f 66 20 5a 69 6d 62 61 62 77 65 22 2c 22 63\n[39889] 6f 6d 6d 6f 6e 22 3a 22 5a 69 6d 62 61 62 77 65 22 7d 2c 22 74 73 6f 22\n[39913] 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f\n[39937] 66 20 5a 69 6d 62 61 62 77 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 69\n[39961] 6d 62 61 62 77 65 22 7d 2c 22 76 65 6e 22 3a 7b 22 6f 66 66 69 63 69 61\n[39985] 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 5a 69 6d 62 61 62 77 65\n[40009] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 69 6d 62 61 62 77 65 22 7d 2c 22\n[40033] 78 68 6f 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c\n[40057] 69 63 20 6f 66 20 5a 69 6d 62 61 62 77 65 22 2c 22 63 6f 6d 6d 6f 6e 22\n[40081] 3a 22 5a 69 6d 62 61 62 77 65 22 7d 2c 22 7a 69 62 22 3a 7b 22 6f 66 66\n[40105] 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 5a 69 6d 62\n[40129] 61 62 77 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 5a 69 6d 62 61 62 77 65\n[40153] 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a\n[40177] 22 53 6c 6f 76 65 6e 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[40201] 65 70 75 62 6c 69 63 20 6f 66 20 53 6c 6f 76 65 6e 69 61 22 2c 22 6e 61\n[40225] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 73 6c 76 22 3a 7b 22 6f 66 66 69 63\n[40249] 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 6b 61 20 53 6c 6f 76 65 6e 69 6a\n[40273] 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 6c 6f 76 65 6e 69 6a 61 22 7d\n[40297] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 47\n[40321] 61 62 6f 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 47 61 62 6f 6e 65\n[40345] 73 65 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[40369] 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3\n[40393] a9 70 75 62 6c 69 71 75 65 20 67 61 62 6f 6e 61 69 73 65 22 2c 22 63 6f\n[40417] 6d 6d 6f 6e 22 3a 22 47 61 62 6f 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n[40441] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 46 61 72 6f 65 20 49 73 6c 61 6e\n[40465] 64 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 61 72 6f 65 20 49 73\n[40489] 6c 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 64 61\n[40513] 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 c3 a6 72 c3 b8 65 72\n[40537] 6e 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 46 c3 a6 72 c3 b8 65 72 6e 65\n[40561] 22 7d 2c 22 66 61 6f 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 c3\n[40585] b8 72 6f 79 61 72 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 46 c3 b8 72 6f 79\n[40609] 61 72 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n[40633] 22 3a 22 4d 6f 72 6f 63 63 6f 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[40657] 4b 69 6e 67 64 6f 6d 20 6f 66 20 4d 6f 72 6f 63 63 6f 22 2c 22 6e 61 74\n[40681] 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63 69\n[40705] 61 6c 22 3a 22 d8 a7 d9 84 d9 85 d9 85 d9 84 d9 83 d8 a9 20 d8 a7 d9 84\n[40729] d9 85 d8 ba d8 b1 d8 a8 d9 8a d8 a9 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[40753] d8 a7 d9 84 d9 85 d8 ba d8 b1 d8 a8 22 7d 2c 22 62 65 72 22 3a 7b 22 6f\n[40777] 66 66 69 63 69 61 6c 22 3a 22 e2 b5 9c e2 b4 b0 e2 b4 b3 e2 b5 8d e2 b4\n[40801] b7 e2 b5 89 e2 b5 9c 20 e2 b5 8f 20 e2 b5 8d e2 b5 8e e2 b5 96 e2 b5 94\n[40825] e2 b5 89 e2 b4 b1 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e2 b5 8d e2 b5 8e\n[40849] e2 b4 b0 e2 b5 96 e2 b5 94 e2 b5 89 e2 b4 b1 22 7d 7d 7d 7d 2c 7b 22 6e\n[40873] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 6f 6e 61 63 6f 22 2c\n[40897] 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 72 69 6e 63 69 70 61 6c 69 74 79\n[40921] 20 6f 66 20 4d 6f 6e 61 63 6f 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22\n[40945] 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 72 69\n[40969] 6e 63 69 70 61 75 74 c3 a9 20 64 65 20 4d 6f 6e 61 63 6f 22 2c 22 63 6f\n[40993] 6d 6d 6f 6e 22 3a 22 4d 6f 6e 61 63 6f 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[41017] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4c 65 73 6f 74 68 6f 22 2c 22\n[41041] 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20 4c 65\n[41065] 73 6f 74 68 6f 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e\n[41089] 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 4b 69 6e 67 64 6f 6d 20\n[41113] 6f 66 20 4c 65 73 6f 74 68 6f 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4c 65\n[41137] 73 6f 74 68 6f 22 7d 2c 22 73 6f 74 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[41161] 22 3a 22 4b 69 6e 67 64 6f 6d 20 6f 66 20 4c 65 73 6f 74 68 6f 22 2c 22\n[41185] 63 6f 6d 6d 6f 6e 22 3a 22 4c 65 73 6f 74 68 6f 22 7d 7d 7d 7d 2c 7b 22\n[41209] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 57 61 6c 6c 69 73 20\n[41233] 61 6e 64 20 46 75 74 75 6e 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[41257] 54 65 72 72 69 74 6f 72 79 20 6f 66 20 74 68 65 20 57 61 6c 6c 69 73 20\n[41281] 61 6e 64 20 46 75 74 75 6e 61 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74\n[41305] 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69\n[41329] 61 6c 22 3a 22 54 65 72 72 69 74 6f 69 72 65 20 64 65 73 20 c3 ae 6c 65\n[41353] 73 20 57 61 6c 6c 69 73 20 65 74 20 46 75 74 75 6e 61 22 2c 22 63 6f 6d\n[41377] 6d 6f 6e 22 3a 22 57 61 6c 6c 69 73 20 65 74 20 46 75 74 75 6e 61 22 7d\n[41401] 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 41\n[41425] 72 75 62 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 41 72 75 62 61 22\n[41449] 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 6e 6c 64 22 3a 7b 22 6f\n[41473] 66 66 69 63 69 61 6c 22 3a 22 41 72 75 62 61 22 2c 22 63 6f 6d 6d 6f 6e\n[41497] 22 3a 22 41 72 75 62 61 22 7d 2c 22 70 61 70 22 3a 7b 22 6f 66 66 69 63\n[41521] 69 61 6c 22 3a 22 41 72 75 62 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 41\n[41545] 72 75 62 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d\n[41569] 6f 6e 22 3a 22 42 75 72 6b 69 6e 61 20 46 61 73 6f 22 2c 22 6f 66 66 69\n[41593] 63 69 61 6c 22 3a 22 42 75 72 6b 69 6e 61 20 46 61 73 6f 22 2c 22 6e 61\n[41617] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63\n[41641] 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 75 20 42 75 72\n[41665] 6b 69 6e 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 75 72 6b 69 6e 61 20\n[41689] 46 61 73 6f 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d\n[41713] 6f 6e 22 3a 22 42 75 72 75 6e 64 69 22 2c 22 6f 66 66 69 63 69 61 6c 22\n[41737] 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 42 75 72 75 6e 64 69 22 2c 22\n[41761] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66\n[41785] 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 75 20 42\n[41809] 75 72 75 6e 64 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 75 72 75 6e 64\n[41833] 69 22 7d 2c 22 72 75 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52\n[41857] 65 70 75 62 6c 69 6b 61 20 79 27 55 62 75 72 75 6e 64 69 20 22 2c 22 63\n[41881] 6f 6d 6d 6f 6e 22 3a 22 55 62 75 72 75 6e 64 69 22 7d 7d 7d 7d 2c 7b 22\n[41905] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 46 69 6e 6c 61 6e 64\n[41929] 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f\n[41953] 66 20 46 69 6e 6c 61 6e 64 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a\n[41977] 7b 22 66 69 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 75 6f 6d\n[42001] 65 6e 20 74 61 73 61 76 61 6c 74 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[42025] 53 75 6f 6d 69 22 7d 2c 22 73 77 65 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[42049] 22 3a 22 52 65 70 75 62 6c 69 6b 65 6e 20 46 69 6e 6c 61 6e 64 22 2c 22\n[42073] 63 6f 6d 6d 6f 6e 22 3a 22 46 69 6e 6c 61 6e 64 22 7d 7d 7d 7d 2c 7b 22\n[42097] 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 54 61 6a 69 6b 69 73\n[42121] 74 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69\n[42145] 63 20 6f 66 20 54 61 6a 69 6b 69 73 74 61 6e 22 2c 22 6e 61 74 69 76 65\n[42169] 4e 61 6d 65 22 3a 7b 22 72 75 73 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22\n[42193] 3a 22 d0 a0 d0 b5 d1 81 d0 bf d1 83 d0 b1 d0 bb d0 b8 d0 ba d0 b0 20 d0\n[42217] a2 d0 b0 d0 b4 d0 b6 d0 b8 d0 ba d0 b8 d1 81 d1 82 d0 b0 d0 bd 22 2c 22\n[42241] 63 6f 6d 6d 6f 6e 22 3a 22 d0 a2 d0 b0 d0 b4 d0 b6 d0 b8 d0 ba d0 b8 d1\n[42265] 81 d1 82 d0 b0 d0 bd 22 7d 2c 22 74 67 6b 22 3a 7b 22 6f 66 66 69 63 69\n[42289] 61 6c 22 3a 22 d2 b6 d1 83 d0 bc d2 b3 d1 83 d1 80 d0 b8 d0 b8 20 d0 a2\n[42313] d0 be d2 b7 d0 b8 d0 ba d0 b8 d1 81 d1 82 d0 be d0 bd 22 2c 22 63 6f 6d\n[42337] 6d 6f 6e 22 3a 22 d0 a2 d0 be d2 b7 d0 b8 d0 ba d0 b8 d1 81 d1 82 d0 be\n[42361] d0 bd 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n[42385] 22 3a 22 45 63 75 61 64 6f 72 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22\n[42409] 52 65 70 75 62 6c 69 63 20 6f 66 20 45 63 75 61 64 6f 72 22 2c 22 6e 61\n[42433] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 73 70 61 22 3a 7b 22 6f 66 66 69 63\n[42457] 69 61 6c 22 3a 22 52 65 70 c3 ba 62 6c 69 63 61 20 64 65 6c 20 45 63 75\n[42481] 61 64 6f 72 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 45 63 75 61 64 6f 72 22\n[42505] 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22\n[42529] 49 74 61 6c 79 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 49 74 61 6c 69\n[42553] 61 6e 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[42577] 22 3a 7b 22 69 74 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65\n[42601] 70 75 62 62 6c 69 63 61 20 69 74 61 6c 69 61 6e 61 22 2c 22 63 6f 6d 6d\n[42625] 6f 6e 22 3a 22 49 74 61 6c 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n[42649] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 79 61 6e 6d 61 72 22 2c 22 6f 66\n[42673] 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 74 68 65\n[42697] 20 55 6e 69 6f 6e 20 6f 66 20 4d 79 61 6e 6d 61 72 22 2c 22 6e 61 74 69\n[42721] 76 65 4e 61 6d 65 22 3a 7b 22 6d 79 61 22 3a 7b 22 6f 66 66 69 63 69 61\n[42745] 6c 22 3a 22 e1 80 95 e1 80 bc e1 80 8a e1 80 ba e1 80 91 e1 80 b1 e1 80\n[42769] ac e1 80 84 e1 80 ba e1 80 85 e1 80 af 20 e1 80 9e e1 80 99 e1 80 b9 e1\n[42793] 80 99 e1 80 90 20 e1 80 99 e1 80 bc e1 80 94 e1 80 ba e1 80 99 e1 80 ac\n[42817] e1 80 94 e1 80 ad e1 80 af e1 80 84 e1 80 ba e1 80 84 e1 80 b6 e1 80 90\n[42841] e1 80 b1 e1 80 ac e1 80 ba 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e1 80 99\n[42865] e1 80 bc e1 80 94 e1 80 ba e1 80 99 e1 80 ac 22 7d 7d 7d 7d 2c 7b 22 6e\n[42889] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4a 61 70 61 6e 22 2c 22\n[42913] 6f 66 66 69 63 69 61 6c 22 3a 22 4a 61 70 61 6e 22 2c 22 6e 61 74 69 76\n[42937] 65 4e 61 6d 65 22 3a 7b 22 6a 70 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[42961] 22 3a 22 e6 97 a5 e6 9c ac 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 e6 97 a5\n[42985] e6 9c ac 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f\n[43009] 6e 22 3a 22 53 6f 75 74 68 20 53 75 64 61 6e 22 2c 22 6f 66 66 69 63 69\n[43033] 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 53 6f 75 74 68 20 53\n[43057] 75 64 61 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67\n[43081] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20\n[43105] 6f 66 20 53 6f 75 74 68 20 53 75 64 61 6e 22 2c 22 63 6f 6d 6d 6f 6e 22\n[43129] 3a 22 53 6f 75 74 68 20 53 75 64 61 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[43153] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4c 69 62 65 72 69 61 22 2c 22\n[43177] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4c\n[43201] 69 62 65 72 69 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65\n[43225] 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69\n[43249] 63 20 6f 66 20 4c 69 62 65 72 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[43273] 4c 69 62 65 72 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n[43297] 6f 6d 6d 6f 6e 22 3a 22 53 6f 75 74 68 20 47 65 6f 72 67 69 61 22 2c 22\n[43321] 6f 66 66 69 63 69 61 6c 22 3a 22 53 6f 75 74 68 20 47 65 6f 72 67 69 61\n[43345] 20 61 6e 64 20 74 68 65 20 53 6f 75 74 68 20 53 61 6e 64 77 69 63 68 20\n[43369] 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n[43393] 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 6f 75 74 68 20\n[43417] 47 65 6f 72 67 69 61 20 61 6e 64 20 74 68 65 20 53 6f 75 74 68 20 53 61\n[43441] 6e 64 77 69 63 68 20 49 73 6c 61 6e 64 73 22 2c 22 63 6f 6d 6d 6f 6e 22\n[43465] 3a 22 53 6f 75 74 68 20 47 65 6f 72 67 69 61 22 7d 7d 7d 7d 2c 7b 22 6e\n[43489] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42 6f 6c 69 76 69 61 22\n[43513] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 50 6c 75 72 69 6e 61 74 69 6f 6e\n[43537] 61 6c 20 53 74 61 74 65 20 6f 66 20 42 6f 6c 69 76 69 61 22 2c 22 6e 61\n[43561] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 79 6d 22 3a 7b 22 6f 66 66 69 63\n[43585] 69 61 6c 22 3a 22 57 75 6c 69 77 79 61 20 53 75 79 75 22 2c 22 63 6f 6d\n[43609] 6d 6f 6e 22 3a 22 57 75 6c 69 77 79 61 22 7d 2c 22 67 72 6e 22 3a 7b 22\n[43633] 6f 66 66 69 63 69 61 6c 22 3a 22 54 65 74 c3 a3 20 56 6f 6c c3 ad 76 69\n[43657] 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 56 6f 6c c3 ad 76 69 61 22 7d 2c\n[43681] 22 71 75 65 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 42 75 6c 69 77\n[43705] 79 61 20 4d 61 6d 61 6c 6c 61 71 74 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[43729] 22 42 75 6c 69 77 79 61 22 7d 2c 22 73 70 61 22 3a 7b 22 6f 66 66 69 63\n[43753] 69 61 6c 22 3a 22 45 73 74 61 64 6f 20 50 6c 75 72 69 6e 61 63 69 6f 6e\n[43777] 61 6c 20 64 65 20 42 6f 6c 69 76 69 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[43801] 22 42 6f 6c 69 76 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22\n[43825] 63 6f 6d 6d 6f 6e 22 3a 22 53 61 6d 6f 61 22 2c 22 6f 66 66 69 63 69 61\n[43849] 6c 22 3a 22 49 6e 64 65 70 65 6e 64 65 6e 74 20 53 74 61 74 65 20 6f 66\n[43873] 20 53 61 6d 6f 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65\n[43897] 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 49 6e 64 65 70 65 6e\n[43921] 64 65 6e 74 20 53 74 61 74 65 20 6f 66 20 53 61 6d 6f 61 22 2c 22 63 6f\n[43945] 6d 6d 6f 6e 22 3a 22 53 61 6d 6f 61 22 7d 2c 22 73 6d 6f 22 3a 7b 22 6f\n[43969] 66 66 69 63 69 61 6c 22 3a 22 4d 61 6c 6f 20 53 61 ca bb 6f 6c 6f 74 6f\n[43993] 20 54 75 74 6f ca bb 61 74 61 73 69 20 6f 20 53 c4 81 6d 6f 61 22 2c 22\n[44017] 63 6f 6d 6d 6f 6e 22 3a 22 53 c4 81 6d 6f 61 22 7d 7d 7d 7d 2c 7b 22 6e\n[44041] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 56 69 65 74 6e 61 6d 22\n[44065] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 6f 63 69 61 6c 69 73 74 20 52\n[44089] 65 70 75 62 6c 69 63 20 6f 66 20 56 69 65 74 6e 61 6d 22 2c 22 6e 61 74\n[44113] 69 76 65 4e 61 6d 65 22 3a 7b 22 76 69 65 22 3a 7b 22 6f 66 66 69 63 69\n[44137] 61 6c 22 3a 22 43 e1 bb 99 6e 67 20 68 c3 b2 61 20 78 c3 a3 20 68 e1 bb\n[44161] 99 69 20 63 68 e1 bb a7 20 6e 67 68 c4 a9 61 20 56 69 e1 bb 87 74 20 4e\n[44185] 61 6d 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 56 69 e1 bb 87 74 20 4e 61 6d\n[44209] 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a\n[44233] 22 52 65 70 75 62 6c 69 63 20 6f 66 20 74 68 65 20 43 6f 6e 67 6f 22 2c\n[44257] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20\n[44281] 74 68 65 20 43 6f 6e 67 6f 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a\n[44305] 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70\n[44329] 75 62 6c 69 71 75 65 20 64 75 20 43 6f 6e 67 6f 22 2c 22 63 6f 6d 6d 6f\n[44353] 6e 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 75 20 43 6f 6e 67 6f\n[44377] 22 7d 2c 22 6b 6f 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65\n[44401] 70 75 62 69 6c 69 6b 61 20 79 61 20 4b 6f 6e 67 6f 22 2c 22 63 6f 6d 6d\n[44425] 6f 6e 22 3a 22 52 65 70 75 62 69 6c 69 6b 61 20 79 61 20 4b 6f 6e 67 6f\n[44449] 22 7d 2c 22 6c 69 6e 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65\n[44473] 70 75 62 6c c3 ad 6b 69 20 79 61 20 4b 6f 6e 67 c3 b3 22 2c 22 63 6f 6d\n[44497] 6d 6f 6e 22 3a 22 52 65 70 75 62 6c c3 ad 6b 69 20 79 61 20 4b 6f 6e 67\n[44521] c3 b3 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e\n[44545] 22 3a 22 49 72 61 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 49 73 6c\n[44569] 61 6d 69 63 20 52 65 70 75 62 6c 69 63 20 6f 66 20 49 72 61 6e 22 2c 22\n[44593] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 61 73 22 3a 7b 22 6f 66 66\n[44617] 69 63 69 61 6c 22 3a 22 d8 ac d9 85 d9 87 d9 88 d8 b1 db 8c 20 d8 a7 d8\n[44641] b3 d9 84 d8 a7 d9 85 db 8c 20 d8 a7 db 8c d8 b1 d8 a7 d9 86 22 2c 22 63\n[44665] 6f 6d 6d 6f 6e 22 3a 22 d8 a7 db 8c d8 b1 d8 a7 d9 86 22 7d 7d 7d 7d 2c\n[44689] 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 46 72 65 6e 63\n[44713] 68 20 53 6f 75 74 68 65 72 6e 20 61 6e 64 20 41 6e 74 61 72 63 74 69 63\n[44737] 20 4c 61 6e 64 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 54 65 72 72\n[44761] 69 74 6f 72 79 20 6f 66 20 74 68 65 20 46 72 65 6e 63 68 20 53 6f 75 74\n[44785] 68 65 72 6e 20 61 6e 64 20 41 6e 74 61 72 63 74 69 63 20 4c 61 6e 64 73\n[44809] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22\n[44833] 6f 66 66 69 63 69 61 6c 22 3a 22 54 65 72 72 69 74 6f 69 72 65 20 64 65\n[44857] 73 20 54 65 72 72 65 73 20 61 75 73 74 72 61 6c 65 73 20 65 74 20 61 6e\n[44881] 74 61 72 63 74 69 71 75 65 73 20 66 72 61 6e c3 a7 61 69 73 65 73 22 2c\n[44905] 22 63 6f 6d 6d 6f 6e 22 3a 22 54 65 72 72 65 73 20 61 75 73 74 72 61 6c\n[44929] 65 73 20 65 74 20 61 6e 74 61 72 63 74 69 71 75 65 73 20 66 72 61 6e c3\n[44953] a7 61 69 73 65 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f\n[44977] 6d 6d 6f 6e 22 3a 22 52 c3 a9 75 6e 69 6f 6e 22 2c 22 6f 66 66 69 63 69\n[45001] 61 6c 22 3a 22 52 c3 a9 75 6e 69 6f 6e 20 49 73 6c 61 6e 64 22 2c 22 6e\n[45025] 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66 69\n[45049] 63 69 61 6c 22 3a 22 49 6c 65 20 64 65 20 6c 61 20 52 c3 a9 75 6e 69 6f\n[45073] 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4c 61 20 52 c3 a9 75 6e 69 6f 6e\n[45097] 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a\n[45121] 22 4c 65 62 61 6e 6f 6e 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 4c 65\n[45145] 62 61 6e 65 73 65 20 52 65 70 75 62 6c 69 63 22 2c 22 6e 61 74 69 76 65\n[45169] 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22\n[45193] 3a 22 d8 a7 d9 84 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8 a9 20 d8 a7 d9\n[45217] 84 d9 84 d8 a8 d9 86 d8 a7 d9 86 d9 8a d8 a9 22 2c 22 63 6f 6d 6d 6f 6e\n[45241] 22 3a 22 d9 84 d8 a8 d9 86 d8 a7 d9 86 22 7d 2c 22 66 72 61 22 3a 7b 22\n[45265] 6f 66 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 6c\n[45289] 69 62 61 6e 61 69 73 65 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4c 69 62 61\n[45313] 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[45337] 3a 22 41 6d 65 72 69 63 61 6e 20 53 61 6d 6f 61 22 2c 22 6f 66 66 69 63\n[45361] 69 61 6c 22 3a 22 41 6d 65 72 69 63 61 6e 20 53 61 6d 6f 61 22 2c 22 6e\n[45385] 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69\n[45409] 63 69 61 6c 22 3a 22 41 6d 65 72 69 63 61 6e 20 53 61 6d 6f 61 22 2c 22\n[45433] 63 6f 6d 6d 6f 6e 22 3a 22 41 6d 65 72 69 63 61 6e 20 53 61 6d 6f 61 22\n[45457] 7d 2c 22 73 6d 6f 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 c4 81\n[45481] 6d 6f 61 20 41 6d 65 6c 69 6b 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53\n[45505] c4 81 6d 6f 61 20 41 6d 65 6c 69 6b 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[45529] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 43 61 6d 65 72 6f 6f 6e 22 2c\n[45553] 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20\n[45577] 43 61 6d 65 72 6f 6f 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b\n[45601] 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62\n[45625] 6c 69 63 20 6f 66 20 43 61 6d 65 72 6f 6f 6e 22 2c 22 63 6f 6d 6d 6f 6e\n[45649] 22 3a 22 43 61 6d 65 72 6f 6f 6e 22 7d 2c 22 66 72 61 22 3a 7b 22 6f 66\n[45673] 66 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 75 20\n[45697] 43 61 6d 65 72 6f 75 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 43 61 6d 65\n[45721] 72 6f 75 6e 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d\n[45745] 6f 6e 22 3a 22 53 65 6e 65 67 61 6c 22 2c 22 6f 66 66 69 63 69 61 6c 22\n[45769] 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 53 65 6e 65 67 61 6c 22 2c 22\n[45793] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 66 72 61 22 3a 7b 22 6f 66 66\n[45817] 69 63 69 61 6c 22 3a 22 52 c3 a9 70 75 62 6c 69 71 75 65 20 64 75 20 53\n[45841] c3 a9 6e c3 a9 67 61 6c 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 c3 a9 6e\n[45865] c3 a9 67 61 6c 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d\n[45889] 6d 6f 6e 22 3a 22 4d 61 63 61 75 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a\n[45913] 22 4d 61 63 61 6f 20 53 70 65 63 69 61 6c 20 41 64 6d 69 6e 69 73 74 72\n[45937] 61 74 69 76 65 20 52 65 67 69 6f 6e 20 6f 66 20 74 68 65 20 50 65 6f 70\n[45961] 6c 65 27 73 20 52 65 70 75 62 6c 69 63 20 6f 66 20 43 68 69 6e 61 22 2c\n[45985] 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 70 6f 72 22 3a 7b 22 6f 66\n[46009] 66 69 63 69 61 6c 22 3a 22 52 65 67 69 c3 a3 6f 20 41 64 6d 69 6e 69 73\n[46033] 74 72 61 74 69 76 61 20 45 73 70 65 63 69 61 6c 20 64 65 20 4d 61 63 61\n[46057] 75 20 64 61 20 52 65 70 c3 ba 62 6c 69 63 61 20 50 6f 70 75 6c 61 72 20\n[46081] 64 61 20 43 68 69 6e 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 63 61\n[46105] 75 22 7d 2c 22 7a 68 6f 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 e4\n[46129] b8 ad e5 8d 8e e4 ba ba e6 b0 91 e5 85 b1 e5 92 8c e5 9b bd e6 be b3 e9\n[46153] 97 a8 e7 89 b9 e5 88 ab e8 a1 8c e6 94 bf e5 8c ba 22 2c 22 63 6f 6d 6d\n[46177] 6f 6e 22 3a 22 e6 be b3 e9 97 a8 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n[46201] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 50 61 6c 65 73 74 69 6e 65 22 2c 22\n[46225] 6f 66 66 69 63 69 61 6c 22 3a 22 53 74 61 74 65 20 6f 66 20 50 61 6c 65\n[46249] 73 74 69 6e 65 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72\n[46273] 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 d8 af d9 88 d9 84 d8 a9\n[46297] 20 d9 81 d9 84 d8 b3 d8 b7 d9 8a d9 86 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[46321] 22 d9 81 d9 84 d8 b3 d8 b7 d9 8a d9 86 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[46345] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 61 75 72 75 22 2c 22 6f 66\n[46369] 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4e 61 75\n[46393] 72 75 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a\n[46417] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66\n[46441] 20 4e 61 75 72 75 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4e 61 75 72 75 22\n[46465] 7d 2c 22 6e 61 75 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70\n[46489] 75 62 6c 69 63 20 6f 66 20 4e 61 75 72 75 22 2c 22 63 6f 6d 6d 6f 6e 22\n[46513] 3a 22 4e 61 75 72 75 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63\n[46537] 6f 6d 6d 6f 6e 22 3a 22 53 61 6e 20 4d 61 72 69 6e 6f 22 2c 22 6f 66 66\n[46561] 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 53 61 6e 20\n[46585] 4d 61 72 69 6e 6f 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 69\n[46609] 74 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 62 6c\n[46633] 69 63 61 20 64 69 20 53 61 6e 20 4d 61 72 69 6e 6f 22 2c 22 63 6f 6d 6d\n[46657] 6f 6e 22 3a 22 53 61 6e 20 4d 61 72 69 6e 6f 22 7d 7d 7d 7d 2c 7b 22 6e\n[46681] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 6f 6c 6f 6d 6f 6e 20\n[46705] 49 73 6c 61 6e 64 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 6f 6c\n[46729] 6f 6d 6f 6e 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d\n[46753] 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53\n[46777] 6f 6c 6f 6d 6f 6e 20 49 73 6c 61 6e 64 73 22 2c 22 63 6f 6d 6d 6f 6e 22\n[46801] 3a 22 53 6f 6c 6f 6d 6f 6e 20 49 73 6c 61 6e 64 73 22 7d 7d 7d 7d 2c 7b\n[46825] 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 41 6e 67 75 69 6c\n[46849] 6c 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 41 6e 67 75 69 6c 6c 61\n[46873] 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22\n[46897] 6f 66 66 69 63 69 61 6c 22 3a 22 41 6e 67 75 69 6c 6c 61 22 2c 22 63 6f\n[46921] 6d 6d 6f 6e 22 3a 22 41 6e 67 75 69 6c 6c 61 22 7d 7d 7d 7d 2c 7b 22 6e\n[46945] 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 69 63 72 6f 6e 65 73\n[46969] 69 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 65 64 65 72 61 74 65\n[46993] 64 20 53 74 61 74 65 73 20 6f 66 20 4d 69 63 72 6f 6e 65 73 69 61 22 2c\n[47017] 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66\n[47041] 66 69 63 69 61 6c 22 3a 22 46 65 64 65 72 61 74 65 64 20 53 74 61 74 65\n[47065] 73 20 6f 66 20 4d 69 63 72 6f 6e 65 73 69 61 22 2c 22 63 6f 6d 6d 6f 6e\n[47089] 22 3a 22 4d 69 63 72 6f 6e 65 73 69 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[47113] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 6c 61 77 69 22 2c 22 6f\n[47137] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4d 61\n[47161] 6c 61 77 69 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67\n[47185] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20\n[47209] 6f 66 20 4d 61 6c 61 77 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d 61 6c\n[47233] 61 77 69 22 7d 2c 22 6e 79 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a\n[47257] 22 43 68 61 6c 6f 20 63 68 61 20 4d 61 6c 61 77 69 2c 20 44 7a 69 6b 6f\n[47281] 20 6c 61 20 4d 61 6c 61 c5 b5 69 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4d\n[47305] 61 6c 61 c5 b5 69 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f\n[47329] 6d 6d 6f 6e 22 3a 22 51 61 74 61 72 22 2c 22 6f 66 66 69 63 69 61 6c 22\n[47353] 3a 22 53 74 61 74 65 20 6f 66 20 51 61 74 61 72 22 2c 22 6e 61 74 69 76\n[47377] 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[47401] 22 3a 22 d8 af d9 88 d9 84 d8 a9 20 d9 82 d8 b7 d8 b1 22 2c 22 63 6f 6d\n[47425] 6d 6f 6e 22 3a 22 d9 82 d8 b7 d8 b1 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65\n[47449] 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42 6f 74 73 77 61 6e 61 22 2c 22\n[47473] 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 42\n[47497] 6f 74 73 77 61 6e 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22\n[47521] 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c\n[47545] 69 63 20 6f 66 20 42 6f 74 73 77 61 6e 61 22 2c 22 63 6f 6d 6d 6f 6e 22\n[47569] 3a 22 42 6f 74 73 77 61 6e 61 22 7d 2c 22 74 73 6e 22 3a 7b 22 6f 66 66\n[47593] 69 63 69 61 6c 22 3a 22 4c 65 66 61 74 73 68 65 20 6c 61 20 42 6f 74 73\n[47617] 77 61 6e 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42 6f 74 73 77 61 6e 61\n[47641] 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a\n[47665] 22 4e 6f 72 74 68 65 72 6e 20 4d 61 72 69 61 6e 61 20 49 73 6c 61 6e 64\n[47689] 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6d 6d 6f 6e 77 65 61\n[47713] 6c 74 68 20 6f 66 20 74 68 65 20 4e 6f 72 74 68 65 72 6e 20 4d 61 72 69\n[47737] 61 6e 61 20 49 73 6c 61 6e 64 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65\n[47761] 22 3a 7b 22 63 61 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f\n[47785] 6d 6d 6f 6e 77 65 61 6c 74 68 20 6f 66 20 74 68 65 20 4e 6f 72 74 68 65\n[47809] 72 6e 20 4d 61 72 69 61 6e 61 20 49 73 6c 61 6e 64 73 22 2c 22 63 6f 6d\n[47833] 6d 6f 6e 22 3a 22 4e 6f 72 74 68 65 72 6e 20 4d 61 72 69 61 6e 61 20 49\n[47857] 73 6c 61 6e 64 73 22 7d 2c 22 63 68 61 22 3a 7b 22 6f 66 66 69 63 69 61\n[47881] 6c 22 3a 22 53 61 6e 6b 61 74 74 61 6e 20 53 69 68 61 20 4e 61 20 49 73\n[47905] 6c 61 73 20 4d 61 72 69 c3 a5 6e 61 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[47929] 22 4e 61 20 49 73 6c 61 73 20 4d 61 72 69 c3 a5 6e 61 73 22 7d 2c 22 65\n[47953] 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6d 6d 6f 6e 77\n[47977] 65 61 6c 74 68 20 6f 66 20 74 68 65 20 4e 6f 72 74 68 65 72 6e 20 4d 61\n[48001] 72 69 61 6e 61 20 49 73 6c 61 6e 64 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a\n[48025] 22 4e 6f 72 74 68 65 72 6e 20 4d 61 72 69 61 6e 61 20 49 73 6c 61 6e 64\n[48049] 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[48073] 3a 22 49 63 65 6c 61 6e 64 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 49\n[48097] 63 65 6c 61 6e 64 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 69\n[48121] 73 6c 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 c3 8d 73 6c 61 6e 64\n[48145] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 c3 8d 73 6c 61 6e 64 22 7d 7d 7d 7d\n[48169] 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4b 65 6e 79\n[48193] 61 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20\n[48217] 6f 66 20 4b 65 6e 79 61 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b\n[48241] 22 65 6e 67 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62\n[48265] 6c 69 63 20 6f 66 20 4b 65 6e 79 61 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22\n[48289] 4b 65 6e 79 61 22 7d 2c 22 73 77 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[48313] 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4b 65 6e 79 61 22 2c 22 63\n[48337] 6f 6d 6d 6f 6e 22 3a 22 4b 65 6e 79 61 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d\n[48361] 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 4b 6f 73 6f 76 6f 22 2c 22 6f\n[48385] 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 63 20 6f 66 20 4b 6f\n[48409] 73 6f 76 6f 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 73 71 69\n[48433] 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 52 65 70 75 62 6c 69 6b 61\n[48457] 20 65 20 4b 6f 73 6f 76 c3 ab 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 4b\n[48481] 6f 73 6f 76 61 22 7d 2c 22 73 72 70 22 3a 7b 22 6f 66 66 69 63 69 61 6c\n[48505] 22 3a 22 d0 a0 d0 b5 d0 bf d1 83 d0 b1 d0 bb d0 b8 d0 ba d0 b0 20 d0 9a\n[48529] d0 be d1 81 d0 be d0 b2 d0 be 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d0 9a\n[48553] d0 be d1 81 d0 be d0 b2 d0 be 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a\n[48577] 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 53 69 6e 74 20 4d 61 61 72 74 65 6e 22\n[48601] 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 69 6e 74 20 4d 61 61 72 74 65\n[48625] 6e 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b\n[48649] 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 69 6e 74 20 4d 61 61 72 74 65 6e\n[48673] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 69 6e 74 20 4d 61 61 72 74 65 6e\n[48697] 22 7d 2c 22 66 72 61 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 53 61\n[48721] 69 6e 74 2d 4d 61 72 74 69 6e 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 53 61\n[48745] 69 6e 74 2d 4d 61 72 74 69 6e 22 7d 2c 22 6e 6c 64 22 3a 7b 22 6f 66 66\n[48769] 69 63 69 61 6c 22 3a 22 53 69 6e 74 20 4d 61 61 72 74 65 6e 22 2c 22 63\n[48793] 6f 6d 6d 6f 6e 22 3a 22 53 69 6e 74 20 4d 61 61 72 74 65 6e 22 7d 7d 7d\n[48817] 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 42 61 68\n[48841] 61 6d 61 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 43 6f 6d 6d 6f 6e\n[48865] 77 65 61 6c 74 68 20 6f 66 20 74 68 65 20 42 61 68 61 6d 61 73 22 2c 22\n[48889] 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a 7b 22 6f 66 66\n[48913] 69 63 69 61 6c 22 3a 22 43 6f 6d 6d 6f 6e 77 65 61 6c 74 68 20 6f 66 20\n[48937] 74 68 65 20 42 61 68 61 6d 61 73 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 42\n[48961] 61 68 61 6d 61 73 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f\n[48985] 6d 6d 6f 6e 22 3a 22 41 6c 67 65 72 69 61 22 2c 22 6f 66 66 69 63 69 61\n[49009] 6c 22 3a 22 50 65 6f 70 6c 65 27 73 20 44 65 6d 6f 63 72 61 74 69 63 20\n[49033] 52 65 70 75 62 6c 69 63 20 6f 66 20 41 6c 67 65 72 69 61 22 2c 22 6e 61\n[49057] 74 69 76 65 4e 61 6d 65 22 3a 7b 22 61 72 61 22 3a 7b 22 6f 66 66 69 63\n[49081] 69 61 6c 22 3a 22 d8 a7 d9 84 d8 ac d9 85 d9 87 d9 88 d8 b1 d9 8a d8 a9\n[49105] 20 d8 a7 d9 84 d8 af d9 8a d9 85 d9 82 d8 b1 d8 a7 d8 b7 d9 8a d8 a9 20\n[49129] d8 a7 d9 84 d8 b4 d8 b9 d8 a8 d9 8a d8 a9 20 d8 a7 d9 84 d8 ac d8 b2 d8\n[49153] a7 d8 a6 d8 b1 d9 8a d8 a9 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 d8 a7 d9\n[49177] 84 d8 ac d8 b2 d8 a7 d8 a6 d8 b1 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22\n[49201] 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 47 72 65 65 63 65 22 2c 22 6f 66 66\n[49225] 69 63 69 61 6c 22 3a 22 48 65 6c 6c 65 6e 69 63 20 52 65 70 75 62 6c 69\n[49249] 63 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6c 6c 22 3a 7b\n[49273] 22 6f 66 66 69 63 69 61 6c 22 3a 22 ce 95 ce bb ce bb ce b7 ce bd ce b9\n[49297] ce ba ce ae 20 ce 94 ce b7 ce bc ce bf ce ba cf 81 ce b1 cf 84 ce af ce\n[49321] b1 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 ce 95 ce bb ce bb ce ac ce b4 ce\n[49345] b1 22 7d 7d 7d 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22\n[49369] 3a 22 4e 65 70 61 6c 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 46 65 64\n[49393] 65 72 61 6c 20 44 65 6d 6f 63 72 61 74 69 63 20 52 65 70 75 62 6c 69 63\n[49417] 20 6f 66 20 4e 65 70 61 6c 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a\n[49441] 7b 22 6e 65 70 22 3a 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 e0 a4 a8 e0\n[49465] a5 87 e0 a4 aa e0 a4 be e0 a4 b2 20 e0 a4 b8 e0 a4 82 e0 a4 98 e0 a5 80\n[49489] e0 a4 af 20 e0 a4 b2 e0 a5 8b e0 a4 95 e0 a4 a4 e0 a4 be e0 a4 a8 e0 a5\n[49513] 8d e0 a4 a4 e0 a5 8d e0 a4 b0 e0 a4 bf e0 a4 95 20 e0 a4 97 e0 a4 a3 e0\n[49537] a4 a4 e0 a4 a8 e0 a5 8d e0 a4 a4 e0 a5 8d e0 a4 b0 22 2c 22 63 6f 6d 6d\n[49561] 6f 6e 22 3a 22 e0 a4 a8 e0 a5 87 e0 a4 aa e0 a4 be e0 a4 b2 22 7d 7d 7d\n[49585] 7d 2c 7b 22 6e 61 6d 65 22 3a 7b 22 63 6f 6d 6d 6f 6e 22 3a 22 55 6e 69\n[49609] 74 65 64 20 53 74 61 74 65 73 20 56 69 72 67 69 6e 20 49 73 6c 61 6e 64\n[49633] 73 22 2c 22 6f 66 66 69 63 69 61 6c 22 3a 22 56 69 72 67 69 6e 20 49 73\n[49657] 6c 61 6e 64 73 20 6f 66 20 74 68 65 20 55 6e 69 74 65 64 20 53 74 61 74\n[49681] 65 73 22 2c 22 6e 61 74 69 76 65 4e 61 6d 65 22 3a 7b 22 65 6e 67 22 3a\n[49705] 7b 22 6f 66 66 69 63 69 61 6c 22 3a 22 56 69 72 67 69 6e 20 49 73 6c 61\n[49729] 6e 64 73 20 6f 66 20 74 68 65 20 55 6e 69 74 65 64 20 53 74 61 74 65 73\n[49753] 22 2c 22 63 6f 6d 6d 6f 6e 22 3a 22 55 6e 69 74 65 64 20 53 74 61 74 65\n[49777] 73 20 56 69 72 67 69 6e 20 49 73 6c 61 6e 64 73 22 7d 7d 7d 7d 5d\n\n\nAbove we see that the content that is received from the API is delivered as a compressed binary string, which must be decompressed and converted back to character encoding before processing. We do this using a built-in httr function:\n\nbdy &lt;- content(response, \"text\")\n\nNo encoding supplied: defaulting to UTF-8.\n\n\nOnce we have the content extracted and converted, we can begin to process it. We previously examined the response and determined that it is in JSON format, so our step will be to load the content into a JSON object for ease of traversal:\n\nbdy_json &lt;- fromJSON(bdy)\n\nIf you examine the class and structure of the bdy_json object, you will see that jsonlite has converted the JSON structure into a nice R data frame where you can begin the process of exploration and cleaning in preparation for research.",
    "crumbs": [
      "Week 7",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Getting Data from the Web</span>"
    ]
  },
  {
    "objectID": "chapters/week08/writing-functions.html",
    "href": "chapters/week08/writing-functions.html",
    "title": "25  Writing Functions",
    "section": "",
    "text": "25.1 Why Write a Function?\nAt this point, you’ve learned all of the basic skills necessary to explore a dataset in R. This chapter and the next focus on how to organize and automate your code so that it’s concise, clear, and effective. This will help you and your collaborators avoid tedious, redundant work, reproduce results efficiently, and run code in specialized environments for research computing, such as high-performance computing clusters.\nThe main way to interact with R is by calling functions, which was first explained way back in Section 12.3.4. This chapter explains how to write your own functions.\nTwo reasons to write functions are to:\nFunctions are the building blocks for solving problems. Break problems down into steps and write a function for each step. That way you can:\nAt first, you might not know how small (or big) each step should be. As a suggestion, most functions should be more than one line of code but short enough to fit on one screen.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/week08/writing-functions.html#why-write-a-function",
    "href": "chapters/week08/writing-functions.html#why-write-a-function",
    "title": "25  Writing Functions",
    "section": "",
    "text": "Reuse general-purpose code. Suppose, for example, that every time you read a data frame into R, you standardize the column names by making them lowercase and replacing spaces with underscores. By turning the code to do this into a function, you can reuse it across many projects without having to copy, paste, and edit (which can introduce errors).\nEncapsulate code that carries out a single step in a multi-step computation. For instance, imagine you need to read a dataset, clean it, and finally transform some of the features before you do any analysis. Writing a function for each step (read_data, clean_data, transform_data, and so on) makes it easier to describe the entire computation in code without being overwhelmed by the details. It also means you can focus on one specific step at a time: while you work on clean_data, the focus is on cleaning, and you can assume reading the data will be handled elsewhere. You can also test each function to make sure that step works correctly before moving on to the next.\n\n\n\nTest that each step works correctly.\nDescribe the sequence of steps concisely.\nModify, reuse, or repurpose steps.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWriting code is a lot like writing an essay. Each line of code is a sentence that expresses one or two ideas. Each function definition is a paragraph, and should have a singular purpose and coherent flow. You can use comments to make an outline and as clarifying footnotes.\nAs with any writing, you’ll typically need to go through several revisions before you arrive at exactly what you want.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/week08/writing-functions.html#function-definitions",
    "href": "chapters/week08/writing-functions.html#function-definitions",
    "title": "25  Writing Functions",
    "section": "25.2 Function Definitions",
    "text": "25.2 Function Definitions\nThink of a function as a factory. It takes raw materials, runs them through some machinery, and produces a final product:\n\nRaw materials: the inputs to a function are called arguments. Each argument is assigned to a parameter, a placeholder variable.\nMachinery: code in the body of a function computes something from the arguments.\nFinal product: the output of a function is called the return value.\n\nThe function keyword defines a new function. It can have any number of parameters. The body must be enclosed in curly braces { } if it’s more than one line of code. The function will automatically return the value of its last line. So a function definition looks like this:\n\nfunction(parameter1, parameter2, ...) {\n  # The body of the function (the code) goes between the curly braces { }.\n\n  # The return value (the result) goes on the last line.\n}\n\nWhen you define a function, you’ll usually want to save it in a variable so that you can call it later. Choose a descriptive name that describes what the function does. Since functions do things, it often makes sense to include a verb in the name.\n\n\n\n\n\n\nImportant\n\n\n\nIndent lines of code between curly braces by 2 or 4 spaces to make it easier to see where the block of code starts and ends.\n\n\n\n\n\n\n\n\nNoteViewing Function Definitions\n\n\n\n\n\nAlmost every command in R is a function, even the arithmetic operators and the parentheses! You can view the definition of a function by typing its name without trailing parentheses (in contrast to how you call functions).\nFor example, let’s look at the body of the append function, which appends a value to the end of a list or vector:\n\nappend\n\nfunction (x, values, after = length(x)) \n{\n    lengx &lt;- length(x)\n    if (!after) \n        c(values, x)\n    else if (after &gt;= lengx) \n        c(x, values)\n    else c(x[1L:after], values, x[(after + 1L):lengx])\n}\n&lt;bytecode: 0x558c98f842f0&gt;\n&lt;environment: namespace:base&gt;\n\n\nDon’t worry if you can’t understand everything the append function’s code does yet. It will make more sense later on, after you’ve written a few functions of your own.\nMany of R’s built-in functions are not entirely written in R code. You can spot these by calls to the special .Primitive or .Internal functions in their code.\nFor instance, the sum function is not written in R code:\n\nsum\n\nfunction (..., na.rm = FALSE)  .Primitive(\"sum\")\n\n\n\n\n\nAs a demonstration, let’s create a function that detects negative numbers. It should take a vector of numbers as input, compare them to zero, and then return the logical result from the comparison as output. Here’s the code:\n\nis_negative = function(x) x &lt; 0\n\nThe name of the function, is_negative, describes what the function does and includes a verb. The parameter x is the input. The return value is the result x &lt; 0.\nAny time you write a function, the first thing you should do afterwards is test that it actually works. Try the is_negative function out on a few test cases:\n\nis_negative(6)\n\n[1] FALSE\n\nis_negative(-1.1)\n\n[1] TRUE\n\nx = c(5, -1, -2, 0, 3)\nis_negative(x)\n\n[1] FALSE  TRUE  TRUE FALSE FALSE\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe cautious about using variables in the body of a function if they’re assigned somewhere else. For example:\n\nx = 100\n\nf = function() x ** 2\n\nf()\n\n[1] 10000\n\n\nDoing so makes your function harder for other people (and future you) to use because the inputs are not clearly labeled. It can also introduce subtle bugs. Use parameters for inputs instead:\n\nf = function(z) z ** 2\n\nf(x)\n\n[1] 10000\n\n\nIt’s also okay to use variables assigned in the body of the function:\n\nf = function(z) {\n  result = z ** 2\n  result\n}\n\nf(x)\n\n[1] 10000",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/week08/writing-functions.html#sec-how-to-write-function",
    "href": "chapters/week08/writing-functions.html#sec-how-to-write-function",
    "title": "25  Writing Functions",
    "section": "25.3 How to Write a Function",
    "text": "25.3 How to Write a Function\nBefore you write a function, you should:\n\nWrite down what you want the function to do, in detail. Pay particular attention to what the inputs and outputs are. Use comments, a separate document, or even a piece of paper. If it’s difficult to explain in words, try drawing a picture. The key is to make sure that the goal is clear before you start to write code.\nCheck whether there’s already a built-in function. Search online and in the R documentation.\n\nIf there isn’t a function that does what you want, it’s time to write one. To do so:\n\nWrite the code for a simple case. For data problems, use a small dataset. Don’t worry about turning the code into a function yet. Make sure that the result is correct.\nOnce you’ve got the code for the simple case working, wrap it in a function definition. Identify the parts of the code that will change for other cases. The dataset is usually one of these, but there might also be others. Replace these with parameters. Make sure the function returns the result.\nTest calling the function with the inputs for the simple case. It should return exactly the same result as it did in step 3. If not, figure out why and fix it.\nTest calling the function with the inputs for a different case. Make sure that the result is correct. If it isn’t, figure out why and fix it.\nRepeat step 6 for a few more cases. Make sure to test cases that are uncommon or unusual, but realistically possible. Also test what happens in cases of erroneous inputs (does your function emit an error? does the error message explain the problem clearly?). By the end of this step, you should feel fairly confident that your function works correctly.\n\nKeep in mind that writing a function is a process of refinement, even for experienced programmers. Even after going through all of the steps above, you might come across a new, unexpected case or discover a bug that requires you to edit your function.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/week08/writing-functions.html#example-getting-largest-values",
    "href": "chapters/week08/writing-functions.html#example-getting-largest-values",
    "title": "25  Writing Functions",
    "section": "25.4 Example: Getting Largest Values",
    "text": "25.4 Example: Getting Largest Values\nSuppose we want a function to get the top 3 largest values in a vector. The main input to the function is the vector. We could also try to write the function so that it returns the top n values instead of being fixed at 3. Then n is another input. The output is a vector of 3 (or n) values.\nThere’s no function built into R to do this. There might be a function in a package, but we’ll go ahead and write the function ourselves for the sake of learning.\nWe’ll start by writing the code for a simple case. Suppose we’re interested in this vector:\n\nx = c(1, 10, 20, -3)\n\nWe can sort the values with the sort function, and then use the head function to get the first 3:\n\nsorted = sort(x, decreasing = TRUE)\nhead(sorted, 3)\n\n[1] 20 10  1\n\n\nNow that we’ve solved the simple case, let’s wrap the code in a function definition. The vector will change from case to case, so we’ll replace x with a parameter named vec. We might also want to change the number of values returned, so we’ll also replace 3 with a parameter named n. So the function definition is:\n\nget_largest = function(vec, n) {\n  sorted = sort(vec, decreasing = TRUE)\n  head(sorted, n)\n}\n\nThe name of the function, get_largest, describes what the function does and includes a verb. If this function will be used frequently, a shorter name, such as largest, might be preferable (compare to the head function).\nTry the get_largest function on the simple case to make sure it returns the same result:\n\nget_largest(x, 3)\n\n[1] 20 10  1\n\n\nThat looks correct, so let’s try some other cases:\n\n# What if we ask for fewer elements?\nget_largest(x, 2)\n\n[1] 20 10\n\n# What if we ask for too many elements?\nget_largest(x, 5)\n\n[1] 20 10  1 -3\n\ny = c(-1, -2, -3)\nget_largest(y, 2)\n\n[1] -1 -2\n\n# What if the vector contains strings?\nz = c(\"d\", \"a\", \"t\", \"a\", \"l\", \"a\", \"b\")\nget_largest(z, 5)\n\n[1] \"t\" \"l\" \"d\" \"b\" \"a\"\n\n\nNotice that the parameters vec and n inside the function do not exist as variables outside of the function:\n\nvec\n\nError: object 'vec' not found\n\n\nIn general, R keeps parameters and variables you define inside of a function separate from variables you define outside of a function. You can read more about the specific rules for how R searches for variables in DataLab’s Intermediate R workshop reader.\n\n\n\n\n\n\nNoteNote: Default Arguments\n\n\n\n\n\nAs a function for quickly summarizing data, get_largest would be more convenient if the parameter n for the number of values to return was optional (again, compare to the head function). You can make the parameter n optional by setting a default argument: an argument assigned to the parameter if no argument is assigned in the call to the function. You can use = to assign default arguments to parameters when you define a function with the function keyword.\nHere’s a new definition of get_largest with the default n = 5:\n\nget_largest = function(vec, n = 5) {\n  sorted = sort(vec, decreasing = TRUE)\n  head(sorted, n)\n}\n\nAfter making a change, it’s a good idea to test the function again:\n\nget_largest(x)\n\n[1] 20 10  1 -3\n\nget_largest(y)\n\n[1] -1 -2 -3\n\nget_largest(z)\n\n[1] \"t\" \"l\" \"d\" \"b\" \"a\"\n\n\n\n\n\n\n\n\n\n\n\nNoteNote: Returning Multiple Values\n\n\n\n\n\nA function returns one R object, but sometimes computations have multiple results. In that case, return the results in a vector, list, or other data structure.\nFor example, let’s make a function that computes the mean and median for a vector. We’ll return the results in a named list, although we could also use a named vector:\n\ncompute_mean_med = function(x) {\n  m1 = mean(x)\n  m2 = median(x)\n  list(mean = m1, median = m2)\n}\n\ncompute_mean_med(c(1, 2, 3, 1))\n\n$mean\n[1] 1.75\n\n$median\n[1] 1.5\n\n\nThe names make the result easier to understand for the caller of the function, although they certainly aren’t required here.\n\n\n\n\n\n\n\n\n\nNoteNote: The return Keyword\n\n\n\n\n\nWe’ve already seen that a function will automatically return the value of its last line. The return keyword causes a function to return a result immediately, without running any subsequent code in its body.\nIt only makes sense to use return from inside of an if-expression. If your function doesn’t have any if-expressions, you don’t need to use return.\nFor example, suppose you want the get_largest function to immediately return NULL if the argument for vec is a list. Here’s the code, along with some test cases:\n\nget_largest = function(vec, n = 5) {\n  if (is.list(vec))\n    return(NULL)\n\n  sorted = sort(vec, decreasing = TRUE)\n  head(sorted, n)\n}\n\nget_largest(x)\n\n[1] 20 10  1 -3\n\nget_largest(z)\n\n[1] \"t\" \"l\" \"d\" \"b\" \"a\"\n\nget_largest(list(1, 2))\n\nNULL\n\n\nAlternatively, you could make the function raise an error by calling the stop function. Whether it makes more sense to return NULL or print an error depends on how you plan to use the get_largest function.\nNotice that the last line of the get_largest function still doesn’t use the return keyword. It’s idiomatic to only use return when strictly necessary.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/week08/writing-functions.html#sec-case-study-fuel-1",
    "href": "chapters/week08/writing-functions.html#sec-case-study-fuel-1",
    "title": "25  Writing Functions",
    "section": "25.5 Case Study: U.S. Alternative Fueling Stations, Part I",
    "text": "25.5 Case Study: U.S. Alternative Fueling Stations, Part I\nThe United States Department of Energy collects data about alternative (non-petroleum) fuel distribution and use within the country. They publish the data they collect on their Alternative Fuels Data Center (AFDC) website. In this case study, which consists of three parts, we’ll clean and analyze an AFDC dataset about alternative fueling stations.\n\n\n\n\n\n\nImportant\n\n\n\nClick here to download the 2007-2013 U.S. Alternative Fueling Stations dataset.\nIf you haven’t already, we recommend you create a directory for this workshop. In your workshop directory, create a data/ subdirectory. Download and save the dataset in the data/ subdirectory.\n\n\n\n\n\n\n\n\nNoteDocumentation for the U.S. Alternative Fueling Stations Dataset\n\n\n\n\n\nThe dataset is an Excel file with a documentation sheet and one data sheet for each year. Each data sheet contains counts broken down by state and fuel type. The format of the data sheets changes in 2014.\nThe source of this dataset is the historical counts on the U.S. DoE’s Alternative Fueling Station Counts by State page.\n\n\n\nThe dataset is in an Excel file with a separate sheet for each year. We’d like to be able to read the data for every year and combine it all into a single data frame. To get started, let’s focus on a single step: reading the data for just one year. We’ll use the first year, 2007, since the data for the pre-2014 years have a simpler structure.\nR doesn’t provide a built-in function to read Excel files, so we’ll use the readxl package. Install the package if you haven’t already, and then load it:\n\n# install.packages(\"readxl\")\nlibrary(\"readxl\")\n\nThe readxl package’s read_excel function reads a single sheet from an Excel file. We can set the function’s sheet parameter to select the sheet by position or name. In the alternative fueling stations dataset, each data sheet is named after the associated year. So the code to read and print the 2007 data is:\n\npath = \"data/2007-2023_us_alt_fuels.xlsx\"\nstations = read_excel(path, sheet = \"2007\")\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n\nhead(stations)\n\n# A tibble: 6 × 9\n  Station Counts by State and …¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 \n  &lt;chr&gt;                          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 State                          Biod… CNG   E85   &lt;NA&gt;  &lt;NA&gt;  LNG   &lt;NA&gt;  Total\n2 &lt;NA&gt;                           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n3 &lt;NA&gt;                           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  Elec… Hydr… &lt;NA&gt;  Prop… &lt;NA&gt; \n4 Alabama                        13    3     3     0     0     0     52    71   \n5 &lt;NA&gt;                           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n6 Alaska                         0     1     0     0     0     0     10    11   \n# ℹ abbreviated name: ¹​`Station Counts by State and Fuel Type`\n\n\nFrom this, we can see that the column names are actually in the 2nd row of the sheet (row 1 in the data frame). So let’s change the call to read_excel to skip the 1st row:\n\nstations = read_excel(path, sheet = \"2007\", skip = 1)\n\nNew names:\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...8`\n\nhead(stations)\n\n# A tibble: 6 × 9\n  State   Biodiesel   CNG   E85 ...5     ...6       LNG ...8    Total\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 &lt;NA&gt;           NA    NA    NA &lt;NA&gt;     &lt;NA&gt;        NA &lt;NA&gt;       NA\n2 &lt;NA&gt;           NA    NA    NA Electric Hydrogen    NA Propane    NA\n3 Alabama        13     3     3 0        0            0 52         71\n4 &lt;NA&gt;           NA    NA    NA &lt;NA&gt;     &lt;NA&gt;        NA &lt;NA&gt;       NA\n5 Alaska          0     1     0 0        0            0 10         11\n6 &lt;NA&gt;           NA    NA    NA &lt;NA&gt;     &lt;NA&gt;        NA &lt;NA&gt;       NA\n\n\nColumns 5, 6, and 8 are still missing names. The names of these columns are in row 2 of the data frame. We’ll use the stringr package, which provides functions for processing strings, to help here. Install the package if you haven’t already, and then load it:\n\n# install.packages(\"stringr\")\nlibrary(\"stringr\")\n\nWe can use stringr’s str_starts function to detect the column names that start with .... The first argument is the strings and the second is the pattern to detect. Call the fixed function on the pattern make stringr treat it as-is. Then we can use indexing to get the column names from row 2:\n\nnames = names(stations)\nis_dot_name = str_starts(names, fixed(\"...\"))\nnames[is_dot_name] = as.character(stations[2, is_dot_name])\nnames\n\n[1] \"State\"     \"Biodiesel\" \"CNG\"       \"E85\"       \"Electric\"  \"Hydrogen\" \n[7] \"LNG\"       \"Propane\"   \"Total\"    \n\n\nLet’s also make the column names lowercase, so that they’re easy to type. We can use stringr’s str_to_lower function to do this:\n\nnames = str_to_lower(names)\nnames\n\n[1] \"state\"     \"biodiesel\" \"cng\"       \"e85\"       \"electric\"  \"hydrogen\" \n[7] \"lng\"       \"propane\"   \"total\"    \n\n\nAssign the names back to the column names:\n\nnames(stations) = names\nhead(stations)\n\n# A tibble: 6 × 9\n  state   biodiesel   cng   e85 electric hydrogen   lng propane total\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 &lt;NA&gt;           NA    NA    NA &lt;NA&gt;     &lt;NA&gt;        NA &lt;NA&gt;       NA\n2 &lt;NA&gt;           NA    NA    NA Electric Hydrogen    NA Propane    NA\n3 Alabama        13     3     3 0        0            0 52         71\n4 &lt;NA&gt;           NA    NA    NA &lt;NA&gt;     &lt;NA&gt;        NA &lt;NA&gt;       NA\n5 Alaska          0     1     0 0        0            0 10         11\n6 &lt;NA&gt;           NA    NA    NA &lt;NA&gt;     &lt;NA&gt;        NA &lt;NA&gt;       NA\n\n\nMany rows are partially or completely blank. We only want to keep the rows that contain counts. So let’s use dplyr’s filter function to remove all of the rows where biodiesel is missing:\n\n# install.packages(\"dplyr\")\nlibrary(\"dplyr\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nstations = filter(stations, !is.na(biodiesel))\nhead(stations)\n\n# A tibble: 6 × 9\n  state      biodiesel   cng   e85 electric hydrogen   lng propane total\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Alabama           13     3     3 0        0            0 52         71\n2 Alaska             0     1     0 0        0            0 10         11\n3 Arizona           10    37    13 12       1            3 58        134\n4 Arkansas           3     3     4 0        0            0 41         51\n5 California        39   186     6 367      23          29 206       856\n6 Colorado          30    21    45 2        0            0 59        157\n\n\nThe electric, hydrogen, and propane columns are character columns, but they should be numeric. Let’s convert the electric column. We’ll leave hydrogen and propane as-is (with a comment in the code), since we’re not going to use them in our analysis. While we’re at it, we’ll also add a year column with the year:\n\nstations$electric = as.numeric(stations$electric)\n# TODO: hydrogen, propane\nstations$year = 2007\n\nhead(stations)\n\n# A tibble: 6 × 10\n  state      biodiesel   cng   e85 electric hydrogen   lng propane total  year\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Alabama           13     3     3        0 0            0 52         71  2007\n2 Alaska             0     1     0        0 0            0 10         11  2007\n3 Arizona           10    37    13       12 1            3 58        134  2007\n4 Arkansas           3     3     4        0 0            0 41         51  2007\n5 California        39   186     6      367 23          29 206       856  2007\n6 Colorado          30    21    45        2 0            0 59        157  2007\n\n\nThis data frame is clean enough for analysis.\nWe want to carry out the same steps for other years of data, so let’s turn the code into a function. Gather together all of the code and enclose it in a function definition:\n\nfunction() {\n  path = \"data/2007-2023_us_alt_fuels.xlsx\"\n  stations = read_excel(path, sheet = \"2007\", skip = 1)\n\n  names = names(stations)\n  is_dot_name = str_starts(names, fixed(\"...\"))\n  names[is_dot_name] = as.character(stations[2, is_dot_name])\n  names = str_to_lower(names)\n  names(stations) = names\n\n  stations = filter(stations, !is.na(biodiesel))\n\n  stations$electric = as.numeric(stations$electric)\n  # TODO: hydrogen, propane\n  stations$year = 2007\n}\n\nThe inputs are the path to the dataset and the year (which is also the sheet name). The output is the cleaned data frame. Add parameters for the inputs and put the output on the last line. Let’s call the function read_fuel_sheet. You can also optionally add some comments to clarify how the function works:\n\nread_fuel_sheet = function(path, year) {\n  sheet = as.character(year)\n  stations = read_excel(path, sheet = sheet, skip = 1)\n\n  # Clean up the column names.\n  names = names(stations)\n  is_dot_name = str_starts(names, fixed(\"...\"))\n  names[is_dot_name] = as.character(stations[2, is_dot_name])\n  names = str_to_lower(names)\n  names(stations) = names\n\n  # Remove blank rows.\n  stations = filter(stations, !is.na(biodiesel))\n\n  # Correct column types and add year column.\n  stations$electric = as.numeric(stations$electric)\n  # TODO: hydrogen, propane\n  stations$year = year\n\n  stations\n}\n\nTest the new function to make sure it returns the same result on the example case:\n\nread_fuel_sheet(path, 2007)\n\nNew names:\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...8`\n\n\n# A tibble: 52 × 10\n   state       biodiesel   cng   e85 electric hydrogen   lng propane total  year\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama            13     3     3        0 0            0 52         71  2007\n 2 Alaska              0     1     0        0 0            0 10         11  2007\n 3 Arizona            10    37    13       12 1            3 58        134  2007\n 4 Arkansas            3     3     4        0 0            0 41         51  2007\n 5 California         39   186     6      367 23          29 206       856  2007\n 6 Colorado           30    21    45        2 0            0 59        157  2007\n 7 Connecticut         1    10     2        3 0            0 16         32  2007\n 8 Delaware            3     1     1        0 0            0 3           8  2007\n 9 District o…         1     1     3        0 1            0 0           6  2007\n10 Florida            14    17    11        2 1            0 49         94  2007\n# ℹ 42 more rows\n\n\nThe result looks okay, so try a few more cases:\n\nread_fuel_sheet(path, 2008)\n\nNew names:\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...8`\n\n\n# A tibble: 52 × 10\n   state       biodiesel   cng   e85 electric hydrogen   lng propane total  year\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama            11     3     6        0 0            0 40         60  2008\n 2 Alaska              0     1     0        0 0            0 10         11  2008\n 3 Arizona            10    40    23        5 1            5 51        135  2008\n 4 Arkansas            2     3     7        0 0            0 37         49  2008\n 5 California         36   184    13      376 26          28 199       862  2008\n 6 Colorado           18    18    65        0 0            0 43        144  2008\n 7 Connecticut         1     9     4        3 1            0 13         31  2008\n 8 Delaware            3     1     1        0 0            0 2           7  2008\n 9 District o…         1     1     3        0 1            0 0           6  2008\n10 Florida            12    15    18        3 2            0 47         97  2008\n# ℹ 42 more rows\n\nread_fuel_sheet(path, 2010)\n\nNew names:\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...8`\n\n\n# A tibble: 52 × 10\n   state       biodiesel   cng   e85 electric hydrogen   lng propane total  year\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama             5     4    15        0 0            1 128       153  2010\n 2 Alaska              1     2     0        0 0            0 8          11  2010\n 3 Arizona            14    36    32        8 1            5 61        157  2010\n 4 Arkansas            6     4    13        1 0            0 51         75  2010\n 5 California         35   213    54      428 22          32 227      1011  2010\n 6 Colorado           14    23    86        5 1            0 50        179  2010\n 7 Connecticut         1    13     1        4 2            0 16         37  2010\n 8 Delaware            3     1     1        0 0            0 3           8  2010\n 9 District o…         1     2     3        1 1            0 0           8  2010\n10 Florida            16    14    42        7 0            0 75        154  2010\n# ℹ 42 more rows\n\n\nThe function seems to generalize well to other years where the data have the same structure. The structure of the data changed in 2014. Let’s test the function on one of these years:\n\nread_fuel_sheet(path, 2015)\n\nWarning: Unknown or uninitialised column: `electric`.\n\n\nError in `$&lt;-`:\n! Assigned data `as.numeric(stations$electric)` must be compatible with\n  existing data.\n✖ Existing data has 52 rows.\n✖ Assigned data has 0 rows.\nℹ Only vectors of size 1 are recycled.\nCaused by error in `vectbl_recycle_rhs_rows()`:\n! Can't recycle input of size 0 to size 52.\n\n\nThe function raises an error, which isn’t too surprising. It looks like we still have some work to do to make the function general enough to read the data for 2014 and subsequent years. We’ll save that for part II of this case study (Section 26.2}.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html",
    "href": "chapters/week08/control-flow.html",
    "title": "26  Control Flow",
    "section": "",
    "text": "26.1 Conditional Expressions\nSuppose you want to read and clean 10 different mass measurement datasets. All of the datasets have the same structure, so the code to read and clean them is the same. Based on what you learned in Chapter 25, you decide to write a function, read_mass_data, and reuse the code. But you realize there are still two problems:\nBoth of these problems are related to the order in which code runs (the flow of control). R provides several control flow expressions that change how code runs. You can use control flow expressions to choose which code to run based on a condition or to run code multiple times. Many of the concepts in this chapter generalize to other programming languages, which provide similar commands to alter control flow.\nA conditional expression is one where the computer must make a decision about which code to run next. The computer makes its decision by checking whether a condition is true. In a flowchart, a conditional looks like a branch (two or more ways to get somewhere):\nIn most programming languages, the keyword if (or some variation of this) creates a conditional. Because of this, conditionals are also called if statements. In R, the syntax for a conditional is:\nif (condition) {\n  # This code runs if the condition is TRUE.\n} else {\n  # This code runs if the condition is FALSE.\n}\nThe condition must return a single TRUE or FALSE value. The else part of the conditional is optional, so if you only want to do something in the true case, you can just write:\nif (condition) {\n  # This code runs if the condition is TRUE.\n}\nYou can test multiple conditions by replacing the curly braces { } after else with another conditional:\nif (condition1) {\n  # This code runs if condition1 is TRUE.\n} else if (condition2) {\n  # This code runs if condition2 is TRUE (and condition1 is FALSE).\n} else {\n  # This code runs if both condition1 and condition2 are FALSE.\n}\nAs an example, suppose we want our code to produce a different greeting depending on the hour:\nWe can use a conditional to do this (see Section 15.4 if you need a refresher on how to write conditions that return TRUE and FALSE values):\nhour = 10\n\nif (hour &gt;= 6 && hour &lt;= 11) {\n  greeting = \"Good morning!\"\n} else if (hour &gt;= 12 && hour &lt;= 17) {\n  greeting = \"Good afternoon!\"\n} else {\n  greeting = \"Hello.\"\n}\n\ngreeting\n\n[1] \"Good morning!\"\nNotice how changing the value of hour changes the result:\nhour = 13\n\nif (hour &gt;= 6 && hour &lt;= 11) {\n  greeting = \"Good morning!\"\n} else if (hour &gt;= 12 && hour &lt;= 17) {\n  greeting = \"Good afternoon!\"\n} else {\n  greeting = \"Hello.\"\n}\n\ngreeting\n\n[1] \"Good afternoon!\"\nTry out a few values for hour yourself. Then try modifying the code to give another greeting, \"Good evening!\" if it’s the evening (hours 18-19).",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html#conditional-expressions",
    "href": "chapters/week08/control-flow.html#conditional-expressions",
    "title": "26  Control Flow",
    "section": "",
    "text": "flowchart LR\n  start(Start) --&gt; condition{Is condition true?}\n    condition -- True --&gt; do_something[Do something]\n    condition -- False --&gt; do_something_else[\"Do something else\"]\n  do_something --&gt; done(End)\n  do_something_else --&gt; done\n\n\n\n\n\n\n\n\nFigure 26.1: A flowchart that shows a conditional. The computer checks a condition to decide which code to run.\n\n\n\n\n\n\n\n\n\n\n\nIf it’s the morning (hours 6-11), the code should produce \"Good morning!\".\nIf it’s the afternoon (hours 12-17), the code should produce \"Good   afternoon!\".\nFor any other hour, the code should produce \"Hello.\".\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn R, conditionals automatically return the value on the last line that runs (similar to functions). So another way to write the greeter example above is:\n\nhour = 13\n\ngreeting = if (hour &gt;= 6 && hour &lt;= 11) {\n  \"Good morning!\"\n} else if (hour &gt;= 12 && hour &lt;= 17) {\n  \"Good afternoon!\"\n} else {\n  \"Hello.\"\n}\n\ngreeting\n\n[1] \"Good afternoon!\"\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can nest one conditional inside the curly braces of another. Nesting is useful when you want to check a condition, do some computations, and then check another condition under the assumption that the first condition was TRUE.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nConditionals correspond to special cases in your code. Having lots of special cases can make code harder to understand and maintain.\nIt’s not bad to use conditionals, but be mindful of how you use them. Think about whether there’s a more general way to do things instead. Be especially wary of nested conditionals.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html#sec-case-study-fuel-2",
    "href": "chapters/week08/control-flow.html#sec-case-study-fuel-2",
    "title": "26  Control Flow",
    "section": "26.2 Case Study: U.S. Alternative Fueling Stations, Part II",
    "text": "26.2 Case Study: U.S. Alternative Fueling Stations, Part II\nLet’s continue the U.S. Alternative Fueling Stations case study from Section 25.5. In the previous part, we wrote a function, read_fuel_sheet, to read and clean sheets from the dataset Excel file:\n\nlibrary(\"dplyr\")\nlibrary(\"readxl\")\nlibrary(\"stringr\")\n\nread_fuel_sheet = function(path, year) {\n  sheet = as.character(year)\n  stations = read_excel(path, sheet = sheet, skip = 1)\n\n  # Clean up the column names.\n  names = names(stations)\n  is_dot_name = str_starts(names, fixed(\"...\"))\n  names[is_dot_name] = as.character(stations[2, is_dot_name])\n  names = str_to_lower(names)\n  names(stations) = names\n\n  # Remove blank rows.\n  stations = filter(stations, !is.na(biodiesel))\n\n  # Correct column types and add year column.\n  stations$electric = as.numeric(stations$electric)\n  # TODO: hydrogen, propane\n  stations$year = year\n\n  stations\n}\n\nWe designed the function around the 2007 data sheet, but the format of the sheets changed in 2014. So the read_fuel_sheet function emits an error for 2014 and later. Let’s edit the function so that it works well for every year. We can use conditionals for code that should only run for some of the years.\nTo get started, let’s write code to read and clean the data for 2023, since it’s the last year. We can use the code in read_fuel_sheet as a reference. We’ll still use the readxl package’s read_excel function to read the sheet:\n\npath = \"data/2007-2023_us_alt_fuels.xlsx\"\nstations = read_excel(path, sheet = \"2023\", skip = 1)\nhead(stations)\n\n# A tibble: 6 × 10\n  State   Biodiesel   CNG   E85 Electrica               Hydrogenb   LNG Propanec\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   \n1 &lt;NA&gt;           NA    NA    NA (stations / charging o… (retail …    NA (primar…\n2 &lt;NA&gt;           NA    NA    NA Level 1 / Level 2 / DC… &lt;NA&gt;         NA &lt;NA&gt;    \n3 Alabama         9    28    34 424 | 1,096             0 | 0 | 0     2 25 | 36…\n4 &lt;NA&gt;           NA    NA    NA 35 | 704 | 357          &lt;NA&gt;         NA &lt;NA&gt;    \n5 Alaska          0     0     0 65 | 124                0 | 0 | 0     0 1 | 0 |…\n6 &lt;NA&gt;           NA    NA    NA 3 | 87 | 34             &lt;NA&gt;         NA &lt;NA&gt;    \n# ℹ 2 more variables: `Renewable Diesel` &lt;dbl&gt;, Totald &lt;dbl&gt;\n\n\nThis immediately reveals one reason why the read_fuel_sheet function didn’t work. For 2014 and later, all of the column names are in the 2nd row of the sheet. There’s no need to get names from other rows. Let’s make a note of this in a comment.\nWe still need to convert the column names to lowercase:\n\nnames = names(stations)\n# For 2014 and later, no need to get column names from two different rows.\nnames = str_to_lower(names)\n\nOne of the columns is named renewable diesel, which will be difficult to use in analysis because it contains a space. We can use stringr’s str_replace_all to replace the space with an underscore:\n\nnames = str_replace_all(names, fixed(\" \"), \"_\")\nnames\n\n [1] \"state\"            \"biodiesel\"        \"cng\"              \"e85\"             \n [5] \"electrica\"        \"hydrogenb\"        \"lng\"              \"propanec\"        \n [9] \"renewable_diesel\" \"totald\"          \n\n\nFour of the names seem incorrect: electrica, hydrogenb, propanec, and totald. If we examine the dataset with spreadsheet software, we can see that these column names have alphabetic superscripts in 2014 and later. There are many ways to fix this, but we’ll use dplyr’s rename function:\n\nnames(stations) = names\n\n# For 2014 and later:\nstations = rename(\n  stations,\n  electric = electrica,\n  hydrogen = hydrogenb,\n  propane = propanec,\n  total = totald\n)\nhead(stations)\n\n# A tibble: 6 × 10\n  state   biodiesel   cng   e85 electric hydrogen   lng propane renewable_diesel\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n1 &lt;NA&gt;           NA    NA    NA (statio… (retail…    NA (prima…               NA\n2 &lt;NA&gt;           NA    NA    NA Level 1… &lt;NA&gt;        NA &lt;NA&gt;                  NA\n3 Alabama         9    28    34 424 | 1… 0 | 0 |…     2 25 | 3…                0\n4 &lt;NA&gt;           NA    NA    NA 35 | 70… &lt;NA&gt;        NA &lt;NA&gt;                  NA\n5 Alaska          0     0     0 65 | 124 0 | 0 |…     0 1 | 0 …                0\n6 &lt;NA&gt;           NA    NA    NA 3 | 87 … &lt;NA&gt;        NA &lt;NA&gt;                  NA\n# ℹ 1 more variable: total &lt;dbl&gt;\n\n\nThe next step in read_fuel_sheet is to remove blank rows (rows with missing values). The 2023 data frame has rows that are partially blank, but they do have values in the electric column. By inspecting the sheet, we can see that for 2014 and later, the electric column contains multiple values separated by the pipe character |. The electric values for each state are also split across two rows. Since we want to compare the data over time, we only need the station count, which is in the first row. So we’ll use the code from read_fuel_sheet that filters out the blank rows:\n\nstations = filter(stations, !is.na(biodiesel))\nhead(stations)\n\n# A tibble: 6 × 10\n  state   biodiesel   cng   e85 electric hydrogen   lng propane renewable_diesel\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n1 Alabama         9    28    34 424 | 1… 0 | 0 |…     2 25 | 3…                0\n2 Alaska          0     0     0 65 | 124 0 | 0 |…     0 1 | 0 …                0\n3 Arizona        77    26    21 1,198 |… 0 | 0 |…     5 42 | 3…                0\n4 Arkans…        34    14    74 334 | 8… 0 | 0 |…     0 17 | 1…                0\n5 Califo…        48   292   369 16,381 … 57 | 7 …    39 136 | …              597\n6 Colora…         5    25    95 2,165 |… 0 | 1 |…     1 28 | 2…                0\n# ℹ 1 more variable: total &lt;dbl&gt;\n\n\nWith that done, let’s split the electric column into two columns, since it contains two values. The first value is the number of stations, which we’ll keep in the electric column. The second value is the number of outlets, which we’ll put in a column called electric_outlets. To make it easier to convert the columns to numbers, we’ll remove all of the commas (,) with stringr’s str_replace_all function. Then we can use stringr’s str_split_fixed function to split the column in two. Finally, we can use as.numeric to convert the columns to numbers before saving them back into the data frame:\n\n# For 2014 and later:\nelectric = str_replace_all(stations$electric, fixed(\",\"), \"\")\nelectric = str_split_fixed(electric, fixed(\" | \"), 2)\nstations$electric = as.numeric(electric[, 1])\nstations$electric_outlets = as.numeric(electric[, 2])\n\nThe hydrogen and propane columns have the same problem. We’ll skip fixing them, since the code to do so is almost identical.\nThe 2023 data frame is now clean enough for analysis. The code to read and clean it is:\n\nstations = read_excel(path, sheet = \"2023\", skip = 1)\n\nnames = names(stations)\n# For 2014 and later, no need to get column names from two different rows.\nnames = str_to_lower(names)\nnames = str_replace_all(names, fixed(\" \"), \"_\")\nnames(stations) = names\n\n# For 2014 and later:\nstations = rename(\n  stations,\n  electric = electrica,\n  hydrogen = hydrogenb,\n  propane = propanec,\n  total = totald\n)\n\nstations = filter(stations, !is.na(biodiesel))\n\n# For 2014 and later:\nelectric = str_replace_all(stations$electric, fixed(\",\"), \"\")\nelectric = str_split_fixed(electric, fixed(\" | \"), 2)\nstations$electric = as.numeric(electric[, 1])\nstations$electric_outlets = as.numeric(electric[, 2])\n\nWe can use conditionals to combine this with code in the body of the read_fuel_sheet function. Specifically, we need to check year &lt; 2014 or year &gt;= 2014 at a few points. One way to write the code for the combined function is:\n\nread_fuel_sheet = function(path, year) {\n  sheet = as.character(year)\n  stations = read_excel(path, sheet = sheet, skip = 1)\n\n  # Clean up the column names.\n  names = names(stations)\n  if (year &lt; 2014) {\n    is_dot_name = str_starts(names, fixed(\"...\"))\n    names[is_dot_name] = as.character(stations[2, is_dot_name])\n  }\n  names = str_to_lower(names)\n  names = str_replace_all(names, fixed(\" \"), \"_\")\n  names(stations) = names\n  if (year &gt;= 2014) {\n    stations = rename(\n      stations,\n      electric = electrica,\n      hydrogen = hydrogenb,\n      propane = propanec,\n      total = totald\n    )\n  }\n\n  # Remove blank rows.\n  stations = filter(stations, !is.na(biodiesel))\n\n  # Correct column types and add year column.\n  if (year &gt;= 2014) {\n    electric = str_replace_all(stations$electric, fixed(\",\"), \"\")\n    electric = str_split_fixed(electric, fixed(\" | \"), 2)\n    stations$electric = as.numeric(electric[, 1])\n    stations$electric_outlets = as.numeric(electric[, 2])\n    # TODO: hydrogen, propane\n  }\n  stations$electric = as.numeric(stations$electric)\n  # TODO: hydrogen, propane\n  stations$year = year\n\n  stations\n}\n\nNotice how using multiple conditionals makes it harder to understand what the function does. We could instead use one conditional, but we’d have to repeat parts of the code that are the same. Alternatively, we could move some of the cleaning steps to other functions, so that read_fuel_sheet is shorter and easier to understand (without changing what it does).\nLet’s test the new read_fuel_sheet function on the 2023 data:\n\nread_fuel_sheet(path, 2023)\n\n# A tibble: 52 × 12\n   state  biodiesel   cng   e85 electric hydrogen   lng propane renewable_diesel\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1 Alaba…         9    28    34      424 0 | 0 |…     2 25 | 3…                0\n 2 Alaska         0     0     0       65 0 | 0 |…     0 1 | 0 …                0\n 3 Arizo…        77    26    21     1198 0 | 0 |…     5 42 | 3…                0\n 4 Arkan…        34    14    74      334 0 | 0 |…     0 17 | 1…                0\n 5 Calif…        48   292   369    16381 57 | 7 …    39 136 | …              597\n 6 Color…         5    25    95     2165 0 | 1 |…     1 28 | 2…                0\n 7 Conne…         1    13     3      865 0 | 0 |…     0 5 | 15…                0\n 8 Delaw…         0     3     2      197 0 | 1 |…     0 7 | 12…                0\n 9 Distr…         9     2     4      369 0 | 0 |…     0 0 | 6 …                0\n10 Flori…         5    64   150     3430 0 | 0 |…     3 74 | 5…                0\n# ℹ 42 more rows\n# ℹ 3 more variables: total &lt;dbl&gt;, electric_outlets &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nWe should also make sure that it still returns the same result for the 2007 data:\n\nread_fuel_sheet(path, 2007)\n\nNew names:\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...8`\n\n\n# A tibble: 52 × 10\n   state       biodiesel   cng   e85 electric hydrogen   lng propane total  year\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama            13     3     3        0 0            0 52         71  2007\n 2 Alaska              0     1     0        0 0            0 10         11  2007\n 3 Arizona            10    37    13       12 1            3 58        134  2007\n 4 Arkansas            3     3     4        0 0            0 41         51  2007\n 5 California         39   186     6      367 23          29 206       856  2007\n 6 Colorado           30    21    45        2 0            0 59        157  2007\n 7 Connecticut         1    10     2        3 0            0 16         32  2007\n 8 Delaware            3     1     1        0 0            0 3           8  2007\n 9 District o…         1     1     3        0 1            0 0           6  2007\n10 Florida            14    17    11        2 1            0 49         94  2007\n# ℹ 42 more rows\n\n\nThis looks good. We’ve now got a function that can read any of the years. In part III of this case study (Section 26.7), we’ll see how to use the function to read all of them.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html#loops",
    "href": "chapters/week08/control-flow.html#loops",
    "title": "26  Control Flow",
    "section": "26.3 Loops",
    "text": "26.3 Loops\nComputers are great at iterative tasks, where some of the steps need to be repeated. Examples of tasks like this include:\n\nApplying a transformation to (each element of) an entire column of data.\nComputing distances between all pairs for a set of points.\nReading a collection of files in order to combine and analyze their data.\nSimulating how a complex system evolves from one time step to the next.\nScraping the pages of a website.\n\nEach run of a repeated step is called an iteration. So in the example about reading a collection of files, one iteration means reading one file.\nIn programming languages, we use control flow expressions called loops to make the computer repeat some code. There are many different kinds of loops, but in a flowchart, they always look like a path that begins and ends at the same place (hence the name “loop”):\n\n\n\n\n\n\n\n\nflowchart LR\n  start(Start) --&gt; condition{Is condition true?}\n    condition -- True --&gt; body[Do something]\n    body --&gt; condition\n    condition -- False ----&gt; done(End)\n\n\n\n\n\n\n\n\nFigure 26.2: A flowchart that shows a while loop (one kind of loop). The computer checks a condition at the beginning of each iteration to decide whether to continue iterating or stop.\n\n\n\nIn R, there are four distinct categories of loops:\n\nVectorization, where a function is implicitly called on each element of a vector.\nMap functions (& apply functions), where a function is explicitly called on each element of a data structure.\nFor loops (& while loops), where an expression is evaluated repeatedly until some condition is met.\nRecursion, where a function calls itself.\n\nWe introduced vectorization in Section 13.1.3, several chapters ago. The next section, Section 26.4, introduces the purrr package and map functions. Section 26.5 introduces for loops. In Section 26.6, we’ll describe how to write loops to solve iterative problems and how to decide which kind of loop to use.\n\n\n\n\n\n\nNote\n\n\n\nWe don’t cover recursion because it tends to be difficult for people to understand. Recursion is also the least efficient kind of loop in R.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html#sec-the-purrr-package",
    "href": "chapters/week08/control-flow.html#sec-the-purrr-package",
    "title": "26  Control Flow",
    "section": "26.4 The purrr Package",
    "text": "26.4 The purrr Package\nSection 13.1.3 introduced vectorization, a convenient and efficient way to compute multiple results. That section also mentioned that some of R’s functions—the ones that summarize or aggregate data—are not vectorized.\nThe class function is an example of a function that’s not vectorized. If we call the class function on the least terns dataset, we get just one result for the dataset as a whole:\n\nclass(terns)\n\n[1] \"data.frame\"\n\n\nWe can get the class of a single column by selecting the column with $, the dollar sign operator:\n\nclass(terns$year)\n\n[1] \"integer\"\n\n\nWhat if we want the classes of all the columns? We could write a call to class for each column, but that would be tedious. When you’re working with a programming language, you should try to avoid tedium; there’s usually a better, more automated way.\nData frames are technically lists (Section 13.2.1), where each column is one element. With that in mind, what we need here is a line of code that calls class on each element of the data frame. The idea is similar to vectorization, but since we have a list and a non-vectorized function, we have to do a bit more than just call class(terns).\nThe purrr package is a collection of functions to help you do things repeatedly or for each element of a data structure. Install and load the package in order to follow along:\n\n# install.packages(\"purrr\")\nlibrary(\"purrr\")\n\nThe package’s map function calls a function on each element of a vector, list, or other data structure. We say it maps or applies a function over the elements. In a flowchart, the map function looks like this:\n\n\n\n\n\n\n\n\nflowchart LR\n  start(Start) --&gt; condition{Are there more elements?}\n    condition -- Yes --&gt; get_elt[\"Get the next element\"]\n    get_elt --&gt; body[Do something]\n    body --&gt; condition\n    condition -- No ----&gt; done(End)\n\n\n\n\n\n\n\n\nFigure 26.3: A flowchart that shows a map function, apply function, or for loop. At the beginning of each iteration, the computer checks whether there is another element in the data to decide whether to continue iterating. If there is, the computer automatically assigns the next element to a variable.\n\n\n\nThe syntax of the map function is:\n\nmap(data, fun, ...)\n\nThe map function calls the function fun once for each element of data. It passes the element to fun as the first argument. It also passes the ... arguments to fun, which are constant across all of the calls.\nLet’s try this out with the least terns data and the class function:\n\nmap(terns, class)\n\n$year\n[1] \"integer\"\n\n$site_name\n[1] \"character\"\n\n$site_name_2013_2018\n[1] \"character\"\n\n$site_name_1988_2001\n[1] \"character\"\n\n$site_abbr\n[1] \"character\"\n\n$region_3\n[1] \"character\"\n\n$region_4\n[1] \"character\"\n\n$event\n[1] \"character\"\n\n$bp_min\n[1] \"numeric\"\n\n$bp_max\n[1] \"numeric\"\n\n$fl_min\n[1] \"integer\"\n\n$fl_max\n[1] \"integer\"\n\n$total_nests\n[1] \"integer\"\n\n$nonpred_eggs\n[1] \"integer\"\n\n$nonpred_chicks\n[1] \"integer\"\n\n$nonpred_fl\n[1] \"integer\"\n\n$nonpred_ad\n[1] \"integer\"\n\n$pred_control\n[1] \"character\"\n\n$pred_eggs\n[1] \"integer\"\n\n$pred_chicks\n[1] \"integer\"\n\n$pred_fl\n[1] \"integer\"\n\n$pred_ad\n[1] \"integer\"\n\n$pred_pefa\n[1] \"character\"\n\n$pred_coy_fox\n[1] \"character\"\n\n$pred_meso\n[1] \"character\"\n\n$pred_owlspp\n[1] \"character\"\n\n$pred_corvid\n[1] \"character\"\n\n$pred_other_raptor\n[1] \"character\"\n\n$pred_other_avian\n[1] \"character\"\n\n$pred_misc\n[1] \"character\"\n\n$total_pefa\n[1] \"integer\"\n\n$total_coy_fox\n[1] \"integer\"\n\n$total_meso\n[1] \"integer\"\n\n$total_owlspp\n[1] \"integer\"\n\n$total_corvid\n[1] \"integer\"\n\n$total_other_raptor\n[1] \"integer\"\n\n$total_other_avian\n[1] \"integer\"\n\n$total_misc\n[1] \"integer\"\n\n$first_observed\n[1] \"character\"\n\n$last_observed\n[1] \"character\"\n\n$first_nest\n[1] \"character\"\n\n$first_chick\n[1] \"character\"\n\n$first_fledge\n[1] \"character\"\n\n\nThe result is similar to if the class function was vectorized. In fact, if we use a vector and a vectorized function with map, the result is nearly identical to the result from vectorization:\n\nx = c(1, 2, pi)\n\nsin(x)\n\n[1] 8.414710e-01 9.092974e-01 1.224647e-16\n\nmap(x, sin)\n\n[[1]]\n[1] 0.841471\n\n[[2]]\n[1] 0.9092974\n\n[[3]]\n[1] 1.224647e-16\n\n\nThe only difference is that the result from map is a list. In fact, the map function always returns a list with one element for each element of the input data.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to use map to repeat arbitrary code rather than a specific function, you can use \\() or the function keyword to create an anonymous function (one that isn’t assigned to a variable). Then the syntax of the map function is:\n\nmap(data, \\(element) {\n  # This code runs once for each element in data.\n})\n\n\n\n\n\n\n\n\n\nNoteNote: Apply Functions\n\n\n\nR’s apply functions are a built-in equivalent to map functions. The lapply, sapply, and tapply functions are the three most important functions in the family of apply functions, but there are many more. The lapply function is nearly identical to the map function.\nWe focus on and recommend the map functions rather than the apply functions because they are more consistent in their syntax and specific in their return types. You can learn more about R’s apply functions by reading this StackOverflow post.\n\n\n\n26.4.1 Other Map Functions\nThe purrr package provides many different map functions, all of which have names that start with map. All of them call another function on each element of a data structure. They also all have the same syntax. Where they differ is in how they return results. A few of these are shown in Table 26.1.\n\n\n\nTable 26.1\n\n\n\n\n\nFunction\nReturn Type\n\n\n\n\nmap_lgl\nlogical\n\n\nmap_int\ninteger\n\n\nmap_dbl\nnumeric (double)\n\n\nmap_chr\ncharacter\n\n\nmap\nlist\n\n\n\n\n\n\n\nLet’s look at some examples of the other map functions. If we use map_chr to find the classes of the columns in the least terns data, we get a character vector:\n\nmap_chr(terns, class)\n\n               year           site_name site_name_2013_2018 site_name_1988_2001 \n          \"integer\"         \"character\"         \"character\"         \"character\" \n          site_abbr            region_3            region_4               event \n        \"character\"         \"character\"         \"character\"         \"character\" \n             bp_min              bp_max              fl_min              fl_max \n          \"numeric\"           \"numeric\"           \"integer\"           \"integer\" \n        total_nests        nonpred_eggs      nonpred_chicks          nonpred_fl \n          \"integer\"           \"integer\"           \"integer\"           \"integer\" \n         nonpred_ad        pred_control           pred_eggs         pred_chicks \n          \"integer\"         \"character\"           \"integer\"           \"integer\" \n            pred_fl             pred_ad           pred_pefa        pred_coy_fox \n          \"integer\"           \"integer\"         \"character\"         \"character\" \n          pred_meso         pred_owlspp         pred_corvid   pred_other_raptor \n        \"character\"         \"character\"         \"character\"         \"character\" \n   pred_other_avian           pred_misc          total_pefa       total_coy_fox \n        \"character\"         \"character\"           \"integer\"           \"integer\" \n         total_meso        total_owlspp        total_corvid  total_other_raptor \n          \"integer\"           \"integer\"           \"integer\"           \"integer\" \n  total_other_avian          total_misc      first_observed       last_observed \n          \"integer\"           \"integer\"         \"character\"         \"character\" \n         first_nest         first_chick        first_fledge \n        \"character\"         \"character\"         \"character\" \n\n\nLikewise, if we use map_dbl to compute the sin values, we get a numeric vector, the same as from vectorization:\n\nmap_dbl(x, sin)\n\n[1] 8.414710e-01 9.092974e-01 1.224647e-16\n\n\nIn spite of that, vectorization is still more efficient than sapply, so use vectorization instead when possible.\n\nThe purrr documentation provides more details about how to use the many functions in the package.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html#sec-for-loops",
    "href": "chapters/week08/control-flow.html#sec-for-loops",
    "title": "26  Control Flow",
    "section": "26.5 For Loops",
    "text": "26.5 For Loops\nA for loop runs the code in its body once for each element of a data structure. Figure 26.3 shows what for loop looks like in a flowchart.\nIn most programming languages, the keyword for creates a for loop. In R, the syntax of a for loop is:\n\nfor (element in data) {\n  # This code runs once for each element in data.\n}\n\nThe loop automatically assigns the next element of data to the variable element at the beginning of each iteration. The loop iterates once for each element, unless a keyword causes the loop to end early.\n\n\n\n\n\n\nTip\n\n\n\nFor loops and map functions do almost the same thing. Here’s how the for loop syntax above translates into a map function:\n\nmap(data, \\(element) {\n  # This code runs once for each element in data.\n})\n\nMap functions tend to be easier to use because they automatically return the result from each iteration. For loops don’t, and leave it up to you to figure out how to store the results.\nThe limitation of map functions is that the each iteration must be independent. You can’t have an iteration that depends on the result of a prior iteration. For loops don’t have this limitation. Dependent iterations are common in simulations, but not so common in data cleaning and analysis tasks.\n\n\nAs a demonstration, let’s print out a message with the current iteration number for 5 iterations:\n\nfor (i in 1:5) {\n  message(\"Hi from iteration \", i)\n}\n\nHi from iteration 1\n\n\nHi from iteration 2\n\n\nHi from iteration 3\n\n\nHi from iteration 4\n\n\nHi from iteration 5\n\n\nUnlike map functions, loops don’t return a result automatically. It’s up to you to use variables to store any results you want to use later. To do this:\n\nBefore the loop, create a result vector (or list) that’s the same length as the number of iterations. Fill vector with 0s or some other placeholder value. You can use functions such as integer, numeric, and character to do this.\nIn the loop, use indexing to replace the elements of the result vector as they’re computed.\n\nThis approach to storing results is called pre-allocation. Here’s an example of pre-allocation for a loop that computes what happens if you repeatedly call the sine function on a value:\n\n# Number of iterations:\nn = 1 + 99\n\n# Create a numeric vector with n elements and set the first element to 1.\nresult = numeric(n)\nresult[1] = 1\n\nfor (i in 2:n) {\n  # Get the result from the previous iteration.\n  # This is why this loop starts at i = 2.\n  prev_result = result[i - 1]\n\n  # Compute the sine of the previous result.\n  # Save it into the results vector at position i.\n  result[i] = sin(prev_result)\n}\n\nPlotting the result vector makes it easier to see the pattern in the values:\n\nlibrary(\"ggplot2\")\n\nresult_df = data.frame(x = 1:n, y = result)\nggplot(result_df) + geom_point() + aes(x = x, y = y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe break keyword causes a for loop to immediately exit. It only makes sense to use break inside of a conditional (otherwise, the loop will exit in the first iteration).\nThe next keyword causes a for loop to immediately go to the next iteration. As with break, it only makes sense to use next inside of a conditional.\nYou can also use break and next in while loops.\n\n\n\n\n\n\n\n\nNoteNote: While Loops\n\n\n\nA while loop makes the computer check a condition at the beginning of each iteration to decide whether to run the code in the loop’s body. Figure 26.2 shows what a while loop looks like in a flowchart.\nIn most programming languages, the keyword while creates a while loop. In R, the syntax of a while loop is:\n\nwhile (condition) {\n  # This code runs repeatedly until the condition is FALSE.\n}\n\nWhile loops are a generalization of for loops, and only do the bare minimum necessary to iterate. They tend to be most useful when you don’t know how many iterations are necessary to complete a task.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html#sec-how-to-write-loop",
    "href": "chapters/week08/control-flow.html#sec-how-to-write-loop",
    "title": "26  Control Flow",
    "section": "26.6 How to Write a Loop",
    "text": "26.6 How to Write a Loop\nBefore you write a loop, write down what you want the code to do. Think about whether the code needs to do something over and over. If it doesn’t, you probably shouldn’t use a loop. Do this in comments, in a separate document, or even on paper.\nOnce you’ve finished writing out and clarifying the goal, it’s time to write the code. Go through these steps:\n\nWrite the code for one iteration. Don’t worry about putting the code in a loop yet. Make sure that the result is correct. Sometimes the best way to approach this is by writing a function (see Section 25.3).\nMake a copy of the code from step 1 and edit it to run a different iteration. Pay attention to what you have to change. Once again, make sure that the result is correct.\nOnce you’ve got the code working for two iterations, make another copy and wrap it in a loop. The parts of the code that had to change in step 2 are a hint about what will change in every iteration. Replace changing data with the loop’s automatic variable. Put code that should only run on some of the iterations in conditionals.\nTest that the loop code works correctly. If it doesn’t, try to pinpoint which iteration is causing the problem. One way to do this is to use message to print out information. Then write out the code for just the broken iteration, get it working, and repeat this process from step 3.\n\nTo decide what kind of loop to use, go down this list:\n\nVectorization\nMap (or apply) functions\n\nTry an apply function if iterations are independent.\n\nFor and while loops\n\nTry a for-loop if some iterations depend on others.\nTry a while-loop if the number of iterations is unknown.\n\nRecursion\n\nConvenient for naturally recursive tasks (like Fibonacci), but often there are faster solutions.\n\n\nThe list items are organized from fastest to slowest, although the speed difference between map functions and for loops is negligible.\nVectorization is the most efficient and concise, but only works with specific functions on vectors. Map functions work with any function and any data structure with elements, while still returning the results automatically. For and while loops provide the most flexibility, but leave it up to you to save the results. Recursion works well for specific kinds of problems, but can be confusing and is the slowest kind of loop in R.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html#sec-case-study-fuel-3",
    "href": "chapters/week08/control-flow.html#sec-case-study-fuel-3",
    "title": "26  Control Flow",
    "section": "26.7 Case Study: U.S. Alternative Fueling Stations, Part III",
    "text": "26.7 Case Study: U.S. Alternative Fueling Stations, Part III\nLet’s finish the U.S. Alternative Fueling Stations case study we began in Section 25.5 and continued in Section 26.2. In the previous part, we modified the read_fuel_sheet function so that it can read and clean sheets for any year in the dataset Excel file:\n\nread_fuel_sheet = function(path, year) {\n  sheet = as.character(year)\n  stations = read_excel(path, sheet = sheet, skip = 1)\n\n  # Clean up the column names.\n  names = names(stations)\n  if (year &lt; 2014) {\n    is_dot_name = str_starts(names, fixed(\"...\"))\n    names[is_dot_name] = as.character(stations[2, is_dot_name])\n  }\n  names = str_to_lower(names)\n  names = str_replace_all(names, fixed(\" \"), \"_\")\n  names(stations) = names\n  if (year &gt;= 2014) {\n    stations = rename(\n      stations,\n      electric = electrica,\n      hydrogen = hydrogenb,\n      propane = propanec,\n      total = totald\n    )\n  }\n\n  # Remove blank rows.\n  stations = filter(stations, !is.na(biodiesel))\n\n  # Correct column types and add year column.\n  if (year &gt;= 2014) {\n    electric = str_replace_all(stations$electric, fixed(\",\"), \"\")\n    electric = str_split_fixed(electric, fixed(\" | \"), 2)\n    stations$electric = as.numeric(electric[, 1])\n    stations$electric_outlets = as.numeric(electric[, 2])\n    # TODO: hydrogen, propane\n  }\n  stations$electric = as.numeric(stations$electric)\n  # TODO: hydrogen, propane\n  stations$year = year\n\n  stations\n}\n\nIn this part, let’s use the function to read all of the sheets, then combine them into a single data frame and make a plot of number of electric stations over time. Imagine what it would look like if we wrote out the code to read the data for each year separately:\n\npath = \"data/2007-2023_us_alt_fuels.xlsx\"\nread_fuel_sheet(path, 2007)\nread_fuel_sheet(path, 2008)\n# ...\nread_fuel_sheet(path, 2023)\n\nThe year is what differs between the calls, so we need to iterate over the years. We could create a vector of years with 2007:2023. Instead, let’s use readxl’s excel_sheets function to get the sheet names, which correspond to years, and convert them to numbers:\n\nyears = excel_sheets(path)\n# Skip the first sheet (which is documentation) and convert to numbers.\nyears = as.numeric(years[-1])\nyears\n\n [1] 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009\n[16] 2008 2007\n\n\nNow we need to call the read_fuel_sheet function with each year in the years vector. We can use purrr’s map function to do this. Since the year should be the second argument to read_fuel_sheet, we have to write the call out explicitly (alternatively, we could modify read_fuel_sheet so that the year is the first argument):\n\nsheets = map(years, \\(year) {\n  read_fuel_sheet(path, year)\n})\n\nThis gives a list of data frames, one for each year. We want to bind them into a single data frame, stacking the rows of each one atop the next. We can use dplyr’s bind_rows function to do this:\n\nstations = bind_rows(sheets)\nhead(stations)\n\n# A tibble: 6 × 12\n  state   biodiesel   cng   e85 electric hydrogen   lng propane renewable_diesel\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n1 Alabama         9    28    34      424 0 | 0 |…     2 25 | 3…                0\n2 Alaska          0     0     0       65 0 | 0 |…     0 1 | 0 …                0\n3 Arizona        77    26    21     1198 0 | 0 |…     5 42 | 3…                0\n4 Arkans…        34    14    74      334 0 | 0 |…     0 17 | 1…                0\n5 Califo…        48   292   369    16381 57 | 7 …    39 136 | …              597\n6 Colora…         5    25    95     2165 0 | 1 |…     1 28 | 2…                0\n# ℹ 3 more variables: total &lt;dbl&gt;, electric_outlets &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nWith the data in good shape, we’re ready to make a plot. Let’s make a line plot with the year on the x-axis, the number of electric stations on the y-axis, and the lines color-coded by state:\n\nggplot(stations) +\n  geom_line() +\n  aes(x = year, y = electric, color = state)\n\n\n\n\n\n\n\n\nThis plot is a bit hard to read because there are so many states. Let’s use dplyr’s filter function to focus only on states on the west coast:\n\nwest = filter(\n  stations,\n  state %in% c(\"California\", \"Oregon\", \"Washington\", \"Alaska\", \"Hawaii\")\n)\n\nggplot(west) +\n  geom_line() +\n  aes(x = year, y = electric, color = state)\n\n\n\n\n\n\n\n\nNow we can see that California has substantially more electric stations than the other west coast states. This could be due to its larger population or due to increased funding for alternative fueling stations.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html#case-study-ca-hospital-utilization",
    "href": "chapters/week08/control-flow.html#case-study-ca-hospital-utilization",
    "title": "26  Control Flow",
    "section": "26.8 Case Study: CA Hospital Utilization",
    "text": "26.8 Case Study: CA Hospital Utilization\nThe California Department of Health Care Access and Information (HCAI) requires hospitals in the state to submit detailed information each year about how many beds they have and the total number of days for which each bed was occupied. The HCAI publishes the data to the California Open Data Portal. Let’s use R to read data from 2016 to 2023 and investigate whether hospital utilization is noticeably different in and after 2020.\nThe dataset consists of a separate Microsoft Excel file for each year. Before 2018, HCAI used a data format (in Excel) called ALIRTS. In 2018, they started collecting more data and switched to a data format called SIERA. The 2018 data file contains a crosswalk that shows the correspondence between SIERA columns and ALIRTS columns.\n\n\n\n\n\n\nImportant\n\n\n\nClick here to download the CA Hospital Utilization dataset (8 Excel files).\nIf you haven’t already, we recommend you create a directory for this workshop. In your workshop directory, create a data/ca_hospitals subdirectory. Download and save the dataset in the data/ca_hospitals subdirectory.\n\n\nWhen you need to solve a programming problem, get started by writing some comments that describe the problem, the inputs, and the expected output. Try to be concrete. This will help you clarify what you’re trying to achieve and serve as a guiding light while you work.\nAs a programmer (or any kind of problem-solver), you should always be on the lookout for ways to break problems into smaller, simpler steps. Think about this when you frame a problem. Small steps are easier to reason about, implement, and test. When you complete one, you also get a nice sense of progress towards your goal.\nFor the CA Hospital Utilization dataset, our goal is to investigate whether there was a change in hospital utilization in 2020. Before we can do any investigation, we need to read the files into R. The files all contain tabular data and have similar formats, so let’s try to combine them into a single data frame. We’ll say this in the framing comments:\n\n# Read the CA Hospital Utilization dataset into R. The inputs are yearly Excel\n# files (2016-2023) that need to be combined. The pre-2018 files have a\n# different format from the others. The result should be a single data frame\n# with information about bed and patient counts.\n#\n# After reading the dataset, we'll investigate utilization in 2020.\n\n“Investigate utilization” is a little vague, but for an exploratory data analysis, it’s hard to say exactly what to do until you’ve started working with the data.\nWe need to read multiple files, but we can simplify the problem by starting with just one. Let’s start with the 2023 data. It’s in an Excel file, which you can read with the read_excel function from the readxl package. If it’s your first time using the readxl package, you’ll need to install it:\n\ninstall.packages(\"readxl\")\n\nThe read_excel function requires the path to the file as the first argument. You can optionally provide the sheet name or number (starting from 1) as the second argument. Open up the Excel file in your computer’s spreadsheet program and take a look. There are multiple sheets, and the data about beds and patients are in the second sheet. Back in R, read just the second sheet:\n\nlibrary(\"readxl\")\n\npath = \"data/ca_hospitals/hosp23_util_data_final.xlsx\"\nsheet = read_excel(path, sheet = 2)\n\nNew names:\n• `` -&gt; `...355`\n\nhead(sheet)\n\n# A tibble: 6 × 355\n  Description            FAC_NO FAC_NAME FAC_STR_ADDR FAC_CITY FAC_ZIP FAC_PHONE\n  &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    \n1 FINAL 2023 UTILIZATIO… FINAL… FINAL 2… FINAL 2023 … FINAL 2… FINAL … FINAL 20…\n2 Page                   1.0    1.0      1.0          1.0      1.0     1.0      \n3 Column                 1.0    1.0      1.0          1.0      1.0     1.0      \n4 Line                   2.0    1.0      3.0          4.0      5.0     6.0      \n5 &lt;NA&gt;                   10601… ALAMEDA… 2070 CLINTO… ALAMEDA  94501   51023337…\n6 &lt;NA&gt;                   10601… ALTA BA… 2450 ASHBY … BERKELEY 94705   510-655-…\n# ℹ 348 more variables: FAC_ADMIN_NAME &lt;chr&gt;, FAC_OPERATED_THIS_YR &lt;chr&gt;,\n#   FAC_OP_PER_BEGIN_DT &lt;chr&gt;, FAC_OP_PER_END_DT &lt;chr&gt;,\n#   FAC_PAR_CORP_NAME &lt;chr&gt;, FAC_PAR_CORP_BUS_ADDR &lt;chr&gt;,\n#   FAC_PAR_CORP_CITY &lt;chr&gt;, FAC_PAR_CORP_STATE &lt;chr&gt;, FAC_PAR_CORP_ZIP &lt;chr&gt;,\n#   REPT_PREP_NAME &lt;chr&gt;, SUBMITTED_DT &lt;chr&gt;, REV_REPT_PREP_NAME &lt;chr&gt;,\n#   REVISED_DT &lt;chr&gt;, CORRECTED_DT &lt;chr&gt;, LICENSE_NO &lt;chr&gt;,\n#   LICENSE_EFF_DATE &lt;chr&gt;, LICENSE_EXP_DATE &lt;chr&gt;, LICENSE_STATUS &lt;chr&gt;, …\n\n\nThe first four rows contain metadata about the columns. The first hospital, Alameda Hospital, is listed in the fifth row. So let’s remove the first four rows:\n\nsheet = sheet[-(1:4), ]\nhead(sheet)\n\n# A tibble: 6 × 355\n  Description FAC_NO    FAC_NAME         FAC_STR_ADDR FAC_CITY FAC_ZIP FAC_PHONE\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    \n1 &lt;NA&gt;        106010735 ALAMEDA HOSPITAL 2070 CLINTO… ALAMEDA  94501   51023337…\n2 &lt;NA&gt;        106010739 ALTA BATES SUMM… 2450 ASHBY … BERKELEY 94705   510-655-…\n3 &lt;NA&gt;        106010776 UCSF BENIOFF CH… 747 52ND ST… OAKLAND  94609   510-428-…\n4 &lt;NA&gt;        106010811 FAIRMONT HOSPIT… 15400 FOOTH… SAN LEA… 94578   51043748…\n5 &lt;NA&gt;        106010844 ALTA BATES SUMM… 2001 DWIGHT… BERKELEY 94704   510-655-…\n6 &lt;NA&gt;        106010846 HIGHLAND HOSPIT… 1411 EAST 3… OAKLAND  94602   51043748…\n# ℹ 348 more variables: FAC_ADMIN_NAME &lt;chr&gt;, FAC_OPERATED_THIS_YR &lt;chr&gt;,\n#   FAC_OP_PER_BEGIN_DT &lt;chr&gt;, FAC_OP_PER_END_DT &lt;chr&gt;,\n#   FAC_PAR_CORP_NAME &lt;chr&gt;, FAC_PAR_CORP_BUS_ADDR &lt;chr&gt;,\n#   FAC_PAR_CORP_CITY &lt;chr&gt;, FAC_PAR_CORP_STATE &lt;chr&gt;, FAC_PAR_CORP_ZIP &lt;chr&gt;,\n#   REPT_PREP_NAME &lt;chr&gt;, SUBMITTED_DT &lt;chr&gt;, REV_REPT_PREP_NAME &lt;chr&gt;,\n#   REVISED_DT &lt;chr&gt;, CORRECTED_DT &lt;chr&gt;, LICENSE_NO &lt;chr&gt;,\n#   LICENSE_EFF_DATE &lt;chr&gt;, LICENSE_EXP_DATE &lt;chr&gt;, LICENSE_STATUS &lt;chr&gt;, …\n\n\nSome datasets also have metadata in the last rows, so let’s check for that here:\n\ntail(sheet)\n\n# A tibble: 6 × 355\n  Description FAC_NO    FAC_NAME         FAC_STR_ADDR FAC_CITY FAC_ZIP FAC_PHONE\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    \n1 &lt;NA&gt;        106574010 SUTTER DAVIS HO… 2000 SUTTER… DAVIS    95616   (530) 75…\n2 &lt;NA&gt;        106580996 ADVENTIST HEALT… 726 FOURTH … MARYSVI… 95901   530-751-…\n3 &lt;NA&gt;        206100718 COMMUNITY SUBAC… 3003 NORTH … FRESNO   93703   559-459-…\n4 &lt;NA&gt;        206274027 WESTLAND HOUSE   100 BARNET … MONTEREY 93940   83162453…\n5 &lt;NA&gt;        206351814 HAZEL HAWKINS M… 900 SUNSET … HOLLIST… 95023   831-635-…\n6 &lt;NA&gt;        &lt;NA&gt;      n = 506          &lt;NA&gt;         &lt;NA&gt;     &lt;NA&gt;    &lt;NA&gt;     \n# ℹ 348 more variables: FAC_ADMIN_NAME &lt;chr&gt;, FAC_OPERATED_THIS_YR &lt;chr&gt;,\n#   FAC_OP_PER_BEGIN_DT &lt;chr&gt;, FAC_OP_PER_END_DT &lt;chr&gt;,\n#   FAC_PAR_CORP_NAME &lt;chr&gt;, FAC_PAR_CORP_BUS_ADDR &lt;chr&gt;,\n#   FAC_PAR_CORP_CITY &lt;chr&gt;, FAC_PAR_CORP_STATE &lt;chr&gt;, FAC_PAR_CORP_ZIP &lt;chr&gt;,\n#   REPT_PREP_NAME &lt;chr&gt;, SUBMITTED_DT &lt;chr&gt;, REV_REPT_PREP_NAME &lt;chr&gt;,\n#   REVISED_DT &lt;chr&gt;, CORRECTED_DT &lt;chr&gt;, LICENSE_NO &lt;chr&gt;,\n#   LICENSE_EFF_DATE &lt;chr&gt;, LICENSE_EXP_DATE &lt;chr&gt;, LICENSE_STATUS &lt;chr&gt;, …\n\n\nSure enough, the last row contains what appears to be a count of the hospitals rather than a hospital. Let’s remove it by calling head with a negative number of elements, which removes that many elements from the end:\n\nsheet = head(sheet, -1)\ntail(sheet)\n\n# A tibble: 6 × 355\n  Description FAC_NO    FAC_NAME         FAC_STR_ADDR FAC_CITY FAC_ZIP FAC_PHONE\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    \n1 &lt;NA&gt;        106571086 WOODLAND MEMORI… 1325 COTTON… WOODLAND 95695   (530) 66…\n2 &lt;NA&gt;        106574010 SUTTER DAVIS HO… 2000 SUTTER… DAVIS    95616   (530) 75…\n3 &lt;NA&gt;        106580996 ADVENTIST HEALT… 726 FOURTH … MARYSVI… 95901   530-751-…\n4 &lt;NA&gt;        206100718 COMMUNITY SUBAC… 3003 NORTH … FRESNO   93703   559-459-…\n5 &lt;NA&gt;        206274027 WESTLAND HOUSE   100 BARNET … MONTEREY 93940   83162453…\n6 &lt;NA&gt;        206351814 HAZEL HAWKINS M… 900 SUNSET … HOLLIST… 95023   831-635-…\n# ℹ 348 more variables: FAC_ADMIN_NAME &lt;chr&gt;, FAC_OPERATED_THIS_YR &lt;chr&gt;,\n#   FAC_OP_PER_BEGIN_DT &lt;chr&gt;, FAC_OP_PER_END_DT &lt;chr&gt;,\n#   FAC_PAR_CORP_NAME &lt;chr&gt;, FAC_PAR_CORP_BUS_ADDR &lt;chr&gt;,\n#   FAC_PAR_CORP_CITY &lt;chr&gt;, FAC_PAR_CORP_STATE &lt;chr&gt;, FAC_PAR_CORP_ZIP &lt;chr&gt;,\n#   REPT_PREP_NAME &lt;chr&gt;, SUBMITTED_DT &lt;chr&gt;, REV_REPT_PREP_NAME &lt;chr&gt;,\n#   REVISED_DT &lt;chr&gt;, CORRECTED_DT &lt;chr&gt;, LICENSE_NO &lt;chr&gt;,\n#   LICENSE_EFF_DATE &lt;chr&gt;, LICENSE_EXP_DATE &lt;chr&gt;, LICENSE_STATUS &lt;chr&gt;, …\n\n\nThere are a lot of columns in sheet, so let’s make a list of just a few that we’ll use for analysis. We’ll keep:\n\nColumns with facility name, location, and operating status\nAll of the columns whose names start with TOT, because these are totals for number of beds, number of census-days, and so on.\nColumns about acute respiratory beds, with names that contain RESPIRATORY, because they might also be relevant.\n\nWe can use the stringr package, which provides string processing functions, to help us get the TOT and RESPIRATORY column names. If it’s your first time using the stringr package, you’ll have to install it:\ninstall.packages(\"stringr\")\nWe can use stringr’s str_starts function to check whether column names start with TOT and its str_detect function to check whether column names contain RESPIRATORY:\n\nlibrary(\"stringr\")\n\nfacility_cols = c(\n  \"FAC_NAME\", \"FAC_CITY\", \"FAC_ZIP\", \"FAC_OPERATED_THIS_YR\", \"FACILITY_LEVEL\",\n  \"TEACH_HOSP\", \"COUNTY\", \"PRIN_SERVICE_TYPE\"\n)\n\ncols = names(sheet)\ntot_cols = cols[str_starts(cols, \"TOT\")]\nrespiratory_cols = cols[str_detect(cols, \"RESPIRATORY\")]\n\nThe TOT and RESPIRATORY columns all contain numbers, but the element type is character, so let’s cast to numbers them with as.numeric. We’ll also add a column with the year:\n\nnumeric_cols = c(tot_cols, respiratory_cols)\nsheet[numeric_cols] = lapply(sheet[numeric_cols], as.numeric)\n\nsheet$year = 2023\n\nNow we’ll select only the columns we identified as useful:\n\nsheet = sheet[c(\"year\", facility_cols, numeric_cols)]\n\nhead(sheet)\n\n# A tibble: 6 × 22\n   year FAC_NAME FAC_CITY FAC_ZIP FAC_OPERATED_THIS_YR FACILITY_LEVEL TEACH_HOSP\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;                &lt;chr&gt;          &lt;chr&gt;     \n1  2023 ALAMEDA… ALAMEDA  94501   Yes                  Parent Facili… No        \n2  2023 ALTA BA… BERKELEY 94705   Yes                  Parent Facili… No        \n3  2023 UCSF BE… OAKLAND  94609   Yes                  Parent Facili… No        \n4  2023 FAIRMON… SAN LEA… 94578   Yes                  Consolidated … No        \n5  2023 ALTA BA… BERKELEY 94704   Yes                  Consolidated … No        \n6  2023 HIGHLAN… OAKLAND  94602   Yes                  Parent Facili… No        \n# ℹ 15 more variables: COUNTY &lt;chr&gt;, PRIN_SERVICE_TYPE &lt;chr&gt;,\n#   TOT_LIC_BEDS &lt;dbl&gt;, TOT_LIC_BED_DAYS &lt;dbl&gt;, TOT_DISCHARGES &lt;dbl&gt;,\n#   TOT_CEN_DAYS &lt;dbl&gt;, TOT_ALOS_CY &lt;dbl&gt;, TOT_ALOS_PY &lt;dbl&gt;,\n#   ACUTE_RESPIRATORY_CARE_LIC_BEDS &lt;dbl&gt;,\n#   ACUTE_RESPIRATORY_CARE_LIC_BED_DAYS &lt;dbl&gt;,\n#   ACUTE_RESPIRATORY_CARE_DISCHARGES &lt;dbl&gt;,\n#   ACUTE_RESPIRATORY_CARE_INTRA_TRANSFERS &lt;dbl&gt;, …\n\n\nLowercase names are easier to type, so let’s also make all of the names lowercase with stringr’s str_to_lower function:\n\nnames(sheet) = str_to_lower(names(sheet))\nhead(sheet)\n\n# A tibble: 6 × 22\n   year fac_name fac_city fac_zip fac_operated_this_yr facility_level teach_hosp\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;                &lt;chr&gt;          &lt;chr&gt;     \n1  2023 ALAMEDA… ALAMEDA  94501   Yes                  Parent Facili… No        \n2  2023 ALTA BA… BERKELEY 94705   Yes                  Parent Facili… No        \n3  2023 UCSF BE… OAKLAND  94609   Yes                  Parent Facili… No        \n4  2023 FAIRMON… SAN LEA… 94578   Yes                  Consolidated … No        \n5  2023 ALTA BA… BERKELEY 94704   Yes                  Consolidated … No        \n6  2023 HIGHLAN… OAKLAND  94602   Yes                  Parent Facili… No        \n# ℹ 15 more variables: county &lt;chr&gt;, prin_service_type &lt;chr&gt;,\n#   tot_lic_beds &lt;dbl&gt;, tot_lic_bed_days &lt;dbl&gt;, tot_discharges &lt;dbl&gt;,\n#   tot_cen_days &lt;dbl&gt;, tot_alos_cy &lt;dbl&gt;, tot_alos_py &lt;dbl&gt;,\n#   acute_respiratory_care_lic_beds &lt;dbl&gt;,\n#   acute_respiratory_care_lic_bed_days &lt;dbl&gt;,\n#   acute_respiratory_care_discharges &lt;dbl&gt;,\n#   acute_respiratory_care_intra_transfers &lt;dbl&gt;, …\n\n\nWe’ve successfully read one of the files! Since the 2018-2023 files all have the same format, it’s likely that we can use almost the same code for all of them. Any time you want to reuse code, it’s a sign that you should write a function, so that’s what we’ll do. We’ll take all of the code we have so far and put it in the body of a function called read_hospital_data, adding some comments to indicate the steps:\nread_hospital_data = function() {\n  # Read the 2nd sheet of the file.\n  path = \"data/ca_hospitals/hosp23_util_data_final.xlsx\"\n  sheet = read_excel(path, sheet = 2)\n\n  # Remove the first 4 and last row.\n  sheet = sheet[-(1:4), ]\n  sheet = head(sheet, -1)\n\n  # Select only a few columns of interest.\n  facility_cols = c(\n    \"FAC_NAME\", \"FAC_CITY\", \"FAC_ZIP\", \"FAC_OPERATED_THIS_YR\",\n    \"FACILITY_LEVEL\", \"TEACH_HOSP\", \"COUNTY\", \"PRIN_SERVICE_TYPE\"\n  )\n\n  cols = names(sheet)\n  tot_cols = cols[str_starts(cols, \"TOT\")]\n  respiratory_cols = cols[str_detect(cols, \"RESPIRATORY\")]\n\n  numeric_cols = c(tot_cols, respiratory_cols)\n  sheet[numeric_cols] = lapply(sheet[numeric_cols], as.numeric)\n\n  sheet$year = 2023\n\n  sheet = sheet[c(\"year\", facility_cols, numeric_cols)]\n\n  # Rename the columns to lowercase.\n  names(sheet) = str_to_lower(names(sheet))\n\n  sheet\n}\nAs it is, the function still only reads the 2023 file. The other files have different paths, so the first thing we need to do is make the path variable a parameter. We’ll also make a year parameter, for the year value inserted as a column:\n\nread_hospital_data = function(path, year) {\n  # Read the 2nd sheet of the file.\n  sheet = read_excel(path, sheet = 2)\n\n  # Remove the first 4 and last row.\n  sheet = sheet[-(1:4), ]\n  sheet = head(sheet, -1)\n\n  # Select only a few columns of interest.\n  facility_cols = c(\n    \"FAC_NAME\", \"FAC_CITY\", \"FAC_ZIP\", \"FAC_OPERATED_THIS_YR\",\n    \"FACILITY_LEVEL\", \"TEACH_HOSP\", \"COUNTY\", \"PRIN_SERVICE_TYPE\"\n  )\n\n  cols = names(sheet)\n  tot_cols = cols[str_starts(cols, \"TOT\")]\n  respiratory_cols = cols[str_detect(cols, \"RESPIRATORY\")]\n\n  numeric_cols = c(tot_cols, respiratory_cols)\n  sheet[numeric_cols] = lapply(sheet[numeric_cols], as.numeric)\n\n  sheet$year = year\n\n  sheet = sheet[c(\"year\", facility_cols, numeric_cols)]\n\n  # Rename the columns to lowercase.\n  names(sheet) = str_to_lower(names(sheet))\n\n  sheet\n}\n\nTest the function out on a few of the files to make sure it works correctly:\n\nhead(\n  read_hospital_data(\n    \"data/ca_hospitals/hosp23_util_data_final.xlsx\", 2023\n  )\n)\n\nNew names:\n• `` -&gt; `...355`\n\n\n# A tibble: 6 × 22\n   year fac_name fac_city fac_zip fac_operated_this_yr facility_level teach_hosp\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;                &lt;chr&gt;          &lt;chr&gt;     \n1  2023 ALAMEDA… ALAMEDA  94501   Yes                  Parent Facili… No        \n2  2023 ALTA BA… BERKELEY 94705   Yes                  Parent Facili… No        \n3  2023 UCSF BE… OAKLAND  94609   Yes                  Parent Facili… No        \n4  2023 FAIRMON… SAN LEA… 94578   Yes                  Consolidated … No        \n5  2023 ALTA BA… BERKELEY 94704   Yes                  Consolidated … No        \n6  2023 HIGHLAN… OAKLAND  94602   Yes                  Parent Facili… No        \n# ℹ 15 more variables: county &lt;chr&gt;, prin_service_type &lt;chr&gt;,\n#   tot_lic_beds &lt;dbl&gt;, tot_lic_bed_days &lt;dbl&gt;, tot_discharges &lt;dbl&gt;,\n#   tot_cen_days &lt;dbl&gt;, tot_alos_cy &lt;dbl&gt;, tot_alos_py &lt;dbl&gt;,\n#   acute_respiratory_care_lic_beds &lt;dbl&gt;,\n#   acute_respiratory_care_lic_bed_days &lt;dbl&gt;,\n#   acute_respiratory_care_discharges &lt;dbl&gt;,\n#   acute_respiratory_care_intra_transfers &lt;dbl&gt;, …\n\n\n\nhead(\n  read_hospital_data(\n    \"data/ca_hospitals/hosp21_util_data_final-revised-06.15.2023.xlsx\", 2021\n  )\n)\n\n# A tibble: 6 × 22\n   year fac_name fac_city fac_zip fac_operated_this_yr facility_level teach_hosp\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;                &lt;chr&gt;          &lt;chr&gt;     \n1  2021 ALAMEDA… ALAMEDA  94501   Yes                  Parent Facili… No        \n2  2021 ALTA BA… BERKELEY 94705   Yes                  Parent Facili… No        \n3  2021 UCSF BE… OAKLAND  94609   Yes                  Parent Facili… No        \n4  2021 FAIRMON… SAN LEA… 94578   Yes                  Consolidated … No        \n5  2021 ALTA BA… BERKELEY 94704   Yes                  Consolidated … No        \n6  2021 HIGHLAN… OAKLAND  94602   Yes                  Parent Facili… No        \n# ℹ 15 more variables: county &lt;chr&gt;, prin_service_type &lt;chr&gt;,\n#   tot_lic_beds &lt;dbl&gt;, tot_lic_bed_days &lt;dbl&gt;, tot_discharges &lt;dbl&gt;,\n#   tot_cen_days &lt;dbl&gt;, tot_alos_cy &lt;dbl&gt;, tot_alos_py &lt;dbl&gt;,\n#   acute_respiratory_care_lic_beds &lt;dbl&gt;,\n#   acute_respiratory_care_lic_bed_days &lt;dbl&gt;,\n#   acute_respiratory_care_discharges &lt;dbl&gt;,\n#   acute_respiratory_care_intra_transfers &lt;dbl&gt;, …\n\n\nThe function appears to work correctly for two of the files, so let’s work towards trying it on all of the 2018-2023 files. We can use the built-in list.files function to get the paths to the files by setting full.names = TRUE (otherwise it just returns the names of the files):\n\npaths = list.files(\"data/ca_hospitals/\", full.names = TRUE)\npaths\n\n[1] \"data/ca_hospitals//hosp16_util_data_final.xlsx\"                   \n[2] \"data/ca_hospitals//hosp17_util_data_final.xlsx\"                   \n[3] \"data/ca_hospitals//hosp18_util_data_final.xlsx\"                   \n[4] \"data/ca_hospitals//hosp19_util_data_final.xlsx\"                   \n[5] \"data/ca_hospitals//hosp20_util_data_final-revised-06.15.2023.xlsx\"\n[6] \"data/ca_hospitals//hosp21_util_data_final-revised-06.15.2023.xlsx\"\n[7] \"data/ca_hospitals//hosp22_util_data_final_revised_11.28.2023.xlsx\"\n[8] \"data/ca_hospitals//hosp23_util_data_final.xlsx\"                   \n\n\nIn addition to the file paths, we also need the year for each file. Fortunately, the last two digits of the year are included in each file’s name. We can write a function to get these. We’ll use R’s built-in basename function to get the file names from the paths, and stringr’s str_sub function to get a substring (the year) from the name:\n\nget_hospital_year = function(path) {\n  name = basename(path)\n  year = str_sub(name, 5, 6)\n  as.integer(year) + 2000\n}\n\nget_hospital_year(paths[1])\n\n[1] 2016\n\n\n\nWith that done, we need to read the pre-2018 files. In the 2018 file, the fourth sheet is a crosswalk that shows which columns in the pre-2018 files correspond to columns in the later files. Let’s write some code to read the crosswalk. First, read the sheet:\n\npath = \"data/ca_hospitals/hosp18_util_data_final.xlsx\"\ncwalk = read_excel(path, sheet = 4)\nhead(cwalk)\n\n# A tibble: 6 × 6\n   Page  Line Column `SIERA Dataset Header (2018)` ALIRTS Dataset Header…¹ Notes\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                         &lt;chr&gt;                   &lt;chr&gt;\n1     1     1      1 FAC_NAME                      FAC_NAME                &lt;NA&gt; \n2     1     2      1 FAC_NO                        OSHPD_ID                &lt;NA&gt; \n3     1     3      1 FAC_STR_ADDR                  &lt;NA&gt;                    Addr…\n4     1     4      1 FAC_CITY                      FAC_CITY                &lt;NA&gt; \n5     1     5      1 FAC_ZIP                       FAC_ZIPCODE             &lt;NA&gt; \n6     1     6      1 FAC_PHONE                     FAC_PHONE               &lt;NA&gt; \n# ℹ abbreviated name: ¹​`ALIRTS Dataset Header (2017)`\n\n\nThe new and old column names are in the fourth and fifth columns, respectively, so we’ll get just those:\n\ncwalk = cwalk[, 4:5]\nhead(cwalk)\n\n# A tibble: 6 × 2\n  `SIERA Dataset Header (2018)` `ALIRTS Dataset Header (2017)`\n  &lt;chr&gt;                         &lt;chr&gt;                         \n1 FAC_NAME                      FAC_NAME                      \n2 FAC_NO                        OSHPD_ID                      \n3 FAC_STR_ADDR                  &lt;NA&gt;                          \n4 FAC_CITY                      FAC_CITY                      \n5 FAC_ZIP                       FAC_ZIPCODE                   \n6 FAC_PHONE                     FAC_PHONE                     \n\n\nFinally, let’s turn the cwalk data frame into a named vector. We’ll make the old column names the names and the new column names the elements. This way we can easily look up the new name for any of the old columns by indexing. Some of the column names in cwalk have extra spaces at the end, so we’ll use stringr’s str_trim function to remove them:\n\ncwalk_names = str_trim(cwalk[[2]])\ncwalk = str_trim(cwalk[[1]])\nnames(cwalk) = cwalk_names\n\nhead(cwalk)\n\n      FAC_NAME       OSHPD_ID           &lt;NA&gt;       FAC_CITY    FAC_ZIPCODE \n    \"FAC_NAME\"       \"FAC_NO\" \"FAC_STR_ADDR\"     \"FAC_CITY\"      \"FAC_ZIP\" \n     FAC_PHONE \n   \"FAC_PHONE\" \n\n\nWe can now define a new version of the read_hospital_data function that uses the crosswalk to change the column names when year &lt; 2018. Let’s also change function to exclude columns with ALOS in the name, because they have no equivalent in the pre-2018 files:\n\nread_hospital_data = function(path, year) {\n  # Read the 2nd sheet of the file.\n  sheet = read_excel(path, sheet = 2)\n\n  # Remove the first 4 and last row.\n  sheet = sheet[-(1:4), ]\n  sheet = head(sheet, -1)\n\n  # Fix pre-2018 column names.\n  if (year &lt; 2018) {\n    new_names = cwalk[names(sheet)]\n    new_names[is.na(new_names)] = names(sheet)[is.na(new_names)]\n    names(sheet) = new_names\n  }\n\n  # Select only a few columns of interest.\n  facility_cols = c(\n    \"FAC_NAME\", \"FAC_CITY\", \"FAC_ZIP\", \"FAC_OPERATED_THIS_YR\",\n    \"FACILITY_LEVEL\", \"TEACH_HOSP\", \"COUNTY\", \"PRIN_SERVICE_TYPE\"\n  )\n\n  cols = names(sheet)\n  tot_cols = cols[str_starts(cols, \"TOT\")]\n  respiratory_cols = cols[str_detect(cols, \"RESPIRATORY\")]\n\n  numeric_cols = c(tot_cols, respiratory_cols)\n  numeric_cols = numeric_cols[!str_detect(numeric_cols, \"ALOS\")]\n  sheet[numeric_cols] = lapply(sheet[numeric_cols], as.numeric)\n\n  sheet$year = year\n\n  sheet = sheet[c(\"year\", facility_cols, numeric_cols)]\n\n  # Rename the columns to lowercase.\n  names(sheet) = str_to_lower(names(sheet))\n\n  sheet\n}\n\nNow we can test the function on all of the files. We’ll use a for-loop to iterate over all of the paths, read the data for each one, and store the result in a list:\n\nhosps = list()\n\nfor (i in seq_along(paths)) {\n  path = paths[[i]]\n  year = get_hospital_year(path)\n  hosp = read_hospital_data(path, year)\n  hosps[[i]] = hosp\n}\n\nNew names:\nNew names:\nNew names:\n• `PSY_CENS_PATIENT_TOTL` -&gt; `PSY_CENS_PATIENT_TOTL...126`\n• `PSY_CENS_PATIENT_TOTL` -&gt; `PSY_CENS_PATIENT_TOTL...130`\n• `PSY_CENS_PATIENT_TOTL` -&gt; `PSY_CENS_PATIENT_TOTL...141`\n\nlength(hosps)\n\n[1] 8\n\n\nWe can use the do.call and rbind functions to bind the rows, or stack, the list of data frames:\n\nhosps = do.call(rbind, hosps)\nhead(hosps)\n\n# A tibble: 6 × 18\n   year fac_name fac_city fac_zip fac_operated_this_yr facility_level teach_hosp\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;                &lt;chr&gt;          &lt;chr&gt;     \n1  2016 ALTA BA… BERKELEY 94705   Yes                  Parent Facili… NO        \n2  2016 CHILDRE… OAKLAND  94609   Yes                  Parent Facili… NO        \n3  2016 THUNDER… OAKLAND  94609   Yes                  Parent Facili… NO        \n4  2016 FAIRMON… SAN LEA… 94578   Yes                  Consolidated … NO        \n5  2016 ALTA BA… BERKELEY 94704   Yes                  Consolidated … NO        \n6  2016 HIGHLAN… OAKLAND  94602   Yes                  Parent Facili… YES       \n# ℹ 11 more variables: county &lt;chr&gt;, prin_service_type &lt;chr&gt;,\n#   tot_lic_beds &lt;dbl&gt;, tot_lic_bed_days &lt;dbl&gt;, tot_discharges &lt;dbl&gt;,\n#   tot_cen_days &lt;dbl&gt;, acute_respiratory_care_lic_beds &lt;dbl&gt;,\n#   acute_respiratory_care_lic_bed_days &lt;dbl&gt;,\n#   acute_respiratory_care_discharges &lt;dbl&gt;,\n#   acute_respiratory_care_intra_transfers &lt;dbl&gt;,\n#   acute_respiratory_care_cen_days &lt;dbl&gt;\n\n\nWe’ve finally got all of the data in a single data frame!\nTo begin to address whether hospital utilization changed in 2020, let’s make a bar plot of total census-days:\n\nlibrary(\"ggplot2\")\n\n(\n    ggplot(hosps) +\n    aes(x = year, weight = tot_cen_days) +\n    geom_bar()\n)\n\n\n\n\n\n\n\n\nAccording to the plot, total census-days was slightly lower in 2020 than in 2019. This is a bit surprising, but it’s possible that California hospitals typically operate close to maximum capacity and were not able to substantially increase the number of beds in 2020 in response to the COVID-19 pandemic. You can use other columns in the dataset, such as tot_lic_beds or tot_lic_bed_days, to check this.\nLet’s also look at census-days for acute respiratory care:\n\n(\n    ggplot(hosps) +\n    aes(x = year, weight = acute_respiratory_care_cen_days) +\n    geom_bar()\n)\n\n\n\n\n\n\n\n\nIn this plot, there’s a clear uptick in census-days in 2020, and then an interesting decrease to below 2019 levels in the years following. Again, you could use other columns in the dataset to investigate this further. We’ll end this case study here, having accomplished the difficult task of reading the data and the much easier task of doing a cursory preliminary analysis of the data.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week08/control-flow.html#case-study-the-collatz-conjecture",
    "href": "chapters/week08/control-flow.html#case-study-the-collatz-conjecture",
    "title": "26  Control Flow",
    "section": "26.9 Case Study: The Collatz Conjecture",
    "text": "26.9 Case Study: The Collatz Conjecture\nThe Collatz Conjecture is a conjecture in math that was introduced in 1937 by Lothar Collatz and remains unproven today, despite being relatively easy to explain. Here’s a statement of the conjecture:\n\nStart from any positive integer. If the integer is even, divide by 2. If the integer is odd, multiply by 3 and add 1.\nIf the result is not 1, repeat using the result as the new starting value.\nThe result will always reach 1 eventually, regardless of the starting value.\n\nThe sequences of numbers this process generates are called Collatz sequences. For instance, the Collatz sequence starting from 2 is 2, 1. The Collatz sequence starting from 12 is 12, 6, 3, 10, 5, 16, 8, 4, 2, 1.\nYou can use iteration to compute the Collatz sequence for a given starting value. Since each number in the sequence depends on the previous one, and since the length of the sequence varies, a while-loop is the most appropriate iteration strategy:\n\nn = 5\ni = 0\nwhile (n != 1) {\n  i = i + 1\n  if (n %% 2 == 0) {\n    n = n / 2\n  } else {\n    n = 3 * n + 1\n  }\n  message(n, \" \", appendLF = FALSE)\n}\n\n16 8 4 2 1\n\n\nAs of 2020, scientists have used computers to check the Collatz sequences for every number up to approximately \\(2^{64}\\).\n\n\n\n\n\n\nNoteSee also\n\n\n\nFor more details about the Collatz Conjecture, check out this video.",
    "crumbs": [
      "Week 8",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Control Flow</span>"
    ]
  },
  {
    "objectID": "chapters/week09/web-scraping.html",
    "href": "chapters/week09/web-scraping.html",
    "title": "27  Web Scraping",
    "section": "",
    "text": "27.1 What’s in a Web Page?\nModern web pages usually consist of many files:\nHTML is the only component that always has to be there. Since HTML is what gives a web page structure, it’s what we’ll focus on when scraping.\nHTML is closely related to eXtensible markup language (XML). Both languages use tags to mark structural elements of data. In HTML, the elements literally correspond to the elements of a web page: paragraphs, links, tables, and so on.\nMost tags come in pairs. The opening tag marks the beginning of an element and the closing tag marks the end. Opening tags are written &lt;NAME&gt;, where NAME is the name of the tag. Closing tags are written &lt;/NAME&gt;.\nA singleton tag is a tag that stands alone, rather than being part of a pair. Singleton tags are written &lt;NAME /&gt;. In HTML (but not XML) they can also be written &lt;NAME&gt;. Fortunately, HTML only has a few singleton tags, so they can be distinguished by name regardless of which way they’re written.\nFor example, here’s some HTML that uses the em (emphasis, usually italic) and strong (usually bold) tags, as well as the singleton br (line break) tag:\nA pair of tags can contain other elements (paired or singleton tags), but not a lone opening or closing tag. This creates a strict, treelike hierarchy.\nOpening and singleton tags can have attributes that contain additional information. Attributes are name-value pairs written NAME=\"VALUE\" after the tag name.\nFor instance, the HTML a (anchor) tag creates a link to the URL provided for the href attribute:\nIn this case the tag also has a value set for the id attribute.\nNow let’s look at an example of HTML for a complete, albeit simple, web page:\nIn most web browsers, you can examine the HTML for a web page by right-clicking and choosing “View Page Source”.\nSee here for a more detailed explanation of HTML, and here for a list of valid HTML elements.",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "chapters/week09/web-scraping.html#whats-in-a-web-page",
    "href": "chapters/week09/web-scraping.html#whats-in-a-web-page",
    "title": "27  Web Scraping",
    "section": "",
    "text": "Hypertext markup language (HTML) for structure and formatting\nCascading style sheets (CSS) for more formatting\nJavaScript (JS) for interactivity\nImages\n\n\n\n\n\n\n&lt;em&gt;&lt;strong&gt;This text&lt;/strong&gt; is emphasized.&lt;br /&gt;&lt;/em&gt; Not emphasized\n\n\n\n&lt;a href=\"http://www.google.com/\" id=\"mytag\"&gt;My Search Engine&lt;/a&gt;\n\n\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;This is the page title!&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;This is a header!&lt;/h1&gt;\n    &lt;p&gt;This is a paragraph.\n      &lt;a href=\"http://www.r-project.org/\"&gt;Here's a website!&lt;/a&gt;\n    &lt;/p&gt;\n    &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "chapters/week09/web-scraping.html#rs-xml-parsers",
    "href": "chapters/week09/web-scraping.html#rs-xml-parsers",
    "title": "27  Web Scraping",
    "section": "27.2 R’s XML Parsers",
    "text": "27.2 R’s XML Parsers\nA parser converts structured data into familiar data structures. R has two popular packages for parsing XML (and HTML):\n\nThe XML package\nThe xml2 package\n\nThe XML package has more features. The xml2 package is more user-friendly, and as part of the Tidyverse, it’s relatively well-documented. This lesson focuses on xml2, since most of the additional features in the XML package are related to writing (rather than parsing) XML documents.\nThe xml2 package is often used in conjunction with the rvest package, which provides support for CSS selectors (described later in this lesson) and automates scraping HTML tables.\nThe first time you use these packages, you’ll have to install them:\ninstall.packages(\"xml2\")\ninstall.packages(\"rvest\")\nLet’s start by parsing the example of a complete web page from earlier. The xml2 function read_xml reads an XML document, and the rvest function read_html reads an HTML document. Both accept an XML/HTML string or a file path (including URLs):\n\nhtml = r\"(\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;This is the page title!&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;This is a header!&lt;/h1&gt;\n    &lt;p&gt;This is a paragraph.\n      &lt;a href=\"http://www.r-project.org/\"&gt;Here's a website!&lt;/a&gt;\n    &lt;/p&gt;\n    &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt; )\"\n\nlibrary(\"xml2\")\nlibrary(\"rvest\")\n\ndoc = read_html(html)\ndoc\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;meta charset=\"UTF-8\"&gt;\\n&lt;title&gt;This is the page title!&lt;/title&gt;\\n&lt; ...\n[2] &lt;body&gt;\\n    &lt;h1&gt;This is a header!&lt;/h1&gt;\\n    &lt;p&gt;This is a paragraph.\\n     ...\n\n\nThe xml_children function returns all of the immediate children of a given element.\nThe top element of our document is the html tag, and its immediate children are the head and body tags:\n\ntags = xml_children(doc)\n\nThe result from xml_children is a node set (xml_nodeset object). Think of a node set as a vector where the elements are tags rather than numbers or strings. Just like a vector, you can access individual elements with the indexing (square bracket [) operator:\n\nlength(tags)\n\n[1] 2\n\nhead = tags[1]\nhead\n\n{xml_nodeset (1)}\n[1] &lt;head&gt;\\n&lt;meta charset=\"UTF-8\"&gt;\\n&lt;title&gt;This is the page title!&lt;/title&gt;\\n&lt; ...\n\n\nThe xml_text function returns the text contained in a tag. Let’s get the text in the title tag, which is beneath the head tag. First we isolate the tag, then use xml_text:\n\ntitle = xml_children(head)\nxml_text(title)\n\n[1] \"This is the page title!\"\n\n\nNavigating through the tags by hand is tedious and easy to get wrong, but fortunately there’s a better way to find the tags we want.",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "chapters/week09/web-scraping.html#xpath",
    "href": "chapters/week09/web-scraping.html#xpath",
    "title": "27  Web Scraping",
    "section": "27.3 XPath",
    "text": "27.3 XPath\nAn XML document is a tree, similar to the file system on your computer:\nhtml\n├── head\n│   └── title\n└── body\n    ├── h1\n    ├── p\n    └── p\n        └── a\nWhen we wanted to find files, we wrote file paths. We can do something similar to find XML elements.\nXPath is a language for writing paths to elements in an XML document. XPath is not R-specific. At a glance, an XPath looks similar to a file path:\n\n\n\nXPath\nDescription\n\n\n\n\n/\nroot, or element separator\n\n\n.\ncurrent tag\n\n\n..\nparent tag\n\n\n*\nany tag (wildcard)\n\n\n\nThe xml2 function xml_find_all finds all elements at given XPath:\n\nxml_find_all(doc, \"/html/body/p\")\n\n{xml_nodeset (2)}\n[1] &lt;p&gt;This is a paragraph.\\n      &lt;a href=\"http://www.r-project.org/\"&gt;Here's ...\n[2] &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n\n\nUnlike a file path, an XPath can identify multiple elements. If you only want a specific element, use indexing to get it from the result.\nXPath also has some features that are different from file paths. The // separator means “at any level beneath.” It’s a useful shortcut when you want to find a specific element but don’t care where it is.\nLet’s get all of the p elements at any level of the document:\n\nxml_find_all(doc, \"//p\")\n\n{xml_nodeset (2)}\n[1] &lt;p&gt;This is a paragraph.\\n      &lt;a href=\"http://www.r-project.org/\"&gt;Here's ...\n[2] &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n\n\nLet’s also get all a elements at any level beneath a p element:\n\nxml_find_all(doc, \"//p/a\")\n\n{xml_nodeset (1)}\n[1] &lt;a href=\"http://www.r-project.org/\"&gt;Here's a website!&lt;/a&gt;\n\n\nThe vertical bar | means “or.” You can use it to get two different sets of elements in one query.\nLet’s get all h1 or p tags:\n\nxml_find_all(doc, \"//h1|//p\")\n\n{xml_nodeset (3)}\n[1] &lt;h1&gt;This is a header!&lt;/h1&gt;\n[2] &lt;p&gt;This is a paragraph.\\n      &lt;a href=\"http://www.r-project.org/\"&gt;Here's ...\n[3] &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n\n\n\n27.3.1 Predicates\nIn XPath, the predicate operator [] gets elements at a position or matching a condition. Most conditions are about the attributes of the element. In the predicate operator, attributes are always prefixed with @.\nFor example, suppose we want to find all tags where the id attribute is equal to \"hello\":\n\nxml_find_all(doc, \"//*[@id = 'hello']\")\n\n{xml_nodeset (1)}\n[1] &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n\n\nNotice that the equality operator in XPath is =, not ==. Strings in XPath can be quoted with single or double quotes.\nYou can combine multiple conditions in the predicate operator with and and or. There are also several XPath functions you can use in the predicate operator. These functions are not R functions, but rather built into XPath. Here are a few:\n\n\n\nXPath\nDescription\n\n\n\n\nnot()\nnegation\n\n\ncontains()\ncheck string x contains y\n\n\ntext()\nget tag text\n\n\nsubstring()\nget a substring\n\n\n\nFor instance, suppose we want to get elements that contain the word “paragraph”:\n\nxml_find_all(doc, \"//*[contains(text(), 'paragraph')]\")\n\n{xml_nodeset (2)}\n[1] &lt;p&gt;This is a paragraph.\\n      &lt;a href=\"http://www.r-project.org/\"&gt;Here's ...\n[2] &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n\n\nFinally, note that you can also use the predicate operator to get elements at a specific position. For example, to get the second p element anywhere in the document:\n\nxml_find_all(doc, \"//p[2]\")\n\n{xml_nodeset (1)}\n[1] &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n\n\nNotice that this is the same as if we had used R to get the second element:\n\nxml_find_all(doc, \"//p\")[2]\n\n{xml_nodeset (1)}\n[1] &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBeware that although the XPath predicate operator resembles R’s indexing operator, the syntax is not always the same.\n\n\nWe’ll learn more XPath in the examples. There’s a complete list of XPath functions on Wikipedia.",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "chapters/week09/web-scraping.html#the-web-scraping-workflow",
    "href": "chapters/week09/web-scraping.html#the-web-scraping-workflow",
    "title": "27  Web Scraping",
    "section": "27.4 The Web Scraping Workflow",
    "text": "27.4 The Web Scraping Workflow\nScraping a web page is part technology, part art. The goal is to find an XPath that’s concise but specific enough to identify only the elements you want. If you plan to scrape the web page again later or want to scrape a lot of similar web pages, the XPath also needs to be general enough that it still works even if there are small variations.\nFirefox and Chrome include “web developer tools” that are invaluable for planning a web scraping strategy. Press Ctrl + Shift + i (Cmd + Shift + i on OS X) in Firefox or Chrome to open the web developer tools.\nWe can also use the web developer tools to interactively identify the element that corresponds to a specific part of a web page. Press Ctrl + Shift + c and then click on the part of the web page you want to identify.\nThe best way to approach web scraping (and programming in general) is as an incremental, iterative process. Use the web developer tools to come up with a basic strategy, try it out in R, check which parts don’t work, and then repeat to adjust the strategy. Expect to go back and forth between your web browser and R several times when you’re scraping.\nMost scrapers follow the same four steps, regardless of the web page and the language of the scraper:\n\nDownload pages with an HTTP request (usually GET)\nParse pages to extract text\nClean up extracted text with string methods or regex\nSave cleaned results\n\nIn R, xml2’s read_xml function takes care of step 1 for you, although you can also use httr functions to make the request yourself.\n\n27.4.1 Being Polite\nMaking an HTTP request is not free! It has a real cost in CPU time and also cash. Server administrators will not appreciate it if you make too many requests or make requests too quickly. So:\n\nIf you’re making multiple requests, slow them down by using R’s Sys.sleep function to make R do nothing for a moment. Aim for no more than 20-30 requests per second, unless you’re using an API that says more are okay.\nAvoid requesting the same page twice. One way to do this is by caching (saving) the results of the requests you make. You can do this manually, or use a package that does it automatically, like the httpcache package.\n\n\n\n\n\n\n\nImportant\n\n\n\nFailing to be polite can get you banned from websites! Also check the website’s terms of service to make sure scraping is not explicitly forbidden.\n\n\n\n\n27.4.2 Case Study: CA Cities\nWikipedia has many pages that are just tables of data. For example, there’s this list of cities and towns in California. Let’s scrape the table to get a data frame.\nStep 1 is to download the page:\nwiki_url =\n  \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_California\"\n\nwiki_doc = read_html(wiki_url)\nStep 2 is to extract the table element from the page. We can use Firefox or Chrome’s web developer tools to identify the table. HTML tables usually use the table tag. Let’s see if it’s the only table in the page:\n\ntables = xml_find_all(wiki_doc, \"//table\")\ntables\n\n{xml_nodeset (4)}\n[1] &lt;table class=\"wikitable sortable\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scope=\"row\" style=\"b ...\n[2] &lt;table class=\"wikitable plainrowheaders sortable\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scop ...\n[3] &lt;table class=\"nowraplinks hlist mw-collapsible autocollapse navbox-inner\" ...\n[4] &lt;table class=\"nowraplinks mw-collapsible autocollapse navbox-inner\" style ...\n\n\nThe page has 4 tables. We can either make our XPath more specific, or use indexing to get the table we want. Refining the XPath makes our scraper more robust, but indexing is easier.\nFor the sake of learning, let’s refine the XPath. Going back to the browser, we can see that the table includes \"wikitable\" and \"sortable\" in its class attribute. So let’s search for these among the table elements:\n\ntab = xml_find_all(tables, \"//*[contains(@class, 'sortable')]\")\ntab\n\n{xml_nodeset (2)}\n[1] &lt;table class=\"wikitable sortable\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scope=\"row\" style=\"b ...\n[2] &lt;table class=\"wikitable plainrowheaders sortable\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scop ...\n\n\nNow we get just two tables, and the second one is the one we want! Here we used a second XPath applied only to the results from the first, but we also could’ve done this all with one XPath: //table[contains(@class, 'sortable')].\nThe next part of extracting the data is to extract the value from each individual cell in the table. HTML tables have a strict layout order, with tags to indicate rows and cells. We could extract each cell by hand and then reassemble them into a data frame, but the rvest function html_table can do it for us automatically:\n\ncities = html_table(tab, fill = TRUE)\ncities = cities[[2]]\nhead(cities)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nCounty\nPopulation (2020)[1]\nPopulation (2010)[9]\nChange\nLand area[10]\nLand area[10]\nPopulation density[10]\nIncorporated[8]\n\n\n\n\nName\nType\nCounty\nPopulation (2020)[1]\nPopulation (2010)[9]\nChange\nsq mi\nkm2\nPopulation density[10]\nIncorporated[8]\n\n\nAdelanto\nCity\nSan Bernardino\n38,046\n31,765\n+19.8%\n52.87\n136.9\n719.6/sq mi (277.8/km2)\nDecember 22, 1970\n\n\nAgoura Hills\nCity\nLos Angeles\n20,299\n20,330\n−0.2%\n7.80\n20.2\n2,602.4/sq mi (1,004.8/km2)\nDecember 8, 1982\n\n\nAlameda\nCity\nAlameda\n78,280\n73,812\n+6.1%\n10.45\n27.1\n7,490.9/sq mi (2,892.3/km2)\nApril 19, 1854\n\n\nAlbany\nCity\nAlameda\n20,271\n18,539\n+9.3%\n1.79\n4.6\n11,324.6/sq mi (4,372.4/km2)\nSeptember 22, 1908\n\n\nAlhambra\nCity\nLos Angeles\n82,868\n83,089\n−0.3%\n7.63\n19.8\n10,860.8/sq mi (4,193.4/km2)\nJuly 11, 1903\n\n\n\n\n\n\nThe fill = TRUE argument ensures that empty cells are filled with NA. We’ve successfully imported the data from the web page into R, so we’re done with step 2.\nStep 3 is to clean up the data frame. The column names contain symbols, the first row is part of the header, and the column types are not correct.\n\n# Fix column names.\nnames(cities) = c(\n  \"city\", \"type\", \"county\", \"population2020\", \"population2010\",\n  \"population_change\", \"mi2\", \"km2\", \"density\", \"date\"\n)\n\n# Remove fake first row.\ncities = cities[-1, ]\n# Reset row names.\nrownames(cities) = NULL\n\nHow can we clean up the date column? The as.Date function converts a string into a date R understands. The idea is to match the date string to a format string where the components of the date are indicated by codes that start with %. For example, %m stands for the month as a two-digit number. You can read about the different date format codes in ?strptime.\nHere’s the code to convert the dates in the data frame:\n\ndates = as.Date(cities$date, \"%B %m, %Y\")\ncities$date = dates\n\nWe can also convert the population to a number:\n\nclass(cities$population2020)\n\n[1] \"character\"\n\n# Remove commas and footnotes (e.g., [1]) before conversion\nlibrary(\"stringr\")\n\npop = str_replace_all(cities$population2020, \",\", \"\")\npop = str_replace_all(pop, \"\\\\[[0-9]+\\\\]\", \"\")\npop = as.numeric(pop)\n\n# Check for missing values, which can mean conversion failed\nany(is.na(pop))\n\n[1] FALSE\n\ncities$population2020 = pop\n\n\n\n27.4.3 Case Study: The CA Aggie\nSuppose we want to scrape The California Aggie.\nIn particular, we want to get all the links to news articles on the features page https://theaggie.org/category/features/. This could be one part of a larger scraping project where we go on to scrape individual articles.\nFirst for Step 1, let’s download the features page so we can extract the links:\nurl = \"https://theaggie.org/category/features/\"\ndoc = read_html(url)\nWe know that links are in a tags, but we only want links to articles. Looking at the features page with the web developer tools, the links to feature articles are all inside of a div tag with class td_block_inner. So for Step 2, let’s get that tag:\n\nxml_find_all(doc, \"//div[contains(@class, 'td_block_inner')]\")\n\n{xml_nodeset (9)}\n[1] &lt;div id=\"tdi_50\" class=\"td_block_inner td-fix-index\"&gt;\\n&lt;div class=\"tdb-ma ...\n[2] &lt;div id=\"tdi_51\" class=\"td_block_inner\"&gt;\\n        &lt;div class=\"tdb_module_ ...\n[3] &lt;div id=\"tdi_52\" class=\"td_block_inner\"&gt;\\n        &lt;div class=\"tdb_module_ ...\n[4] &lt;div id=\"tdi_53\" class=\"td_block_inner\"&gt;\\n        &lt;div class=\"tdb_module_ ...\n[5] &lt;div id=\"tdi_54\" class=\"td_block_inner\"&gt;\\n        &lt;div class=\"tdb_module_ ...\n[6] &lt;div id=\"tdi_55\" class=\"td_block_inner\"&gt;\\n        &lt;div class=\"tdb_module_ ...\n[7] &lt;div id=\"tdi_56\" class=\"td_block_inner\"&gt;\\n        &lt;div class=\"tdb_module_ ...\n[8] &lt;div id=\"tdi_57\" class=\"td_block_inner\"&gt;\\n        &lt;div class=\"tdb_module_ ...\n[9] &lt;div id=\"tdi_74\" class=\"td_block_inner tdb-block-inner td-fix-index\"&gt;\\n   ...\n\n# OR html_nodes(doc, \"div.td-block-inner\")\n\nThat returns a lot of results, so let’s try using the id attribute, which is \"tdi_113\", instead. Usually the id of an element is unique, so this ensures that we get the right section.\nWe can also add in a part about getting links now:\n\ndiv = xml_find_all(doc, \"//div[@id = 'tdi_113']\")\n# OR html_nodes(doc, \"div#tdi_113\")\n\nlinks = xml_find_all(div, \".//a\")\n# OR html_nodes(div, \"a\")\n\nlength(links)\n\n[1] 0\n\n\nThat gives us 0 links, but there are only 15 articles on the page, so something’s still not right. Inspecting the page again, there are actually three links to each article: on the image, on the title, and on “Continue Reading”.\nLet’s focus on the title link. All of the title links are inside of an h3 tag. Generally it’s more robust to rely on tags (structure) than to rely on attributes (other than id and class). So let’s use the h3 tag here:\n\nlinks = xml_find_all(div, \".//h3/a\")\n# OR html_nodes(div, \"h3 &gt; a\")\n\nlength(links)\n\n[1] 0\n\n\nNow we’ve got the 15 links, so let’s get the URLs from the href attribute.\n\nfeature_urls = xml_attr(links, \"href\")\n\nThe other article listings (Sports, Science, etc) on The Aggie have a similar structure, so we can potentially reuse our code to scrape those.\nSo let’s turn our code into a function. The input will be a downloaded page, and the output will be the article links.\n\nparse_article_links = function(page) {\n  div = xml_find_all(page, \"//div[@id = 'tdi_113']\")\n  links = xml_find_all(div, \".//h3/a\")\n  xml_attr(links, \"href\")\n}\n\nWe can test this out on the Sports page. First we download the page:\nsports = read_html(\"https://theaggie.org/category/sports\")\nThen we call the function on the document:\n\nsports_urls = parse_article_links(sports)\nhead(sports_urls)\n\ncharacter(0)\n\n\nIt looks like the function works even on other pages! We can also set up the function to extract the link to the next page, in case we want to scrape multiple pages of links.\nThe link to the next page of features (an arrow at the bottom) is an a tag with attribute aria-label in a div with class page-nav. Let’s see if that’s specific enough to isolate the tag:\n\nnav = xml_find_all(doc, \"//div[contains(@class, 'page-nav')]\")\n# OR html_nodes(doc, \"div.page-nav\")\nnext_page = xml_find_all(nav, \".//a[contains(@aria-label, 'next-page')]\")\n# OR html_nodes(nav, \"a[aria-label *= 'next-page']\")\n\nIt looks like it is. We use contains here rather than = because it is common for the class attribute to have many parts. Using contains makes our code robust against changes in the future.\nWe can now modify our parser function to return the link to the next page:\n\nparse_article_links = function(page) {\n  # Get article URLs\n  div = xml_find_all(page, \"//div[@id = 'tdi_113']\")\n  links = xml_find_all(div, \".//h3/a\")\n  urls = xml_attr(links, \"href\")\n\n  # Get next page URL\n  nav = xml_find_all(page, \"//div[contains(@class, 'page-nav')]\")\n  next_page = xml_find_all(nav, \".//a[contains(@aria-label, 'next-page')]\")\n  next_url = xml_attr(next_page, \"href\")\n\n  # Using a list allows us to return two objects\n  list(urls = urls, next_url = next_url)\n}\n\nSince our function gets URL for the next page, what happens on the last page?\nLooking at the last page in the browser, there is no link to the next page. Let’s see what our scraper function does:\nlast_page = read_html(\"https://theaggie.org/category/features/page/187/\")\n\nparse_article_links(last_page)\n\n$urls\ncharacter(0)\n\n$next_url\n[1] \"https://theaggie.org/category/features/page/188/\"\n\n\nWe get an empty character vector as the URL for the next page. This is because the xml_find_all function returns an empty node set for the next page URL, so there aren’t any href fields for xml_attr to extract. It’s convenient that the xml2 functions behave this way, but we could also add an if-statement to the function to check (and possibly return NA as the next URL in this case).\nThen the code becomes:\n\nparse_article_links = function(page) {\n  # Get article URLs\n  div = xml_find_all(page, \"//div[@id = 'tdi_113']\")\n  links = xml_find_all(div, \".//h3/a\")\n  urls = xml_attr(links, \"href\")\n\n  # Get next page URL\n  nav = xml_find_all(page, \"//div[contains(@class, 'page-nav')]\")\n  next_page = xml_find_all(nav, \".//a[contains(@aria-label, 'next-page')]\")\n  if (length(next_page) == 0) {\n    next_url = NA\n  } else {\n    next_url = xml_attr(next_page, \"href\")\n  }\n\n  # Using a list allows us to return two objects\n  list(urls = urls, next_url = next_url)\n}\n\nNow our function should work well even on the last page.\nIf we want to scrape links to all of the articles in the features section, we can use our function in a loop:\n# NOTE: This code is likely to take a while to run, and is meant more for\n# reading than for you to run and try out.\n\nurl = \"https://theaggie.org/category/features/\"\narticle_urls = list()\ni = 1\n\n# On the last page, the next URL will be `NA`.\nwhile (!is.na(url)) {\n  # Download and parse the page.\n  page = read_html(url)\n  result = parse_article_links(page)\n\n  # Save the article URLs in the `article_urls` list. The variable `i` is the\n  # page number.\n  article_urls[[i]] = result$url\n  i = i + 1\n\n  # Set the URL to the next URL.\n  url = result$next_url\n\n  # Sleep for 1/30th of a second so that we never make more than 30 requests\n  # per second.\n  Sys.sleep(1/30)\n}\nNow we’ve got the basis for a simple scraper.",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "chapters/week09/web-scraping.html#css-selectors",
    "href": "chapters/week09/web-scraping.html#css-selectors",
    "title": "27  Web Scraping",
    "section": "27.5 CSS Selectors",
    "text": "27.5 CSS Selectors\nCascading style sheets (CSS) is a language used to control the formatting of an XML or HTML document.\nCSS selectors are the CSS way to write paths to elements. CSS selectors are more concise than XPath, so many people prefer them. Even if you prefer CSS selectors, it’s good to know XPath because CSS selectors are less flexible.\nHere’s the basic syntax of CSS selectors:\n\n\n\nCSS\nDescription\n\n\n\n\na\ntags a\n\n\na &gt; b\ntags b directly beneath a\n\n\na b\ntags b anywhere beneath a\n\n\na, b\ntags a or b\n\n\n#hi\ntags with attribute id=\"hi\"\n\n\n.hi\ntags with attribute class that contains \"hi\"\n\n\n[foo=\"hi\"]\ntags with attribute foo=\"hi\"\n\n\n[foo*=\"hi\"]\ntags with attribute foo that contains \"hi\"\n\n\n\nIf you want to learn more, CSS Diner is an interactive tutorial that covers the entire CSS selector language.\nIn Firefox, you can get CSS selectors from the web developer tool. Right-click the tag you want a selector for and choose “Copy Unique Selector”. Beware that the selectors Firefox generates are often too specific to be useful for anything beyond the simplest web scrapers.\nThe rvest package uses CSS selectors by default. Behind the scenes, the package translates these into XPath and passes them to xml2.\nHere are a few examples of CSS selectors, using rvest’s html_nodes function:\n\nhtml = r\"(\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;This is the page title!&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;This is a header!&lt;/h1&gt;\n    &lt;p&gt;This is a paragraph.\n      &lt;a href=\"http://www.r-project.org/\"&gt;Here's a website!&lt;/a&gt;\n    &lt;/p&gt;\n    &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt; )\"\n\ndoc = read_html(html)\n\n# Get all p elements\nhtml_nodes(doc, \"p\")\n\n{xml_nodeset (2)}\n[1] &lt;p&gt;This is a paragraph.\\n      &lt;a href=\"http://www.r-project.org/\"&gt;Here's ...\n[2] &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;\n\n# Get all links\nhtml_nodes(doc, \"a\")\n\n{xml_nodeset (1)}\n[1] &lt;a href=\"http://www.r-project.org/\"&gt;Here's a website!&lt;/a&gt;\n\n# Get all tags with id=\"hello\"\nhtml_nodes(doc, \"#hello\")\n\n{xml_nodeset (1)}\n[1] &lt;p id=\"hello\"&gt;This is another paragraph.&lt;/p&gt;",
    "crumbs": [
      "Week 9",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "chapters/week10/collaboration.html",
    "href": "chapters/week10/collaboration.html",
    "title": "28  Collaboration in Data-Driven Work",
    "section": "",
    "text": "CautionWork in Progress\n\n\n\nWe’re still developing this section and will post it soon.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Collaboration in Data-Driven Work</span>"
    ]
  },
  {
    "objectID": "chapters/week10/branches-merges.html",
    "href": "chapters/week10/branches-merges.html",
    "title": "29  Branches & Merges",
    "section": "",
    "text": "29.1 What’s a Branch?\nAs you make commits to a Git repository, the commits become that repository’s history. You can explore the history with commands like git log and git restore (see Section 8.4). For many repositories, the history is linear—each commit has one commit before it and possibly one after it, as in Figure 29.1.\nA branch is a distinct path through a repository’s history, usually formed by branching off of or splitting the history at a particular commit, as in Figure 29.2. When a repository’s history is completely linear, the repository has just one branch.\nBranches are really useful! With branches, you can:\nThese are just a few ways to use branches. Git wasn’t the first version control system to support branches, but it does make using them especially quick and easy.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Branches & Merges</span>"
    ]
  },
  {
    "objectID": "chapters/week10/branches-merges.html#whats-a-branch",
    "href": "chapters/week10/branches-merges.html#whats-a-branch",
    "title": "29  Branches & Merges",
    "section": "",
    "text": "Figure 29.1: Visualization of a Git repository with a linear history. There is exactly one branch, named a. Solid arrows show the order of commits. Dashed arrows show where the next commit will be on each branch. Each circle represents a commit.\n\n\n\n\n\n\n\n\n\n\n\nFigure 29.2: Visualization of a Git repository with two branches, a and b. Solid arrows show the order of commits. Dashed arrows show where the next commit will be on each branch. Each circle represents a commit. Commits only on branch a are blue , and commits only on branch b are yellow . Commits on both branches before the merge are gray .\n\n\n\n\n\nTry something that might not work out. Suppose you get an idea for a new feature to add to your lab’s analysis package, but developing it will take time and might temporarily break the package. You also need to use the package for your day-to-day work. By developing the new feature on a branch, you can switch between the stable branch and the development branch whenever you need. This makes it easy to keep using the package while you develop the new feature, to test the new feature, and to pause or abandon the new feature if it doesn’t work out (just stop working on the branch).\nTemporarily store some work. If you start working on something but get interrupted, you can store your work on a branch until you’re ready to come back to it.\nExplore an earlier version of the repository. If you need to look at an earlier version of a single file, git restore and git show are great. But what if you want to know what the entire repository was like in the past or don’t know which file you need? You can make a branch at the commit in which you’re interested, switch to the branch, and then explore the repository without need for any special tools or Git commands.\nControl when conflicting changes are resolved. When multiple people work on a repository, sharing work frequently makes it easier to get feedback and coordinate everyone’s efforts. By having each person work on a separate branch, they can share their work at any time, even if changes they made to a particular file conflict with changes someone else made. Resolving the conflicts can wait until everyone is ready to combine their work.\nPresent a repository differently to different audiences. For instance, if your repository contains a package, you can use the default branch for files users of the package need and a separate development branch for files contributors to the package need.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Branches & Merges</span>"
    ]
  },
  {
    "objectID": "chapters/week10/branches-merges.html#switching-branches",
    "href": "chapters/week10/branches-merges.html#switching-branches",
    "title": "29  Branches & Merges",
    "section": "29.2 Switching Branches",
    "text": "29.2 Switching Branches\nLet’s explore Git’s commands for working with branches. Along the way, we’ll also review some of the fundamental Git commands we explained in Chapter 8. As an example, we’ll use DataLab’s Git sandbox repository.\nTo get started, clone a copy of the repository from GitHub to your computer. Open a terminal and navigate to your home directory:\ncd\nGo to the repository’s page on GitHub, click the green “Code” button, and copy the URL under the SSH tab. Then go back to the terminal, type the git clone command, and paste the URL after the command. The full command should look like this:\ngit clone git@github.com:ucdavisdatalab/sandbox_git.git\nRun the command, wait for Git to clone the repository, and then change directories into it:\ncd sandbox_git\nTake a moment to explore the files in the repository.\nAt any given time, the working tree (the collection of actual files and directories) in a repository corresponds to a single branch. We call this the current branch, and say that the current branch is checked out or that the repository is “on” the current branch. When you clone a repository, Git automatically checks out the default branch.\nThe git status command lists the current branch in its output:\ngit status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\nThe repository is on the main branch, which makes sense you just cloned the repository, and main is the default branch.\nOn GitHub, the repository has two branches: the default branch main and another branch called demo. You can use git branch to list the branches in a repository:\ngit branch\n* main\nThe active branch (main in this case) is marked with a star *. But where’s the demo branch? By default, git branch only lists branches you create or switch to locally. Each of these is called a local branch. The demo branch exists on the remote (GitHub), but you’ve never switched to a local copy, so it’s a remote branch. You can use the -a flag (short for --all) to make the git branch command show all branches, local and remote:\ngit branch -a\n* main\n  remotes/origin/HEAD -&gt; origin/main\n  remotes/origin/demo\n  remotes/origin/main\n\nIn the output, remote branches are prefixed with remotes/ and the name of the remote (in this case, origin). You should see one local branch, main, and three remote branches.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the -v flag (short for --verbose) to make the git branch command show more information about each branch.\n\n\nLet’s take a look at the demo branch. To switch to a different branch, use the git switch command with the name of the branch you want:\ngit switch demo\nbranch 'demo' set up to track 'origin/demo'.\nSwitched to a new branch 'demo'\n\nOpen the README.md file in a text editor. You should now see a message in the file that wasn’t there on the main branch:\n**You're probably on the `demo` branch!**\n\nWhat's your favorite color?\nAnswer the question in README.md and save the file. Don’t worry about committing your answer. Git will now say that you’ve made some changes to README.md:\ngit status\nOn branch demo\nYour branch is up to date with 'origin/demo'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nTry switching back to the main branch:\ngit switch main\nerror: Your local changes to the following files would be overwritten by checkout:\n        README.md\nPlease commit your changes or stash them before you switch branches.\nAborting\nGit refuses to switch branches because doing so would overwrite the changes you made to README.md. You haven’t committed the changes, so there wouldn’t be any way to restore them later. There are several different ways to handle this:\n\n\nKeep the changes on the demo branch. To do this, add the changes to the staging area, make a commit, and then switch branches.\nMove the changes to the main branch. To do this, switch branches with the -m flag (short for --merge).\nDiscard the changes. To do this, switch branches with the -f flag (short for --force).\n\nFor the sake of this example, take the third option and throw away the changes:\ngit switch -f main\nSwitched to branch 'main'\nYour branch is up to date with 'origin/main'.\nCheck that the changes you made to README.md are gone:\ngit status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\nThen switch back to the demo branch:\ngit switch demo\nSwitched to branch 'demo'\nYour branch is up to date with 'origin/demo'.\nNotice that your changes are gone on this branch too:\ngit status\nOn branch demo\nYour branch is up to date with 'origin/demo'.\n\nnothing to commit, working tree clean\nSince we didn’t commit the changes, Git has no record of them.\n\n\n\n\n\n\nWarning\n\n\n\nOnly use git switch with -f when you’re absolutely sure you want to discard your changes. There’s no undo.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Branches & Merges</span>"
    ]
  },
  {
    "objectID": "chapters/week10/branches-merges.html#creating-a-branch",
    "href": "chapters/week10/branches-merges.html#creating-a-branch",
    "title": "29  Branches & Merges",
    "section": "29.3 Creating a Branch",
    "text": "29.3 Creating a Branch\nWhen you start working on something new in a repository, like a new feature or a bug fix, it’s usually a good idea to create a new branch and do the work there. Then you can switch back to the original branch any time you need to use the repository in the state it was in before you started working. Working on a branch also makes collaboration easier (see Section 30.3) and can make a repository’s history easier to understand.\nBranch names should be short but descriptive enough to serve as a reminder about the purpose of the branch. Git allows a wide variety of characters in branch names, but using only lowercase letters, numbers, underscores (_), and dashes (-) ensures that your branch name is safe for Git-related tools and easy to type. Branch names cannot contain spaces or begin with a dash. See this StackOverflow post for the exact rules.\n\n\n\n\n\n\nTip\n\n\n\nFor branches where you’ll work mostly or entirely alone, we recommend names of the form XX-DESCRIPTION, where XX are your initials and DESCRIPTION is a short description of the branch. For instance, nu-fix-indexing is a good branch name for a branch about fixing an indexing bug.\n\n\nYou can also use the git branch command to create a new branch. Branches almost always start from an existing commit in the repository’s history; the default is the most recent commit on the current branch. Make sure that you’re on the demo branch, and then create a new branch called my-first-branch:\ngit branch my-first-branch\nThe git branch command doesn’t print anything when it successfully creates a branch. Switch to the new branch:\ngit switch my-first-branch\nSwitched to branch 'my-first-branch'\nExamine README.md and the most recent commit in the Git log on this branch. They’re identical to demo, since this is a branch off of the most recent commit to demo.\nOpen README.md and answer the question again. For instance, you might answer:\nMy favorite color is blue.\nSave the file, then add the changes to the staging area:\ngit add README.md\nThen commit the changes, making sure to write a descriptive commit message:\ngit commit\nWith your changes committed, you can safely switch back to the demo branch:\ngit switch demo\nSwitched to branch 'demo'\nYour branch is up to date with 'origin/demo'.\nYou can switch back to my-first-branch any time you want to work on that version of the repository.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Branches & Merges</span>"
    ]
  },
  {
    "objectID": "chapters/week10/branches-merges.html#whats-a-merge",
    "href": "chapters/week10/branches-merges.html#whats-a-merge",
    "title": "29  Branches & Merges",
    "section": "29.4 What’s a Merge?",
    "text": "29.4 What’s a Merge?\nAfter working on a branch for a while, you might reach a point where you feel like your work is ready to copy back to the original branch. You can do this with a merge, which applies commits from one branch onto another.\nTo practice merging, let’s merge the my-first-branch branch into the demo branch. You can use git merge BRANCH to merge the branch BRANCH into the current branch. Make sure the repository is on the demo branch, and then run:\ngit merge my-first-branch\nUpdating d734c25..7d96756\nFast-forward\n README.md | 2 ++\n 1 file changed, 2 insertions(+)\nWhen you run git merge, Git will try to merge the commits automatically. An automatic merge is possible as long as there are not changes to a file on one branch that conflict with changes to that file on the other. There are two kinds of automatic merges (also see Figure 29.3):\n\nA regular merge, for which Git creates a merge commit that combines the changes from each branch.\nA fast-forward merge, where only one branch has new commits, so Git can apply the new commits without any need for a merge commit.\n\nWhen we merged my-first-branch into demo, Git did a fast-forward merge, since we didn’t make any commits to demo after creating my-first-branch. Git notes this in the output. This means that Git did not need to create a merge commit.\n\n\n\n\n\n\nFigure 29.3: Visualization of merging and fast-forward merging branch b into a. Solid arrows show the order of commits. Dashed arrows show where the next commit will be on each branch. Each circle represents a commit. Commits only on branch b are yellow . Commits on both branches before the merge are gray . Commits on both branches after the merge are green . The merge commit is white .\n\n\n\nIn a regular merge, Git will create a new commit and give you a chance to enter a commit message (similar to when you run git commit). Git will automatically fill in a commit message describing the merge, and in most cases it’s best to accept the default message.\n\n\n\n\n\n\nNoteMerge versus Rebase\n\n\n\nThere’s another way to apply commits from one branch onto another: a rebase updates a collection of commits so that they appear as if they were made after any new commits on the original branch, and then applies them. Rebasing makes a repository’s history linear (and thus simpler) even if most work was done on branches. On the other hand, rebasing can make the actual development process harder to fathom, since commits are not necessarily in chronological order and information about branches is discarded. You can rebase one branch onto another with the git rebase command.\n\n\n\n\n\n\nFigure 29.4: Visualization of merging branch b into a and rebasing branch a onto b. Solid arrows show the order of commits. Dashed arrows show where the next commit will be on each branch. Each circle represents a commit. Commits only on branch a are blue , and commits only on branch b are yellow . Commits on both branches before the rebase are gray . Commits on both branches after the rebase are green .\n\n\n\nWe recommend merging over rebasing when you’re dealing with multiple branches. That said, learning to use git rebase can be beneficial for managing commits within a single branch—you can use the command to reorganize or combine commits before you share them with anyone else, so that the history of your work is well-organized even if your development process is messy.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Branches & Merges</span>"
    ]
  },
  {
    "objectID": "chapters/week10/branches-merges.html#merge-conflicts",
    "href": "chapters/week10/branches-merges.html#merge-conflicts",
    "title": "29  Branches & Merges",
    "section": "29.5 Merge Conflicts",
    "text": "29.5 Merge Conflicts\nBranches can have commits that change files in different ways. For example, you might answer the favorite color question with “red” on the demo branch and save and commit your work. Later, you switch to my-first-branch, think about how your favorite color is actually more of an “orange”, and answer the question again, saving and committing your new answer to the branch. The commit on demo changes README.md to mention “red”, while the commit on my-first-branch changes README.md to mention “orange”.\nThe example is a bit silly, but in a repository with many files, it’s easy to accidentally change a file in different ways on different branches. You might also do it intentionally, perhaps because the branches originally had separate purposes. When you’re working collaboratively, you and your collaborator might both decide to make changes to the same file, unaware of what the other is doing.\nWe say branches like the two in the example have diverged: a file is changed in a different way by the commits on each branch. Git can’t merge divergent branches automatically. Instead, when you run git merge, Git will report that there’s a merge conflict and ask you to resolve the conflict manually. In other words, it’s up to you to decide what the contents of the file should be after the merge. That might mean keeping the changes from one branch and discarding the changes from the other, or it might mean combining the changes from both. What’s correct depends on the file, the conflicting changes, and the reason for the merge. Git can’t guess your intent, so you have to resolve the conflict.\nTo practice resolving a merge conflict, let’s deliberately create the one in the example. Make sure the repository is on the demo branch. Open README.md with a text editor and edit it so that the last line is:\nMy favorite color is red.\nSave the file, then git add and git commit the changes.\nNext, switch to the my-first-branch branch:\ngit switch my-first-branch\nOnce again, open README.md with a text editor. This time, edit it so that the last line is:\nI really like orange.\nSave the file, then git add and git commit the changes.\nThe demo and my-first-branch branches have now diverged. Each has different changes to README.md. Let’s merge demo into my-first-branch so that we can resolve the merge conflict. Make sure the repository is on my-first-branch, then run:\ngit merge demo\nAuto-merging README.md\nCONFLICT (content): Merge conflict in README.md\nAutomatic merge failed; fix conflicts and then commit the result.\nAs expected, Git reports that there’s a merge conflict in README.md. At this point, the branches are partially merged, but Git is waiting for you to fix the conflict. You can see more information about this by checking the repository’s status:\ngit status\nOn branch my-first-branch\nYou have unmerged paths.\n  (fix conflicts and run \"git commit\")\n  (use \"git merge --abort\" to abort the merge)\n\nUnmerged paths:\n  (use \"git add &lt;file&gt;...\" to mark resolution)\n        both modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nThe “unmerged paths” mentioned in the output are the files with conflicts, where Git was unable to merge the changes automatically. To resolve the conflicts and complete the merge, edit each file with a text editor, then git add and git commit your changes. To cancel the merge instead, run git merge --abort.\nLet’s fix the conflict. Open README.md with a text editor. The last few lines should look something like this:\nWhat's your favorite color?\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nI really like orange.\n=======\nMy favorite color is red.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; demo\nWhen you run git merge and there are merge conflicts, Git adds markers to the associated files to indicate the conflicts. The markers have this form:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; a\n\n=======\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; b\nThe &lt;&lt;&lt;&lt;&lt;&lt;&lt; a line shows where the conflict begins and the &gt;&gt;&gt;&gt;&gt;&gt;&gt; line shows where it ends. Each also names one of the two branches being merged. The ======= line separates the conflicting changes on each branch. So Git puts changes only on branch a between these markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; a\n\n=======\nAnd puts changes only on branch b between these markers:\n=======\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; b\nEverything outside of conflict markers is the same on both branches.\nIn the README.md file, the first part of the conflict is:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nI really like orange.\n=======\nHEAD is a special name Git uses to refer to the current branch. The current branch is my-first-branch, since we switched to it before starting the merge. So this part shows the conflicting changes on my-first-branch. And sure enough, I really like orange. is what we committed there.\nThe second part of the conflict is:\n=======\nMy favorite color is red.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; demo\nThis part shows the conflicting changes on demo. That’s where we committed My favorite color is red..\nTo fix a conflict, delete the conflict markers and edit the associated lines to be the way you want. For instance, we can edit the last few lines of README.md to be:\nWhat's your favorite color?\n\nI really like orange, but I also really like red.\nThis combines the changes from each branch nicely. Save the file and git add the changes. Take a look at the status now:\nOn branch my-first-branch\nAll conflicts fixed but you are still merging.\n  (use \"git commit\" to conclude merge)\n\nChanges to be committed:\n        modified:   README.md\nThere are no longer any unmerged paths, meaning there are no more conflicts to resolve. With all of the conflicts resolved, we can complete the merge with:\ngit commit\n[my-first-branch fbadd8f] Merge branch 'demo' into my-first-branch\nBecause this is a merge, Git creates a merge commit and automatically provides a commit message. Congratulations, you’ve just resolved your first merge conflict! 🎉",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Branches & Merges</span>"
    ]
  },
  {
    "objectID": "chapters/week10/branches-merges.html#merging-remote-branches",
    "href": "chapters/week10/branches-merges.html#merging-remote-branches",
    "title": "29  Branches & Merges",
    "section": "29.6 Merging Remote Branches",
    "text": "29.6 Merging Remote Branches\nYou’ve probably used the git pull command to pull changes from a repository. The command is actually just a shortcut for running two simpler commands:\n\ngit fetch to download branches from a remote\ngit merge to merge a remote branch onto the current local branch\n\nFor each local branch, the remote branch that git pull merges is called the upstream or tracking branch. You can set or change a local branch’s upstream branch by running git pull or git branch with the -u (short for --set-upstream) flag and the name of a remote branch. You can also configure Git to automatically set an upstream branch for each local branch your create.\nUnderstanding how git pull works might help you simplify your mental model of Git: every copy of a repository is just a collection of branches, and getting commits from a remote just means downloading a branch—often with the same name as a local branch—and merging it.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Branches & Merges</span>"
    ]
  },
  {
    "objectID": "chapters/week10/working-with-github.html",
    "href": "chapters/week10/working-with-github.html",
    "title": "30  Working with GitHub",
    "section": "",
    "text": "30.1 What’s an Issue?\nWhat makes GitHub special is the fact that, more than being simply a place to store files, it is above all a communication channel. Where GitHub extends the functionality of version control is not just where it offers various forms of cloud hosting; it is also where GitHub provides tools that let people talk about the code they’re working on. It’s a place where team members can propose and explain the changes they make, look at changes others have made, track and discuss any bugs that might come up, get feedback from others, and plan for any future changes the team intends to make.\nLearning how to use GitHub, then, is as much about learning how to communicate effectively through the different facets of the service as it is about acquainting yourself with new technical skills (i.e., using your computer to track code remotely). In this chapter, we’ll discuss both parts of using the service and do so with an eye toward how GitHub can facilitate collaborative and reproducible research. Although we focus on GitHub, many of the concepts and workflows presented also apply to other Git hosting services, such as GitLab and Bitbucket.\nA short summary of the different facets of communication GitHub provides includes:\nAdditionally, GitHub users can monitor and modify other projects’ code using “Watch”, “Star”, and “Fork” functionalities. The service also provides teams with the ability to specify licensing information for their projects.\nWhen you’re working collaboratively, you and your collaborators can use Git to send changes back and forth. If you’re using GitHub with Git, it provides a variety of tools to organize and coordinate your work. This chapter describes some of those tools.\nAn issue or ticket is a description of a specific task to be completed in a project. The name “issue” originated in software development, where posting an issue is the primary way to report a bug (a literal issue with how the software works). Nowadays, most projects use issues to keep track of all development tasks, not just those related to bugs. An issue tracker is a website where people can create, view, and comment on issues for a project.\nWe recommend that researchers use issues in two ways. First, issues are a great way to plan, discuss, and coordinate a project, even if computing is only a small part of it. Think of the issue tracker as a to-do list. GitHub provides a variety of features for organizing issues, such as tags, deadlines, assigned owners, checklists, sub-issues, and more.\nSecond, for any project that depends on open-source software, the issue trackers for those software can be a good source of information when trying to diagnose problems. You can search the issue tracker for related issues, which might include workarounds to use until the problem is fixed. If there isn’t an issue for the problem, and you’re confident it’s a bug, you can create an issue to (hopefully) get help from the developers. In the event that you fix a bug or add a feature to the software yourself, the issue tracker is also the first place to go to get your contributions officially added.\nLet’s practice using issues on GitHub. Make sure you’re logged in to GitHub, then go to the page for DataLab’s Git sandbox repository. Click on the “Issues” tab near the top of the page. You should see a page like the one in Figure 30.1. This is the issue tracker for the repository. In the screenshot, there are no issues shown, but you might see issues in the tracker on your screen.\nAn issue can be open or closed; an open issue is one that is still in progress, while a closed issue is one that’s not, typically because it’s either complete or canceled. By default, GitHub issue trackers only show open issues. You can see closed issues instead by clicking on the “Closed” tab.\nClick on the green “New Issue” button to create a new issue on the repository. You’ll be taken to a page like the one in Figure 30.2.\nLet’s fill in the issue with a title and a description. In the title, put a non-invasive question for your fellow learners. For example, you could put a question like, “What’s your favorite tree on campus?”. Try to think of a unique question. In the description, state the question again, and write 1-2 sentences explaining why you think it’s an interesting question. You can use Markdown to format the description of an issue.\nWhen you’re done writing, click on the green “Create” button. GitHub will create the new issue and take you to its page, as in Figure 30.3. Notice that GitHub assigns a unique number to each issue in a repository, so that you can easily cross-reference issues. The issue in the screenshot is number 19. GitHub also allows you to comment on your issue, which is a great way to post follow-up information (for corrections to the issue description, it’s usually better to use the “Edit” option in the issue’s “…” menu).\nNow that you’ve created an issue on the sandbox repository, find a partner and exchange issue numbers. Then find your partner’s issue on the tracker. When writing an issue description or comment, you can reference another issue by typing a number sign (#) and the issue number. GitHub will automatically create a link with the issue’s title. Try this out by replying to your partner’s issue. Answer their question and mention your issue, as in Figure 30.4.\nFinally, go back to the issue your created and read your partner’s comment. Then close the issue by pressing the “Close issue” button. You can close any issue you create, as well as any issue anyone creates on your repositories. Generally, you shouldn’t close an issue unless you feel it’s been completed or resolved.\nOn public-facing open-source projects, issues are the primary way to report bugs and request features. On research projects, issues are a good way to plan next steps, document your work as you go, and coordinate with collaborators. Issues are only public on public repositories, so if you choose to work in a private repository, your issues will be private as well.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Working with GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/week10/working-with-github.html#sec-whats-an-issue",
    "href": "chapters/week10/working-with-github.html#sec-whats-an-issue",
    "title": "30  Working with GitHub",
    "section": "",
    "text": "Figure 30.1: The issue tracker for DataLab’s Git sandbox repository.\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.2: GitHub’s new issue page.\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.3: An issue just after being created.\n\n\n\n\n\n\n\n\n\n\nFigure 30.4: A comment on an issue.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen you create an issue on someone else’s repository, especially if it’s an open-source project and you want to report a bug, craft the description carefully. Make sure to include information about the context in which you encountered the problem and details about how to reproduce it, such as your operating system, version of the software, and the steps you took that led to the problem. Be polite and remember that developers of open-source projects are usually unpaid volunteers, so they might not be able to address your issue right away. If it’s urgent, consider trying to fix it yourself and if you succeed, contribute the fix to the project.\nSee this guide for more advice about how to write a good issue.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Working with GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/week10/working-with-github.html#forks-remotes",
    "href": "chapters/week10/working-with-github.html#forks-remotes",
    "title": "30  Working with GitHub",
    "section": "30.2 Forks & Remotes",
    "text": "30.2 Forks & Remotes\nA fork is a copy on GitHub of some other repository on GitHub. Forking a repository has two purposes:\n\nThe fork is your copy, so you can make whatever changes you want (as long as you satisfy the project’s license agreement). If you develop a bug fix or a cool new feature on the fork, you can contribute it back to the original repository. If the original developers are not interested or don’t respond to communications, you can still work on your fork.\nThe fork serves as a backup in case the original repository is deleted.\n\nAs an example, suppose you want to contribute a change to DataLab’s Git sandbox repository. Since you don’t own the repository, you can’t push changes to it directly. If you know the owner and they trust you, you can ask them to give you access as a collaborator (as in Section 9.3). But that won’t be the case for most projects.\nInstead, you can get your changes on GitHub by forking the repository. Once they’re on GitHub, you can, if you want, make a request to the owner of the original repository to incorporate your changes (more about this in Section 30.3). Let’s do this with the Git sandbox repository.\nMake sure you’re logged in to GtiHub and open the repository’s GitHub page in your web browser. In the row of buttons in the top right part of the page, click on “Fork”. GitHub will take you to a page like the one shown in Figure 30.5, where you can edit the name and description for the fork. Leave the name and description as-is, and click on the “Create fork” button.\n\n\n\n\n\n\nFigure 30.5: GitHub’s “Create a new fork” page.\n\n\n\nAfter clicking the button, GitHub will take you to the page for the new fork, as in Figure 30.6. Notice that the owner of the repository is you rather than ucdavisdatalab. There are also two special buttons on the page: “Contribute” and “Sync fork”. We’ll explain more about the “Contribute” button soon. The “Sync fork” button merges the latest commits from the original repository into your fork—you can use it to update your fork if the original repository changes.\n\n\n\n\n\n\nFigure 30.6: The fork’s GitHub page.\n\n\n\nIf you’ve been following along with the examples, at this point you’ve worked with three different copies of the Git sandbox repository:\n\nDataLab’s original repository on GitHub\nThe local copy you cloned in Chapter 29\nThe fork you just created\n\nMost of the time, if you want to make changes to a repository, it’s easiest to do so with a local copy. So go back to your terminal and the local copy. Switch to the main branch:\ngit switch main\nChange to the directory with the current date in YYYY-MM-DD format, or use 2025-05-01 if you don’t see a directory for the current date. Inside the directory there’s a README.md file. Open the file in a text editor. It should look something like this, but might have a different date and question:\n# 2025-05-01\n\nWhat are your 3 favorite places to eat in Davis, CA?\nClose README.md without saving. In the same directory, use a text editor to make a new file called USERNAME.md, where USERNAME is your GitHub username. In the file, answer the question (one answer per line). Save the file, then git add and git commit the changes.\nNow that we’ve changed the local repository, we need to get the changes to GitHub. The local repository already knows about DataLab’s repository on GitHub, since it was cloned from there. More precisely, Git knows that DataLab’s repository is a remote for the local repository. You can see all of the remotes for a repository with the git remote command:\ngit remote\norigin\nWhen you clone a repository, Git names the remote origin by default.\nMost of the time, it’s best to run git remote with the -v flag (short for --verbose), so that you can also see the URLs for each remote:\ngit remote -v\norigin  git@github.com:ucdavisdatalab/sandbox_git.git (fetch)\norigin  git@github.com:ucdavisdatalab/sandbox_git.git (push)\nIn the verbose output, Git prints each remote twice, since remotes can have different URLs for fetching and pushing commits (although this is uncommon). More importantly, we can see that origin does indeed refer to DataLab’s repository.\nIn order to push changes from the local repository to the fork, you first need to let Git know that the fork exists by adding it as a remote. On the fork’s GitHub page, click on the green “Code” button, and copy the URL under the SSH tab. Then go back to the terminal, type git remote add fork, and paste the URL after the command. The full command should look like this:\ngit remote add fork git@github.com:nick-ulle/sandbox_git.git\nThis command tells Git to add a new remote named fork to the repository, with the provided URL. Check the repository’s remotes one more time:\ngit remote -v\nfork    git@github.com:nick-ulle/sandbox_git.git (fetch)\nfork    git@github.com:nick-ulle/sandbox_git.git (push)\norigin  git@github.com:ucdavisdatalab/sandbox_git.git (fetch)\norigin  git@github.com:ucdavisdatalab/sandbox_git.git (push)\nWe didn’t have to call the new remote fork, but it’s a good name here because it makes it clear which remote is which. When dealing with multiple remotes, try to choose descriptive, unambiguous names.\nWith the remote added, you can push your changes there. Push the main branch to the remote fork:\ngit push fork main\nEnumerating objects: 6, done.\nCounting objects: 100% (6/6), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (4/4), 1.58 KiB | 1.58 MiB/s, done.\nTotal 4 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nTo github.com:nick-ulle/sandbox_git.git\n   f1976a1..6bebf75  main -&gt; main\nRefresh the GitHub page for your fork to make sure you can see the changes there. The page should look something like Figure 30.7.\n\n\n\n\n\n\nNote\n\n\n\nYou can remove remotes you’re no longer using from a repository with the git remote remove command. For instance, if you no longer want to use the fork remote:\ngit remote remove fork",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Working with GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/week10/working-with-github.html#sec-making-a-pull-request",
    "href": "chapters/week10/working-with-github.html#sec-making-a-pull-request",
    "title": "30  Working with GitHub",
    "section": "30.3 Making a Pull Request",
    "text": "30.3 Making a Pull Request\nA pull request is a request in GitHub to merge one branch into another. Pull requests provide a way to contribute changes back to a repository you don’t control. You can also use pull requests in your own repositories to discuss and refine work, manage contributions from collaborators, and document major changes.\n\n\n\n\n\n\nFigure 30.7: The fork after pushing a commit.\n\n\n\nThe changes you made to the Git sandbox repository are now online on your fork, but they’re still not in DataLab’s original repository. You can contribute your changes by making a pull request. GitHub provides several different ways to create a pull request; one is to click the “Contribute” button on a fork’s GitHub page (as in Figure 30.7).\nOn your fork of the sandbox, go ahead and click the “Contribute” button, and click the green “Open pull request” button in the pop-up box. GitHub will take you to a “Comparing changes” page like the one in Figure 30.8. The page has spaces for a title and description, similar to when we created an issue in Section 30.1. In fact, pull requests are just a special kind of issue.\n\n\n\n\n\n\nFigure 30.8: GitHub’s “Comparing changes” page, to create a new pull request.\n\n\n\nWhen you create a pull request, GitHub automatically fills in the title with the commit message from the source branch’s most recent commit. If the branch has multiple new commits, it’s a good practice to change the title to something that summarizes the overall effect of the pull request. You should also fill in the description with a longer explanation of what your pull request does—don’t leave it blank!\nOnce you’ve filled in the title and description, click the green “Create pull request” button. GitHub will take you to the page for the new pull request, as in Figure 30.9.\n\n\n\n\n\n\nFigure 30.9: The new pull request.\n\n\n\nA pull request stays open until it is merged or closed without merging. While a pull request is open, anyone with access to the source branch can push changes, and GitHub will automatically update the request. This way the author of the request can address feedback, or collaborators can even jump in and make corrections themselves.\nThe tabs along the top of the pull request page show information about the changes to be merged. On the “Conversation” tab, you and your collaborators can comment on the request just like you would on an issue.\nUnder the “Files changed” tab, you can see diffs for all of the files changed by the pull request, as in Figure 30.10. You can also add comments on specific lines. Examining the files changed is a great way to make sense of what a pull request will actually do, and commenting on lines is a convenient way to provide feedback to collaborators.\n\n\n\n\n\n\nFigure 30.10: The “Files changed” tab on a pull request.\n\n\n\nThe “Files changed” tab is also the main interface for reviewing a pull request. Reviewing pull requests is generally the responsibility of the owner of the destination branch. The can choose to either accept the pull request, marking it as ready to merge, or flag it as in need of more work. In the latter case, the author can push new commits to their branch and then request another review.\nWith a pull request open, now you have to wait until your request is approved. Congratulations on making your first pull request!",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Working with GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/week10/working-with-github.html#github-flow",
    "href": "chapters/week10/working-with-github.html#github-flow",
    "title": "30  Working with GitHub",
    "section": "30.4 GitHub Flow",
    "text": "30.4 GitHub Flow\nPull requests are the foundation of GitHub flow, a workflow for collaborating on repositories and projects on GitHub. To follow GitHub flow for a repository, first you need to do some setup:\n\nIf you don’t have write access to the repository, create a fork.\nClone a copy of the repository (if you have write access) or your fork (if you don’t).\n\nOnce you’re set up, each time you want to make a new contribution, the steps in GitHub flow are:\n\nCreate a new, local branch with a short, descriptive name.\nCommit changes on the branch.\nPush your changes to GitHub.\nCreate a pull request when you’re ready to contribute the changes back to the original repository.\nAddress feedback from reviewers through discussion on the pull request and/or by pushing additional changes.\nMerge the pull request.\nDelete the branch.\n\nYou can read more about GitHub flow in the official documentation.\n\n\n\n\n\n\nNoteSee Also\n\n\n\nGitHub flow is simplification of Git flow, a workflow designed for collaboratively developing software for a large audience, where maintaining stability and rolling out changes gradually is paramount.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Working with GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/week10/making-git-comfortable.html",
    "href": "chapters/week10/making-git-comfortable.html",
    "title": "31  Making Git Comfortable",
    "section": "",
    "text": "31.1 Ignoring Files\nLet’s briefly consider files that you don’t want Git to track. These are typically:\nAs an example, here’s a file listing for a directory containing resources for a workshop about text mining in R that DataLab offers:\nNotice the .DS_Store file and .git/ directory. These are hidden configuration files for macOS and Git, respectively. Git is pretty smart, and it knows to ignore its own hidden configuration files, but we need to tell it explicitly to ignore the other one using a special file called .gitignore.\nYou can create a .gitignore file from scratch with a text editor. In the file, simply add the names of files and directories that you want Git to ignore, putting one per line. For example:\nNote in this example that we’ve also added the data/ folder and any of its contents to .gitignore. This is for two reasons, which we discussed earlier: 1) it’s usually not a good idea to track data files with Git; 2) the free version of GitHub (which you’re probably using) puts a cap on the total size of a repository. It would be a waste of space to have data eating away at that size limit.\nGenerally speaking, you should place your .gitignore file in the root of your repository, where it will control Git behavior for the repository. GitHub provides a nice repository of template .gitignore files for various types of development here.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Making Git Comfortable</span>"
    ]
  },
  {
    "objectID": "chapters/week10/making-git-comfortable.html#ignoring-files",
    "href": "chapters/week10/making-git-comfortable.html#ignoring-files",
    "title": "31  Making Git Comfortable",
    "section": "",
    "text": "Private files that shouldn’t be shared. Examples in this category include configuration files with passwords, authentication tokens, SSH keys, and data sets with personally identifiable information or other sensitive information.\nNuisance files that aren’t a meaningful part of your project. Examples include hidden files created automatically by your computer’s operating system and intermediate files generated by your project’s workflows.\nLarge files (over 1-10 MB), because most Git hosting services have repository size limits and large files tend to slow down Git. Examples in this category include data sets and outputs. It’s usually easiest to store these files on a private server or a regular file hosting service, such as Google Drive or Box. If you need to distribute large files with your repository, consider using tools such as Git Large File Storage and Data Version Control.\n\n\nls -a\n.   .DS_Store   D3VIS   gephi_tutorial   r_networks   scraping\n..  .git        data    kumu_tutorial    readme.txt   text_mining\n\n\n.httr-oauth\n.DS_Store\n.config\ndata/*",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Making Git Comfortable</span>"
    ]
  },
  {
    "objectID": "chapters/week10/making-git-comfortable.html#configuring-git",
    "href": "chapters/week10/making-git-comfortable.html#configuring-git",
    "title": "31  Making Git Comfortable",
    "section": "31.2 Configuring Git",
    "text": "31.2 Configuring Git\nProper configuration can make Git more comfortable and convenient to use. The git config manual page lists Git’s many configuration options. You can set options globally or per-repository; in practice, we’ve found the former more useful. To set an option, use git config set --global with the option’s name and setting, as explained in Section 8.1. Settings are stored in a configuration file, so if you prefer, you can edit the file directly. To open the global configuration file in Git’s default text editor, run:\ngit config edit --global\nThe file contains sections with key-value pairs. Each section begins with a header in square brackets [ ]. Key-value pairs are indented within each section and written key = value. Lines beginning with the number sign # are ignored as comments. For example:\n# Name and email.\n[user]\n  name = Nick Ulle\n  email = naulle@ucdavis.edu\nOptions listed in the manual always have two parts separated by a dot (.); these are the section and key. For instance, the option user.name corresponds to the section user and key name.\nGit’s default text editor is usually vi, but if you don’t like vi you can set it to something else with the core.editor option. For example, if you prefer micro, you can set:\n[core]\n  editor = micro\n\nThe setting should either be the name of a program you can run from the terminal or the absolute path to a program.\n\n\n\n\n\n\nTip\n\n\n\nTwo settings we recommend for a smoother Git experience are:\n[push]\n  autoSetupRemote = true\n[pull]\n  ff = only\nThe push.autoSetupRemote option controls whether Git automatically sets a local branch’s upstream when you run git push. With the option set to true, the first time you run git push on a branch, its upstream will be set to the specified remote.\nThe pull.ff option controls what Git does when you run git pull. With pull.ff set to only, Git will abort any pull that requires a regular (not fast-forward) merge. This is helpful for avoiding accidental merge commits, and you can still run a regular merge when needed with git merge.\n\n\n\n31.2.1 Aliases\nIn Git, an alias is a name for another command. Aliases are great shortcuts for commands that are long or difficult to remember. You can set aliases in the alias section of Git’s configuration file; the key should be whatever you want the alias’ name to be. The value of the alias should be a Git command without git at the beginning. For example, to make an alias unstage that runs git reset -- to remove a change from the staging area:\n[alias]\n  unstage = reset --\nYou can also set aliases with git config set --global.\nAfter setting an alias, you can use it like any other Git command. So to use the unstage alias in a repository:\ngit unstage\nAny arguments you pass to an alias are appended to the end of the command. For instance, if you only want to remove changes to README.md from the staging area, you can specify this when you run git unstage:\ngit unstage README.md\n\n\n\n\n\n\nTip\n\n\n\nHere are a few more useful aliases:\n[alias]\n  # Show a diff for changes in the staging area.\n  staged = diff --staged\n\n  # Show a graph of the repository's 10 most recent commits.\n  ls = log --graph --oneline --all -10\n\n  # Show the first commit for a file or directory.\n  origin = log --follow --diff-filter=A --\n\n  # Show number of commits per author for a file or directory.\n  authors = shortlog --numbered --summary --\n\n  # Print the path to the top level of a repository.\n  root = rev-parse --show-toplevel\nWith the root alias, to quickly change to the top level of a repository from any of its subdirectories, run:\ncd `git root`",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Making Git Comfortable</span>"
    ]
  },
  {
    "objectID": "chapters/week10/making-git-comfortable.html#getting-help",
    "href": "chapters/week10/making-git-comfortable.html#getting-help",
    "title": "31  Making Git Comfortable",
    "section": "31.3 Getting Help",
    "text": "31.3 Getting Help\nGit has a well-deserved reputation as a tool that’s difficult to learn and use. One of the reasons for this is the minimal design of Git’s original command line interface. Since Git’s first release in 2005, contributors have made several improvements to the interface, but it still has rough edges. Hopefully it will continue to improve.\nIn this reader, we’ve focused a small set of tasks we consider important, and where the Git interface provides multiple ways to do something, we’ve attempted to choose the simplest (which is generally the newest). Nevertheless, if you continue using Git, you’ll eventually encounter a problem or need to do something that we didn’t explain.\nThere’s a wealth of information about Git online. The book Pro Git by Chacon and Straub is a great reference to learn more and look for help. The website Dangit, Git!?! explains how to solve several frequently-encountered problems, and Julia Evans’ blog post Confusing Git Terminology explains Git jargon. Stack Overflow, a programming question and answer site, contains many questions and answers about Git, and if you can’t find one that helps you, you can post a new question.",
    "crumbs": [
      "Week 10",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Making Git Comfortable</span>"
    ]
  },
  {
    "objectID": "chapters/11_optical-character-recognition.html",
    "href": "chapters/11_optical-character-recognition.html",
    "title": "32  Optical Character Recognition",
    "section": "",
    "text": "32.1 What is Optical Character Recognition?\nThis lesson focuses on extracting data from non-digital sources, such as printed documents, using several packages for optical character recognition (OCR).\nMuch of the data we’ve used in the course thus far has been born-digital. That is, we’ve used data that originates from a digital source and does not exist elsewhere in some other form. Think back, for example, to the lecture on strings in R (?sec-strings-and-regular-expressions): your homework required you to type text directly into RStudio, manipulate it, and print it to screen. But millions, even billions, of data-rich documents do not originate from digital sources. The United States Census, for example, dates back to 1790; we still have these records and could go study them to get a sense of what the population was like hundreds of years ago. Likewise, printing and publishing far precedes the advent of computers; much of the literary record is still bound up between the covers books or stowed away in archives. Computers, however, can’t read the way we read, so if we wanted to use digital methods to analyze such materials, we’d need to convert them into a computationally tractable form. How do we do so?\nOne way would be to transcribe documents by hand, either by typing out plain text versions with word processing software or by using other data entry methods like keypunching to record the information those documents contain. Amazon’s Mechanical Turk service is an example of this kind of data entry. It’s also worth noting that, for much of the history of computing, data entry was highly gendered and considered to be “dumb”, secretarial work that young women would perform. Much of the divisions between “cool” coding and computational grunt work that, in a broad, cultural sense, continue to inform how we think about programming, and indeed who gets to program, stem from such perceptions. In spite of (or perhaps because of) such perceptions, huge amounts of data owe their existence to manual data entry. That said, the process itself is expensive, time consuming, error-prone, and, well, dull.\nOptical character recognition, or OCR, is an attempt to offload the work of digitization onto computers. Speaking in a general sense, this process ingests images of print pages (such as those available on Google Books or HathiTrust), applies various preprocessing procedures to those images to make them a bit easier to read, and then scans through them, trying to match the features it finds with a “vocabulary” of text elements it keeps as a point of reference. When it makes a match, OCR records a character and enters it into a text buffer (a temporary data store). Oftentimes this buffer also includes formatting data for spaces, new lines, paragraphs, and so on. When OCR is finished, it outputs its matches as a data object, which you can then further manipulate or analyze using other code.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Optical Character Recognition</span>"
    ]
  },
  {
    "objectID": "chapters/11_optical-character-recognition.html#loading-page-images",
    "href": "chapters/11_optical-character-recognition.html#loading-page-images",
    "title": "32  Optical Character Recognition",
    "section": "32.2 Loading Page Images",
    "text": "32.2 Loading Page Images\nOCR “reads” by tracking pixel variations across page images. This means every page you want to digitize must be converted into an image format. For the purposes of introducing you to OCR, we won’t go through the process of creating these images from scratch; instead, we’ll be using ready-made examples. The most common page image formats you’ll encounter are PDF and PNG. They’re lightweight, portable, and usually retain the image quality OCR software needs to find text.\nThe pdftools package is good for working with these files:\n\n# install.packages(\"pdftools\")\nlibrary(\"pdftools\")\n\nOnce you’ve downloaded/installed it, you can load a PDF into RStudio from your computer by entering its path as a string and assigning that string to a variable, like so:\n\npdf &lt;- \"data/pdf_sample.pdf\"\n\nNote that we haven’t used a special reater function, like read.csv or readRDS. The pdftools package will grab this file from its location and load it properly when you run a process on it.\n\n\n\n\n\n\nNote\n\n\n\nYou can also just write the string out in whatever function you want to call, but we’ll keep our pdf variable for the sake of clarity.\n\n\nThe same method works with web addresses. We’ll be using web material. First, write out an address and assign it to a variable.\npdf &lt;- \"https://datalab.ucdavis.edu/adventures-in-datascience/pdf_sample.pdf\"\nSome PDF files will have text data already encoded into them. This is especially the case if someone made a file with word processing software (like when you write a paper in Word and email a PDF to your TA or professor). You can check whether a PDF has text data with the pdf_text function. Assign the function’s output to a variable and print it to screen with message, like so:\n\ntext_data &lt;- pdf_text(pdf)\nmessage(text_data)\n\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\n\n\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\n\n                                             The quick brown fox jumps over the lazy dog.\n                                             The quick brown fox jumps over the lazy dog.\n                                             The quick brown fox jumps over the lazy dog.\n                                             The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\n\n                       The quick brown fox jumps over the lazy dog.\n                       The quick brown fox jumps over the lazy dog.\n                       The quick brown fox jumps over the lazy dog.\n                       The quick brown fox jumps over the lazy dog.\n                       The quick brown fox jumps over the lazy dog.\n\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\n\n                                               The quick brown fox jumps over the lazy dog.\n                                               The quick brown fox jumps over the lazy dog.\n                                               The quick brown fox jumps over the lazy dog.\n                                               The quick brown fox jumps over the lazy dog.\n\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\n\n                       The quick brown fox jumps over the lazy dog.\n                       The quick brown fox jumps over the lazy dog.\n                       The quick brown fox jumps over the lazy dog.\n                       The quick brown fox jumps over the lazy dog.\n\n\nNotice the printout recreates the original formatting from the PDF. If you were to use the print function on the text output, you’d see all the line breaks and spaces pdf_text created to match its output with the file. This re-creation would be even more apparent if you were to save the output to a new file with write. Doing so would produce a close, plain text approximation of the original PDF.\nYou can also process multi-page PDF files with pdf_text. It can transcribe whole books and will keep them in a single text buffer, which you can then assign to a variable or save to a file. Keep in mind, however, that if your PDF files have headers, footers, page numbers, chapter breaks, or other such paratextual information, pdf_text will include these in its output.\nIf, when you run pdf_text, you find that your file already contains text data, you’re set! There’s no need to perform OCR and you can immediately start working with your data. However, if you run the function and find that it outputs a blank character string, you’ll need to OCR it. The next section shows you how.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Optical Character Recognition</span>"
    ]
  },
  {
    "objectID": "chapters/11_optical-character-recognition.html#running-ocr",
    "href": "chapters/11_optical-character-recognition.html#running-ocr",
    "title": "32  Optical Character Recognition",
    "section": "32.3 Running OCR",
    "text": "32.3 Running OCR\nFirst, you’ll need to download/install another package, tesseract, which complements pdftools. The latter only loads/reads PDFs, whereas tesseract actually performs OCR. Download/install tesseract:\n\n# install.packages(\"tesseract\")\nlibrary(\"tesseract\")\n\nLet’s use a different PDF to try out tesseract:\nnew_pdf &lt;- \"https://jeroen.github.io/images/ocrscan.pdf\"\nTo run OCR on this PDF, use the following:\n\nocr_output &lt;- ocr(new_pdf)\n\nPrint the output to screen with message and see if the process worked:\n\nmessage(ocr_output)\n\nTHE SLEREXE COMPANY LIMITED\nSAPORS LANE - BOOLE - DORSET - BH 25 8ER\ne sous (4513) 617 - Tk 12345\nOur Ref. 350/BIC/EAC 186 Janvary, 1972.\nDe. PN, Cundall,\nKining Surveys Lid.,\nHolroyd Road,\nReading,\nBerks.\nDear Pece,\n\nPernit ne to introduce you to the facility of facsinile\ntransmission.\n\nIn facainile a photocell is coused to perforn a raster scan over\nthe subject copy. The varistions of princ density on the docunent\ncause the photecell o generate an analogous electrical video signal.\nThis signal is used to mdulate a carrier, vhich is cransmitted to o\ncemote destination over & radio or cable commnications link.\n\n¢ the remote cerminal, demodulation reconstructs the video\nsignal, which is used to modulate the density of print produced by @\nprinting device. Tnis device is scanning in 4 raster scan synchronised\nUich that at the cransmitring terminal. As & result, a facsimile\ncopy of the subject document is produced.\n\nProbably you have uees for this facility in your organisation.\n\nYours sincerely,\nP.J. cross\nGroup Leader - Facsinile Research\n\n\nVoila! You’ve just digitized text. The formatting is a little off, but things look good overall. And most importantly, it looks like everything has been transcribed correctly.\nAs you ran this process, you might’ve noticed that a new PNG file briefly appeared on your computer. This is because tesseract converts the PDF file to PNG file as part of its behind-the-scenes pre-processing work and silently deletes the PNG file when it finishes running. If you have a collection of PDF files that you’d like to OCR, it can sometimes be faster and less memory intensive to convert them all to PNG files first. You can perform this conversion like so:\n\npng &lt;- pdf_convert(\n  new_pdf, format = \"png\", filenames = \"images/ch11/png_example.png\"\n)\n\nWarning in sprintf(filenames, pages, format): 2 arguments not used by format\n'images/ch11/png_example.png'\n\n\nConverting page 1 to images/ch11/png_example.png... done!\n\n\nIn addition to returning the a PNG object in your R session, the pdf_convert function will also save the file in your working directory. You could, for example, use a for-loop and a vector of paths to PDF files to convert all of them to PNG files. Since pdf_convert saves them to disk, they can be stored until you’re ready to OCR them.\npdfs &lt;- c(\"list.pdf\", \"of.pdf\", \"files.pdf\", \"to.pdf\", \"convert.pdf\")\noutfiles &lt;- c(\"list.png\", \"of.png\", \"files.png\", \"to.png\", \"convert.png\")\n\nfor (i in 1:length(pdfs)) {\n  pdf_convert(pdfs[i], format=\"png\", filenames=outfiles[i])\n}\nThe ocr function works with a number of different file types (typically images). For instance, it accepts PNGs as well as PDFs:\n\npng_ocr_output &lt;- ocr(png)",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Optical Character Recognition</span>"
    ]
  },
  {
    "objectID": "chapters/11_optical-character-recognition.html#accuracy",
    "href": "chapters/11_optical-character-recognition.html#accuracy",
    "title": "32  Optical Character Recognition",
    "section": "32.4 Accuracy",
    "text": "32.4 Accuracy\nIf you use message to print the output from OCRing the PNG file in the example above, you might notice that the text is messier than it was when we used pdf_text_ocr:\n\nmessage(png_ocr_output)\n\nTHE SLEREXE COMPANY LIMITED\nSAPORS LANE - BOOLE - DORSET - BH 25 8ER\ne sous (4513) 617 - Tk 12345\nOur Ref. 350/BIC/EAC 186 Janvary, 1972.\nDe. PN, Cundall,\nKining Surveys Lid.,\nHolroyd Road,\nReading,\nBerks.\nDear Pece,\n\nPernit ne to introduce you to the facility of facsinile\ntransmission.\n\nIn facainile a photocell is coused to perforn a raster scan over\nthe subject copy. The varistions of princ density on the docunent\ncause the photecell o generate an analogous electrical video signal.\nThis signal is used to mdulate a carrier, vhich is cransmitted to o\ncemote destination over & radio or cable commnications link.\n\n¢ the remote cerminal, demodulation reconstructs the video\nsignal, which is used to modulate the density of print produced by @\nprinting device. Tnis device is scanning in 4 raster scan synchronised\nUich that at the cransmitring terminal. As & result, a facsimile\ncopy of the subject document is produced.\n\nProbably you have uees for this facility in your organisation.\n\nYours sincerely,\nP.J. cross\nGroup Leader - Facsinile Research\n\n\nThis doesn’t have to do with the PNG file format per se but rather with the way we created our file. If you open it, you’ll see that it’s quite blurry, which has made it harder for ocr to match the text it represents:\n\n\n\n\n\n\nFigure 32.1\n\n\n\nThis blurriness is because pdf_convert defaults to 72 dots per inch (DPI). DPI is a measure of how many pixels, or dots, a digital image file uses to represent an inch of the image. DPI quantifies resolution and originated in inkjet printing. More pixels means higher image resolution, though this comes with a trade off: images with a high DPI are also bigger and take up more space on your computer. Usually, a DPI of 150 is sufficient for most OCR jobs, especially if your documents were printed with technologies like typewriters, dot matrix printers, and so on, and if they feature fairly legible typefaces (Times New Roman, for example). A DPI of 300, however, is ideal. You can set the DPI in pdf_convert by adding a dpi argument in the call:\n\nhi_res_png &lt;- pdf_convert(\n  new_pdf, format=\"png\", dpi=150,\n  filenames=\"images/ch11/hi_res_png_example.png\"\n)\n\nWarning in sprintf(filenames, pages, format): 2 arguments not used by format\n'images/ch11/hi_res_png_example.png'\n\n\nConverting page 1 to images/ch11/hi_res_png_example.png... done!\n\n\nAnother function, ocr_data, outputs a data frame that contains all of the words tesseract found when it scanned through your image, along with a column of confidence scores. These scores, which range from 0-100, provide valuable information about how well the OCR process has performed, which in turn may tell you whether you need to modify your PDF or PNG files further before OCRing them (more on this below). Generally, you can trust scores of 93 and above.\nTo get confidence scores for an OCR job, call ocr_data and subset the confidence column, like so:\n\nocr_data &lt;- ocr_data(hi_res_png)\nconfidence_scores &lt;- ocr_data$confidence\nconfidence_scores\n\n  [1] 92.00555 92.16185 91.26955 91.26955 93.30071 92.80251 93.25327 71.22106\n  [9] 93.01385 87.86584 89.25087 89.25087 39.46595 39.46595 96.18760 92.64400\n [17] 88.31395 96.62357 91.38167 88.73389 88.73389 87.54987 87.54987 92.04705\n [25] 89.52077 91.29449 90.80710 92.46021 92.75455 89.72379 92.27145 92.01097\n [33] 91.00894 89.22992 91.89604 91.02854 92.30912 90.34227 90.34227 91.70280\n [41] 92.35736 91.17986 91.17986 92.30919 89.62785 82.76822 40.42221 91.66215\n [49] 91.92680 93.24316 90.77785 90.77785 91.95331 91.95331 92.47021 93.19882\n [57] 91.73550 91.52353 91.88087 61.49971 91.50322 92.83280 88.38924 88.38924\n [65] 92.01638 92.67615 91.98049 43.38534 90.48783 92.81612 92.41699 91.47611\n [73] 91.84003 91.84003 92.20630 86.01997 91.97392 92.16437 91.10802 92.11120\n [81] 89.69808 92.23833 92.46779 91.11872 91.73578 91.94142 91.98083 92.07896\n [89] 92.07896 92.64109 91.36734 91.36734 91.05901 92.81266 92.59015 91.31464\n [97] 90.42278 90.42278 92.18396 92.17403 92.28313 91.59138 90.33145 92.01377\n[105] 92.01377 91.19365 91.39406 91.58421 79.49812 91.90497 91.70367 91.50892\n[113] 92.86395 93.07679 91.78714 92.10359 92.12737 91.94938 89.91416 91.05914\n[121] 92.03944 93.12405 93.26999 93.26999 92.23213 90.81768 70.74520 92.39217\n[129] 89.55022 86.20355 89.34053 88.06285 92.31555 93.13564 92.01375 91.13263\n[137] 91.13263 92.17674 92.80893 91.49842 90.36536 90.36536 91.31169 92.65086\n[145] 92.95564 91.97475 91.96663 91.11794 91.92394 91.60635 91.32370 91.32370\n[153] 91.62907 93.08089 92.95688 92.60538 92.07363 91.69480 92.84332 92.31062\n[161] 91.96275 91.96792 91.56029 91.68050 92.09031 84.39836 88.10213 92.19061\n[169] 92.44096 93.29742 92.41998 91.57234 84.95621 85.53573 85.33029 91.52122\n[177] 96.01897 88.61853 82.43198 89.51816 77.73746 76.97222 71.24612 58.29185\n[185] 66.72427 46.87411\n\n\nThe mean is a good indicator of the overall OCR quality:\n\nconfidence_mean &lt;- mean(confidence_scores)\nconfidence_mean\n\n[1] 88.93843\n\n\nLooks pretty good, though there were a few low scores that dragged the score down a bit. Let’s look at the median:\n\nconfidence_median &lt;- median(confidence_scores)\nconfidence_median\n\n[1] 91.68765\n\n\nWe can work with that!\nIf we want to check our output a bit more closely, we can do two things. First, we can look directly at ocr_data and compare, row by row, a given word and its confidence score.\n\nhead(ocr_data, 25)\n\n          word confidence              bbox\n1       SAPORS   92.00555   422,194,497,208\n2         LANE   92.16185   508,194,560,208\n3            -   91.26955   570,203,575,205\n4        BOOLE   91.26955   585,193,651,208\n5            -   93.30071   661,202,666,205\n6       DORSET   92.80251   676,193,755,208\n7            -   93.25327   764,203,769,205\n8         BH25   71.22106   780,193,831,208\n9            8   93.01385   842,193,850,208\n10          ER   87.86584   856,194,883,208\n11   TELEPHONE   89.25087   449,232,534,243\n12       BOOLE   89.25087   544,232,589,243\n13        (945   39.46595   600,229,634,246\n14         13)   39.46595   640,229,664,246\n15       51617   96.18760   675,229,719,244\n16           -   92.64400   730,237,735,240\n17       TELEX   88.31395   746,232,792,243\n18      123456   96.62357   804,228,857,244\n19         Our   91.38167   211,392,246,408\n20        Ref.   88.73389   261,391,306,407\n21 350/PJC/EAC   88.73389   325,389,459,409\n22        18th   87.54987   863,389,910,405\n23    January,   87.54987  924,389,1020,408\n24       1972.   92.04705 1038,388,1095,405\n25         Dr.   89.52077   212,492,244,508\n\n\nThat’s a lot of information though. Something a little more sparse might be better. We can use base R’s table function to count the number of times unique words appear in the OCR data. We do this with the word column in our ocr_data variable from above:\n\nocr_vocabulary &lt;- table(ocr_data$word)\nocr_vocabulary &lt;- as.data.frame(ocr_vocabulary)\n\nLet’s look at the first 30 words:\n\nhead(ocr_vocabulary, 30)\n\n             Var1 Freq\n1               -    5\n2               .    1\n3            (945    1\n4               1    1\n5          123456    1\n6             13)    1\n7            18th    1\n8           1972.    1\n9            2038    1\n10    350/PJC/EAC    1\n11          51617    1\n12              8    1\n13             80    1\n14              a    9\n15             an    1\n16      analogous    1\n17             As    1\n18             at    1\n19             At    1\n20         Berks.    1\n21           BH25    1\n22          BOOLE    2\n23             by    1\n24          cable    1\n25       carrier,    1\n26          cause    1\n27         caused    1\n28 communications    1\n29           copy    1\n30          copy.    1\n\n\nThis representation makes it easy to spot errors like discrepancies in spelling. We could correct those either manually or with string matching. One way to further examine this table is to look for words that only appear once or twice in the output; among such entries you’ll often find misspellings. The table does, however, have its limitations. Looking at this data can quickly become overwhelming if you send in too much text. Additionally, notice that punctuation “sticks” to words and that uppercase and lowercase variants of words are counted separately, rather than together. These quirks are fine, useful even, if we’re just spot-checking for errors, but we’d need to further clean this data if we wanted to use it in computational text analysis. A later lecture will discuss other methods that we can use to clean text.\nWhen working in a data-forensic mode with page images, it’s a good idea to pull a few files at random and run them through ocr_data to see what you’re working with. OCR accuracy is often wholly reliant on the quality of the page images, and most of the work that goes into digitizing text involves properly preparing those images for OCR. Adjustments include making sure images are converted to black and white, increasing image contrast and brightness, increasing DPI, and rotating images so that their text is more or less horizontal. The tesseract package performs some of these tasks itself, but you can also do them ahead of time and often you’ll have more control over quality this way. The tesseract documentation goes into detail about what you can do to improve accuracy before even opening R; we can’t cover this in depth, but keep the resource in mind as you work with this type of material. And remember: the only way to completely trust your accuracy is to go through the OCR output yourself. It’s a very common thing to have to make small tweaks to output. In this sense, we haven’t quite left the era of hand transcription.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Optical Character Recognition</span>"
    ]
  },
  {
    "objectID": "chapters/11_optical-character-recognition.html#unreadable-text",
    "href": "chapters/11_optical-character-recognition.html#unreadable-text",
    "title": "32  Optical Character Recognition",
    "section": "32.5 Unreadable Text",
    "text": "32.5 Unreadable Text\nAll that said, these various strategies for improving accuracy will only get you so far if your page images are composed in a way OCR just can’t read. OCR systems contain a lot of in-built assumptions about what “normal” text is, and they are incredibly brittle when they encounter text that diverges from that norm. Early systems, for example, required documents to be printed with special, machine-readable typefaces; texts that contained anything other than this design couldn’t be read. Now, OCR is much better at handling a variety of text styling, but systems still struggle with old print materials like blackletter.\n\n\n\n\n\n\nFigure 32.2\n\n\n\nRunning:\n\nballad &lt;- \"https://ebba.english.ucsb.edu/images/cache/hunt_1_18305_2448x2448.jpg\"\nballad_out &lt;- ocr(ballad)\n\nProduces:\n\nmessage(ballad_out)\n\n- @€ QADdifcriptionof Roxtons faleehod ’\nafPozkie byze, and of hig fatall favewel.\nfLhe fatal fineof Lraitours loes ;\n% 4Bp Fufice due, deferupngfoe. - 1\nT Iate (alas) the great bntrath  Ehe Crane wwoldefipebptothe punte,  Roto, bis futerpug long (be fure) o\nDf Traitours, hoto (£ fped g heardit once of olve s _ @diplipay bis foes atlaf: : ,\nT boliff to knoin, Hal berez.wae Sndwoith the Lpng of bpaoes did frine Pig mercpe moued once atwap, |\nPotv late allegeance fev, 1Bp Fame, 3 beardittolves = Pe Hall them &uigbt out caff i\n« 3f Reucrsrage agamnit the Hea. GAnddofvoeihe welve not fal benoy aaith fentence infk fo2 theit butruth, .\nAnd ffwell Wwith oddeine rapnes 1But higher Epld nipMOUAL 2 and bacakpng of his fwpll s :\nPotu glao are they to fall agapne, ST il pait her teach((aithoroevepote) £Lhe fruits oftheir fedicfous (s, :\nAndtracetheic onted traine? Shame madea backevecour SChebarnes of cavth thail fpll.\n8l 36 five by fozce Wwolde fozge the fall 3 touch no Grmes herefn atall, TCheir foules God wot foze clogd 1 crime\nDf any fumptuoule place, AButfhetna fable fopfes __ Qnd theirpofteritie\nB 36 water flods byd bim leaueof, T hofemozall fence doth repr 4Befpotted foze With theirabule,\ns flames he wyll difgrace, DEclpmers bye theguple. Ano fEand by theit follie.\n| 3t Goo command the Wwpides to ceafe, Tuho buploes a houle of many 5 heiclinpngs left theiv namea hante,\ni is blaffes ave lapd full lotu ¢ anbdlaith not ground Wogkr heir dedes ith poplon fped: :\ni 3fBod command the feas focalme, 4Butdoth ertotethe groundk i3&gt;  Theivdeathesawage foz wantof grace §\n: 2Chey wyll notrage 02 flolu. Pis bufldpng can notdure. SCheir honours quite is dead.\n4 Gl thinges at Gods commandemet be, @ TWhofekes furmifingtodilp SCheir flefh tofedethe kptes and crolues |\n3f betheir fateregarde s : a Ruler fentbp GOD 2 ACheir arnies a mase foa men s b\nHl andnoman liues whole veffinie 35 fubiect fure, Deuoide of grace. Cheir guerdon as examples are ¥\n1By hum i8 bupzeparde. Checauleof his otonerod. 2Co dafh dolte Dunces den.\n{15t when a man foxfakes theip, - Q byve that Wyl hernefEdefple SChaotn bp pout fouts pou Auggifh foxte  §\n8 - anorowlesin allowing Wwaues 1By vight Hould lofe a wyngs 3 oumumming malkyng route : -;\nAnd of bis boluntarfe wyll, Gud then is e no fying fo Wi CErtoll your erclamations bp, g\n! Bis onite god hap depraues 1But ot agother thyng. ABaals chapletnes,champions ffoute,\ni Wow fal hehopeto lcape the gulfe 2 anbhe that lofeth all at games, Datke fute fo2 pardons, papilts braue,\n: Bot Hal be thinke todeale 2 D3 fpendes infoiole ercefle: Foztraitours indulgence\n. 1o Mal bis fanfie baing him lound Qnbhopes by hapsto bealehisharme, . fend ot fome purgatosic (craps, i\n£ 4T o @afties MHoze Wwith laple 2 Mufk vzinke of deare piffrefle, Some WBulls tuith Peter pence.\n; Botv hall is fratght in fine (uccede 2 o fpeatie of bapdles fovelirapne D fwavime of D2ones, how dare pefipl |\n4 Qlas what Hall hegapne 2 Trhis wylfull wapardereive $ Twith labourpng Wees contend\n| wubatfeare by oams Do matke bimquake hey cave not foz the boke of Gov, Pou fonght fo2honie from thehiues, v\ni Hotw ofte [ubleceto papne 2 1o P2inces, men bntrue. 1B ut gall you foundinend. b\not funozie times in Dangers ven o cuntepe, caufers of much woe, SChefe tafpes oo Wwafk, their fings beont &\n33 thootone theman bnivple 2 ﬁts'i‘faitbfull frdendes, afall: heir fpight topll nofanaple: i\n8l @uhoclimes Withouten bolde ot bye, b ftheiv otone effates, afipng, ZChefle Peacocks pronvearenaked lefee [\n= ABewware, 3 hint aduise. T 5 others, avpeasgall. DF theiv difplayed taple. h\n8 Qllfuch as teatt to falle confrads, D 1Loz0e, hotu long thefe Liserdsluvke, Chele Lurkpe cocks fu cullourred,\n3 D2 feseet harmes confpive? BGos GO D, holv greata fuhple _ Holong banelurkt alofe &\n{ 15¢(ure, with Hoxtonsithey hal taffe  Wlere thepin hand with fefgnedbarts  Ehe WBeare (althongh but fot of fwote) \"\nQ right deferucd bire, heir cuntrye to defple? Path pluct bis wynges by piofe,\n& Thep can not loke foz better (pede, oto did thep frame their turniture?  SCbe Mone ber bozowenlighthath loff, [\nDo death foz (uch to fell 7 Poiv fitthey mave theirtoles : Sbe wapnedas wele\n& Godgrantthe futticeof the Wozive Poiw Hymon feught our englplh Trofe  TWhohoped by bap of othersharnes, 4\nPut by the papnes of bell, o bapng to Romaine feoles. 4 full Poneonce tobie,\ni oz fuchapentiuccale it is, Potu Himon Pagus playd bis parte, 4The Lpon (uffred long the Wull,\n: TChat Cnglifh barts diddare 39t 1Babilon balvoe dibrage: 1is noble mpnd totrye:\n& 700 palle the boundes of dutieslatve, Potw WBafan bulles begon to bell, Wntpll the 1Bull Wwas rageypng wod, :\n: D3 of their cuntriecare. Hofu Judas fought bis wage. dndfrombisfakedid hye. 3\ni dnomerciehath (o longreleal 13oin Jannes and Jambzes 0id abyde fChen time it was to bid him Fap\n] Dfenvours (God doth knofv) TChe baunt of baaineficke ads, Perfozce, bishomestocut\ni andbountic of our curteous Nuécne Yot Dathan, Choze, Abivam (md Andmake him leauebis rageing tunes ¢\nS T long hath fpared her foe. o dath out Poyples faas. 3In(cilence to be put. £\nb 1But Gov, Wwhofegrace fpiresherharte, ol Romaine marchant feta frefh Andall the calues of WBalan kynd i\ni AW pll not abyoethe fpight g pardons baauea fale, Are weaned from their Wwifh s i\ni Df RAebels rage, Wwho rampets veach Potw aliapes fomeagain( the aenty  ELheBivcan Tigers tanmed notn, 0\ni Fromber, hee title quight. Tioloe dzeame afencelestale.  dlemathon eatesnofifh). i\nB4 Qithough e dotwe inpititull seale, BGos bicar frombis god receaucd 4Beholoe befoze pour balefull epes\nAnd loueth to fucke noblod 3 Che kepes to lofeand bpnds Lhepurchace of pour pacte,\ni1 ¢t Gona caueat topll her lend 4Baals chaplein thoght he fire wolk ™ 1o SHutuey pour fodefneforolful fight\nI appealethole Wipers mode, Huch was his pagan mynd. i ith fighes of dubble harte,\nB q man that (s bis bouleon fire, Gob 1Lozve hotn bits the tert theie ts  Lament thelackeof pouralies i\nvl feke to quench the flame : TThat faith fuch men thall bé  Religiousrebellgalls 5\nit Clsfrom thefpoylefomepacteconuey, Futheic reltgﬂonbo;no;tnlhz ABetuepethat pll fuccele of pours, b\nC1s feke the heate to tame, D much bavietie. Come curfe pour (odeine fall, G\nR wnho (@ea penthoule wether beate, and fund2p (02tsof fects furk nd Wben pe hauc pour guiles out fought\n| And heares a boiftroule fopndes iuifion Mall appeare &lt; And all your craft appzoued,\nil s3utheoefull fafetic of himlelte, dgaintE thefatber; fonnefhe  ue, Peccanimus Hall be your fong .\nT pll foace him fuccour fynde 2 Gaint mother, daughter 2 Pour ground wozke is renoued,\nAChepitifull pacient Pellican, 9 it not come to pafe trofy pra? QAud lokehotv Poztons (ped their wills\n; Per blod although Hee Hed e ea, baftards furethey bees €uen (o thetr fee MHall haue, e\nil 3oct Wopll (hee femeber dateto end, qrho our gwd mother Nuane o, . 4 o better et thenthope to gapne e\n!‘ n ﬂbzcarle hﬁzwnung befped. b m(lgthtzang rzbelllnu?xz.g ol 15ut gallotwes without graue, :\ni1 he Cagle fipnges ber yong ones dotone Lan o0 bis bengeance long etais ey 3\nit what ﬁfght lnf fg]nne vefule 2 mbell;z})is e (pruants ficle CEINTS léha ibl'nn. L\n8l Wnperfed foiles (e deadly hates, Fniuvfoule (pights of godlelemeit, @Xg * i\n. Anv rightlp fuch mirble, \\ wuhe turne as doth & Whele2 A 0 @‘@ @A@ 2\n£ » it\n¢ @ Tmp2inted at London by Fleyandet Lacfe, foz Hentrie Tipskeham, divellyng at the figne\no o of the blacke 3Bope, at themivdle {po2th Doze of Paules chuveh, 2 :\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy the way, this example used a JPG file. The ocr function can handle those too!\n\n\nEven though every page is an “image” to OCR, OCR struggles with imagistic or unconventional page layouts, as well as inset graphics. Add to that sub-par scans of archival documents, as in the newspaper page below, and the output will contain way more errors than correct matches.\n\n\n\n\n\n\nFigure 32.3\n\n\n\n\nnewspaper &lt;- \"https://chroniclingamerica.loc.gov/data/batches/mimtptc_inkster_ver01/data/sn88063294/00340589130/1945120201/0599.pdf\"\nnewspaper_ocr &lt;- ocr(newspaper)\n\nBeautifully messy output results:\n\nmessage(newspaper_ocr)\n\n\n\n\nOne strategy you might use to work with sources like this is to crop out everything you don’t want to OCR. This would be especially effective if, for example, you had a newspaper column that always appeared in the top left-hand corner of the page. You could preprocess your page images so that they only showed that part of the newspaper and left out any ads, images, or extra text. Doing so would likely increase the quality of your OCR output. Such a strategy can be achieved outside of R with software ranging from Adobe Photoshop or the open-source GIMP to Apple’s Automator workflows. Within R, packages like tabulizer and magick enable this. You won’t, however, be required to use these tools in the course, though we may have a chance to demonstrate some of them during lecture.\nThere are several other scenarios where OCR might not be able to read text. Two final (and major) ones are worth highlighting. First, for a long time OCR support for non-alphabetic writing systems was all but nonexistent. New datasets have been released in recent years that mostly rectify these absences, but sometimes support remains spotty and your mileage may vary. Second, OCR continues to struggle with handwriting. While it is possible to train unsupervised learning processes on datasets of handwriting and get good results, as of yet there is no general purpose method for OCRing handwritten texts. The various ways people write just don’t conform to the standardized methods of printing that enable computers to recognize text in images. If, someday, you figure out a solution for this, you’ll have solved one of the most challenging problems in computer vision and pattern recognition to date!",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Optical Character Recognition</span>"
    ]
  },
  {
    "objectID": "chapters/12_statistics.html",
    "href": "chapters/12_statistics.html",
    "title": "33  Statistics",
    "section": "",
    "text": "33.1 Introduction\nIt is useful to begin with some concrete examples of statistical questions to motivate the material that we’ll cover in this lesson. This will also help confirm that your R environment is working.\n# install.packages(\"ggformula\")\n# install.packages(\"mosaic\")\n# remotes::install_github(\"ProjectMOSAIC/mosaicModel\")\nlibrary(\"ggformula\")\nlibrary(\"mosaic\")\nlibrary(\"mosaicModel\")\nNow load the data sets:\nmice_pot = read.csv(\"https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/mice_pot.csv\")\nbarnacles = read.csv(\"https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/barnacles.csv\")\nBirths78 = read.csv(\"https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/births.csv\")\nsmoking = read.csv(\"https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/smoking.csv\")\nadipose = read.csv(\"https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/adipose.csv\")",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/12_statistics.html#introduction",
    "href": "chapters/12_statistics.html#introduction",
    "title": "33  Statistics",
    "section": "",
    "text": "Note\n\n\n\nThe examples that follow use several data sets, which we read directly from CSV files.\nThe data sets come from the fosdata package, which you can optionally install to your computer in order to get access to all of the associated help files. The fosdata package is hosted on GitHub but not CRAN, so to install it you need another package, remotes, for its install_github function. Here’s how to install both:\n# This is optional!\n# install.packages(\"remotes\")\nremotes::install_github(\"speegled/fosdata\")\n\n\n\n\n\n\n33.1.1 The mice_pot Data Set\nThe mice_pot data set comes from an experiment where four groups of mice were dosed with different levels of THC. There was a low, medium, and high dosage group, as well as a control group that got no THC. The mice were then observed for a while and their total movement was quantified as a percentage of the baseline group mean. Two statistical questions that might arise here are:\n\nWere there differences in the typical amount of movement between mice of different groups?\nWhat was the average amount of movement by mice in the medium dose group?\n\nBoth of these questions can be approached by summarizing the sample with descriptive statistics. Here’s one way to compute the average (mean) movement for each group:\n\naggregate(mice_pot[\"percent_of_act\"], mice_pot[\"group\"], mean)\n\n  group percent_of_act\n1   0.3       97.32250\n2     1       99.05235\n3     3       70.66787\n4   VEH      100.00000\n\n\nThe means aren’t identical! So there are clearly differences between all of the groups, right? Yes, in terms of this sample. But if you want to generalize your conclusion to cover what would happen to other mice that weren’t in the study, then you need to think about the population. In this case, that’s the population of all the mice that could have been dosed with THC.\nBecause we can’t see data from mice that weren’t part of the study, we rely on statistical inference to reach conclusions about the population. How is that possible? Statistical methods can tell us about the distribution of the sample relative to the population.\n\n\n33.1.2 The barnacles Data Set\nThis data set was collected by counting the barnacles in 88 grid squares on the Flower Garden Banks coral reef in the Gulf of Mexico. The counts were normalized to barnacles per square meter. Some questions that you might approach with statistical methods are:\n\nWhat is the average number of barnacles per square meter, and is it greater than 300?\n\nYou can use R to compute the average:\n\nmean(barnacles$per_m)\n\n[1] 332.0186\n\n\nFrom that calculation, we see that the mean is 332 barnacles per square meter, which is greater than 300. But again, the first calculation has told us only about the mean of the particular locations that were sampled. Wouldn’t it be better to answer the questions in reference to the number of barnacles per square meter of reef, rather than square meter of measurement? Here, the population is the entire area of the Flower Garden Banks reef. Again, we will be able to answer the questions relative to the entire reef by working out the sample mean’s distribution relative to the population.\n\n\n33.1.3 Sample and Population\nSamples and populations are fundamental concepts in statistics. A sample is data—the hard numbers that go into your calculations. The population is trickier: it’s the units to which you are able to generalize your conclusions.\nFor the barnacles data, in order to draw conclusions about the population (the entire Flower Garden Banks reef), the sample must be carefully selected to ensure it is representative. For instance, randomly sampling locations so that any location on the reef might be selected is one sampling strategy.\nFor the mice_pot data, the population is all the mice that might have been selected for use in the experiment. How big that population is depends on how the mice were selected for the experiment. Randomly selecting the experimental units from a group is a common way of ensuring that the results can generalize to that whole group.\nA non-random sample tends to mean that the population to which you can generalize is quite limited. What sort of population do you think we could generalize about if we recorded the age of everyone in this class?",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/12_statistics.html#uses-of-simulation",
    "href": "chapters/12_statistics.html#uses-of-simulation",
    "title": "33  Statistics",
    "section": "33.2 Uses of Simulation",
    "text": "33.2 Uses of Simulation\nThe study of statistics started in the 1800s, but slowly. Most statistical methodology and theory was developed during the first half of the 20th century—a time when data and processing power were in short supply. Today, that’s not so much the case. If you did the assigned reading, then you saw that statisticians are very much still grappling with how to teach statistics in light of the advances in computing over the past 40 years.\nTraditionally, statisticians are very concerned with assessing the normality of a sample, because the conclusions you get from traditional statistical methods depend on a sample coming from a normal distribution. Nowadays, there are a lot of clever methods that can avoid the need to assume normality. We’re going to learn some of those methods, because they usually don’t require any complicated math. If you want to know more, one of the assigned readings was the introduction to a book that would be a great reference for self-guided study.\nWe will use simulation-based methods extensively today.\nThis is the density curve of a standard normal distribution:\n\n\n\n\n\n\n\n\n\nAnd this is a histogram of samples taken from that same distribution:\n\n# Sample 20 numbers from a standard normal and draw the histogram\nx = rnorm(20)\nround(sort(x), 2)\n\n [1] -1.86 -1.83 -1.39 -1.22 -0.84 -0.76 -0.57 -0.29 -0.09  0.06  0.08  0.20\n[13]  0.31  0.36  0.48  0.57  0.75  0.78  0.85  1.45\n\nhist(x)\n\n\n\n\n\n\n\n\nDo the numbers seem to come from the high-density part of the normal density curve? Are there any that don’t? It isn’t surprising if some of your x samples are not particularly close to zero. One out of twenty (that’s five percent) samples from a standard normal population are greater than two or less than negative two, on average. That’s “on average” over the population. Your sample may be different.\nHere is the density of the exponential distribution:\n\n\n\n\n\n\n\n\n\nAnd here is a histogram of 20 samples taken from that distribution:\n\n# Sample 20 numbers from a histogram and plot the histogram\nex = rexp(20)\nround(sort(ex), 2)\n\n [1] 0.04 0.09 0.22 0.23 0.28 0.29 0.45 0.46 0.56 0.56 0.61 0.64 0.76 0.84 0.93\n[16] 0.97 1.04 1.19 1.38 3.21\n\nhist(ex)\n\n\n\n\n\n\n\n\nThe histograms are clearly different, but it would be difficult to definitively name the distribution of the data by looking at a sample.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/12_statistics.html#mathematical-statistics",
    "href": "chapters/12_statistics.html#mathematical-statistics",
    "title": "33  Statistics",
    "section": "33.3 Mathematical Statistics",
    "text": "33.3 Mathematical Statistics\nThe mean has some special properties: you’ve seen how we can calculate the frequency of samples being within an interval based on known distributions. But we need to know the distribution. It turns out that the distribution of the sample mean approaches the normal distribution as the sample size increases, for almost any independent data. That allows us to create intervals and reason about the distribution of real data, even though the data’s distribution is unknown.\n\n33.3.1 Law of Large Numbers\nThe Law of Large Numbers says that if the individual measurements are independent, then the mean of a sample tends toward the mean of the population as the sample size gets larger. This is what we’d expect, since we showed the rate at which the variance of the sample mean gets smaller is \\(1/n\\).\n\nnn = c(1, 2, 4, 8, 12, 20, 33, 45, 66, 100)\nmeans = sapply(nn, function(n) mean(rnorm(n)))\n\nplot(nn, means, bty = 'n', ylab = \"sample mean\")\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\n\n33.3.2 Central Limit Theorem\nThe most important mathematical result in statistics, the Central Limit Theorem, says that if you take (almost) any sample of random numbers and calculate its mean, the distribution of the mean tends toward a normal distribution. We illustrate the “tending toward” with an arrow and it indicates that the distribution of a sample mean is only approximately normal. But if the original samples were from a normal distribution then the sample mean has an exactly normal distribution. From here, I’ll start writing the mean of a random variable \\(X\\) as \\(\\bar{X}\\) and the mean of a sample \\(x\\) as \\(\\bar{x}\\).\n\\[ \\bar{X} \\rightarrow N(\\mu, \\frac{\\sigma^2}{n}) \\]\nAnd because of the identities we learned before, you can write this as\n\\[\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\rightarrow N(0, 1) \\]\nThis is significant because we can use the standard normal functions on the right, and the data on the left, to start answering questions like, “what is the 95% confidence interval for the population mean?”\n\n# Generate 20 samples from a uniform distribution and plot their histogram\nN = 20\nu = rexp(N)\nhist(u)\n\n\n\n\n\n\n\n# Generate 100 repeated samples of the same size, calculate the mean of each\n# one, and plot the histogram of the means.\nB = 100\nmeans = numeric(B)\nfor (i in 1:B) {\n  means[[i]] = mean(rexp(N))\n}\n\nhist(means)\n\n\n\n\n\n\n\n\nWhat happens as B and N get larger or smaller? Do they play different roles?",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/12_statistics.html#statistical-inference",
    "href": "chapters/12_statistics.html#statistical-inference",
    "title": "33  Statistics",
    "section": "33.4 Statistical Inference",
    "text": "33.4 Statistical Inference\n\n33.4.1 Confidence Intervals\nRecall the mice_pot data set, which contains data from an experiment where mice were dosed with THC and then measured for motor activity as a percentage of their baseline activity. We are going to look at the group that got a medium dose of THC.\n\n# Extract just the mice that got the medium dose of THC\nmice_med = mice_pot[ mice_pot$group == 1, ]\n\n# Assess normality with histogram and QQ plot\nhist(mice_med$percent_of_act)\n\n\n\n\n\n\n\nqqnorm(mice_med$percent_of_act)\n\n\n\n\n\n\n\n\n\n33.4.1.1 Finding Confidence Intervals\nNow we are using our sample to make some determination about the population, so this is statistical inference. Our best guess of the population mean is the sample mean, mean(mice_med$percent_of_act), which is 99.1%. But to get a confidence interval, we need to use the formula:\n\\[ \\bar{x} \\pm t_{n-1, 0.1} * S / \\sqrt{n} \\]\nFortunately, R can do all the work for us:\n\n# 80% confidence interval for location of mice_med mean:\nt.test(mice_med$percent_of_act, conf.level = 0.8)\n\n\n    One Sample t-test\n\ndata:  mice_med$percent_of_act\nt = 13.068, df = 11, p-value = 4.822e-08\nalternative hypothesis: true mean is not equal to 0\n80 percent confidence interval:\n  88.71757 109.38712\nsample estimates:\nmean of x \n 99.05235 \n\n\n\n\n\n33.4.2 Two-population Test\nThe test of \\(\\mu_0 = 100\\) is a one-population test because it seeks to compare a single population against a specified standard. On the other hand, you may wish to assess the null hypothesis that the movement of mice in the high-THC group is equal to the movement of mice in the medium-THC group. This is called a two-population test, since there are two populations to compare against each other. The null hypothesis is \\(\\mu_{0, med} = \\mu_{0, high}\\). Testing a two-population hypothesis requires first assessing normality and also checking whether the variances are equal. There are separate procedures when the variances are equal vs. unequal.\n\n# Extract the samples to be compared\ngroup1 = mice_pot$percent_of_act[mice_pot$group == 1]\ngroup3 = mice_pot$percent_of_act[mice_pot$group == 3]\n\n# Check for equal variances---these are close enough\nvar(group1)\n\n[1] 689.4729\n\nvar(group3)\n\n[1] 429.4551\n\n# Confirm equal variances with a boxplot\nboxplot(group1, group3)\n\n\n\n\n\n\n\n# Check whether the high-THC mice movement is normal\n# (we already checked for the medium-dose mice)\nqqnorm(group3)\n\n\n\n\n\n\n\n# Two-pop test\nt.test(group1, group3, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  group1 and group3\nt = 2.7707, df = 20, p-value = 0.0118\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  7.014608 49.754345\nsample estimates:\nmean of x mean of y \n 99.05235  70.66787 \n\n\n\n\n33.4.3 Hypothesis Tests for Non-normal Data\nJust as with the confidence intervals, there is a bootstrap hypothesis test that can be used where the data are not normal. There are other options, too, with clever derivations. The one I’ll show you is the Wilcoxon test, which is based on the ranks of the data.\nSince we’ve already seen that the barnacles per square meter data are not normal, I will illustrate testing the null hypothesis that \\(\\mu_0 = 300\\) barnacles per square meter. This is a one-population test, and a two-sided alternative.\n\n# Wilcoxon test for 300 barnacles per square meter\nwilcox.test(barnacles$per_m)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  barnacles$per_m\nV = 3916, p-value = 3.797e-16\nalternative hypothesis: true location is not equal to 0",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/12_statistics.html#regression",
    "href": "chapters/12_statistics.html#regression",
    "title": "33  Statistics",
    "section": "33.5 Regression",
    "text": "33.5 Regression\nRegression is a mathematical tool that allows you to estimate how some response variable is related to some predictor variable(s). There are methods that handle continuous or discrete responses of many different distributions, but we are going to focus on linear regression here.\nLinear regression means that the relationship between the predictor variable(s) and the response is a linear one. To illustrate, we’ll create a plot of the relationship between the waist measurement and body mass index (BMI) of 81 adults:\n\n# Plot the relationship between the waist_cm and bmi variables\nwith(adipose, plot(waist_cm, bmi), bty = 'n')\n\n\n\n\n\n\n\n\nThe relationship between the two is apparently linear (you can imagine drawing a straight line through the data). The general mathematical form of a linear regression line is:\n\\[ y = a + \\beta x + \\epsilon \\]\nHere, the response variable (BMI) is called \\(y\\) and the predictor (waist measurement) is \\(x\\). The coefficient \\(\\beta\\) indicates how much the response changes for a change in the predictors (that is, the expected change in BMI with a 1 cm change in waist measurement). Variable \\(a\\) denotes the intercept, which is a constant offset that aligns the mean of \\(y\\) with the mean of \\(x\\). Finally, \\(\\epsilon\\) is the so-called residual error in the relationship. It represents the variation in the response that is not due to the predictor(s).\n\n33.5.1 Fitting a Regression Line\nThe R function to fit the model is called lm. Let’s take a look at an example:\n\n# Fit the linear regression BMI vs waist_cm\nfit = lm(bmi ~ waist_cm, data = adipose)\n\n# Plot the fitted regression: begin with the raw data\nwith(adipose, plot(waist_cm, bmi, bty = 'n'))\n\n# Now plot the fitted regression line (in red)\nabline(coef(fit)[[1]], coef(fit)[[2]], col = 'red')\n\n\n\n\n\n\n\n\n\n\n33.5.2 Assumptions and Diagnostics\n“Fitting” a linear regression model involves estimating \\(a\\) and \\(\\beta\\) in the regression equation. You can can do this fitting procedure using any data, but the results won’t be reliable unless some conditions are met. The conditions are:\n\nA linear model is appropriate (linearity).\nThe residual error is normally distributed.\nThe variance of the residual error is constant for all observations.\nObservations are independent.\n\nThe first of these conditions can’t be checked—it has to do with the design of the experiment. The rest can be checked, though, and I’ll take them in order.\n\n33.5.2.1 Checking Linearity\nIn the case of a simple linear regression model (one predictor variable), you can check this by plotting the predictor against the response and looking for a linear trend. If you have more than one predictor variable, then you need to plot the predictions against the response to look for a linear trend. We’ll see an example by adding height as a predictor for BMI (in addition to waist measurement).\n\n# Linear model for BMI using waist size and height as predictors\nfit2 = lm(bmi ~ waist_cm + stature_cm, data=adipose)\n\n# Plot the fitted versus the predicted values\nplot(fit2$fitted.values, adipose$bmi, bty = 'n')\n\n\n\n\n\n\n\n\n\n\n33.5.2.2 Checking that the Residuals Are Normally Distributed\nWe have already learned about the QQ plot, which shows visually whether some values are Normally distributed. In order to depend upon the fit from a linear regression model, we need to see that the residuals are Normally distributed, and we use the QQ plot to check.\n\n\n33.5.2.3 Checking that the Variance Is Constant\nIn an earlier part, we saw that the variance is the average of the squared error. But that would just be a single number, when we want to see if there is a trend. So like the QQ plot, you’ll plot the residuals and use your eyeball to discern whether there is a trend in the residuals or if they are approximately constant - this is called the scale-location plot. The QQ plot and scale-location plot are both created by plotting the fitted model object\n\n# Set up the pattern of the panels\nlayout(matrix(1:4, 2, 2))\n\n# Make the diagnostic plots\nplot(fit)\n\n\n\n\n\n\n\n\nThe “Residuals vs. Fitted” plot is checking whether the linear model is correct. There should be no obvious pattern if the data are linear (as is the case here). The Scale-Location plot will have no obvious pattern if the variance of the residuals is constant, as is the case here (you might see a slight pattern in the smoothed red line but it isn’t obvious). And the QQ plot will look like a straight line if the residuals are from a Normal distribution, as is the case here. So this model is good. The fourth diagnostic plot is the Residuals vs. Leverage plot, which is used to identify influential outliers. We won’t get into that here.\n\n\n\n33.5.3 Functions for Inspecting Regression Fits\nWhen you fit a linear regression model, you are estimating the parameters of the regression equation. In order to see those estimates, use the summary() function on the fitted model object.\n\n# Get the model summary\nsummary(fit2)\n\n\nCall:\nlm(formula = bmi ~ waist_cm + stature_cm, data = adipose)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1290 -1.0484 -0.2603  1.2661  5.2572 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.38196    3.82700   3.758 0.000329 ***\nwaist_cm     0.29928    0.01461  20.491  &lt; 2e-16 ***\nstature_cm  -0.08140    0.02300  -3.539 0.000680 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.724 on 78 degrees of freedom\nMultiple R-squared:  0.844, Adjusted R-squared:   0.84 \nF-statistic:   211 on 2 and 78 DF,  p-value: &lt; 2.2e-16\n\n\nHere you can see that the average marginal effect of one additional centimeter of waist measurement is to increase BMI by 0.3 and an additional centimeter of height is associated with a change to BMI of r round(coef(fit2)[[3]], 2). You can get the coefficients from the fitted model object using the coef() function, and there are some other functions that allow you to generate the values shown in the summary table.\n\n# Get the coefficients of the fitted regression\nbeta = coef(fit2)\nround(beta, 2)\n\n(Intercept)    waist_cm  stature_cm \n      14.38        0.30       -0.08 \n\n\nGet the variance-covariance matrix:\n\nround(vcov(fit2), 4)\n\n\n# Compare the square root of the diagonals of the variance-covariance matrix to\n# the standard errors are reported in the summary table:\nse = sqrt(diag(vcov(fit2)))\n\n# Here are the standard errors:\nround(se, 3)\n\n(Intercept)    waist_cm  stature_cm \n      3.827       0.015       0.023 \n\n\n\n# Calculate the t-statistics for the regression coefficients (compare these to\n# the t-statistics reported in the summary table)\nt_stats = beta / se\n\n# Show the t-statistics:\nround(t_stats, 2)\n\n(Intercept)    waist_cm  stature_cm \n       3.76       20.49       -3.54 \n\n\n\n# Calculate the p-values:\npval = 2 * pt(abs(t_stats), df=78, lower.tail=FALSE)\nround(pval, 4)\n\n(Intercept)    waist_cm  stature_cm \n      3e-04       0e+00       7e-04 \n\n\n\n# This is the residual standard error:\nsd(fit2$residuals) * sqrt(80 / 78)\n\n[1] 1.72357\n\n# R-squared is the proportion of variance\n# explained by the regression model\nround(1 - var(fit2$residuals) / var(adipose$bmi), 3)\n\n[1] 0.844\n\n\n\n\n33.5.4 A Model that Fails Diagnostics\nWe’ve seen a model that has good diagnostics. Now let’s look at one that doesn’t. This time, we’ll use linear regression to make a model of the relationship between waist measurement and the visceral adipose tissue fat (measured in grams). The visceral adipose tissue fat is abbreviated vat in the data. First, since the model uses a single predictor variable, let’s look at the relationship with a pair plot.\n\n# plot the relationship between waist_cm and vat\nwith(adipose, plot(waist_cm, vat, bty = 'n'))\n\n\n\n\n\n\n\n\nThe plot is obviously not showing a linear relationship, which will violate one of the conditions for linear regression. Also, you can see that there is less variance of vat among the observations that have smaller waist measurements. So that will violate the assumption that the residual variance has no relationship to the fitted values. To see how these will show up in the diagnostic plots, we need to fit the linear regression model.\n\n# Estimate the model for vat\nfit_vat = lm(vat ~ waist_cm, data = adipose)\n\n# There is no problem creating the summary table:\nsummary(fit_vat)\n\n\nCall:\nlm(formula = vat ~ waist_cm, data = adipose)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-996.25 -265.96  -61.87  191.24 1903.46 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3604.196    334.241  -10.78   &lt;2e-16 ***\nwaist_cm       51.353      3.937   13.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 479 on 79 degrees of freedom\nMultiple R-squared:  0.6829,    Adjusted R-squared:  0.6789 \nF-statistic: 170.2 on 1 and 79 DF,  p-value: &lt; 2.2e-16\n\n# Show the diagnostic plots\nlayout(matrix(1:4, 2, 2))\nplot(fit_vat)\n\n\n\n\n\n\n\n\nThere is obviously a curved pattern in the Residuals vs. Fitted plot, and in the Scale vs. Location plot. Residuals vs. Fitted shows a fan-shaped pattern, too, which reflects the increasing variance among the greater fitted values. The QQ plot is not a straight line, although the difference is not as obvious. In particular, the upper tail of residuals is heavier than expected. Together, all of these are indications that we may need to do a log transformation of the response. A log transformation helps to exaggerate the differences between smaller numbers (make the lower tail heavier) and collapse some difference among larger numbers (make the upper tail less heavy).\n\n# Fit a regression model where the response is log-transformed\nfit_log = lm(log(vat) ~ waist_cm, data = adipose)\n\n# Plot the diagnostics for the log-transformed model\nplot(fit_log)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe diagnostics do not look good after the log transformation, but now the problem is the opposite: a too-heavy lower tail and residual variance decreases as the fitted value increases. Perhaps a better transformation is something in between the raw data and the log transform. Try a square-root transformation:\n\n# Fit a model where the vat is square root transformed\nfit_sqrt = lm(sqrt(vat) ~ waist_cm, data = adipose)\n\n# Plot the diagnostics for the log-transformed model\nplot(fit_sqrt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese look acceptable for real-world data.\n\n\n33.5.5 Predictions and Variability\nThere are two scales of uncertainty for a regression model: uncertainty in the fitted relationship, and the uncertainty of a predicted outcome. The uncertainty of a prediction is always greater because it is calculated by adding the uncertainty of the fitted line to the uncertainty of a single data point around that fitted line. We can illustrate using the example of the model we just created to relate the waist measurement to the square root of vat.\nFor this example, we’ll need the mvtnorm library to be loaded:\n\n# install.packages(\"mvtnorm\")\nlibrary(\"mvtnorm\")\n\n# Draw the data on the transformed scale\nwith(adipose, plot(waist_cm, sqrt(vat), bty = 'n'))\n\n# Plot the fitted regression line\nabline(coef(fit_sqrt)[[1]], coef(fit_sqrt)[[2]], col = 'red')\n\n# Plot 100 samples from the distribution of the regression line.\nfor (i in 1:100) {\n  cc = rmvnorm(n = 1, mean = coef(fit_sqrt), sigma = vcov(fit_sqrt))\n  abline(cc[[1]], cc[[2]], col = grey(0.8))\n}\n\n\n\n\n\n\n\n\nClearly, the variability of the data points is greater than the variability of the fitted line (that’s why they lie outside the envelope of the fitted lines). We can extract a confidence interval for fitted values or predictions with the predict function.\n\n# Draw the data on the transformed scale\nwith(adipose, plot(waist_cm, sqrt(vat), bty = 'n'))\n\n# Plot the fitted regression line\nabline(coef(fit_sqrt)[[1]], coef(fit_sqrt)[[2]], col = 'red')\n\n# Define some waist measurements where we'll construct confidence intervals\npred_pts = data.frame(waist_cm = c(70, 85, 110))\n\n# Calculate the 90% CI at each of the pred_pts\nff = predict(fit_sqrt, pred_pts, interval = \"confidence\", level = 0.9)\npp = predict(fit_sqrt, pred_pts, interval = \"prediction\", level = 0.9)\n\n# Convert the confidence intervals to data.frames\nff = as.data.frame(ff)\npp = as.data.frame(pp)\n\n# Add the three confidence intervals to the plots\n# (offset them a bit for clarity in the plot)\nfor (i in 1:3) {\n  lines(\n    x = rep(pred_pts$waist_cm[[i]] - 0.5, 2),\n    y = c(ff$lwr[[i]], ff$upr[[i]]),\n    col = 'blue',\n    lwd = 2\n  )\n\n  lines(\n    x = rep(pred_pts$waist_cm[[i]] + 0.5, 2),\n    y = c(pp$lwr[[i]], pp$upr[[i]]),\n    col = 'orange',\n    lwd = 2\n  )\n}\n\n# Add a legend\nlegend(\n  c(\"90% CI (fitted values)\", \"90% CI (predicted values)\"),\n  col = c(\"blue\", \"orange\"),\n  x = \"topleft\", lwd = 2, bty = 'n'\n)\n\n\n\n\n\n\n\n\nOne thing to notice about the confidence intervals is that the interval is smallest (so the precision of the estimation is greatest) at the mean of the predictor variable. This is a general rule of fitting regression.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/12_statistics.html#model-selection",
    "href": "chapters/12_statistics.html#model-selection",
    "title": "33  Statistics",
    "section": "33.6 Model Selection",
    "text": "33.6 Model Selection\nChoosing how to represent your data is a common task in statistics. The most common target is to choose the representation (or model) that does the best job of predicting new data. We set this target because if we have a representation that predicts the future, then we can say it must accurately represent the process that generates the data.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/12_statistics.html#cross-validation",
    "href": "chapters/12_statistics.html#cross-validation",
    "title": "33  Statistics",
    "section": "33.7 Cross-validation",
    "text": "33.7 Cross-validation\nUnfortunately, we rarely have information about the future, so there isn’t new data to predict. One way to do prediction with the available data is to break it into a training part and a testing part. You make represent the training part with a model, and then use it to predict the left-out testing part. If you then swap the to parts and repeat the process, you’ll have a prediction for every data point. This would be called two-fold cross validation because the data was broken into two parts.\nIt’s more common to break the data into more than two parts - typically five or ten or one per data point. Then one part is taken as the testing part and all the others go into the training part. The result is five-fold or ten-fold, or leave-one-out cross validation.\nLet’s use cross-validation to do model selection. The model this time is a representation of the number of births per day in 1978 in the United States.\n\n# Plot the data\ngf_point(births ~ day_of_year, color = ~wknd, data = Births78)\n\n\n\n\n\n\n\n# Make models with two through ten knots in the spline for day_of_year\nbmod2  = lm(births ~ wknd + ns(day_of_year,  2), data = Births78)\nbmod4  = lm(births ~ wknd + ns(day_of_year,  4), data = Births78)\nbmod6  = lm(births ~ wknd + ns(day_of_year,  6), data = Births78)\nbmod8  = lm(births ~ wknd + ns(day_of_year,  8), data = Births78)\nbmod10 = lm(births ~ wknd + ns(day_of_year, 10), data = Births78)\n\n# Plot the 2 and 10 knot models\nmod_plot(bmod2, births ~ day_of_year + wknd) +\n  geom_point(\n    mapping = aes(x = day_of_year, y = births, color = wknd),\n    data = Births78\n  )\n\n\n\n\n\n\n\nmod_plot(bmod10, births ~ day_of_year + wknd) +\n  geom_point(\n    mapping = aes(x = day_of_year, y = births, color = wknd),\n    data = Births78\n  )\n\n\n\n\n\n\n\n# Cross-validate to choose the best model\nmod_cv(bmod2, bmod4, bmod6, bmod8, bmod10, k = nrow(Births78), ntrials = 1)\n\n       mse  model\n1 190815.9  bmod2\n2 143305.1  bmod4\n3 104875.7  bmod6\n4 106094.2  bmod8\n5 107130.5 bmod10\n\n# Plot the data\nmod_plot(bmod6, births ~ day_of_year + wknd) +\n  geom_point(\n    mapping = aes(x = day_of_year, y = births, color = wknd),\n    data = Births78\n  )\n\n\n\n\n\n\n\n\nCross-validation suggests that six knots is the ideal number, because it has the smallest mean-squared error (mse). The resulting model looks good, too.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/13_reshaping-tabular-data.html",
    "href": "chapters/13_reshaping-tabular-data.html",
    "title": "34  Reshaping Tabular Data",
    "section": "",
    "text": "34.1 Introduction\nThis lesson focuses on how to identify untidy tabular data sets and reshape them to be tidy, in the sense described in Section 11.2.\nLet’s look at some examples of tidy and untidy data sets. The tidyr package provides examples, and as we’ll see later, it also provides functions to make untidy data sets tidy. As usual, we first need to load the package:\n# install.packages(\"tidyr\")\nlibrary(\"tidyr\")\nLet’s start with an example of tidy data. This data set is included in the tidyr package and records the number of tuberculosis cases across several different countries and years:\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\nWhen you first look at a data set, think about what the observations are and what the features are. If the data set comes with documentation, it may help you figure this out. Since this data set is a tidy data set, we already know each row is an observation and each column is a feature.\nFeatures in a data set tend to take one of two roles. Some features are identifiers that describe the observed subject. These are usually not what the researcher collecting the data is trying to find out. For example, in the tuberculosis data set, the country and year columns are identifiers.\nOther features are measurements. These are usually the reason the researcher collected the data. For the tuberculosis data set, the cases and population columns are measurements.\nThinking about whether features are identifiers or measurements can be helpful when you need to use tidyr to rearrange a data set.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Reshaping Tabular Data</span>"
    ]
  },
  {
    "objectID": "chapters/13_reshaping-tabular-data.html#columns-into-rows",
    "href": "chapters/13_reshaping-tabular-data.html#columns-into-rows",
    "title": "34  Reshaping Tabular Data",
    "section": "34.2 Columns into Rows",
    "text": "34.2 Columns into Rows\nTidy data rule 1 says each observation must have its own row. Here’s a table that breaks rule 1:\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\nAll of the numbers measure the same thing: cases. To make the data tidy, we must rotate the 1999 and 2000 column names into rows, one for each value in the columns. The new columns are year and cases.\nThis process means less columns (generally) and more rows, so the data set becomes longer.\nWe can use the pivot_longer function to rotate columns into rows. We need to specify:\n\nColumns to rotate as cols.\nName(s) of new identifier column(s) as names_to.\nName(s) of new measurement column(s) as values_to.\n\nHere’s the code:\n\npivot_longer(table4a, -country, names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\n\n\n\n\n\nNoteHow to Pivot Longer without tidyr\n\n\n\n\n\nYou also can do this without tidyr:\n\nSubset columns to separate 1999 and 2000 into two data frames.\nAdd a year column to each.\nRename the 1999 and 2000 columns to cases.\nStack the two data frames with rbind.\n\n\n# Step 1\ndf99 = table4a[-3]\ndf00 = table4a[-2]\n\n# Step 2\ndf99$year = \"1999\"\ndf00$year = \"2000\"\n\n# Step 3\nnames(df99)[2] = \"cases\"\nnames(df00)[2] = \"cases\"\n\n# Step 4\nrbind(df99, df00)\n\n# A tibble: 6 × 3\n  country      cases year \n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;\n1 Afghanistan    745 1999 \n2 Brazil       37737 1999 \n3 China       212258 1999 \n4 Afghanistan   2666 2000 \n5 Brazil       80488 2000 \n6 China       213766 2000",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Reshaping Tabular Data</span>"
    ]
  },
  {
    "objectID": "chapters/13_reshaping-tabular-data.html#rows-into-columns",
    "href": "chapters/13_reshaping-tabular-data.html#rows-into-columns",
    "title": "34  Reshaping Tabular Data",
    "section": "34.3 Rows into Columns",
    "text": "34.3 Rows into Columns\nTidy data rule 2 says each feature must have its own column. Let’s look at a table that breaks rule 2:\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\nHere the count column contains two different features: cases and population. To make the data tidy, we must rotate the count values into columns, one for each type value. New columns are cases and population.\nThis process means less rows and more columns, so the data set becomes wider.\nWe can use pivot_wider to rotate rows into columns. We need to specify:\n\nColumn names to rotate as names_from.\nMeasurements to rotate as values_from.\n\nHere’s the code:\n\npivot_wider(table2, names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n\n\n\n\nNoteHow to Pivot Wider without tidyr\n\n\n\n\n\nYou can also do this without tidyr:\n\nSubset rows to separate cases and population values.\nRemove the type column from each.\nRename the count column to cases and population.\nMerge the two subsets by matching country and year.\n\n\n# Step 1\ncases = table2[table2$type == \"cases\", ]\npop = table2[table2$type == \"population\", ]\n\n# Step 2\ncases = cases[-3]\npop = pop[-3]\n\n# Step 3\nnames(cases)[3] = \"cases\"\nnames(pop)[3] = \"population\"\n\n# Step 4\ntidy = cbind(cases, pop[3])\n\nThis code uses the cbind function to merge the two subsets, but it would be better to use the merge function. The cbind function does not use identifier columns to check that the rows in each subset are from the same observations.\n\n\n\n\n\n\n\n\n\nNoteSee also\n\n\n\nRun vignette(\"pivot\") for more examples of how to use tidyr.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Reshaping Tabular Data</span>"
    ]
  },
  {
    "objectID": "chapters/13_reshaping-tabular-data.html#separating-values",
    "href": "chapters/13_reshaping-tabular-data.html#separating-values",
    "title": "34  Reshaping Tabular Data",
    "section": "34.4 Separating Values",
    "text": "34.4 Separating Values\nTidy data rule 3 says each value must have its own cell. Here’s a table that breaks rule 3:\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nCells in the rate column contain two values: cases and population. These are two different features, so to make the data set tidy, we need to separate them into two different columns.\nSo how can we separate the rate column? The rate column is a character vector (you can check this with str(table3)), so we can use the string processing functions in the stringr package. In particular, we can use the str_split_fixed function:\n\nlibrary(\"stringr\")\n\ncolumns = str_split_fixed(table3$rate, fixed(\"/\"), 2)\n\nNow we have a character matrix where the values are in separate columns. Now we need to combine these with the original data frame. There are several ways to approach this, but to be safe, let’s make a new data frame rather than overwrite the original. First we make a copy of the original:\n\ntidy_tb = table3\n\nNext, we need to assign each column in the character matrix to a column in the tidy_tb data frame. Since the columns contain numbers, we can also use the as.numeric function to convert them to the correct data type:\n\ntidy_tb$cases = as.numeric(columns[, 1])\ntidy_tb$population = as.numeric(columns[, 2])\n\nExtracting values, converting to appropriate data types, and then combining everything into a single data frame is an extremely common pattern in data science.\nUsing stringr functions is the most general way to separate out values in a column, but the tidyr package also provides a function separate specifically for the case we just worked through. Either package is appropriate for solving this problem.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Reshaping Tabular Data</span>"
    ]
  },
  {
    "objectID": "chapters/14_network-analysis.html",
    "href": "chapters/14_network-analysis.html",
    "title": "35  Network Analysis",
    "section": "",
    "text": "35.1 What Is a Network?\nThis lesson will introduce the theoretical and logistical underpinnings of network analysis. It will define what networks are, their limitations, and their use cases. It will then cover some of the most commonly used network measures, what they mean, and how to generate them.\nYou are all most likely familiar now with tabular data; rows and columns containing information. It looks like this:\nWhile this is a tidy way to store data, it artificially atomizes or separates many of the things we are interested in as researchers, social or otherwise. Network analysis is a tool to work with relational data: information about how entities are connected with each other.\nFor example, the diagram below shows the same data as the table above, with the added benefit of showing how these individuals are connected to each other. Hover over the people to reveal the data about them.\nRather than looking only at attributes of specific data points, we are looking at the connections between data. In network analysis, data points are called nodes or vertices, and the connections between them are called edges or ties. Vertices can be anything—people, places, words, concepts—they are usually mapped into rows in a data frame. Edges contain any information on how these things connect or are related to each other. These components create a network or graph, defined as “finite set or sets of actors and the relation or relations defined on them” (Wasserman and Faust 1994).",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/14_network-analysis.html#what-is-a-network",
    "href": "chapters/14_network-analysis.html#what-is-a-network",
    "title": "35  Network Analysis",
    "section": "",
    "text": "Person\nName\nAge\nWidgets\n\n\n\n\n\nJ\n30\n1\n\n\n\nY\n21\n3\n\n\n\nG\n32\n4\n\n\n\nZ\n48\n8\n\n\n\n\n\n\n\n\n35.1.1 Networks in Research\n\n35.1.1.1 Social Sciences\n\nOne of the first instances of social network analysis was originally published in 1932 as part of Jacob Moreno’s Who Shall Survive (1953). This study used the friendship networks of girls within a reform school to show that the ties between them were a stronger predictor of runaways than any attribute of the girls themselves. Since then, networks have been used widely in the social sciences, but only really picked up as the tools to understand SNA became more available.\n\n\n35.1.1.2 Neuroscience\n\nNeuroscientists use networks to study the brain, given their ready application to neurons and pathways. Bassett and Sporns (2017) provide an overview of how to translate neuroscience problems into network ones, and the tools available to study them.\n\n\n35.1.1.3 Chemistry\n\nChemistry was quick to see the applications of networks. As early at 1985 papers were published detailing the potential networks provided in terms of understanding and finding new ways to measure and understand the bonds between atoms and molecules (Balaban 1985).\n\n\n35.1.1.4 The Internet\n\nThe internet is a network! Beyond the various social network sites, servers themselves act as nodes and the information flows between them along edges. Google used this property in the first version of their search engine, which used the network metric of PageRank to determine which sites to show at the top of search results (Page 2001).\n\n\n35.1.1.5 Infrastructure\n\nFand and Mostafavi (2019) showed how you can use social media network data to find where infrastructure is failing during disasters, such as hurricane Harvey in 2017. Their system promises a method to monitor physical infrastructure like roads, bridges, and barriers like more easily monitored infrastructure like the electrical grid.\n\n\n35.1.1.6 Security\n\nNetwork analysis has also been used for offensive purposes. One of the most prominent uses is mapping crime or terror networks (Krebs 2002), though it is fraught with ethical concerns. There are specific tools made for this purpose, such as the keyplayer package (An and Liu 2016), which helps find what nodes in a network would fragment them the most if removed.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/14_network-analysis.html#network-data",
    "href": "chapters/14_network-analysis.html#network-data",
    "title": "35  Network Analysis",
    "section": "35.2 Network Data",
    "text": "35.2 Network Data\nNetworks are based on relational data. This means the core data requirement is that we have some measure of how nodes are connected. The two most common network data formats are the edgelist and adjacency matrix. Either of these will work for nearly any network purpose, and it is easy to convert between them. You will also need an attributes file, which gives information about the nodes being connected.\n\n35.2.1 Edgelist\nAn edgelist is a two-column data frame with a from and to column. Each row represents one edge or tie, with the possibility of adding in more information. Here’s an example of a basic edgelist:\n\n# Load in the data\ntoy_edgelist = read.csv(\n  \"data/toy_edgelist.csv\", header = TRUE, stringsAsFactors = FALSE\n)\n\n# Show the first 10 rows\nhead(toy_edgelist, n = 10)\n\n\n\n\n\nto\nfrom\n\n\n\n\nb33f00bd1109e1ae3ffa757d0aef0a25942f2ba3\nzuko\n\n\n19d5b2694036f6fab966564c1c44bc74330f22c2\nzuko\n\n\n9483b16c4904908115f4538525e37f776f4596d4\nzuko\n\n\nf8452649773eb7e024bfa59c395afa0c302d1928\nzuko\n\n\neea677240a425ed7ccdeff69feb2d377a5542599\nzuko\n\n\n9dbcce359070c879f20843e19564aee545f80d2d\nzuko\n\n\n749e81272630eb4755e4a7bca10fe3e3524d77ce\nzuko\n\n\ntoph\nzuko\n\n\n5737a840aa867025dcb506f24cb5546f16b4d777\nzuko\n\n\n028f5d1f351d38cd6553ab4674b19725d5ea3d3c\nzuko\n\n\n\n\n\n\n\n\n35.2.2 Adjacency Matrix\nThe same data can also be displayed in a table format. The information is the same, but it is presented in a way more usable by our code to create measures we care out. In this format, every node has both a row and column. If there is an edge between two nodes, a 1 is placed in the intersection of their row and column.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb33f00bd1109e1ae3ffa757d0aef0a25942f2ba3\n19d5b2694036f6fab966564c1c44bc74330f22c2\n9483b16c4904908115f4538525e37f776f4596d4\nf8452649773eb7e024bfa59c395afa0c302d1928\neea677240a425ed7ccdeff69feb2d377a5542599\n9dbcce359070c879f20843e19564aee545f80d2d\n749e81272630eb4755e4a7bca10fe3e3524d77ce\ntoph\n5737a840aa867025dcb506f24cb5546f16b4d777\n028f5d1f351d38cd6553ab4674b19725d5ea3d3c\nzuko\n\n\n\n\nb33f00bd1109e1ae3ffa757d0aef0a25942f2ba3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n19d5b2694036f6fab966564c1c44bc74330f22c2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n9483b16c4904908115f4538525e37f776f4596d4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\nf8452649773eb7e024bfa59c395afa0c302d1928\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\neea677240a425ed7ccdeff69feb2d377a5542599\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n9dbcce359070c879f20843e19564aee545f80d2d\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n749e81272630eb4755e4a7bca10fe3e3524d77ce\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\ntoph\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n5737a840aa867025dcb506f24cb5546f16b4d777\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n028f5d1f351d38cd6553ab4674b19725d5ea3d3c\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\nzuko\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n35.2.3 Edge Weights\nEdges can also have weights, meaning some edges are valued more than others. In an edgelist, you can add a third “weight” column, entering higher numbers to denote a more important connection. In an adjacency matrix, you can put numbers other than 1 in the intersection to denote more important connections. For our example, we’ll stick with un-weighted connections for now.\n\n\n35.2.4 Attributes\nEach network also typically has an attributes table, which looks just like typical tabular data, with each row belonging to a specific node in our network. Let’s load in and look at the sample attributes file.\n\n# Load in data\ntoy_attributes = read.csv(\n  \"data/toy_attributes.csv\", header = TRUE, stringsAsFactors = FALSE\n)\n\n# Show top of attributes table\nhead(toy_attributes, n = 10)\n\n\n\n\n\nid\nyear\ncolor\n\n\n\n\nzuko\n2\npurple\n\n\nnezuko\n2\npurple\n\n\nwinnie the pooh\n2\nblue\n\n\ntoph\n2\npurple\n\n\nchicken joe\n2\ngreen\n\n\nthe rat from ratatouille\n5\nblue\n\n\nspider-man\n3\nblue\n\n\nyamaguchi tadashi\n1\npurple\n\n\njude sweetwine\n2\npurple\n\n\nlord future\n2\nblue\n\n\n\n\n\n\n\n\n35.2.5 Create an Example Network\nBefore we start exploring specific measures, we’ll create a toy network to use as an example. Let’s start by loading in some packages.\nThe statnet package is a popular package for network analysis in R. It allows you to compute many of the most common network measures, and run simulations called exponential random graph models. We’ll stick with the basics for now!\n\n# Run this to load statnet, if you need to install it, do so now.\n# install.packages(\"statnet\")\nlibrary(\"statnet\")\n\nNow that we have our tools loaded, let’s create out first network. We’ll use the data you loaded in before. This toy network will be used as a visual for learning the measurements below.\nWe are going to turn the attributes file and edgelist into a statnet network object. A network object is a special kind of list in R. It is formatted in a way that the other statnet functions expect. While you could edit it like a normal list, it is highly recommended you use the other statnet functions to manipulate this object to make sure you don’t break any of the data expectations.\nWe’ll use the network function to create our network object. Before we create it, we will sort our attributes file alphabetically. This is super important, as the network object will automatically sort things itself.\n\n\n\n\n\n\nImportant\n\n\n\nIf we do not sort our attributes data frame to match, all of our measures later will be misaligned!\n\n\n\n# Sort your attributes frame alphabetically. Super important!\ntoy_attributes = toy_attributes[order(toy_attributes$id), ]\n\n# Make network!\n# We will cover the `directed = FALSE` argument soon.\ntoy_network = network(toy_edgelist, directed = FALSE)\n\nBefore we move on, let’s add a net_id column to our attributes data frame. This will let us easily check what the network object IDs are for our nodes.\n\n# Add ID column\ntoy_attributes$net_id = 1:nrow(toy_attributes)\n\nWe can inspect our new network by calling the summary function on it. Don’t worry too much about the output yet.\n\nsummary(toy_network)\n\nNetwork attributes:\n  vertices = 96\n  directed = FALSE\n  hyper = FALSE\n  loops = FALSE\n  multiple = FALSE\n  bipartite = FALSE\n total edges = 88 \n   missing edges = 0 \n   non-missing edges = 88 \n density = 0.01929825 \n\nVertex attributes:\n  vertex.names:\n   character valued attribute\n   96 valid vertex names\n\nNo edge attributes\n\nNetwork edgelist matrix:\n      [,1] [,2]\n [1,]    1   25\n [2,]    2   25\n [3,]    3   25\n [4,]    4   25\n [5,]    5   25\n [6,]    6   25\n [7,]    7   25\n [8,]    8   25\n [9,]    9   25\n[10,]   10   25\n[11,]   11   87\n[12,]   12   87\n[13,]   13   87\n[14,]   14   87\n[15,]   15   87\n[16,]   16   88\n[17,]   17   88\n[18,]   18   88\n[19,]   19   88\n[20,]   20   88\n[21,]   21   88\n[22,]   22   88\n[23,]   23   88\n[24,]   24   88\n[25,]   25   88\n[26,]   26    8\n[27,]   27    8\n[28,]   28    8\n[29,]   29    8\n[30,]   30    8\n[31,]   31   89\n[32,]   32   89\n[33,]   33   89\n[34,]   34   89\n[35,]   35   89\n[36,]   36   89\n[37,]   37   89\n[38,]   38   89\n[39,]   39   90\n[40,]   40   90\n[41,]   41   90\n[42,]   42   90\n[43,]   43   90\n[44,]   44   90\n[45,]   45   90\n[46,]   46   90\n[47,]   47   90\n[48,]   48   91\n[49,]   49   91\n[50,]   50   91\n[51,]   51   91\n[52,]   52   91\n[53,]   53   91\n[54,]   54   91\n[55,]   55   24\n[56,]   56   24\n[57,]   57   24\n[58,]   58   24\n[59,]   59   24\n[60,]   17   24\n[61,]   60   24\n[62,]   61   24\n[63,]   62   24\n[64,]   63   92\n[65,]   64   92\n[66,]   65   92\n[67,]   66   92\n[68,]   17   92\n[69,]   67   93\n[70,]   68   93\n[71,]   69   93\n[72,]   70   93\n[73,]   71   93\n[74,]   72   94\n[75,]   73   94\n[76,]   74   94\n[77,]   75   94\n[78,]   76   94\n[79,]   77   95\n[80,]   78   95\n[81,]   79   95\n[82,]   80   95\n[83,]   81   95\n[84,]   82   96\n[85,]   83   96\n[86,]   84   96\n[87,]   85   96\n[88,]   86   96\n\n\nThen we’ll add the node attributes to the network object. If you run summary again you should see the values from our toy_attributes have been added.\n\n# Add each attribute to network.\n# Do this by looking at every column, then adding it to the network\nfor(col_name in colnames(toy_attributes)) {\n    toy_network = set.vertex.attribute(\n      x = toy_network, attrname = col_name, value = toy_attributes[,col_name]\n    )\n}\n\nLet’s see what out network looks like!\n\nplot(toy_network)\n\n\n\n\n\n\n\n\nThe default plotting in statnet is ugly. For the sake of our eyes, and for exploring some of the measure we create, we’ll use the visNetwork package to visualize our networks. It will make the code a bit more cumbersome, but it will be worth it. From now on, we will need to use the edges and attributes data frames for plotting. This means we will often need to run commands twice, once for the network and once for the data frames. When you are working with networks for research, you would usually do everything you need on your network, than create a data frame from it all at once. We will need to deal with a bit of redundancy to take things one step at a time.\nLet’s try plotting again with visNetwork, using the data frames. We’ll give the visNetwork function our edgelist and attributes data frame. We’ll also tell it to plot the names from our attributes data frame so we can see them when we hover over the nodes in the plot.\n\n# Add pop-up tooltips with names\n# visNetwork uses the \"title\" column to create pop-up boxes\ntoy_attributes$title = toy_attributes$id\n\n# Plot!\nvisNetwork(nodes = toy_attributes, edges = toy_edgelist) %&gt;%\n  visInteraction(zoomView = FALSE)\n\n\n\n\n\nNice.\n\n\n35.2.6 Components\nMost often when working with networks you want to limit your analysis to one cluster or component, typically the largest one in your network. If segments of your network aren’t connected, you can’t answer many of the relational questions network analysis is good for! Let’s limit our network to the largest component:\n\n# Find what nodes are part of the largest component\ntoy_network%v%\"lc\" = component.largest(toy_network)\n\n# Delete those nodes that are not in the network\nin_lc = toy_network%v%\"lc\"\ntoy_network = delete.vertices(toy_network, which(!in_lc))\n\n# In our data frames\ntoy_attributes = toy_attributes[\n  toy_attributes$id %in% as.character(toy_network%v%\"id\"),\n]\n\nin_lc = toy_edgelist$to %in% toy_attributes$id |\n  toy_edgelist$from %in% toy_attributes$id\ntoy_edgelist = toy_edgelist[which(in_lc), ]\n\n# Plot!\nvisNetwork(nodes = toy_attributes, edges = toy_edgelist) %&gt;%\n  visInteraction(zoomView = FALSE)\n\n\n\n\n\n\n\n35.2.7 Limitations of Network Data\nBefore we move on we should take a moment to talk about some the the caveats when using network data. While powerful, network analysis is particularly picky when in comes to data requirements. I’ll cover the two biggest ones below. You should always keep these in mind when using or interpreting network tools.\n\n35.2.7.1 Missing Data\nNetwork analysis is very vulnerable to missing data. A simple way to understand why is to make a small adjustment to our network. I’ve highlighted one node in green. This node is structurally vital to the network; without it, the shape of the network as a whole will change.\n\n\n\n\n\n\nIf we remove this node, the network changes in a major way! Imagine these nodes are people, and that missing node is the one person you forget to survey, or was sick the day data was collected. This could massively change the outcome of your analyses. There is some advanced research going on to detect and replace missing data like this if you have enough context, but it is not something to rely on.\n\n\n\n\n\n\n\n\n35.2.7.2 Network Boundaries\nNetwork analysis is all about looking at the relationships between entities. However, following all connections an entity has can quickly spiral out of hand. For example, if you wanted to map your own social network, where would you start? You would include yourself, then your friends and family, but what about after that? Your friends and family have friends and family, as do their friends and family, and so on. If you are looking at human networks, every human will be included if you look far enough, so how do you decide when to stop?\nThere is no easy answer. If you are looking at a pre-defined group (for example, this class), you can set the boundaries to include everyone in this class and the connection between them. However, that doesn’t really capture the social networks of people in this class as most people will have friends elsewhere.\nAnother common method is setting an arbitrary number of “steps” or connections from a target population. If we were interested in a 2-step network from an individual, we would collect all of their relevant connections, and then ask all the people they nominated about their connections. Some sort of justification will be needed as to why you picked the number of steps that you did.\n\n\n\n\n\n\nFigure 35.1\n\n\n\n\n\n\n35.2.8 Projected Networks\nOften, you will not have individual level network data, but you will have data on group membership. For example, if you wanted to map the social networks of student, but don’t know who they actually hang around with, you may be able to use class rosters to build an approximate network. This is call a bipartite network, two-mode network, or projected network. You can see an example below.\n\n\n\n\n\n\nFigure 35.2: In this figure there are two kinds of nodes, students and classes. You can “collapse” this into a student network by assuming every student connected to a class is connected to each other. The same is true with classes, such that classes are related to each other if a single student is enrolled in both. This assumption may not always be correct, and you need to take care if you are going to make it in your research. If a class has 300 students, it is most likely not correct to assume every student knows every other student in that class.\n\n\n\nFor reference, this is what out projected class network looks like:",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/14_network-analysis.html#graph-level-properties",
    "href": "chapters/14_network-analysis.html#graph-level-properties",
    "title": "35  Network Analysis",
    "section": "35.3 Graph Level Properties",
    "text": "35.3 Graph Level Properties\nNow that we know what networks are and have some examples of how they are used and the data required, let’s get into actually analyzing them. There are a number of measures we can compute to understand the structure of a network as a whole. We will go over some basic network level ones here. These are single measures or attributes used to describe the entire network, and can be used to compare one network against another.\n\nDirected or Undirected\nDensity\nCentralization\n\n\n35.3.1 Directed or Undirected\nNetworks can either be directed or undirected. A directed network treats the edges between nodes as having a specific direction of flow, while an undirected network considers all edges to be mutual. An example of each is presented below.\nBoth edgelist and adjacency matrix datasets are inherently directed. For edgelists, the sender is often the first column, and the receiver is the second. For adjacency matrices the rows are considered senders and columns are receivers. Directionality is often specified when the network objects are created. When we created our toy network, we specified directed = FALSE to simplify things. If you want a directed network, the default is directed = TRUE for statnet networks.\nA directed network tracks which node is the source and which node is the receiver for an edge. Take for example the follow mechanic on Twitter. User A can follow User B, creating a directed edge from A to B, but B does not have to follow A in return. This can be useful when trying to understand the flows of resources that are finite such as money or goods.\n\n# visNetwork uses a column called \"arrows\" to show directionality in its plots.\n# For our edgelist, we'll just say every row is \"to\" for now\ntoy_edgelist$arrows = \"to\"\n\n# This will show us what our network would look like if it was directed.\nvisNetwork(toy_attributes, toy_edgelist, main = \"Directed\") %&gt;%\n  visInteraction(zoomView = FALSE)\n\n\n\n\n\nAn undirected network treats all ties as mutual, such that A and B are both involved equally in a tie. An example is the friend mechanic on Facebook. Once a friendship is established, both users are considered equal in the tie. This can be helpful when you do not have information on what node initiates a tie, or when events happen equally to a group of nodes, such as all nodes being connected through co-membership in a group.\n\n# Let's drop the arrow column for now since our network is undirected.\ntoy_edgelist = toy_edgelist[,c(\"from\", \"to\")]\n\n# Plot\nvisNetwork(toy_attributes, toy_edgelist, main = \"Undirected\") %&gt;%\n  visInteraction(zoomView = FALSE)\n\n\n\n\n\nWhich of these will be useful to you will likely change from project to project. However, it is vital to understand what kind of network you are working with, as many network calculations we will talk about later change their behavior based on if the network is directed or not.\n\n\n35.3.2 Density\nDensity is the first real graph level metric that helps you understand what is particular about the network you are looking at. The density of a network is a numerical score showing how many ties exist in a network, given the max possible in that network. Mathematically that is \\(\\frac{Actual Edges}{Possible\nEdges}\\), where actual edges is the number of edges in the network, and possible edges is the number of edges if every single node in the network was connected to every other node.\nNetworks that are more densely connected are considered to be more cohesive and robust. This means that the removal of any specific edge or node will not have a great effect of the network as a whole. It also typically means that any one node in the network will be more likely to have access to whatever resources are in the network, as there are more potential connections in the network to search for resources.\nTo calculate the density of a network, we use the network.density function. You can also see it if you use summary on your network object. Below is our toy network and a less dense version to try and visualize the difference. Density is all about how many edges exist in the network. Notice that there are the same number of nodes in both of these networks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n35.3.3 Centralization\nFreeman centralization (usually just called centralization) gives a sense of the shape of the network, namely how node level measures are distributed in a network. We’ll discuss node level measures next, but for now it is only important to understand that node level measures are numeric scores assigned to specific nodes rather than the network as a whole. This means that each node may have a different value.\nConsider the two networks below. The first “star” network would be considered highly centralized, as one node connects to all the others, while the rest of the nodes have no connections to each other. This star network would have a edge centralization score of 1, as 100% of the ties are connected with one node. The loop network would have a score of 0, as every node is equally connected to each other.\nCentralization is a measure of how unevenly node level metrics are distributed in a network. This is helpful when trying to understand if some nodes in the network have a larger influence, or are is some way more important than others.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/14_network-analysis.html#node-level-properties",
    "href": "chapters/14_network-analysis.html#node-level-properties",
    "title": "35  Network Analysis",
    "section": "35.4 Node Level Properties",
    "text": "35.4 Node Level Properties\nNode level measures are numeric representations of a node’s position and importance in a network. There are several common node level measures, and we will go over some of them here. Each measure tries to quantify a different aspect of a node’s position in the network so we can make an argument about why that specific node or class of nodes is important in some way. We will go over:\n\nDegree\nGeodesic distance\nBetweenness centrality\nEigenvector centrality\n\nMost node level measures are only helpful within the context of the network they were generated for. This is because the measures are created in part using network level measures like density. This means it is alright to compare one node to another within the same network, but toy should node compare the node level measures between networks.\n\n35.4.1 Degree\nDegree counts how many edges are connected to a node. You can count incoming, outgoing, or total (Freeman) degree. Incoming and outgoing degree only matter in directed networks. In undirected networks, only total edges are applicable. Degree gives a very rough measure of how popular or central a node is in the network. If a node has more ties, it may indicate that node as being more central or important the network as a whole.\nDegree is a raw count of the number of edges a node has, this makes the interpretation of degree highly dependent on the size of the network. In a small network with only 25 total edges, having 10 of them would be significant. In a larger network with 250 total edges, 10 edges could be less impressive. Degree should thus be interpreted in the cortex of other nodes in the network.\nLet’s scale the node sizes of our toy network based on their total degree numbers. We’ll get degree counts for each of our nodes using the degree() function. We can save that into our data frame and network for use later. For now I am naming columns to work specifically with visNetwork, we’ll make a proper data frame for analyses later using data we saved in the network object. In our visualization, you can click on any node to highlight only the edges connected to that node.\n\n# Find the degree of each node and save in the network.\n#\n# We will use the special `%v%` operator when assigning values to a network.\n# `%v%` works like `$` for data frames, allowing you to ask for specific values\n# in the network.\n#\n# In this case `%v%` stands for vertex, and you can use `%e%` if you want to\n# work with edges. So let's get the degree counts, and assign them to the\n# \"degree\" variable in our network object\ntoy_network%v%\"degree\" = degree(toy_network)\n\n# visNetwork uses the \"value\" column to determine node size, so let's put it\n# there as well for now.\n#\n# We'll square the values just to make them more distinct\ntoy_attributes$value = degree(toy_network)^2\n\n# Plot!\nvisNetwork(toy_attributes, toy_edgelist, main = \"Degree Example\") %&gt;%\n  visInteraction(zoomView = FALSE)\n\n\n\n\n\n\n\n35.4.2 Geodesic Distance\nGeodesic distance is “the length of the shortest path via the edges or binary connections between nodes” (Kadushin 2012). In other words, if we treat the network as a map we can move along, with the nodes being stopping places and the edges being paths, the geodesic is the shortest possible path we can use to walk between two nodes.\nNodes that on average have a shorter geodesic distance between all the other nodes in the network are considered to have have greater access to the resources in a network. This is because a node with a low average geodesic distance can theoretically “reach” the other nodes with less effort because it does not need to travel as far.\nTo find the mean geodesic distance for each node in the network we will first need to find the geodesic distance from each node to every other node, then take the mean. Not super difficult, but there isn’t a single function to do it for us. First we will use the geodist function to get all the geodesics.\n\n# Get all the geodesics\n# I use the $gdist so we only get geodesics not counts\ngeodist(toy_network)$gdist\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    2    2    2    2    2    2    2    2     2     3     3     3\n [2,]    2    0    2    2    2    2    2    2    2     2     3     3     3\n [3,]    2    2    0    2    2    2    2    2    2     2     3     3     3\n [4,]    2    2    2    0    2    2    2    2    2     2     3     3     3\n [5,]    2    2    2    2    0    2    2    2    2     2     3     3     3\n [6,]    2    2    2    2    2    0    2    2    2     2     3     3     3\n [7,]    2    2    2    2    2    2    0    2    2     2     3     3     3\n [8,]    2    2    2    2    2    2    2    0    2     2     3     3     3\n [9,]    2    2    2    2    2    2    2    2    0     2     3     3     3\n[10,]    2    2    2    2    2    2    2    2    2     0     3     3     3\n[11,]    3    3    3    3    3    3    3    3    3     3     0     2     2\n[12,]    3    3    3    3    3    3    3    3    3     3     2     0     2\n[13,]    3    3    3    3    3    3    3    3    3     3     2     2     0\n[14,]    3    3    3    3    3    3    3    3    3     3     2     2     2\n[15,]    3    3    3    3    3    3    3    3    3     3     2     2     2\n[16,]    3    3    3    3    3    3    3    3    3     3     2     2     2\n[17,]    3    3    3    3    3    3    3    3    3     3     2     2     2\n[18,]    3    3    3    3    3    3    3    3    3     3     2     2     2\n[19,]    3    3    3    3    3    3    3    3    3     3     2     1     2\n[20,]    1    1    1    1    1    1    1    1    1     1     2     2     2\n[21,]    3    3    3    3    3    3    3    1    3     3     4     4     4\n[22,]    3    3    3    3    3    3    3    1    3     3     4     4     4\n[23,]    3    3    3    3    3    3    3    1    3     3     4     4     4\n[24,]    3    3    3    3    3    3    3    1    3     3     4     4     4\n[25,]    3    3    3    3    3    3    3    1    3     3     4     4     4\n[26,]    4    4    4    4    4    4    4    4    4     4     3     2     3\n[27,]    4    4    4    4    4    4    4    4    4     4     3     2     3\n[28,]    4    4    4    4    4    4    4    4    4     4     3     2     3\n[29,]    4    4    4    4    4    4    4    4    4     4     3     2     3\n[30,]    4    4    4    4    4    4    4    4    4     4     3     2     3\n[31,]    4    4    4    4    4    4    4    4    4     4     3     2     3\n[32,]    4    4    4    4    4    4    4    4    4     4     3     2     3\n[33,]    4    4    4    4    4    4    4    4    4     4     3     2     3\n[34,]    5    5    5    5    5    5    5    5    5     5     4     2     4\n[35,]    5    5    5    5    5    5    5    5    5     5     4     2     4\n[36,]    5    5    5    5    5    5    5    5    5     5     4     2     4\n[37,]    5    5    5    5    5    5    5    5    5     5     4     2     4\n[38,]    2    2    2    2    2    2    2    2    2     2     1     1     1\n[39,]    4    4    4    4    4    4    4    4    4     4     3     1     3\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]\n [1,]     3     3     3     3     3     3     1     3     3     3     3     3\n [2,]     3     3     3     3     3     3     1     3     3     3     3     3\n [3,]     3     3     3     3     3     3     1     3     3     3     3     3\n [4,]     3     3     3     3     3     3     1     3     3     3     3     3\n [5,]     3     3     3     3     3     3     1     3     3     3     3     3\n [6,]     3     3     3     3     3     3     1     3     3     3     3     3\n [7,]     3     3     3     3     3     3     1     3     3     3     3     3\n [8,]     3     3     3     3     3     3     1     1     1     1     1     1\n [9,]     3     3     3     3     3     3     1     3     3     3     3     3\n[10,]     3     3     3     3     3     3     1     3     3     3     3     3\n[11,]     2     2     2     2     2     2     2     4     4     4     4     4\n[12,]     2     2     2     2     2     1     2     4     4     4     4     4\n[13,]     2     2     2     2     2     2     2     4     4     4     4     4\n[14,]     0     2     2     2     2     2     2     4     4     4     4     4\n[15,]     2     0     2     2     2     2     2     4     4     4     4     4\n[16,]     2     2     0     2     2     2     2     4     4     4     4     4\n[17,]     2     2     2     0     2     2     2     4     4     4     4     4\n[18,]     2     2     2     2     0     2     2     4     4     4     4     4\n[19,]     2     2     2     2     2     0     2     4     4     4     4     4\n[20,]     2     2     2     2     2     2     0     2     2     2     2     2\n[21,]     4     4     4     4     4     4     2     0     2     2     2     2\n[22,]     4     4     4     4     4     4     2     2     0     2     2     2\n[23,]     4     4     4     4     4     4     2     2     2     0     2     2\n[24,]     4     4     4     4     4     4     2     2     2     2     0     2\n[25,]     4     4     4     4     4     4     2     2     2     2     2     0\n[26,]     3     3     3     3     3     1     3     5     5     5     5     5\n[27,]     3     3     3     3     3     1     3     5     5     5     5     5\n[28,]     3     3     3     3     3     1     3     5     5     5     5     5\n[29,]     3     3     3     3     3     1     3     5     5     5     5     5\n[30,]     3     3     3     3     3     1     3     5     5     5     5     5\n[31,]     3     3     3     3     3     1     3     5     5     5     5     5\n[32,]     3     3     3     3     3     1     3     5     5     5     5     5\n[33,]     3     3     3     3     3     1     3     5     5     5     5     5\n[34,]     4     4     4     4     4     3     4     6     6     6     6     6\n[35,]     4     4     4     4     4     3     4     6     6     6     6     6\n[36,]     4     4     4     4     4     3     4     6     6     6     6     6\n[37,]     4     4     4     4     4     3     4     6     6     6     6     6\n[38,]     1     1     1     1     1     1     1     3     3     3     3     3\n[39,]     3     3     3     3     3     2     3     5     5     5     5     5\n      [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37]\n [1,]     4     4     4     4     4     4     4     4     5     5     5     5\n [2,]     4     4     4     4     4     4     4     4     5     5     5     5\n [3,]     4     4     4     4     4     4     4     4     5     5     5     5\n [4,]     4     4     4     4     4     4     4     4     5     5     5     5\n [5,]     4     4     4     4     4     4     4     4     5     5     5     5\n [6,]     4     4     4     4     4     4     4     4     5     5     5     5\n [7,]     4     4     4     4     4     4     4     4     5     5     5     5\n [8,]     4     4     4     4     4     4     4     4     5     5     5     5\n [9,]     4     4     4     4     4     4     4     4     5     5     5     5\n[10,]     4     4     4     4     4     4     4     4     5     5     5     5\n[11,]     3     3     3     3     3     3     3     3     4     4     4     4\n[12,]     2     2     2     2     2     2     2     2     2     2     2     2\n[13,]     3     3     3     3     3     3     3     3     4     4     4     4\n[14,]     3     3     3     3     3     3     3     3     4     4     4     4\n[15,]     3     3     3     3     3     3     3     3     4     4     4     4\n[16,]     3     3     3     3     3     3     3     3     4     4     4     4\n[17,]     3     3     3     3     3     3     3     3     4     4     4     4\n[18,]     3     3     3     3     3     3     3     3     4     4     4     4\n[19,]     1     1     1     1     1     1     1     1     3     3     3     3\n[20,]     3     3     3     3     3     3     3     3     4     4     4     4\n[21,]     5     5     5     5     5     5     5     5     6     6     6     6\n[22,]     5     5     5     5     5     5     5     5     6     6     6     6\n[23,]     5     5     5     5     5     5     5     5     6     6     6     6\n[24,]     5     5     5     5     5     5     5     5     6     6     6     6\n[25,]     5     5     5     5     5     5     5     5     6     6     6     6\n[26,]     0     2     2     2     2     2     2     2     4     4     4     4\n[27,]     2     0     2     2     2     2     2     2     4     4     4     4\n[28,]     2     2     0     2     2     2     2     2     4     4     4     4\n[29,]     2     2     2     0     2     2     2     2     4     4     4     4\n[30,]     2     2     2     2     0     2     2     2     4     4     4     4\n[31,]     2     2     2     2     2     0     2     2     4     4     4     4\n[32,]     2     2     2     2     2     2     0     2     4     4     4     4\n[33,]     2     2     2     2     2     2     2     0     4     4     4     4\n[34,]     4     4     4     4     4     4     4     4     0     2     2     2\n[35,]     4     4     4     4     4     4     4     4     2     0     2     2\n[36,]     4     4     4     4     4     4     4     4     2     2     0     2\n[37,]     4     4     4     4     4     4     4     4     2     2     2     0\n[38,]     2     2     2     2     2     2     2     2     3     3     3     3\n[39,]     3     3     3     3     3     3     3     3     1     1     1     1\n      [,38] [,39]\n [1,]     2     4\n [2,]     2     4\n [3,]     2     4\n [4,]     2     4\n [5,]     2     4\n [6,]     2     4\n [7,]     2     4\n [8,]     2     4\n [9,]     2     4\n[10,]     2     4\n[11,]     1     3\n[12,]     1     1\n[13,]     1     3\n[14,]     1     3\n[15,]     1     3\n[16,]     1     3\n[17,]     1     3\n[18,]     1     3\n[19,]     1     2\n[20,]     1     3\n[21,]     3     5\n[22,]     3     5\n[23,]     3     5\n[24,]     3     5\n[25,]     3     5\n[26,]     2     3\n[27,]     2     3\n[28,]     2     3\n[29,]     2     3\n[30,]     2     3\n[31,]     2     3\n[32,]     2     3\n[33,]     2     3\n[34,]     3     1\n[35,]     3     1\n[36,]     3     1\n[37,]     3     1\n[38,]     0     2\n[39,]     2     0\n\n\nThis output is just like an adjacency matrix, with row and columns being the network node IDs (net_id in our attributes data frame). Next we would want sum all the columns for each row (so adding up all the geodesics for a node), and divide by the total number of nodes it can have an edge with to get the average geodesic distance for that node. This gives us the average geodesic distance for each node!\n\n# `colSums` gives us the sum of all columns for a row.\n# We subtract one from the denominator because a node cannot have a geodesic\n# distance with itself.\ncolSums(geodist(toy_network)$gdist) / (nrow(as.sociomatrix(toy_network)) - 1)\n\n [1] 3.131579 3.131579 3.131579 3.131579 3.131579 3.131579 3.131579 2.868421\n [9] 3.131579 3.131579 2.947368 2.447368 2.947368 2.947368 2.947368 2.947368\n[17] 2.947368 2.947368 2.368421 2.157895 3.842105 3.842105 3.842105 3.842105\n[25] 3.842105 3.342105 3.342105 3.342105 3.342105 3.342105 3.342105 3.342105\n[33] 3.342105 4.184211 4.184211 4.184211 4.184211 1.973684 3.210526\n\n\nLet’s add this to our network and plot it. We’ll also add color and labels so it’s easier to see what this measure means. The red node has the longest average geodesic distance, and would need to travel through the whole network to reach the nodes on the opposite side. Meanwhile, the blue node has the smallest average geodesic distance because it is located near the middle of the network.\n\n# Add mean geodesic distance to network object\ntoy_network%v%\"mean_distance\" =\n  (colSums(geodist(toy_network)$gdist))/(nrow(as.sociomatrix(toy_network)) - 1)\n\n# Set all node colors in visNetwork to grey as default\ntoy_attributes$color = \"grey\"\n\n# Add label as geodesic distance, rounding to 3 digits\ntoy_attributes$label = round(toy_network%v%\"mean_distance\", 3)\n\n# Replace min average geodesic with blue, max with red\ntoy_attributes$color[which(toy_network%v%\"mean_distance\" == max(toy_network%v%\"mean_distance\"))] = \"red\"\ntoy_attributes$color[which(toy_network%v%\"mean_distance\" == min(toy_network%v%\"mean_distance\"))] = \"blue\"\n\n# Make sure edges are grey too\ntoy_edgelist$color = \"grey\"\n\n# Plot\nvisNetwork(toy_attributes, toy_edgelist, main = \"Geodesic Example\") %&gt;%\n  visInteraction(zoomView = FALSE)\n\n\n\n\n\nNote that while there is a correlation between degree counts (node size) and mean geodesic distance, one does not cause the other. This is our first instance of how network structure, not node attributes, can inform us about the nodes in a network. Essentially, looking at the network as a whole can tell us things about the people in it that is lost if we look only at individuals.\n\n\n35.4.3 Centrality\nCentrality scores encompass a wide range of measures computed at the node level. Each tries to understand the importance of a single node within the structure of a network by looking at the nodes connection patterns to other nodes. Any centrality measure can be used to create a network level centralization score like we discussed above. We will go through some of the common centrality measures here, but know there are several more we will not cover.\n\n35.4.3.1 Betweenness Centrality\nBetweenness centrality is one of the most common centrality measures. It tries to calculate the extent to which a node acts as a gatekeeper or broker in the network. A broker bridges two otherwise disconnected segments in a network. If there are two parts of a network that would otherwise be broken apart if a node was removed, they would have a high betweenness centrality. The fragmenting of a network is not a prerequisite however, simply acting as an effective “shortcut” in a network can also raise a node’s betweenness centrality. Betweenness is calculated using geodesic distances, and gives a higher score to nodes that lie on more geodesic paths.\nThe next network plot shows the size of nodes as their degree, with a label showing their betweenness centrality score. Centrality score are usually normalized such that their scores all sum to 1. This way, you can easily compare nodes within the network (but not between networks), and understand how nodes relate to each other structurally. It is possible for a node to have a 0 betweenness score if no geodesic distances pass through them.\nLike last time I’ve colored the nodes so that the node with the highest betweenness centrality is red, while the lowest is blue. Compared to distance however, it is considered advantageous to have a high betweenness centrality, as this means that nodes acts as a gatekeeper in the network, which can be a powerful position. Contrast this with having a low mean distance, which meant you were closer to all other nodes.\n\n# Add eigenvector centrality to network object as \"norm_betweenness\".\n#\n# We also tell it we are treating our data as undirected (\"graph\"), rather than\n# the default directed (\"digraph\"), same with `cmode = \"undirected\"`.\n#\n# We also say we want a normalized (0-1, sum to 1) score using `rescale =\n# TRUE`.\n\ntoy_network%v%\"norm_betweenness\" =\n  betweenness(\n    dat = toy_network, gmode = \"graph\", rescale = TRUE, cmode = \"undirected\"\n  )\n\n# Add label as geodesic distance, rounding to 3 digits\ntoy_attributes$label = round(toy_network%v%\"norm_betweenness\", 3)\n\n# Reset all nodes to grey\ntoy_attributes$color = c(\"grey\")\n\n# Replace min average geodesic with blue, max with red\ntoy_attributes$color[which(toy_network%v%\"norm_betweenness\" == max(toy_network%v%\"norm_betweenness\"))] = \"red\"\ntoy_attributes$color[which(toy_network%v%\"norm_betweenness\" == min(toy_network%v%\"norm_betweenness\"))] = \"blue\"\n\n# Plot\nvisNetwork(toy_attributes, toy_edgelist, main = \"Betweenness Centrality Example\") %&gt;% visInteraction(zoomView = FALSE)\n\n\n\n\n\n\n\n35.4.3.2 Eigenvector Centrality\nEigenvector Centrality is commonly known as a measure of “popular friends.” Rather than looking at the network position of a node, it looks at the network positions of nodes connected to it. Nodes with a high eigenvector score will be connected to nodes more prominent in the network. Nodes with low degree can have high eigenvector scores if they are connected to important nodes. In real life networks this can be interpreted as being close to influential others in a network.\nI’ve colored the nodes so that the node with the highest eigenvector centrality is red, while the lowest is blue. It is considered advantageous to have a high eigenvector centrality, as this means you are well connected to other popular nodes.\n\n# Add eigenvector centrality to network object as \"evc\".\n#\n# We also tell it we are treating our data as undirected (\"graph\"), rather than\n# the default directed (\"digraph\").\n#\n# We also say we want a normalized (0-1, sum to 1) score using `rescale =\n# TRUE`.\ntoy_network%v%\"evc\" = evcent(toy_network, gmode = \"graph\", rescale = TRUE)\n\n# Add label as geodesic distance, rounding to 3 digits\ntoy_attributes$label = round(toy_network%v%\"evc\", 3)\n\n# Reset all nodes to grey\ntoy_attributes$color = \"grey\"\n\n# Replace min average geodesic with blue, max with red\ntoy_attributes$color[which(toy_network%v%\"evc\" == max(toy_network%v%\"evc\"))] = \"red\"\ntoy_attributes$color[which(toy_network%v%\"evc\" == min(toy_network%v%\"evc\"))] = \"blue\"\n\n# Plot\nvisNetwork(\n  toy_attributes, toy_edgelist, main = \"Eigenvector Centrality Example\"\n) %&gt;% visInteraction(zoomView = FALSE)",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/14_network-analysis.html#network-workflow",
    "href": "chapters/14_network-analysis.html#network-workflow",
    "title": "35  Network Analysis",
    "section": "35.5 Network Workflow",
    "text": "35.5 Network Workflow\nWe have been taking our analyses one step at a time and plotting them. This is useful for learning, but slightly annoying in practice. Below I’ve aggregated how you would actually run analyses in practice so you can refer to it later.\n\n# Data load\ntoy_edgelist = read.csv(\n  \"data/toy_edgelist.csv\", header = TRUE, stringsAsFactors = FALSE\n)\ntoy_attributes = read.csv(\n  \"data/toy_attributes.csv\", header = TRUE, stringsAsFactors = FALSE\n)\n\n# Make a network\n#\n# Sort your attributes frame alphabetically. Super important!\ntoy_attributes = toy_attributes[order(toy_attributes$id), ]\n\n# Make network!\ntoy_network_total = network(toy_edgelist, directed = FALSE)\n\n# Largest component\n#\n# Find what nodes are part of the largest component\ntoy_network_total%v%\"lc\" = component.largest(toy_network_total)\n\n# Delete those nodes that are not in the component\nin_lc = toy_network_total%v%\"lc\"\ntoy_network = delete.vertices(toy_network_total, which(!in_lc))\n\n# In our data frames\ntoy_attributes = toy_attributes[\n  toy_attributes$id %in% as.character(toy_network_total%v%\"vertex.names\"),\n]\n\nin_lc = toy_edgelist$to %in% toy_attributes$id |\n  toy_edgelist$from %in% toy_attributes$id\ntoy_edgelist = toy_edgelist[which(in_lc),]\n\n# Generate measures\n\n# Degree\ntoy_network_total%v%\"degree\" = degree(toy_network_total)\n\n# Mean geodesic\ntoy_network_total%v%\"mean_distance\" =\n  (colSums(geodist(toy_network_total)$gdist)) /\n  (nrow(as.sociomatrix(toy_network_total)) - 1)\n\n# Normalized betweenness\ntoy_network_total%v%\"norm_betweenness\" = betweenness(\n  dat = toy_network_total, gmode = \"graph\", rescale = TRUE,\n  cmode = \"undirected\"\n)\n\n# Eigenvector\ntoy_network_total%v%\"evc\" = evcent(\n  toy_network_total, gmode = \"graph\", rescale = TRUE\n)\n\n# Add back to attributes data frame\n\n# Degree\ntoy_attributes$degree = toy_network_total%v%\"degree\"\n\n# Mean geodesic\ntoy_attributes$mean_distance = toy_network_total%v%\"mean_distance\"\n\n# Normalized betweenness\ntoy_attributes$norm_betweenness = toy_network_total%v%\"norm_betweenness\"\n\n## Eigenvector\ntoy_attributes$evc = toy_network_total%v%\"evc\"\n\nFinally, we can then look at the network measures for our nodes. This data frame can be used for plotting or further analyses.\n\nhead(toy_attributes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyear\ncolor\ndegree\nmean_distance\nnorm_betweenness\nevc\n\n\n\n\n22\n028f5d1f351d38cd6553ab4674b19725d5ea3d3c\nNA\nNA\n2\n3.131579\n0\n0.0218579\n\n\n15\n19d5b2694036f6fab966564c1c44bc74330f22c2\nNA\nNA\n2\n3.131579\n0\n0.0218579\n\n\n30\n1ae1327030b801f0046278d197378603b51de4b7\nNA\nNA\n2\n3.131579\n0\n0.0218579\n\n\n67\n258f00e649e6a452acb67cb9297c88820c05e2a7\nNA\nNA\n2\n3.131579\n0\n0.0218579\n\n\n65\n2e9fed34d6b2d42052850b46aeaa97f9fe6542dc\nNA\nNA\n2\n3.131579\n0\n0.0218579\n\n\n75\n3220545023e21c80db2a4d4e10b3eb4217b90605\nNA\nNA\n2\n3.131579\n0\n0.0218579",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/14_network-analysis.html#network-tools",
    "href": "chapters/14_network-analysis.html#network-tools",
    "title": "35  Network Analysis",
    "section": "35.6 Network Tools",
    "text": "35.6 Network Tools\nThere are several ways to interact with network data in R. Thus far we have been using a combination of statnet for analysis and visNetwork for visualization. Here we’ll gloss over some other tools and what they are used for. Rather than a comprehensive tutorial, this section is just meant to introduce you to what tools are out there so you can investigate them further if you have a need for them.\n\n35.6.1 Network Models\n\n35.6.1.1 statnet\nstatnet is one of the two largest network packages in R. It allows you to create network objects and generate the network measures we’ve been looking at this far. statnet’s claim to fame however is it’s ability to run network simulations, called exponential random graph models (ERGMs). These models allow you to keep some aspect of a network constant and generate random networks that fit your specifications. This can help you highlight one structural aspect of a network and prove that, all else being random, it is important.\n\n\n\n\n\n\nNoteSee also\n\n\n\nTo learn more about ERGMs, see (Robins, Pattison, et al. 2007; Robins, Snijders, et al. 2007). Learn more on the statnet website.\n\n\n\n\n35.6.1.2 igraph\nigraph is the other big network package in R. It has more network measures than statnet, but less robust simulation capabilities. While the same network concepts you’ve learned with statnet will help you understand all networks, the code syntax for igraph is different, so you can’t use the two tools interchangeably. Notably, some functions are named the same in statnet and igraph, so it is advised not to load both at the same time.\n\n\n\n\n\n\nNoteSee also\n\n\n\nLearn more on the igraph website.\n\n\n\n\n35.6.1.3 intergraph\nintergraph is a utility package in R to convert between statnet and igraph network objects. This means you can prepare your data in your package of choice, then convert your network and use what tools you need from the other package.\n\n\n35.6.1.4 tidygraph\ntidygraph is a relatively new tool in R, built to use Tidyverse syntax. It can do many of the same basic network analyses of the two older packages, but lacks the breadth of igraph and the simulation capabilities of statnet.\n\n\n\n\n\n\nNoteSee also\n\n\n\nLearn more on the tidygraph website.\n\n\n\n\n\n35.6.2 Network Visualization\n\n35.6.2.1 Built-in\nWhile we avoided it today, all network packages have built in visualization capabilities that can look nice if you work on it. The advantage of these is that you can use the network objects themselves to pull attributes from the networks for your plots (for example, pull degree centrality from node size).\n\n\n35.6.2.2 visNetwork\nvisNetwork can make some nice interactive network visualizations with relatively simple code. This is great for learning and for exploring networks interactively. However, it does have significant downsides. For one, you have to keep separate data frames for your edges and attributes as it cannon run directly on network objects. Most importantly it cannot produce static network images! You will most likely need more static plots than interactive ones. If you can only dig deeply into one tool, this one may not be the best to specialize in.\n\n\n35.6.2.3 ggraph\nggraph uses ggplot syntax to plot networks. There are several packages that do this in various stages of development. To my understanding, ggraph is the most recent incarnation still under active development.\n\n\n\n\n\n\nNoteSee also\n\n\n\nLearn more on the ggraph website.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/14_network-analysis.html#references",
    "href": "chapters/14_network-analysis.html#references",
    "title": "35  Network Analysis",
    "section": "35.7 References",
    "text": "35.7 References\n\n\nAn, Weihua, and Yu-Hsin Liu. 2016. “Keyplayer: An R\nPackage for Locating Key Players in Social\nNetworks.” The R Journal 8 (1): 257. https://doi.org/10.32614/RJ-2016-018.\n\n\nBalaban, Alexandru T. 1985. “Applications of Graph Theory in\nChemistry.” Journal of Chemical Information and Modeling\n25 (3): 334–43. https://doi.org/10.1021/ci00047a033.\n\n\nBassett, Danielle S., and Olaf Sporns. 2017. “Network\nNeuroscience.” Nature Neuroscience 20 (3): 353–64. https://doi.org/10.1038/nn.4502.\n\n\nFan, Chao, and Ali Mostafavi. 2019. “A Graph-Based Method for\nSocial Sensing of Infrastructure Disruptions in Disasters.”\nComputer-Aided Civil and Infrastructure Engineering 34 (12):\n1055–70. https://doi.org/10.1111/mice.12457.\n\n\nKadushin, Charles. 2012. Understanding Social Networks:\nTheories, Concepts, and\nFindings. New York, NY: Oxford\nUniversity Press.\n\n\nKrebs, Valdis E. 2002. “Mapping Networks of Terrorist\nCells.” Connections 24 (3): 43–52.\n\n\nMoreno, Jacob L. 1953. “Who Shall Survive?:\nFoundations of Sociometry, Group\nPsychotherapy and Sociodrama.” In.\nBeacon, N.Y.: Beacon House Inc.\n\n\nPage, Lawrence. 2001. Method for node ranking in a linked database.\nUS6285999B1, issued September 2001. https://patents.google.com/patent/US6285999/en.\n\n\nRobins, Garry, Pip Pattison, Yuval Kalish, and Dean Lusher. 2007.\n“An Introduction to Exponential Random Graph (p*) Models for\nSocial Networks.” Social Networks, Special\nSection: Advances in Exponential Random\nGraph (p*) Models, 29 (2): 173–91. https://doi.org/10.1016/j.socnet.2006.08.002.\n\n\nRobins, Garry, Tom Snijders, Peng Wang, Mark Handcock, and Philippa\nPattison. 2007. “Recent Developments in Exponential Random Graph\n(p*) Models for Social Networks.” Social Networks,\nSpecial Section: Advances in Exponential\nRandom Graph (p*) Models, 29 (2): 192–215. https://doi.org/10.1016/j.socnet.2006.08.003.\n\n\nWasserman, Stanley, and Katherine Faust. 1994. Social Network\nAnalysis: Methods and Applications. Cambridge,\nUK: Cambridge University Press.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/15_natural-language-processing.html",
    "href": "chapters/15_natural-language-processing.html",
    "title": "36  Natural Language Processing",
    "section": "",
    "text": "36.1 Using a File Manifest\nThis lecture is designed to introduce you to the basics of preprocessing and analyzing natural language text data.\nIn this lesson, we’ll be preparing a collection of texts for computational analysis. Before we start that work in full, we’re going to load in a file manifest, which will help us a) identify what’s in our collection; and b) keep track of things like the order of texts.\nmanifest &lt;- read.csv(\"data/C19_novels_manifest.csv\", row.names = 1)\nmanifest\n\n   last_name  first_name                         title year genre\n1   Beckford     William                        Vathek 1786     G\n2  Radcliffe         Ann              ASicilianRomance 1790     G\n3  Radcliffe         Ann         TheMysteriesofUdolpho 1794     G\n4      Lewis     Matthew                       TheMonk 1795     G\n5     Austen        Jane           SenseandSensibility 1811     N\n6    Shelley        Mary                  Frankenstein 1818     G\n7      Scott      Walter                       Ivanhoe 1820     N\n8        Poe  EdgarAllen TheNarrativeofArthurGordonPym 1838     N\n9     Bronte       Emily              WutheringHeights 1847     G\n10 Hawthorne   Nathaniel      TheHouseoftheSevenGables 1851     N\n11   Gaskell   Elizabeth                 NorthandSouth 1854     N\n12   Collins      Wilkie               TheWomaninWhite 1860     N\n13   Dickens     Charles             GreatExpectations 1861     N\n14     James       Henry               PortraitofaLady 1881     N\n15 Stevenson RobertLouis                TreasureIsland 1882     N\n16 Stevenson RobertLouis                 JekyllandHyde 1886     G\n17     Wilde       Oscar        ThePictureofDorianGray 1890     G\n18    Stoker        Bram                       Dracula 1897     G\nAs you can see, in addition to the author and title listings, we also have a genre tag. G is for “Gothic” literature, while N is “not Gothic.” Let’s convert the datatype for the genre column into a factor, which will make life easier later on.\nmanifest$genre &lt;- as.factor(manifest$genre)",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "chapters/15_natural-language-processing.html#loading-a-corpus",
    "href": "chapters/15_natural-language-processing.html#loading-a-corpus",
    "title": "36  Natural Language Processing",
    "section": "36.2 Loading a Corpus",
    "text": "36.2 Loading a Corpus\nWith our metadata loaded, it’s time to bring in our files. We’ll be using files stored in an RDS format, though you could also load straight from a directory with a combination of lapply and readLines.\n\nfiles &lt;- readRDS(\"data/C19_novels_raw.rds\")\n\nLoading our files like this will create a giant list of vectors, where each vector is a full text file. Those vectors are chunked by paragraph right now, but for our purposes it would be easier if each vector was a single stream of text (like the output of ocr, if you’ll remember). We can collapse them together with paste.\n\nfiles &lt;- lapply(files, paste, collapse = \" \")\n\nFrom here, we can wrap these files in a special “corpus” object, which the tm package enables (a corpus is a large collection of texts). A tm corpus works somewhat like a database. It has a section for “content”, which contains text data, as well as various metadata sections, which we can populate with additional information about our texts, if we wished. Taken together, these features make it easy to streamline workflows with text data.\nTo make a corpus with tm, we call the Corpus function, specifying with VectorSource (because our texts are vectors):\n\nlibrary(\"tm\")\ncorpus &lt;- Corpus(VectorSource(files))\n\nHere’s a high-level glimpse at what’s in this object:\n\ncorpus\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 18\n\n\nZooming in to metadata about a text in the corpus:\n\ncorpus[[6]]\n\n&lt;&lt;PlainTextDocument&gt;&gt;\nMetadata:  7\nContent:  chars: 424017\n\n\nNot much here so far, but we’ll add more later.\nFinally, we can get content from a text:\n\nlibrary(\"stringr\")\n\nstr_sub(corpus[[6]]$content, start = 1, end = 500)\n\n[1] \"FRANKENSTEIN:  OR,  THE MODERN PROMETHEUS.  BY MARY W. SHELLEY.  PREFACE.  The event on which this fiction is founded, has been supposed, by Dr. Darwin, and some of the physiological writers of Germany, as not of impossible occurrence. I shall not be supposed as according the remotest degree of serious faith to such an imagination; yet, in assuming it as the basis of a work of fancy, I have not considered myself as merely weaving a series of supernatural terrors. The event on which the interest \"\n\n\nIn this last view, you can see that the text file is still formatted (at least we didn’t have to OCR it!). This formatting is unwieldy and worse, it makes it so we can’t really access the elements that comprise each novel. We’ll need to do more work to preprocess our texts before we can analyze them.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "chapters/15_natural-language-processing.html#preprocessing",
    "href": "chapters/15_natural-language-processing.html#preprocessing",
    "title": "36  Natural Language Processing",
    "section": "36.3 Preprocessing",
    "text": "36.3 Preprocessing\nPart of preprocessing entails making decisions about the kinds of information we want to know about our data. Knowing what information we want often guides the way we structure data. Put another way: research questions drive preprocessing.\n\n36.3.1 Tokenizing and Bags of Words\nFor example, it’d be helpful to know how many words are in each novel, which might enable us to study patterns and differences between authors’ styles. To get word counts, we need to split the text vectors into individual words. One way to do this would be to first strip out everything in each novel that isn’t an alphabetic character or a space. Let’s grab one text to experiment with.\n\nfrankenstein &lt;- corpus[[6]]$content\nfrankenstein &lt;- str_replace_all(\n  frankenstein, pattern = \"[^A-Za-z]\", replacement = \" \"\n)\n\nFrom here, it would be easy enough to count the words in a novel by splitting its vector on spaces, removing empty elements in the vector, and calling length on the vector. The end result is what we call a bag of words.\n\nfrankenstein &lt;- str_split(frankenstein, \" \")\nfrankenstein &lt;- lapply(frankenstein, function(x) x[x != \"\"])\nlength(frankenstein[[1]])\n\n[1] 76015\n\n\nAnd here are the first nine words (or “tokens”):\n\nfrankenstein[[1]][1:9]\n\n[1] \"FRANKENSTEIN\" \"OR\"           \"THE\"          \"MODERN\"       \"PROMETHEUS\"  \n[6] \"BY\"           \"MARY\"         \"W\"            \"SHELLEY\"     \n\n\n\n\n36.3.2 Text Normalization\nWhile easy, producing our bag of words this way is a bit clunky. And further, this process can’t handle contractions (“I’m”, “don’t”, “that’s”) or differences in capitalization.\n\nfrankenstein[[1]][188:191]\n\n[1] \"Midsummer\" \"Night\"     \"s\"         \"Dream\"    \n\n\nShould be:\nMidsummer Night's Dream\nAnd\n\"FRANKENSTEIN\", \"Frankenstein\"\nShould be:\n\"Frankenstein\"\nOr, even better:\nfrankenstein\nTypically, when we work with text data we want all of our words to be in the same case because this makes it easier to do things like counting operations. Remember that, to a computer, “Word” and “word” are two separate words, and if we want to count them together, we need to pick one version or the other. Making all words lowercase (even proper nouns) is the standard. Doing this is part of what’s called text normalization. (Other forms of normalization might entail handling orthographic differences between British and American English, like “color” and “colour”.)\nAs for contractions, we have some decisions to make. On the one hand, it’s important to retain as much information as we can about the original text, so keeping “don’t” or “what’s” (which would be “don t” and “what s” in our current method) is important. One way corpus linguists handle these words is to lemmatize them. Lemmatizing involves removing inflectional endings to return words to their base form:\n\ncar, cars, car’s, cars’ =&gt; car\ndon’t =&gt; do\n\nThis is a helpful step if what we’re primarily interested in is doing a high- level analysis of semantics. On the other hand, though, many words that feature contractions are high-frequency function words, which don’t have much meaning beyond the immediate context of a sentence or two. Words like “that’s” or “won’t” appear in huge numbers in text data, but they don’t carry much information in and of themselves—it may in fact be the case that we could get rid of them entirely…\n\n\n36.3.3 Stop Words\n…and indeed this is the case! When structuring text data to study it at scale, it’s common to remove, or stop out, words that don’t have much meaning. This makes it much easier to identify significant (i.e. unique) features in a text, without having to swim through all the noise of “the” or “that,” which would almost always show up as the highest-occurring words in an analysis. But what words should we remove? Ultimately, this depends on your text data. We can usually assume that function words will be on our list of stop words, but it may be that you’ll have to add or subtract others depending on your data and, of course, your research question.\nThe tm package has a good starting list. Let’s look at the first 100 words.\n\nhead(stopwords(\"SMART\"), 100)\n\n  [1] \"a\"            \"a's\"          \"able\"         \"about\"        \"above\"       \n  [6] \"according\"    \"accordingly\"  \"across\"       \"actually\"     \"after\"       \n [11] \"afterwards\"   \"again\"        \"against\"      \"ain't\"        \"all\"         \n [16] \"allow\"        \"allows\"       \"almost\"       \"alone\"        \"along\"       \n [21] \"already\"      \"also\"         \"although\"     \"always\"       \"am\"          \n [26] \"among\"        \"amongst\"      \"an\"           \"and\"          \"another\"     \n [31] \"any\"          \"anybody\"      \"anyhow\"       \"anyone\"       \"anything\"    \n [36] \"anyway\"       \"anyways\"      \"anywhere\"     \"apart\"        \"appear\"      \n [41] \"appreciate\"   \"appropriate\"  \"are\"          \"aren't\"       \"around\"      \n [46] \"as\"           \"aside\"        \"ask\"          \"asking\"       \"associated\"  \n [51] \"at\"           \"available\"    \"away\"         \"awfully\"      \"b\"           \n [56] \"be\"           \"became\"       \"because\"      \"become\"       \"becomes\"     \n [61] \"becoming\"     \"been\"         \"before\"       \"beforehand\"   \"behind\"      \n [66] \"being\"        \"believe\"      \"below\"        \"beside\"       \"besides\"     \n [71] \"best\"         \"better\"       \"between\"      \"beyond\"       \"both\"        \n [76] \"brief\"        \"but\"          \"by\"           \"c\"            \"c'mon\"       \n [81] \"c's\"          \"came\"         \"can\"          \"can't\"        \"cannot\"      \n [86] \"cant\"         \"cause\"        \"causes\"       \"certain\"      \"certainly\"   \n [91] \"changes\"      \"clearly\"      \"co\"           \"com\"          \"come\"        \n [96] \"comes\"        \"concerning\"   \"consequently\" \"consider\"     \"considering\" \n\n\nThat looks pretty comprehensive so far, though the only way we’ll know whether it’s a good match for our corpus is to process our corpus with it. At first glance, the extra random letters in this list seem like they could be a big help, on the off chance there’s some noise from OCR. If you look at the first novel in the corpus, for example, there are a bunch of stray p’s, which is likely from a pattern for marking pages (“p. 7”):\n\nmessage(str_sub(corpus[[1]]$content, start = 1, end = 1000))\n\nVATHEK;  AN ARABIAN TALE,    BY  WILLIAM BECKFORD, ESQ.    p. 7VATHEK.  Vathek, ninth Caliph [7a] of the race of the Abassides, was the son of Motassem, and the grandson of Haroun Al Raschid.  From an early accession to the throne, and the talents he possessed to adorn it, his subjects were induced to expect that his reign would be long and happy.  His figure was pleasing and majestic; but when he was angry, one of his eyes became so terrible [7b] that no person could bear to behold it; and the wretch upon whom it was fixed instantly fell backward, and sometimes expired.  For fear, however, of depopulating his dominions, and making his palace desolate, he but rarely gave way to his anger.  Being much addicted to women, and the pleasures of the table, he sought by his affability to procure agreeable companions; and he succeeded the better, p. 8as his generosity was unbounded and his indulgences unrestrained; for he was by no means scrupulous: nor did he think, with the Caliph Omar Ben A\n\n\nOur stop word list would take care of this. With it, we could return to our original collection of novels, split them on spaces as before, and filter out everything that’s stored in our stop_list variable. Before we did the filtering, though, we’d need to transform the novels into lowercase (which can be done with stringr’s str_to_lower function).\n\n\n36.3.4 Tokenizers\nThis whole process is ultimately straightforward so far, but it would be nice to collapse all its steps. Luckily, there are packages we can use to streamline our process. The tokenizers package has functions that split a text vector, turn words into lowercase forms, and remove stop words, all in a few lines of code. Further, we can combine these functions with a special tm_map function in the tm package, which will globally apply our changes.\n\nlibrary(\"tokenizers\")\n\ncleaned_corpus &lt;- tm_map(\n  corpus,\n  tokenize_words,\n  stopwords = stopwords('SMART'),\n  lowercase = TRUE,\n  strip_punct = TRUE,\n  strip_numeric = TRUE\n)\n\nYou may see a “transformation drops documents” warning after this. You can disregard it. It has to do with the way tm references text changes against a corpus’s metadata, which we’ve left blank.\nWe can compare our tokenized output with the text data we had been working with earlier:\n\nlist(\n  untokenized = frankenstein[[1]][1:9],\n  tokenized = cleaned_corpus[[6]]$content[1:5]\n)\n\n$untokenized\n[1] \"FRANKENSTEIN\" \"OR\"           \"THE\"          \"MODERN\"       \"PROMETHEUS\"  \n[6] \"BY\"           \"MARY\"         \"W\"            \"SHELLEY\"     \n\n$tokenized\n[1] \"frankenstein\" \"modern\"       \"prometheus\"   \"mary\"         \"shelley\"     \n\n\nFrom the title alone we can see how much of a difference tokenizing with stop words makes. And while we lose a bit of information by doing this, what we can is a much clearer picture of key words we’d want to further analyze.\n\n\n36.3.5 Document Chunking and N-grams\nFinally, it’s possible to change the way we separate out our text data. Instead of tokenizing on words, we could use the tokenizers package to break apart our texts on paragraphs (tokenize_paragraphs), sentences (tokenize_sentences), and more. There might be valuable information to be learned about the average sentence length of a novel, for example, so we might chunk it accordingly.\nWe might also want to see whether a text contains repeated phrases, or if two or three words often occur in the same sequence. We could investigate this by adjusting the window around which we tokenize individual words. So far we’ve used the “unigram,” or a single word, as our basic unit of counting, but we could break our texts into “bigrams” (two word phrases), “trigrams” (three word phrases), or, well any sequence of \\(n\\) units. Generally, you’ll see these sequences referred to as n-grams:\n\nfrankenstein_bigrams &lt;- tokenize_ngrams(\n  corpus[[6]]$content,\n  n = 2,\n  stopwords = stopwords(\"SMART\")\n)\n\nHere, n = 2 sets the n-gram window at two:\n\nfrankenstein_bigrams[[1]][1:20]\n\n [1] \"frankenstein modern\"   \"modern prometheus\"     \"prometheus mary\"      \n [4] \"mary shelley\"          \"shelley preface\"       \"preface event\"        \n [7] \"event fiction\"         \"fiction founded\"       \"founded supposed\"     \n[10] \"supposed dr\"           \"dr darwin\"             \"darwin physiological\" \n[13] \"physiological writers\" \"writers germany\"       \"germany impossible\"   \n[16] \"impossible occurrence\" \"occurrence supposed\"   \"supposed remotest\"    \n[19] \"remotest degree\"       \"degree faith\"         \n\n\nNote though that, for this function, we’d need to do some preprocessing on our own to remove numeric characters and punctuation; tokenize_ngrams won’t do it for us.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "chapters/15_natural-language-processing.html#counting-terms",
    "href": "chapters/15_natural-language-processing.html#counting-terms",
    "title": "36  Natural Language Processing",
    "section": "36.4 Counting Terms",
    "text": "36.4 Counting Terms\nLet’s return to our single word counts. Now that we’ve transformed our novels into bags of single words, we can start with some analysis. Simply counting the number of times a word appears in some data can tell us a lot about a text. The following steps should feel familiar: we did them with OCR.\nLet’s look at Wuthering Heights, which is our ninth text:\n\nlibrary(\"tidyverse\")\n\nwuthering_heights &lt;- table(cleaned_corpus[[9]]$content)\nwuthering_heights &lt;- data.frame(\n  word = names(wuthering_heights),\n  count = as.numeric(wuthering_heights)\n)\nwuthering_heights &lt;- arrange(wuthering_heights, desc(count))\n\nhead(wuthering_heights, 30)\n\n         word count\n1  heathcliff   422\n2      linton   348\n3   catherine   339\n4          mr   312\n5      master   185\n6     hareton   169\n7    answered   156\n8        till   151\n9       house   144\n10       door   133\n11        mrs   133\n12     joseph   130\n13       miss   129\n14       time   127\n15       back   121\n16    thought   118\n17      cathy   117\n18       good   117\n19    replied   117\n20   earnshaw   116\n21       eyes   116\n22      cried   114\n23      young   107\n24        day   106\n25     father   106\n26      asked   105\n27       make   105\n28      edgar   104\n29      night   104\n30       made   102\n\n\nLooks good! The two main characters in this novel are named Heathcliff and Catherine, so it makes sense that these words would appear a lot. You can see, however, that we might want to fine tune our stop word list so that it removes “mr” and “mrs” from the text. Though again, it depends on our research question. If we’re exploring gender roles in nineteenth-century literature, we’d probably keep those words in.\nIn addition to fine tuning stop words, pausing here at these counts would be a good way to check whether some other form of textual noise is present in your data, which you haven’t yet caught. There’s nothing like that here, but you might imagine how consistent OCR noise could make itself known in this view.\n\n36.4.1 Term Frequency\nAfter you’ve done your fine tuning, it would be good to get a term frequency number for each word in this data frame. Raw counts are nice, but expressing those counts in proportion to the total words in a document will tell us more information about a word’s contribution to the document as a whole. We can get term frequencies for our words by dividing a word’s count by document length (which is the sum of all words in the document).\n\nwuthering_heights$term_frequency &lt;- sapply(\n  wuthering_heights$count,\n  function(x) x / sum(wuthering_heights$count)\n)\nhead(wuthering_heights, 30)\n\n         word count term_frequency\n1  heathcliff   422    0.009619549\n2      linton   348    0.007932709\n3   catherine   339    0.007727552\n4          mr   312    0.007112084\n5      master   185    0.004217101\n6     hareton   169    0.003852379\n7    answered   156    0.003556042\n8        till   151    0.003442066\n9       house   144    0.003282500\n10       door   133    0.003031754\n11        mrs   133    0.003031754\n12     joseph   130    0.002963368\n13       miss   129    0.002940573\n14       time   127    0.002894983\n15       back   121    0.002758212\n16    thought   118    0.002689827\n17      cathy   117    0.002667031\n18       good   117    0.002667031\n19    replied   117    0.002667031\n20   earnshaw   116    0.002644236\n21       eyes   116    0.002644236\n22      cried   114    0.002598646\n23      young   107    0.002439080\n24        day   106    0.002416285\n25     father   106    0.002416285\n26      asked   105    0.002393490\n27       make   105    0.002393490\n28      edgar   104    0.002370695\n29      night   104    0.002370695\n30       made   102    0.002325104\n\n\n\n\n36.4.2 Plotting Term Frequency\nLet’s plot the top 50 words in Wuthering Heights. We’ll call fct_reorder in the aes layer of ggplot to sort words in the descending order of their term frequency.\n\nlibrary(\"ggplot2\")\n\nggplot(wuthering_heights[1:50, ]) +\n  aes(x = fct_reorder(word, -term_frequency), y = term_frequency) +\n  geom_bar(stat =\"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)\n  ) +\n  labs(\n    title = \"Top 50 words in Wuthering Heights\",\n    x = \"Word\",\n    y = \"Term Frequency\"\n  )\n\n\n\n\n\n\n\n\nThis is a good start for creating a high-level view of the novel, but further tuning might be in order. We’ve already mentioned “mrs” and “mr” as two words that we could cut out of the text. Another option would be to collapse these two words together into a base form by stemming them. Though this would overweight their base form (which in this case is “mr”) in terms of term frequency, it would also free up space to see other terms in the document. Other examples of stemming words would be transforming “fishing”, “fished”, and “fisher” all into “fish.”\nThat said, like all preprocessing, lemmatizing words is an interpretive decision, which comes with its own consequences. Maybe it’s okay to transform “mr” and “mrs” into “mr” for some analyses, but it’s also the case that we’d be erasing potentially important gender differences in the text—and would do so by overweighting the masculine form of the word. Regardless of what you decide, it’s important to keep track of these decisions as you make them because they will impact the kinds of claims you make about your data later on.\n\n\n36.4.3 Comparing Term Frequencies Across Documents\nTerm frequency is helpful if we want to start comparing words across two texts. We can make some comparisons by transforming the above code into a function:\n\nterm_table &lt;- function(text) {\n  term_tab &lt;- table(text)\n\n  term_tab &lt;- data.frame(word = names(term_tab), count = as.numeric(term_tab))\n  term_tab$term_frequency &lt;- sapply(\n    term_tab$count,\n    function(x) (x/sum(term_tab$count))\n  )\n\n  arrange(term_tab, desc(count))\n}\n\nWe already have a term table for Wuthering Heights. Let’s make one for Dracula:\n\ndracula &lt;- term_table(cleaned_corpus[[18]]$content)\nhead(dracula, 30)\n\n        word count term_frequency\n1       time   387    0.007280458\n2        van   321    0.006038829\n3    helsing   299    0.005624953\n4       back   261    0.004910076\n5       room   231    0.004345699\n6       good   225    0.004232824\n7       lucy   225    0.004232824\n8        man   224    0.004214012\n9       dear   219    0.004119949\n10      mina   217    0.004082324\n11     night   217    0.004082324\n12      hand   209    0.003931823\n13      face   205    0.003856573\n14      door   201    0.003781323\n15      made   193    0.003630822\n16      poor   192    0.003612010\n17     sleep   190    0.003574385\n18      eyes   186    0.003499135\n19    looked   185    0.003480322\n20    friend   183    0.003442697\n21     great   182    0.003423884\n22  jonathan   182    0.003423884\n23        dr   178    0.003348634\n24    things   174    0.003273384\n25      make   163    0.003066446\n26       day   160    0.003010008\n27 professor   155    0.002915946\n28     count   153    0.002878320\n29     found   153    0.002878320\n30   thought   153    0.002878320\n\n\nNow we can compare the relative frequency of a word across two novels:\n\ncomparison_words &lt;- c(\"dark\", \"night\", \"ominous\")\nfor (i in comparison_words) {\n  wh &lt;- list(wh = subset(wuthering_heights, word == i))\n  drac &lt;- list(drac = subset(dracula, word == i))\n  print(wh)\n  print(drac)\n}\n\n$wh\n    word count term_frequency\n183 dark    32   0.0007294445\n\n$drac\n   word count term_frequency\n90 dark    77    0.001448566\n\n$wh\n    word count term_frequency\n29 night   104    0.002370695\n\n$drac\n    word count term_frequency\n11 night   217    0.004082324\n\n$wh\n        word count term_frequency\n7283 ominous     1   2.279514e-05\n\n$drac\n        word count term_frequency\n7217 ominous     1   1.881255e-05\n\n\nNot bad! We might be able to make a few generalizations from this, but to say anything definitively, we’ll need to scale our method. Doing so wouldn’t be easy with this setup as it stands now. While it’s true that we could write some functions to roll through these two data frames and systematically compare the words in each, it would take a lot of work to do so. Luckily, the tm package (which we’ve used to make our stop word list) features generalized functions for just this kind of thing.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "chapters/15_natural-language-processing.html#text-mining-pipepline",
    "href": "chapters/15_natural-language-processing.html#text-mining-pipepline",
    "title": "36  Natural Language Processing",
    "section": "36.5 Text Mining Pipepline",
    "text": "36.5 Text Mining Pipepline\nBefore going further, we should note that tm has its own functions for preprocessing texts. To send raw files directly through those functions, you’d call tm_map in conjunction with these functions. You can think of tm_map as a cognate to the apply family.\ncorpus_2 &lt;- Corpus(VectorSource(files))\ncorpus_2 &lt;- tm_map(corpus_2, removeNumbers)\ncorpus_2 &lt;- tm_map(corpus_2, removeWords, stopwords(\"SMART\"))\ncorpus_2 &lt;- tm_map(corpus_2, removePunctuation)\ncorpus_2 &lt;- tm_map(corpus_2, stripWhitespace)\nNote the order of operations here: because our stop words list takes into account punctuated words, like “don’t” or “i’m”, we want to remove stop words before removing punctuation. If we didn’t do this, removeWords wouldn’t catch the un-punctuated “dont” or “im”. This won’t always be the case, since we can use different stop word lists, which may have a different set of terms, but in this instance, the order in which we preprocess matters.\nPreparing your text files like this would be fine, and indeed sometimes it’s preferable to sequentially step through each part of the preprocessing workflow. That said, tokenizers manages the order of operations above on its own and its preprocessing functions are generally a bit faster to run (in particular, removeWords is quite slow in comparison to tokenize_words).\nThere is, however, one caveat to using tokenizers. It splits documents up to do text cleaning, but other functions in tm require non-split documents. If we use tokenizers, then, we need to do a quick workaround with paste.\n\ncleaned_corpus &lt;- lapply(cleaned_corpus, paste, collapse = \" \")\n\nAnd then reformat that output as a corpus object:\n\ncleaned_corpus &lt;- Corpus(VectorSource(cleaned_corpus))\n\nUltimately, it’s up to you to decide what workflow makes sense. Personally, I (Tyler) like to do exploratory preprocessing steps with tokenizers, often with a sample set of all the documents. Then, once I’ve settled on my stop word list and so forth, I reprocess all my files with the tm-specific functions above.\nRegardless of what workflow you choose, preprocessing can take a while, so now would be a good place to save your data. That way, you can retrieve your corpus later on.\n\nsaveRDS(cleaned_corpus, \"data/C19_novels_cleaned.rds\")\n\nLoading it back in is straightforward:\n\ncleaned_corpus &lt;- readRDS(\"data/C19_novels_cleaned.rds\")",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "chapters/15_natural-language-processing.html#document-term-matrix",
    "href": "chapters/15_natural-language-processing.html#document-term-matrix",
    "title": "36  Natural Language Processing",
    "section": "36.6 Document Term Matrix",
    "text": "36.6 Document Term Matrix\nThe advantage of using a tm corpus is that it makes comparing data easier. Remember that, in our old workflow, looking at the respective term frequencies in two documents entailed a fair bit of code. And further, we left off before generalizing that code to the corpus as a whole. But what if we wanted to look at a term across multiple documents?\nTo do so, we need to create what’s called a document-term matrix, or DTM. A DTM describes the frequency of terms across an entire corpus (rather than just one document). Rows of the matrix correspond to documents, while columns correspond to the terms. For a given document, we count the number of times that term appears and enter that number in the column in question. We do this even if the count is 0; key to the way a DTM works is that it’s a corpus-wide representation of text data, so it matters if a text does or doesn’t contain a term.\nHere’s a simple example with three documents:\n\nDocument 1: “I like cats”\nDocument 2: “I like dogs”\nDocument 3: “I like both cats and dogs”\n\nTransforming these into a document-term matrix would yield:\n\n\n\nn_doc\nI\nlike\nboth\ncats\nand\ndogs\n\n\n\n\n1\n1\n1\n0\n1\n0\n0\n\n\n2\n1\n1\n0\n0\n0\n1\n\n\n3\n1\n1\n1\n1\n1\n1\n\n\n\nRepresenting texts in this way is incredibly useful because it enables us to easily discern similarities and differences in our corpus. For example, we can see that each of the above documents contain the words “I” and “like.” Given that, if we wanted to know what makes documents unique, we can ignore those two words and focus on the rest of the values.\nNow, imagine doing this for thousands of words! What patterns might emerge?\nLet’s try it on our corpus. We can transform a tm corpus object into a DTM by calling DocumentTermMatrix.\n\n\n\n\n\n\nWarning\n\n\n\nDocumentTermMatrix is one of the functions in the tm package that requires non-split documents, so before you call it make sure you know how you’ve preprocessed your texts!\n\n\n\ndtm &lt;- DocumentTermMatrix(cleaned_corpus)\n\nThis object is quite similar to the one that results from Corpus: it contains a fair bit of metadata, as well as an all-important dimnames field, which records the documents in the matrix and the entire term vocabulary. We access all of this information with the same syntax we use for data frames.\nLet’s look around a bit and get some high-level info.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "chapters/15_natural-language-processing.html#corpus-analytics",
    "href": "chapters/15_natural-language-processing.html#corpus-analytics",
    "title": "36  Natural Language Processing",
    "section": "36.7 Corpus Analytics",
    "text": "36.7 Corpus Analytics\nNumber of columns in the DTM (that is, the vocabulary size):\n\ndtm$ncol\n\n[1] 34925\n\n\nNumber of rows in the DTM (that is, the number of documents this matrix represents):\n\ndtm$nrow\n\n[1] 18\n\n\nRight now, the document names are just a numbers in a vector:\n\ndtm$dimnames$Docs\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\"\n\n\nBut they’re ordered according to the sequence in which the corpus was originally created. This means we can use our metadata from way back when to associate a document with its title:\n\ndtm$dimnames$Docs &lt;- manifest$title\ndtm$dimnames$Docs\n\n [1] \"Vathek\"                        \"ASicilianRomance\"             \n [3] \"TheMysteriesofUdolpho\"         \"TheMonk\"                      \n [5] \"SenseandSensibility\"           \"Frankenstein\"                 \n [7] \"Ivanhoe\"                       \"TheNarrativeofArthurGordonPym\"\n [9] \"WutheringHeights\"              \"TheHouseoftheSevenGables\"     \n[11] \"NorthandSouth\"                 \"TheWomaninWhite\"              \n[13] \"GreatExpectations\"             \"PortraitofaLady\"              \n[15] \"TreasureIsland\"                \"JekyllandHyde\"                \n[17] \"ThePictureofDorianGray\"        \"Dracula\"                      \n\n\nWith this information associated, we can use inspect to get a high-level view of the corpus:\n\ninspect(dtm)\n\n&lt;&lt;DocumentTermMatrix (documents: 18, terms: 34925)&gt;&gt;\nNon-/sparse entries: 145233/483417\nSparsity           : 77%\nMaximal term length: 19\nWeighting          : term frequency (tf)\nSample             :\n                          Terms\nDocs                       back day eyes good great long made man thought time\n  Dracula                   261 160  186  225   182  147  193 224     153  387\n  GreatExpectations         244 216  180  256   198  173  300 307     238  373\n  Ivanhoe                    77 138  100  298   111  154  151 235      46  182\n  NorthandSouth             184 257  197  316   179  211  234 270     332  423\n  PortraitofaLady           210 241  226  520   421  187  381 317     302  339\n  TheHouseoftheSevenGables   79 113   72  100   144  153  144 211      60  113\n  TheMonk                    81 106  184   80    66  108  167  95      72  162\n  TheMysteriesofUdolpho     117 167  225  186   164  359  316 213     341  367\n  TheWomaninWhite           417 351  233  235   112  188  244 443     183  706\n  WutheringHeights          121 106  116  117    63   97  102  88     118  127\n\n\nOf special note here is sparsity. Sparsity measures the amount of 0s in the data. This happens when a document does not contain a term that appears elsewhere in the corpus. In our case, of the 628,650 entries in this matrix, 80% of them are 0. Such is the way of working with DTMs: they’re big, expansive data structures that have a lot of empty space.\nWe can zoom in and filter on term counts with findFreqTerms. Here are terms that appear more than 1,000 times in the corpus:\n\nfindFreqTerms(dtm, 1000)\n\n [1] \"answered\" \"appeared\" \"asked\"    \"back\"     \"day\"      \"dear\"    \n [7] \"death\"    \"door\"     \"eyes\"     \"face\"     \"father\"   \"felt\"    \n[13] \"found\"    \"friend\"   \"gave\"     \"give\"     \"good\"     \"great\"   \n[19] \"half\"     \"hand\"     \"hands\"    \"head\"     \"hear\"     \"heard\"   \n[25] \"heart\"    \"hope\"     \"kind\"     \"knew\"     \"lady\"     \"leave\"   \n[31] \"left\"     \"life\"     \"light\"    \"long\"     \"looked\"   \"love\"    \n[37] \"made\"     \"make\"     \"man\"      \"men\"      \"mind\"     \"moment\"  \n[43] \"morning\"  \"mother\"   \"night\"    \"part\"     \"passed\"   \"people\"  \n[49] \"person\"   \"place\"    \"poor\"     \"present\"  \"put\"      \"replied\" \n[55] \"returned\" \"round\"    \"side\"     \"speak\"    \"stood\"    \"thing\"   \n[61] \"thou\"     \"thought\"  \"till\"     \"time\"     \"told\"     \"turned\"  \n[67] \"voice\"    \"woman\"    \"words\"    \"world\"    \"young\"    \"count\"   \n[73] \"house\"    \"madame\"   \"room\"     \"sir\"      \"emily\"    \"margaret\"\n[79] \"miss\"     \"mrs\"      \"isabel\"  \n\n\nUsing findAssocs, we can also track which words rise and fall in usage alongside a given word. (The number in the third argument position of this function is a cutoff for the strength of a correlation.)\nHere’s “boat”:\n\nfindAssocs(dtm, \"boat\", .85)\n\n$boat\n  thumping scoundrels     midday  direction \n      0.94       0.88       0.87       0.85 \n\n\nHere’s “writing” (there are a lot of terms, so we’ll limit to 15):\n\nwriting &lt;- findAssocs(dtm, \"writing\", .85)\nwriting[[1]][1:15]\n\n     letter        copy    disposal   inquiries    bedrooms   hindrance \n       0.99        0.97        0.97        0.97        0.97        0.97 \n   messages certificate    distrust     plainly    drawings   anonymous \n       0.97        0.97        0.96        0.96        0.96        0.96 \n   ladyship  plantation    lodgings \n       0.96        0.96        0.96 \n\n\n\n36.7.1 Corpus Term Counts\nFrom here, it would be useful to get a full count of all the terms in the corpus. We can transform the DTM into a matrix and then a data frame:\n\nterm_counts &lt;- as.matrix(dtm)\nterm_counts &lt;- data.frame(sort(colSums(term_counts), decreasing = TRUE))\nterm_counts &lt;- cbind(newColName = rownames(term_counts), term_counts)\ncolnames(term_counts) &lt;- c(\"term\", \"count\")\n\nAs before, let’s plot the top 50 terms in these counts, but this time, they will cover the entire corpus:\n\nggplot(term_counts[1:50, ]) +\n  aes(x = fct_reorder(term, -count), y = count) +\n  geom_bar(stat = \"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)\n  ) +\n  labs(\n    title = \"Top 50 words in 18 Nineteenth-Century Novels\",\n    x = \"Word\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\nThis looks good, though the words here are all pretty common. In fact, many of them are simply the most common words in the English language. “Time” is the 64th-most frequent word in English; “make” is the 50th. As it stands, then, this graph doesn’t tell us very much about the specificity of our particular collection of texts; if we ran the same process on English novels from the twentieth century, we’d probably produce very similar output.\n\n\n36.7.2 TF-IDF Scores\nGiven this, if we want to know what makes our corpus special, we need a measure of uniqueness for the terms it contains. One of the most common ways to do this is to get what’s called a TF-IDF score (short for “term frequency-inverse document frequency”) for each term in our corpus. TF-IDF is a weighting method. It increases proportionally to the number of times a word appears in a document but is importantly offset by the number of documents in the corpus that contain this term. This offset adjusts for common words across a corpus, pushing their scores down while boosting the scores of rarer terms in the corpus.\nInverse document frequency can be expressed as:\n\\[\\begin{align*}\nidf_i = log(\\frac{n}{df_i})\n\\end{align*}\\]\nWhere \\(idf_i\\) is the idf score for term \\(i\\), \\(df_i\\) is the number of documents that contain \\(i\\), and \\(n\\) is the total number of documents.\nA TF-IDF score can be calculated by the following:\n\\[\\begin{align*}\nw_i,_j = tf_i,_j \\times idf_i\n\\end{align*}\\]\nWhere \\(w_i,_j\\) is the TF-IDF score of term \\(i\\) in document \\(j\\), \\(tf_i,_j\\) is the term frequency for \\(i\\) in \\(j\\), and \\(idf_i\\) is the inverse document score.\nWhile it’s good to know the underlying equations here, you won’t be tested on the math specifically. And as it happens, tm has a way to perform the above math for each term in a corpus. We can implement TF-IDF scores when making a document-term matrix:\n\ndtm_tfidf &lt;- DocumentTermMatrix(\n  cleaned_corpus,\n  control = list(weighting = weightTfIdf)\n)\ndtm_tfidf$dimnames$Docs &lt;- manifest$title\n\nTo see what difference it makes, let’s plot the top terms in our corpus using their TF-IDF scores:\n\ntfidf_counts &lt;- as.matrix(dtm_tfidf)\ntfidf_counts &lt;- data.frame(sort(colSums(tfidf_counts), decreasing = TRUE))\ntfidf_counts &lt;- cbind(newColName = rownames(tfidf_counts), tfidf_counts)\ncolnames(tfidf_counts) &lt;- c(\"term\", \"tfidf\")\n\n\nggplot(data = tfidf_counts[1:50, ]) +\n  aes(x = fct_reorder(term, -tfidf), y = tfidf) +\n  geom_bar(stat = \"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)\n  ) +\n  labs(\n    title = \"Words with the 50-highest TF-IDF scores in 18 Nineteenth-Century Novels\",\n    x = \"Word\",\n    y = \"TF-IDF\"\n  )\n\n\n\n\n\n\n\n\nLots of names! That makes sense: heavily weighted terms in these novels are going to be terms that are unique to each text. Main characters’ names are used a lot in novels, and the main character names in these novels are all unique.\nTo see in more concrete way how TF-IDF scores might make a difference in the way we analyze our corpus, we’ll do two last things. First, we’ll look again at term correlations, using the same words from above with findAssocs, but this time we’ll use TF-IDF scores.\nHere’s “boat”:\n\nfindAssocs(dtm_tfidf, terms = \"boat\", corlimit = .85)\n\n$boat\n   thumping       shore      bucket      cables         doo       geese \n       0.95        0.93        0.92        0.92        0.92        0.92 \n    pickled         sea      rudder     gunwale  scoundrels       boats \n       0.92        0.91        0.91        0.91        0.91        0.90 \n       keel      sailed        crew    baffling     biscuit    bowsprit \n       0.90        0.89        0.89        0.89        0.89        0.89 \n    hauling     muskets      ripped      splash      anchor         oar \n       0.89        0.89        0.89        0.89        0.88        0.88 \n   rattling       sandy        cook      patted     shipped       beach \n       0.88        0.88        0.88        0.88        0.88        0.87 \n    pistols      seamen     tobacco         lee    bulwarks      hauled \n       0.87        0.87        0.87        0.87        0.87        0.87 \n    inkling      musket  navigation        rags    steering      island \n       0.87        0.87        0.87        0.87        0.87        0.86 \n     bottle     tumbled       avast       belay       bilge   broadside \n       0.86        0.86        0.86        0.86        0.86        0.86 \n   cruising   cutlasses    diagonal   furtively     headway     jupiter \n       0.86        0.86        0.86        0.86        0.86        0.86 \n   mainland      marlin      midday     monthly   mutineers outnumbered \n       0.86        0.86        0.86        0.86        0.86        0.86 \n    plumped     riggers    schooner   schooners   seaworthy    swamping \n       0.86        0.86        0.86        0.86        0.86        0.86 \n     tide's      tiller     tonnage       towed       yawed        sail \n       0.86        0.86        0.86        0.86        0.86        0.85 \n       ship         tap     loading       sails         aft      berths \n       0.85        0.85        0.85        0.85        0.85        0.85 \n     pinned \n       0.85 \n\n\nHere’s “writing”:\n\nfindAssocs(dtm_tfidf, terms = \"writing\", corlimit = .85)\n\n$writing\n    hindrance      messages      disposal     inquiries      bedrooms \n         0.92          0.91          0.90          0.90          0.89 \n     ladyship          copy      lodgings        london    unforeseen \n         0.88          0.87          0.87          0.87          0.87 \n     drawings    plantation  explanations   certificate         dears \n         0.86          0.86          0.86          0.86          0.86 \nneighbourhood    allowances \n         0.85          0.85 \n\n\nThe semantics of these results have changed. For “boats”, we get much more terms related to seafaring. Most probably this is because only a few novels talk about boats so these terms correlate highly with one another. For “writing”, we’ve interestingly lost a lot of the words associated with writing in a strict sense (“copy”, “message”) but we’ve gained instead a list of terms that seem to situate us in where writing takes place in these novels, or what characters write about. So far though this is speculation; we’d have to look into this further to see whether the hypothesis holds.\nFinally, we can disaggregate our giant term count graph from above to focus more closely on the uniqueness of individual novels in our corpus. First, we’ll make a data frame from our TF-IDF DTM. We’ll transpose the DTM so the documents are our variables (columns) and the corpus vocabulary terms are our observations (or rows). Don’t forget the t!\n\ntfidf_df &lt;- as.matrix(dtm_tfidf)\ntfidf_df &lt;- as.data.frame(t(tfidf_df))\ncolnames(tfidf_df) &lt;- manifest$title\n\n\n\n36.7.3 Unique Terms in a Document\nWith this data frame made, we can order our rows by the highest value for a given column. In other words, we can find out not only the top terms for a novel, but the top most unique terms in that novel.\nHere’s Dracula:\n\nordering &lt;- order(tfidf_df$Dracula, decreasing = TRUE)\nrownames(tfidf_df[ordering[1:50], ])\n\n [1] \"helsing\"      \"mina\"         \"lucy\"         \"jonathan\"     \"van\"         \n [6] \"harker\"       \"godalming\"    \"quincey\"      \"seward\"       \"professor\"   \n[11] \"morris\"       \"lucy's\"       \"harker's\"     \"diary\"        \"seward's\"    \n[16] \"arthur\"       \"renfield\"     \"westenra\"     \"whilst\"       \"undead\"      \n[21] \"tonight\"      \"whitby\"       \"dracula\"      \"varna\"        \"carfax\"      \n[26] \"journal\"      \"helsing's\"    \"count\"        \"count's\"      \"hawkins\"     \n[31] \"madam\"        \"galatz\"       \"jonathan's\"   \"mina's\"       \"pier\"        \n[36] \"wolves\"       \"tomorrow\"     \"czarina\"      \"telegram\"     \"boxes\"       \n[41] \"today\"        \"holmwood\"     \"hypnotic\"     \"garlic\"       \"vampire\"     \n[46] \"phonograph\"   \"transylvania\" \"cliff\"        \"piccadilly\"   \"slovaks\"     \n\n\nNote here that some contractions have slipped through. Lemmatizing would take care of this, though we could also go back to the corpus object and add in another step with tm_map and then make another DTM:\n\ncleaned_corpus &lt;- tm_map(\n  cleaned_corpus, str_remove_all, pattern = \"\\\\'s\", replacement = \" \"\n)\n\nWe won’t bother to do this whole process now, but it’s a good example of how iterative the preprocessing workflow is.\nHere’s Frankenstein:\n\nordering &lt;- order(tfidf_df$Frankenstein, decreasing = TRUE)\nrownames(tfidf_df[ordering[1:50], ])\n\n [1] \"clerval\"      \"justine\"      \"elizabeth\"    \"felix\"        \"geneva\"      \n [6] \"frankenstein\" \"safie\"        \"cottagers\"    \"dæmon\"        \"ingolstadt\"  \n[11] \"kirwin\"       \"agatha\"       \"victor\"       \"ernest\"       \"mont\"        \n[16] \"krempe\"       \"lacey\"        \"waldman\"      \"agrippa\"      \"walton\"      \n[21] \"mountains\"    \"creator\"      \"cottage\"      \"sledge\"       \"hovel\"       \n[26] \"switzerland\"  \"ice\"          \"beaufort\"     \"cornelius\"    \"william\"     \n[31] \"protectors\"   \"moritz\"       \"henry\"        \"labours\"      \"chamounix\"   \n[36] \"glacier\"      \"jura\"         \"blanc\"        \"endeavoured\"  \"lake\"        \n[41] \"leghorn\"      \"monster\"      \"rhine\"        \"magistrate\"   \"belrive\"     \n[46] \"lavenza\"      \"salêve\"       \"saville\"      \"strasburgh\"   \"werter\"      \n\n\nAnd here’s Sense and Sensibility:\n\nordering &lt;- order(tfidf_df$SenseandSensibility, decreasing = TRUE)\nrownames(tfidf_df[ordering[1:50], ])\n\n [1] \"elinor\"       \"marianne\"     \"dashwood\"     \"jennings\"     \"willoughby\"  \n [6] \"lucy\"         \"brandon\"      \"barton\"       \"ferrars\"      \"colonel\"     \n[11] \"mrs\"          \"marianne's\"   \"edward\"       \"middleton\"    \"elinor's\"    \n[16] \"norland\"      \"palmer\"       \"steele\"       \"dashwoods\"    \"jennings's\"  \n[21] \"willoughby's\" \"edward's\"     \"delaford\"     \"steeles\"      \"cleveland\"   \n[26] \"mama\"         \"dashwood's\"   \"lucy's\"       \"brandon's\"    \"fanny\"       \n[31] \"allenham\"     \"middletons\"   \"devonshire\"   \"combe\"        \"ferrars's\"   \n[36] \"sister\"       \"morton\"       \"miss\"         \"margaret\"     \"park\"        \n[41] \"charlotte\"    \"exeter\"       \"magna\"        \"berkeley\"     \"harley\"      \n[46] \"john\"         \"middleton's\"  \"parsonage\"    \"beaux\"        \"behaviour\"   \n\n\nNames still rank high, but we can see in these results other words that indeed seem to be particular to each novel. With this data, we now have a sense of what makes each document unique in its relationship with all other documents in a corpus.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "chapters/16_geospatial-data.html",
    "href": "chapters/16_geospatial-data.html",
    "title": "37  Geospatial Data",
    "section": "",
    "text": "37.1 What is Geospatial Data?\nThis lecture is designed to introduce you to the basics of geospatial data.\nGeospatial data (also known as spatial data, GIS data, and other names) is information that can be attributed to a real-world location or can relate to each other in space.\nTechnically, “geospatial” refers to locations on Earth, while “spatial” can be locations anywhere, including other planets or even ficticious places (like J.R.R. Tolkien’s hand-drawn maps for his novels), but quite often the terms are used interchangably.\nYou use geospatial data every day on your smart phone through spatially-enabled apps like Google Maps, food delivery apps, fitness trackers, weather, or games like Pokemon Go.\n\\[\\begin{align*}\n\\textrm{(geo)Spatial Data} &= \\textrm{Attributes} + \\textrm{Locations} \\\\\n\n\\textrm{Location} &= \\textrm{Coordinate Reference System (CRS)} +\n  \\textrm{Coordinates}\n\\end{align*}\\]\nSo…\n\\[\n\\textrm{(geo)Spatial Data} = \\textrm{Attributes} +\n  \\textrm{Coordinate Reference System (CRS)} +\n  \\textrm{Coordinates}\n\\]",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "chapters/16_geospatial-data.html#what-is-geospatial-data",
    "href": "chapters/16_geospatial-data.html#what-is-geospatial-data",
    "title": "37  Geospatial Data",
    "section": "",
    "text": "37.1.1 Attributes\nAttributes are pieces of information about a location. For example, if I’m mapping gas stations, my attributes might be something like the price of gas, the address of the station, and the company that runs it (Shell, Arco, etc.). This isn’t the same thing as metadata, which is information about the entire data set such as who made the data, when they made it, and how the data was created.\n\n\n37.1.2 Coordinate Reference System\nThe earth is generally round. Maps are generally flat, with a few exceptions. If you were to try to flatten out the earth, you would create some fairly major distortions. Next time you eat an orange or a tangerine, try taking off the peel and then try to create a flat solid sheet of peel from it. You’ll end up needing to cut it or smash it to get a flat surface. The same thing happens to geospatial data when we try to translate it from a round globe to a flat map. But there are ways to minimize distortions.\nA coordinate reference system (sometimes called a projection) is a set of mathematical formulas that translate measurements on a round globe to a flat piece of paper. The coordinate reference system also specifies the linear units of measure (i.e. feet, meters, decimal degrees, or something else) and a set of reference lines.\nFor our purposes, we can think of coordinate reference systems coming in two flavors. One is geographic coordinate systems. For simplicity’s sake, we can think of these as coordinate reference systems that apply to latitude and longitude coordinates. Projected coordinate systems translate latitude and longitude coordinates into linear units from a specified baseline and aim to reduce some aspect of the distortion introduced in the round to flat translation.\n\n\n\n\n\n\nNote\n\n\n\nI am very much simplifying these concepts so we can learn the basics without getting overwhelmed.\n\n\nTo work with more than one digital spatial data set, the coordinate reference systems must match. If they don’t match, you can transform your data into a different coordinate reference system.\n\n\n37.1.3 Coordinates\nCoordinates are given in the distance (in the linear units specified in the CRS) from the baselines (specified in the CRS). Coordinates can be plotted just like coordinates on a graph (Cartesian coordinate system). Sometimes we refer to these as \\(x\\) and \\(y\\), just like a graph, but sometimes you’ll hear people refer to the cardinal directions (north, south, east, and west).\nLet’s take a moment to talk about latitude and longitude. You’re probably at least a little familiar with latitude (\\(y\\)) and longitude (\\(x\\)), but this is a special case that’s more complex than we probably initially realize. Latitude and longitude are angular measurements (with units in degrees) from a set of baselines—usually the Equator and the Greenwich Meridian. We can plot latitude and longitude on a Cartesian coordinate system, but this introduces major distortions increasing as you approach the poles. You never want to use straight latitude/longitude coordinates (commonly in North America, you’ll see data in the geographic coordinate reference system called WGS84) for an analysis. Always translate them into a projected coordinate system first. In addition, because the units are degrees, they are rather hard for us to interpret when we make measurements. How many degrees is it from the UC Davis campus to your apartment? It’s probably a very small fraction of a degree. Area measurements make even less sense. (What is a square degree and what does that look like?) Latitude/longitude coordinates are a great starting place, we just need to handle them correctly.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "chapters/16_geospatial-data.html#geospatial-data-models",
    "href": "chapters/16_geospatial-data.html#geospatial-data-models",
    "title": "37  Geospatial Data",
    "section": "37.2 Geospatial Data Models",
    "text": "37.2 Geospatial Data Models\nNow we have an idea of what makes data spatial, but what does spatial data look like in a computer? There are two common data models for geospatial data: Vector and Raster.\n\n\n\n\n\n\n\n\nData Model\nGeometry\nExample\n\n\n\n\nVector\nPoints\nVery small things, like cities at world scale\n\n\n.\nLines\nLinear things, like roads at city scale\n\n\n.\nPolygons\nLarger things that take up space, like parks at a city scale\n\n\nRaster\nGrid\nDigital Photo\n\n\n\n\n\n\n\n\n\nFigure 37.1: A visual table of raster versus vector data as continuous and discrete data.\n\n\n\n\n37.2.1 Vector Data\nVector data represents discrete objects in the real world with points, lines, and polygons in the data set.\nIf you were to draw a map to your house for a friend, you would typically use vector data—roads would be lines, a shopping center included as an important landmark might be a rectangle of sorts, and your house might be a point (perhaps represented by a star or a house icon).\nFor this lecture, we will focus on point data.\n\n\n37.2.2 Raster Data\nRaster data represents continuous fields or discrete objects on a grid, storing measurements or category codes in each cell of the grid.\nDigital photos are raster data you are already familiar with. If you zoom in far enough on a digital photo, you’ll see that photo is made up of pixels, which appear as colored squares. Pixels are cells in a regular grid and each contains the digital code that corresponds to the color that should be displayed there. Satellite images (like you see in Google Maps) are a very similar situation.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "chapters/16_geospatial-data.html#data-structures-applied-to-geospatial-data",
    "href": "chapters/16_geospatial-data.html#data-structures-applied-to-geospatial-data",
    "title": "37  Geospatial Data",
    "section": "37.3 Data Structures Applied to Geospatial Data",
    "text": "37.3 Data Structures Applied to Geospatial Data\nIn Chapter 22, you learned that data can be structured in a number of ways, such as tabular, tree (XML and JSON), relational, and non-hierarchical structures. All of these structures can include spatial information.\n\n\n\n\n\n\n\n\nData Structure\nExample File Type\nHow It’s Implemented\n\n\n\n\nTabular\nCSV\nOne or more columns hold spatial data (like latitude & longitude)\n\n\nTree\ngeoJSON\nTags in the structure indicate spatial information like geometry type and vertex locations\n\n\nRelational Database\nPostGIS or Spatialite\nOne column holds the “geometry” information (vertexes & CRS)\n\n\nNon-Hierarchical Relational Data\nSpatial Graph Databases\nNodes have locations associated with them, edges represent flow (think: transportation networks or stream networks)\n\n\n\nFor visualization purposes, geospatial software typically show all of these data structures as a map where each entity is linked with a table of the attribute data—one row of data in the table relates to one entity on the map. So regardless of the underlying data structure, you can think of these as interactive maps like you find on Google Maps.",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "chapters/16_geospatial-data.html#cleaning-geospatial-data",
    "href": "chapters/16_geospatial-data.html#cleaning-geospatial-data",
    "title": "37  Geospatial Data",
    "section": "37.4 Cleaning Geospatial Data",
    "text": "37.4 Cleaning Geospatial Data\nWhat can go wrong?\n\nLocation data isn’t usable\nLocation data is incorrect\nAttribute data is incorrect\nCoordinate Reference System (CRS) is improperly defined\n\n\n37.4.1 Example Data\nThe data set we’ll be working with as an example contains locations and attributes about lake monsters. Lake monsters are fictional creatures like sea monsters, but they live in lakes and not the ocean. The most famous lake monster is probably Nessie, who lives in Loch Ness. The data set we’re working with today is the early stages of a now much cleaner data set. This data came from a Wikipedia page and the locations were geocoded (a process that matches text locations with real-world locations). We’ll walk through some common processes and challenges with point data stored in a CSV file.\n\n\n37.4.2 Making Location Data Usable\nSomeone sends you a CSV file. At first glance, nothing looks amiss. There is a column for latitude and another for longitude, but how is it formatted? It’s degrees-minutes-seconds (DMS)! DMS looks like this:\n34° 36' 31.774\"\n(That’s 34 degrees, 36 minutes, 31.447 seconds). Sometimes people put in the symbols for degrees (°), minutes ('), and seconds (\"), and sometimes not. The computer can’t read this format, especially the symbols. It has to be converted to decimal degrees (DD), which looks like this:\n34.60882611\nTo convert it, we need to know that there are 60 minutes in a degree and 60 seconds in a minute.\n\\[\n\\textrm{Decimal Degrees} = \\textrm{Degrees} +\n  (\\textrm{Minutes} / 60) + (\\textrm{Seconds} / 3600)\n\\]\n\\[\n34.60882611 = 34 + (36 / 60) + (31.447 / 3600)\n\\]\nFirst, we need to load the libraries we’ll need and then load the data:\n\n# Load libraries\nlibrary(\"sf\")\n\nLinking to GEOS 3.13.0, GDAL 3.10.0, PROJ 9.5.0; sf_use_s2() is TRUE\n\nlibrary(\"mapview\")\nlibrary(\"gdtools\")  # Makes the display (dependency of mapview)\nlibrary(\"leafem\")  # Makes the labels work (dependency of mapview)\nlibrary(\"leaflet\")\n\n# Read data\nmonsters.raw &lt;- read.csv(\n  \"data/lake_monsters.csv\", stringsAsFactors = FALSE, encoding = \"utf-8\"\n)\n\n# Explore the data\nhead(monsters.raw)\n\n  fid field_1               Lake        Area        Country     Continent\n1   1       1      Arenal Lagoon    Alajuela     Costa Rica North America\n2   2       2    Bangweulu Swamp                     Zambia        Africa\n3   3       3 Bassenthwaite Lake     England United Kingdom        Europe\n4   4       4          Bear Lake Idaho, Utah            USA North America\n5   5       5        Brosno Lake Tver Oblast         Russia        Europe\n6   6       6   Bueng Khong Long   Bueng Kan       Thailand          Asia\n                         Name       lat         lon        lat_dms\n1                     unnamed  10.49143  -84.851696 10°29?29.1304?\n2                      Nsanga -11.14741   29.784582 -11°8?50.6760?\n3                       Eachy  54.65279   -3.213612 54°39?10.0359?\n4 Bear Lake Monster, Isabella  42.21721 -111.319881  42°13?1.9643?\n5               Brosno Dragon  56.82407   31.914652 56°49?26.6520?\n6                  Phaya Naga  18.02363  104.014360  18°1?25.0676?\n           lon_dms                                 coords_3395    lon_3395\n1   -84°51?6.1069?  Point (-9445647.63285386 1166706.48735204)  -9445647.6\n2    29°47?4.4935?  Point (3315604.44829715 -1240572.14607131)   3315604.4\n3   -3°12?49.0016?   Point (-357737.60213262 7259890.14217639)   -357737.6\n4 -111°19?11.5709? Point (-12392072.44582391 5164853.16566773) -12392072.4\n5   31°54?52.7472?   Point (3552722.80948453 7688451.06249625)   3552722.8\n6   104°0?51.6974?  Point (11578825.63491604 2027100.69217741)  11578825.6\n  lat_3395\n1  1166706\n2 -1240572\n3  7259890\n4  5164853\n5  7688451\n6  2027101\n\n\nNext, we need to write some functions to deal with our specific DMS data and how its formatted:\n\n# This function splits up the DMS column into three columns - D, M, & S\nsplit.dms &lt;- function(dms.column) {\n  # Separate the pieces of the DMS column\n\n  # Make a matrix of characters\n  variable &lt;- do.call(rbind, args = c(strsplit(dms.column, '[°?]+')))\n\n  # Set the data type to numeric instead of character\n  mode(variable) &lt;- \"numeric\"\n\n  dms.split &lt;- as.data.frame(variable)\n\n  split.string &lt;- strsplit(dms.column, '[°?]+')\n\n  # Name the columns\n  names(dms.split) &lt;- c(\"D\", \"M\", \"S\")\n\n  dms.split\n}\n\n\n# This function coverts a 3 column data frame of DMS to DD, like the data\n# created by split.dms\ndecimaldegrees &lt;- function(dms.df) {\n  dd &lt;- data.frame()\n\n  for (i in 1:dim(dms.df)[1]) {\n    if (dms.df[i, 1] &gt; 0){\n      # Decimal Degrees = Degrees + (Minutes / 60) + (Seconds / 3600)\n      dd.row &lt;- dms.df[i, 1] + (dms.df[i, 2] / 60) + (dms.df[i, 3] / 3600)\n      dd &lt;- rbind(dd, dd.row)\n\n    } else {\n      # -Decimal Degrees = Degrees - (Minutes / 60) - (Seconds / 3600)\n      dd.row &lt;- dms.df[i, 1] - (dms.df[i,2] / 60) - (dms.df[i,3] / 3600)\n      dd &lt;- rbind(dd, dd.row)\n    }\n  }\n  dd\n}\n\nFinally, we can process our DMS data to convert it to Decimal Degreess (DD):\n\n# Process latitude\ndms.split &lt;- split.dms(monsters.raw$lat_dms)\ndd &lt;- decimaldegrees(dms.split)\nmonsters.df &lt;- cbind(monsters.raw, dd)\nnames(monsters.df)[15] &lt;- \"lat_dd\"\n\n# Process longitude\ndms.split &lt;- split.dms(monsters.raw$lon_dms)\ndd &lt;- decimaldegrees(dms.split)\nmonsters.df &lt;- cbind(monsters.df, dd)\nnames(monsters.df)[16] &lt;- \"lon_dd\"\n\n# Look at the data\nhead(monsters.df)\n\n  fid field_1               Lake        Area        Country     Continent\n1   1       1      Arenal Lagoon    Alajuela     Costa Rica North America\n2   2       2    Bangweulu Swamp                     Zambia        Africa\n3   3       3 Bassenthwaite Lake     England United Kingdom        Europe\n4   4       4          Bear Lake Idaho, Utah            USA North America\n5   5       5        Brosno Lake Tver Oblast         Russia        Europe\n6   6       6   Bueng Khong Long   Bueng Kan       Thailand          Asia\n                         Name       lat         lon        lat_dms\n1                     unnamed  10.49143  -84.851696 10°29?29.1304?\n2                      Nsanga -11.14741   29.784582 -11°8?50.6760?\n3                       Eachy  54.65279   -3.213612 54°39?10.0359?\n4 Bear Lake Monster, Isabella  42.21721 -111.319881  42°13?1.9643?\n5               Brosno Dragon  56.82407   31.914652 56°49?26.6520?\n6                  Phaya Naga  18.02363  104.014360  18°1?25.0676?\n           lon_dms                                 coords_3395    lon_3395\n1   -84°51?6.1069?  Point (-9445647.63285386 1166706.48735204)  -9445647.6\n2    29°47?4.4935?  Point (3315604.44829715 -1240572.14607131)   3315604.4\n3   -3°12?49.0016?   Point (-357737.60213262 7259890.14217639)   -357737.6\n4 -111°19?11.5709? Point (-12392072.44582391 5164853.16566773) -12392072.4\n5   31°54?52.7472?   Point (3552722.80948453 7688451.06249625)   3552722.8\n6   104°0?51.6974?  Point (11578825.63491604 2027100.69217741)  11578825.6\n  lat_3395    lat_dd      lon_dd\n1  1166706  10.49143  -84.851696\n2 -1240572 -11.14741   29.784582\n3  7259890  54.65279   -3.213612\n4  5164853  42.21721 -111.319881\n5  7688451  56.82407   31.914652\n6  2027101  18.02363  104.014360\n\n\nAnother common issue with point data is that the latitude and longitude are not in any form of degrees, but instead are in a projected coordinate system with linear units (usually feet or meters). If the data doesn’t come with metadata, you may be left guessing which coordinate system it is in. With experience, you’ll get better at guessing, but sometimes the data is not usable. Our monsters data set has latitude and longitude in the World Mercator (EPSG: 3395) projection as well. Let’s briefly look at that here, but we’ll play with that more later in this document.\n\nmonsters.df[1:10, 13:16]\n\n      lon_3395 lat_3395    lat_dd      lon_dd\n1   -9445647.6  1166706  10.49143  -84.851696\n2    3315604.4 -1240572 -11.14741   29.784582\n3    -357737.6  7259890  54.65279   -3.213612\n4  -12392072.4  5164853  42.21721 -111.319881\n5    3552722.8  7688451  56.82407   31.914652\n6   11578825.6  2027101  18.02363  104.014360\n7   -7845463.9  5433619  43.98618  -70.477001\n8  -12704546.6  6054836  47.87777 -114.126884\n9   -9467608.2  5128572  41.97447  -85.048971\n10 -12525669.2  5011829  41.18707 -112.520000\n\n\nNote that data preparation and cleaning is the vast majority of the work for all data, not just spatial data. All of the code we just looked at was just to get the data in a usable format. We’ll convert it to a spatial data type and map it in the next section.\n\n\n37.4.3 Cleaning Location Data\nSometimes, the locations in your data set are incorrect. This can happen for a number of reasons.\nFor example, it’s fairly common for data to get truncated or rounded if you open a CSV in Excel. Removing decimal places from coordinate data loses precision.\nPeople often swap their latitude and longitude columns as well, which make data show up in the wrong Cartesian coordinate, for example, \\((-119, 34)\\) is a verrry different location than \\((34, -119)\\). \\(-119\\) is actually out of the range of latitude data and will often break your code.\nAnother common source of error is in the way the data was made. If data is produced by geocoding, turning an address or place name into a coordinate, the location may have been matched badly. If the data was made by an analysis process, an unexpected aspect of the data could cause problems, like a one-to-many join when you thought you had a one-to-one join in a database.\nRegardless of how the errors came about, how do we find incorrect locations? Start by mapping the data and see where it lands. Is it where you expect the data to be? Sometimes you can’t tell it’s wrong because the data looks normal.\n\n# Convert the monsters data frame into an sf (spatial) object\n#   Note: x is the data frame, not longitude.\n#   Coordinate Reference System (CRS): we're using lat/long here so we need\n#     WGS84 which is EPSG code 4326 - we just need to tell R what the CRS is,\n#     we don't change it this way. If we want to change it, we need to use\n#     `st_transform`.\n\nmonsters.sf &lt;- st_as_sf(\n  x = monsters.df, coords = c(\"lon_dd\", \"lat_dd\"), crs = 4326\n)\n\n# Notice we added a geometry column!\nnames(monsters.sf)\n\n [1] \"fid\"         \"field_1\"     \"Lake\"        \"Area\"        \"Country\"    \n [6] \"Continent\"   \"Name\"        \"lat\"         \"lon\"         \"lat_dms\"    \n[11] \"lon_dms\"     \"coords_3395\" \"lon_3395\"    \"lat_3395\"    \"geometry\"   \n\n\n\n# Plot a map\nmapview(monsters.sf)\n\n\n\n\n\n\n\nFigure 37.2: This is a screen capture of the output for the mapview function. Running this code in a regular R session (that is, not in Quarto like we do to create this reader) will make an interactive map.\n\n\n\nIn the interactive version of this map, you can pan and zoom to different areas to see more detail. Clicking on a point will open a pop-up with attribute information.\nFirst impressions: This map looks good! The points are all on land masses, none in the ocean. Let’s see if they are on the correct continent…\n\nmapview(monsters.sf, zcol = \"Continent\", legend = TRUE)\n\n\n\n\n\n\n\nFigure 37.3: Map of monster locations by continent.\n\n\n\nIt’s hard to see, but there’s a point in Michigan that’s the wrong color for North America!\n\n\n\n\n\n\nFigure 37.4: Map of monster locations by continent zoomed in to the Great Lakes.\n\n\n\nWhoops! Lakes of Killarney isn’t in Michigan! That point should be in Ireland! If we zoom in, we can see why the geocoder got confused. The lake names are very similar.\n\n\n37.4.4 Cleaning Attribute Data\nAttribute data can be proofed in much the same way tabular data can be proofed. You can look at the statistical properties of numeric data or the unique entities in a list of categorical variables to see if any values are odd or out of place.\nWith spatial data, we can also map the data and visualize it by attribute values to see if anything is out of place spatially. Labels are another helpful tool. Sometimes cleaning attributes uncovers issues with the locations.\nLet’s make sure the lake names match the lakes the points are in. We’ll make a map and if you zoom in enough, the lake names will appear in the background map data.\n\n# Makes a pop-up with attribute information\nmy.label.options &lt;- labelOptions(clickable = TRUE)\n\nmap.lakename &lt;- mapview(monsters.sf, zcol = \"Lake\", legend = FALSE)\nlabels.lakename &lt;- addStaticLabels(\n  map.lakename, label = monsters.sf$Lake, labelOption = my.label.options\n)\n\nlabels.lakename\n\n\n\n\n\n\n\nFigure 37.5: Map of monster locations by lake name zoomed in to the Great Lakes.\n\n\n\n\n\n\n\n\n\nFigure 37.6: Map of monster locations by lake name zoomed in to the Great Lakes.\n\n\n\nAnd for fun, let’s look at the monster names:\n\nmap.monstername &lt;- mapview(monsters.sf, zcol = \"Name\", legend = FALSE)\nlabels.monstername &lt;- addStaticLabels(\n  map.monstername, label = monsters.sf$Name, labelOption = my.label.options\n)\n\nlabels.monstername\n\n\n\n\n\n\n\nFigure 37.7: Map of monster locations by monster name zoomed in to the Great Lakes.\n\n\n\n\n\n\n\n\n\nFigure 37.8: Map of monster locations by monster name zoomed in to the Great Lakes.\n\n\n\nYikes! That needs some clean-up too! The name column is missing some names and some records have extra information in them.\n\n\n37.4.5 Checking Coordinate Reference Systems\n\nWhy is my California data showing up in Arizona?\n\nThis is a common question UC Davis researchers ask on the Geospatial email list. Why does this happen? It’s usually because the CRS for their data is improperly defined. Someone changed the definition but didn’t reproject the data (the mathematical process of switching CRSs). Using the wrong CRS will often shift data just enough to look really funny on a map, but sometimes it won’t show up at all.\n\nWhy don’t my data sets line up in my map?\n\nAgain, it’s your CRS. In this case, they could be correct for all of the data sets you’re using, but each data set has a different CRS. You can think of CRSs as different dimensions in your favorite sci-fi story. Sometimes you can see the other person in the other dimension (CRS), but usually they are too different and you’re nowhere near each other. Data sets have to have the same CRS to make a map or do any analysis.\nOur data came with lat/long data in another coordinate reference system—EPSG 3395 “World Mercator”, a world projection centered on Europe. Notice how the coordinates look very different from the lat/long coordinates in EPSG 4326 “WGS 84”:\n\nmonsters.df[1:10,13:16]\n\n      lon_3395 lat_3395    lat_dd      lon_dd\n1   -9445647.6  1166706  10.49143  -84.851696\n2    3315604.4 -1240572 -11.14741   29.784582\n3    -357737.6  7259890  54.65279   -3.213612\n4  -12392072.4  5164853  42.21721 -111.319881\n5    3552722.8  7688451  56.82407   31.914652\n6   11578825.6  2027101  18.02363  104.014360\n7   -7845463.9  5433619  43.98618  -70.477001\n8  -12704546.6  6054836  47.87777 -114.126884\n9   -9467608.2  5128572  41.97447  -85.048971\n10 -12525669.2  5011829  41.18707 -112.520000\n\n# Let's make our World Mercator data spatial so we can explore its CRS\nmonsters.sf.3395 &lt;- st_as_sf(\n  x = monsters.df, coords = c(\"lon_3395\", \"lat_3395\"), crs = 3395\n)\n\n# `st_crs` tells us what the CRS is in well known text (WKT) and EPSG (if it's\n# available)\nst_crs(monsters.sf)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\nst_crs(monsters.sf.3395)\n\nCoordinate Reference System:\n  User input: EPSG:3395 \n  wkt:\nPROJCRS[\"WGS 84 / World Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"World Mercator\",\n        METHOD[\"Mercator (variant A)\",\n            ID[\"EPSG\",9804]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Very small scale conformal mapping.\"],\n        AREA[\"World between 80°S and 84°N.\"],\n        BBOX[-80,-180,84,180]],\n    ID[\"EPSG\",3395]]\n\n# Check to see if they are identical, returning a logical vector\nidentical(st_crs(monsters.sf), st_crs(monsters.sf.3395))\n\n[1] FALSE",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "chapters/16_geospatial-data.html#conclusions",
    "href": "chapters/16_geospatial-data.html#conclusions",
    "title": "37  Geospatial Data",
    "section": "37.5 Conclusions",
    "text": "37.5 Conclusions\nWe’ve learned some of the basics of geospatial data. We learned that the main components of geospatial data are locations, attributes, and a coordinate reference system. We saw how geospatial data can be represented with different data models, but we focused on point vector data. We learned that the data structures we were already familiar with can be modified to contain spatial data. And finally, we looked at some common processes for cleaning our geospatial data.\nThis was a lot to cover, but we just scratched the surface of all your can do with geospatial data science! If you want to learn more, UC Davis has some fantastic introductory classes for GIS (Geographic Information Systems/Science) and Remote Sensing (working satelite data and air photos).",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "chapters/16_geospatial-data.html#optional-further-reading",
    "href": "chapters/16_geospatial-data.html#optional-further-reading",
    "title": "37  Geospatial Data",
    "section": "37.6 Optional Further Reading",
    "text": "37.6 Optional Further Reading\n\nBolstad, P. 2019. GIS Fundamentals: A first text on geographic information systems. Sixth Edition. XanEdu. Ann Arbor, MI. 764 pp.\nSutton, T., O. Dassau, & M. Sutton. 2021. A Gentle Introduction to GIS. https://docs.qgis.org/3.16/en/docs/gentle_gis_introduction/preamble.html (accessed on 2021-02-11)",
    "crumbs": [
      "Special Topics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  }
]