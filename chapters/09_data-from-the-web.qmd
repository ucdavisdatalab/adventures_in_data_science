---
format:
  html:
    df-print: kable
---

```{r, echo=FALSE, results=FALSE}
source("R/ch09.R")
```

# Getting Data from the Web

The internet in general and the World Wide Web specifically serves as a vast
source of data suitable for answering a wide range of research questions. In
this lesson we cover several common methods for acquiring this data.

::: {.callout-note title="Learning Goals" collapse="false"}
After this lesson, you should be able to:

* Understand the reasoning behind how URLs are formatted
* Explain and read hypertext markup language (HTML)
* View the HTML source of a web page
* Use Firefox or Chrome's web developer tools to locate tags within a web page
* Query a web API with the `httr` package
* Set fields in a URL to customize web API queries
* With the `rvest` package:
    + Read HTML into R
    + Extract HTML tables as data frames
* With the `xml2` package:
    + Use XPath to extract specific elements of a page
:::

##  How the Web Works

The discipline of Data Science was, in large part, ushered into being by the
increasing availability of information available on the World Wide Web or
through other internet sources. Prior to the popularization of the internet as
a publishing and communications platform, the majority of scientific research
involved controlled studies in which researchers would collect their own data
through various direct means (surveys, medical testing, etc.) in order to test
a stated hypothesis.

The vast amount of information available on the internet disrupted this
centuries-long dominance. Today, the dominant form of scientific research
involves using data collected or produced by others for reasons having little
or nothing to do with the research question being investigated by scholar.
Users who post items about their favorite political candidate are not, for
example, doing this so that sociologists can better under how politics function
in America. However, their Tweets are being used in that and many other
unforeseen capacities.

Because the internet provides such a rich trove of information for study,
understanding how to effectively get, process, and prepare information from the
internet for scientific research is a crucial skill for any data scientist. And
in order to understand these workflows, the data scientist must first
understand how the internet itself functions.

### Client-Server Architecture

The base architecture and functioning of the internet is quite simple:

::: {#fig-basic-internet}
![](/images/ch09/basic_internet.png)
:::

1. A content producer puts information on a computer called the *server* for
   others to retrieve;
2. A user uses their local computer, called the *client*, to request the
   information from the sever;
3. The server delivers the information to the client.

Each of the above detailed steps is accomplished using a technically complex
but conceptually simple set of computer protocols. The technical details are
beyond the scope of this course. We are here concerned with their conceptual
architecture.

#### Communication Between Clients and Servers

Anytime a computer connects to any network, that computer is assigned a unique
identifier known as an **internet protocol (IP) address** that uniquely
identifies that computer on the network. IP addresses have the form `x.x.x.x`,
where each `x` can be any integer from 0 to 255. For example, `169.237.102.141`
is the current IP address of the computer that hosts the DataLab website. IP
addresses are sometimes pre-designated for particular computers. A
pre-designated IP address is known as **static IP address**. In other cases IP
addresses are dynamically assigned from a range of available IP Address using a
system known as the **Dynamic Host Configuration Protocol** (DHCP). Servers are
typically assigned static IP addresses and clients are typically assigned
dynamic IP addresses.

::: {#fig-internet-with-ip}
![](/images/ch09/basic_internet_with_ip.png)
:::

As humans, we are used to accessing websites via a domain name (which we'll
discuss shortly), but you can also contact any server on the internet by simply
typing the IP address into your browser address bar where you would normally
enter the URL. For example, you can simply click on <https://169.237.102.144>
to access the DataLab website.

::: {.callout-note}
Your browser may give you a security warning if you try to access a server
directly using an IP address. For the link above, it is safe to proceed to the
site.
:::

#### Domain Name Resolution

IP addresses are the unique identifiers that make the internet work, but they
are not very human friendly. To solve this problem, a system of domain name
resolution was created. Under this system, internet service providers access a
universal domain registry database that associates human readable domain names
with machine readable IP addresses, and a secondary set of of internet
connected servers known as **domain name servers** (DNS) provide a lookup
service that translates domain names into IP addresses in the background. As
the end-user, you enter and see only domain names, but the actual request
process is a multi-step process in which domain names are translated to IP
address in the background:

::: {#fig-internet-with-dns}
![](/images/ch09/basic_internet_dns.png)
:::

1. A content producer puts information on a computer called the **server** for
   others to retrieve;
2. A user uses their local computer, called the **client**, to request the
   information from the sever using a domain name using request software such
   as a web browser;
3. The user's client software first sends a request to a DNS server to retrieve
   the IP address of the server on the network associated with the entered
   domain name;
4. The DNS server returns the associated IP address to the client;
5. The client then makes the information request to the server using its
   retrieved IP address;
6. The server delivers the information to the client.

#### Request Routing

Our simple diagram of the client server process shows only two computers. But
when you connect to the internet you are not, of course, creating a direct
connection to a single computer. Rather, you are connecting to vase network of
literally millions of computers, what we have come to refer to as the cloud.

In order to solve this problem, the internet backbone also deploys a routing
system that directs requests and responses across the network to the
appropriate servers and clients.

When you connect to the WiFi network in your home, office, or the local coffee
house, you are connecting to a router. That router receives all of your
requests and, provided you are not requesting something directly from another
computer that is connected to the same router, passes that request on to a
larger routing network at your **internet service provider** (ISP). When the
ISP routers receive your request, they check to see if you're requesting
something from a computer that is connected to their network. If it is, they
deliver the request. If it is not, they pass the request on to another,
regional routing network. And this routing process is repeated until your
request if finally routed to the correct server.

::: {#fig-internet-routing}
![](/images/ch09/basic_internet_routing.png)
:::

#### The Server Response

When a request is sent to a server across the internet, the request includes
both the specific URL of the resource being request and also an hidden
**request header**. The request header provides information to the server such
as the IP address and the operating system of the client, the transfer protocol
being used, and the software on the client that is making the request. The
server uses this information to properly format its response and to route it
back to the requesting client using the same IP routing process as described
above.

#### Internet Transfer Protocols

All of the information transferred between computers over the network is
transferred as streams of binary data. In order to ensure data integrity, these
streams are usually broken up into smaller **packets** of data which are
transmitted independent of each other and then reassembled by the receiving
computer once it has received all of the packets in the stream. The first
packet returned (a header packet) typically delivers information about how many
packets the client should expect to receive and about how they should be
reassembled to recreate the original data stream.

There are many different standards for how data streams are divided into
packets. One standard might, for example, break the stream into a collection of
50-byte packets, while another might use 100-byte packages. These standards are
called **protocols**. The two protocols that are familiar to most users are
**HTTP** and **HTTPS**, which define the hypertext transfer protocol and its
sibling the hypertext transfer secure protocol. When you type a URL like
<https://datalab.ucdavis.edu> into your browser, you are instructing the
browser to use the HTTPS protocol to exchange information. Because HTTP and
HTTPS are so common, most modern browsers do not require you to type the
protocol name. They will simply insert the protocol for you in the background.

### Understanding URLs

URL is an acronym for **uniform resource locator**. "Uniform" is a key term in
this context. URLs are not arbitrary pointers to information. They are machine
parsable, human readable, and can contain a lot of information.

All URLs are constructed using a standardized format. Consider the following
URL:

```
https://sfbaywildlife.info/species/common_birds.htm
```

There are actually several distinct components to the above URL:

```{r, echo=FALSE}
url_protocol <- c("https://")
url_domain <- c("sfbaywildlife.info")
url_path <- c("/species/common_birds.htm")
birds_columns <- c("protocol", "server", "path to file")
birds_table <- data.frame(url_protocol, url_domain, url_path)
colnames(birds_table) <- birds_columns
birds_table[,]
```

We've already discussed internet protocols and domain names. The file path
portion of the URL can also provide valuable information about the server. It
reads exactly like a Unix file path on the command line. The path
`/species/common_birds.htm` indicates that the file `common_birds.htm` is in
the `species` directory on the server.

#### Dynamic Files

In the above example, when you enter the URL
`https://sfbaywildlife.info/species/common_birds.htm`, your browser requests
the file at `/species/common_birds.html` on the server. The server simply finds
the file and delivers it to your web browser. We call this a **static web
server** because the server itself does not do any processing of files prior to
delivery. It simply receives requests for files living on the server and then
sends them to the client, whose browser renders the file for viewing.

Many websites, however, use dynamic processing. Pages with file extensions such
as `.php` or `.jsp`, for example, include computer code in them. When these
pages are requested by the server, the server executes the code in the
designated file and sends the output of that execution to the requesting client
rather than the actual file. Many sites, such as online stores and blogs, use
this functionality to connect their web pages to active databases that track
inventory and orders, for example.

#### Query Strings

Dynamic websites, such as e-commerce sites that are connected to databases,
require a mechanism for users to submit information to the server for
processing. This is accomplished through one of two HTTP requests: GET or POST.

**POST requests** send submitted information to the server via a hidden **HTTP
header** that is invisible to the end user. Scraping sites that require POST
transactions is possible but can require significant sleuthing to determine the
correct parameters and is beyond the scope of this course.

**GET requests**, which are, happily for web scrapers more ubiquitous than POST
requests, are much easier to understand. They are submitted via a **query
string** that is simply appended to the request URL as in the following
example:

```
https://ebba.english.ucsb.edu/search_combined/?ft=dragon&numkw=52
```

Here we see a query string appended to the end of the actual URL:

```{r, echo=FALSE}
url_protocol <- c("https://")
url_domain <- c("ebba.english.ucsb.edu")
url_path <- c("/search_combined/index.php")
url_query <- c("?ft=dragon&numkw=52")
birds_columns <- c("protocol", "server", "path to file", "query string")
birds_table <- data.frame(url_protocol, url_domain, url_path, url_query)
colnames(birds_table) <- birds_columns
birds_table[,]
```

Query strings always appear at the end of the URL and begin with the `?`
character followed by a series of key/value pairs separated by the `&`
character. In the above example we see that two parameters are submitted to the
server via the query string as follows:

* `ft=dragon`
* `numkw=52`

The server will use these parameter values as input to perform a dynamic
operation, in this case searching a database.

## Accessing Data Online

While there are many methods (and R packages) for acquiring data from the web,
all fall into one of three general categories of data acquisition:

* **Direct download** describes the case where a data provider has provided a
  specific URL or web link from which you can download the data. For example,
  when you download data from the "Files" section of Canvas, you are using the
  direct download method of data acquisition.
* Web **application programming interfaces** (web APIs) are web-accessible
  endpoints that you access via a URL, just as you would any website, but that
  are designed specifically to interact with computers (as opposed to humans).
  APIs receive requests and return data to the requester in machine, as opposed
  to human, readable formats, such as JSON or XML.  We will learn more about
  working with APIs in this unit.
* **Scraping** a web page means extracting information from human readable
  internet sources so that it can be used programmatically (for instance, in
  R).  We will also learn more about Scraping in this unit.

Each of the above general methods can be accomplished by applying any number of
sub-methods and packages. And each brings with it its own degree of complexity
and difficulty. As a general rule, the various ways you can get data from the
web can be ranked according to difficulty from most to least convenient as
follows:

1. Direct download or "data dump"
2. R or Python package (there are packages for many popular web APIs)
3. Documented web API
4. Undocumented web API
5. Scraping


## Web APIs

As noted earlier, a web application programming interface (API) provides a
machine readable gateway for accessing data from the web.  Most APIs provide
programmatic access to the data that lives behind a human readable website. For
example, most social media platforms such as Twitter, Facebook, and Instagram
provide web APIs that allow computers to programmatically access the same data
that you, as a human, see when you interact with these platforms via a web
browser of mobile app.  Some web APIs, however, are standalone, in that they
provide machine access to data sources that have no human readable interface.

One of the challenges with working with web APIs is that, while there are some
conventions for behavior, you need to know what and how to query a specific API
in order to interact with it. Some APIs are well documented, while others are
not. And some, for example the Twitter API, have extensive documentation that
is frequently erroneous, incomplete, or out of date.  As a result, working with
web APIs can sometimes be challenging.

For this unit, we will work with the [RESTCountries
API](https://restcountries.com/) which stores data on countries all over the
world: their currencies, flags, population, etc. There are a large number of
public web APIs out there, such as those listed by the [public-api
project](https://github.com/public-apis/public-apis).

### Querying a Web API

The R community has developed packages to facilitate interaction with many
popular web APIs (Twitter, Facebook, etc.). Because web APIs are
web-accessible, however, all can be accessed programmatically using basic
internet request protocols, just as if you were going to a human readable
webpage, provided you know how to formulate your request as a URL. The
RESTcountries webpage publishes guidelines for accessing its API.

We can see from the documentation that the API will allow us to query a list of
country features that appear in the database using a URL with the following
construction:

```
"https://restcountries.com/v3.1/all?fields=name"
```

You can actually go to this URL in your web browser and see the response, a
portion of which is reproduced here:

```json
[{"name":{"common":"American Samoa","official":"American Samoa","nativeName":{"eng":{"official":"American Samoa","common":"American Samoa"},"smo":{"official":"Sāmoa Amelika","common":"Sāmoa Amelika"}}}},{"name":{"common":"Peru","official":"Republic of Peru","nativeName":{"aym":{"official":"Piruw Suyu","common":"Piruw"},"que":{"official":"Piruw Ripuwlika","common":"Piruw"},"spa":{"official":"República del Perú","common":"Perú"}}}},{"name":{"common":"Tonga","official":"Kingdom of Tonga","nativeName":{"eng":{"official":"Kingdom of Tonga","common":"Tonga"},"ton":{"official":"Kingdom of Tonga","common":"Tonga"}}}}
```

If you look closely at this extract form the response, you will see that it
contains HTML but is not HTML. Remember that APIs exist to deliver machine
readable information. In this case, the API is delivering data in the JSON
format, and some of the fields in the JSON object contain information provided
as HTML.

Because the response is machine readable, we can make better use of the query
if we run it in R rather than our web browser. Before we can do so, we need to
setup our R environment to execute HTTP queries against the API and to process
JSON. We'll use the `httr` package to process our http transactions and the
`jsonlite` package to process the JSON that we receive

```r
install.packages("httr")
install.packages("jsonlite")
```

With our packages installed, we execute our query in R with one simple command:

```{r}
library(httr)
library(jsonlite)

response <- GET("https://restcountries.com/v3.1/all?fields=name")
```

The above executes an HTTP GET request (just like your web browser) to the
identified query URL and loads it into an httr "response" object.  If you take
some time to examine the response object, you will see that it is a container
object that contains a lot of useful information in addition to the actual JSON
response that you see when you load the URL in your browser.  For example, we
can check the status of the response to see if it was successful by look at the
response "status_code" which should be 200 if the query executed successfully.

```{r}
message(response$status_code)
```

The actual content of the response can be found in the response object's
"content" element. Note, however, that we have to do some formatting on the
object in order to access it. Go ahead and look at the actual
`response$content` object:

```{r, attr.output='style="max-height: 200px;"'}
response$content
```

Above we see that the content that is received from the API is delivered as a
compressed binary string, which must be decompressed and converted back to
character encoding before processing. We do this using a built-in `httr`
function:

```{r}
bdy <- content(response, "text")
```

Once we have the content extracted and converted, we can begin to process it.
We previously examined the response and determined that it is in JSON format,
so our step will be to load the content into a JSON object for ease of
traversal:

```{r}
bdy_json <- fromJSON(bdy)
```

If you examine the class and structure of the `bdy_json` object, you will see
that `jsonlite` has converted the JSON structure into a nice R data frame where
you can begin the process of exploration and cleaning in preparation for
research.


## Web Scraping

### What's in a Web Page?

Modern web pages usually consist of many files:

* Hypertext markup language (HTML) for structure and formatting
* Cascading style sheets (CSS) for more formatting
* JavaScript (JS) for interactivity
* Images

HTML is the only component that always has to be there. Since HTML is what
gives a web page structure, it's what we'll focus on when scraping.

HTML is closely related to eXtensible markup language (XML). Both languages use
**tags** to mark structural **elements** of data. In HTML, the elements
literally correspond to the elements of a web page: paragraphs, links, tables,
and so on.

Most tags come in pairs. The **opening tag** marks the beginning of an element
and the **closing tag** marks the end. Opening tags are written `<NAME>`, where
`NAME` is the name of the tag. Closing tags are written `</NAME>`.

A **singleton tag** is a tag that stands alone, rather than being part of a
pair. Singleton tags are written `<NAME />`. In HTML (but not XML) they can
also be written `<NAME>`. Fortunately, HTML only has a few singleton tags, so
they can be distinguished by name regardless of which way they're written.

For example, here's some HTML that uses the `em` (emphasis, usually italic) and
`strong` (usually bold) tags, as well as the singleton `br` (line break) tag:

```html
<em><strong>This text</strong> is emphasized.<br /></em> Not emphasized
```

A pair of tags can contain other elements (paired or singleton tags), but not a
lone opening or closing tag. This creates a strict, treelike hierarchy.

Opening and singleton tags can have **attributes** that contain additional
information. Attributes are name-value pairs written `NAME="VALUE"` after the
tag name.

For instance, the HTML `a` (anchor) tag creates a link to the URL provided for
the `href` attribute:

```html
<a href="http://www.google.com/" id="mytag">My Search Engine</a>
```

In this case the tag also has a value set for the `id` attribute.

Now let's look at an example of HTML for a complete, albeit simple, web page:

```html
<html>
  <head>
    <title>This is the page title!</title>
  </head>
  <body>
    <h1>This is a header!</h1>
    <p>This is a paragraph.
      <a href="http://www.r-project.org/">Here's a website!</a>
    </p>
    <p id="hello">This is another paragraph.</p>
  </body>
</html>
```

In most web browsers, you can examine the HTML for a web page by right-clicking
and choosing "View Page Source".

See [here][html_basics] for a more detailed explanation of HTML, and
[here][html_ref] for a list of valid HTML elements.

[html_basics]: https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics
[html_ref]: https://developer.mozilla.org/en-US/docs/Web/HTML/Element


### R's XML Parsers

A **parser** converts structured data into familiar data structures. R has two
popular packages for parsing XML (and HTML):

* The XML package
* The [xml2 package][xml2]

The XML package has more features. The xml2 package is more user-friendly, and
as part of the Tidyverse, it's relatively well-documented. This lesson focuses
on xml2, since most of the additional features in the XML package are related
to writing (rather than parsing) XML documents.

The xml2 package is often used in conjunction with the [rvest package][rvest],
which provides support for CSS selectors (described later in this lesson) and
automates scraping HTML tables.

[xml2]: https://xml2.r-lib.org/
[rvest]: https://rvest.tidyverse.org/

The first time you use these packages, you'll have to install them:

```r
install.packages("xml2")
install.packages("rvest")
```

Let's start by parsing the example of a complete web page from earlier. The
xml2 function `read_xml` reads an XML document, and the rvest function
`read_html` reads an HTML document. Both accept an XML/HTML string or a file
path (including URLs):

```{r}
html = r"(
<html>
  <head>
    <title>This is the page title!</title>
  </head>
  <body>
    <h1>This is a header!</h1>
    <p>This is a paragraph.
      <a href="http://www.r-project.org/">Here's a website!</a>
    </p>
    <p id="hello">This is another paragraph.</p>
  </body>
</html> )"

library(xml2)
library(rvest)

doc = read_html(html)
doc
```

The `xml_children` function returns all of the immediate children of a given
element.

The top element of our document is the `html` tag, and its immediate children
are the `head` and `body` tags:

```{r}
tags = xml_children(doc)
```

The result from `xml_children` is a **node set** (`xml_nodeset` object). Think
of a node set as a vector where the elements are tags rather than numbers or
strings. Just like a vector, you can access individual elements with the
indexing (square bracket `[`) operator:

```{r}
length(tags)
head = tags[1]
head
```

The `xml_text` function returns the text contained in a tag. Let's get the text
in the `title` tag, which is beneath the `head` tag. First we isolate the tag,
then use `xml_text`:

```{r}
title = xml_children(head)
xml_text(title)
```

Navigating through the tags by hand is tedious and easy to get wrong, but
fortunately there's a better way to find the tags we want.

### XPath

An XML document is a tree, similar to the file system on your computer:

```
html
├── head
│   └── title
└── body
    ├── h1
    ├── p
    └── p
        └── a
```

When we wanted to find files, we wrote file paths. We can do something similar
to find XML elements.

**XPath** is a language for writing paths to elements in an XML document. XPath
is not R-specific. At a glance, an XPath looks similar to a file path:

XPath | Description
----- | --------------------------
`/`   | root, or element separator
`.`   | current tag
`..`  | parent tag
`*`   | any tag (wildcard)

: {.striped .hover}

The xml2 function `xml_find_all` finds all elements at given XPath:
```{r}
xml_find_all(doc, "/html/body/p")
```

Unlike a file path, an XPath can identify multiple elements. If you only want a
specific element, use indexing to get it from the result.

XPath also has some features that are different from file paths. The `//`
separator means "at any level beneath." It's a useful shortcut when you want to
find a specific element but don't care where it is.

Let's get all of the `p` elements at any level of the document:
```{r}
xml_find_all(doc, "//p")
```

Let's also get all `a` elements at any level beneath a `p` element:
```{r}
xml_find_all(doc, "//p/a")
```

The vertical bar `|` means "or." You can use it to get two different sets of
elements in one query.

Let's get all `h1` or `p` tags:
```{r}
xml_find_all(doc, "//h1|//p")
```


#### Predicates

In XPath, the predicate operator `[]` gets elements at a position or matching a
condition. Most conditions are about the attributes of the element. In the
predicate operator, attributes are always prefixed with `@`.

For example, suppose we want to find all tags where the `id` attribute is equal
to `"hello"`:
```{r}
xml_find_all(doc, "//*[@id = 'hello']")
```

Notice that the equality operator in XPath is `=`, not `==`. Strings in XPath
can be quoted with single or double quotes.

You can combine multiple conditions in the predicate operator with `and` and
`or`. There are also several XPath functions you can use in the predicate
operator. These functions are *not* R functions, but rather built into XPath.
Here are a few:

XPath         | Description
------------- | -----------
`not()`       | negation
`contains()`  | check string x contains y
`text()`      | get tag text
`substring()` | get a substring

: {.striped .hover}

For instance, suppose we want to get elements that contain the word
"paragraph":

```{r}
xml_find_all(doc, "//*[contains(text(), 'paragraph')]")
```

Finally, note that you can also use the predicate operator to get elements at a
specific position. For example, to get the second `p` element anywhere in the
document:

```{r}
xml_find_all(doc, "//p[2]")
```

Notice that this is the same as if we had used R to get the second element:

```{r}
xml_find_all(doc, "//p")[2]
```

::: {.callout-warning}
Beware that although the XPath predicate operator resembles R's indexing
operator, the syntax is not always the same.
:::

We'll learn more XPath in the examples. There's a complete list of XPath
functions on [Wikipedia][wiki-xpath].

[wiki-xpath]: https://en.wikipedia.org/wiki/XPath#Functions_and_operators


### The Web Scraping Workflow

Scraping a web page is part technology, part art. The goal is to find an XPath
that's concise but specific enough to identify only the elements you want. If
you plan to scrape the web page again later or want to scrape a lot of similar
web pages, the XPath also needs to be general enough that it still works even
if there are small variations.

[Firefox][] and [Chrome][] include "web developer tools" that are invaluable
for planning a web scraping strategy. Press `Ctrl + Shift + i` (`Cmd + Shift +
i` on OS X) in Firefox or Chrome to open the web developer tools.

We can also use the web developer tools to interactively identify the element
that corresponds to a specific part of a web page. Press `Ctrl + Shift + c` and
then click on the part of the web page you want to identify.

The best way to approach web scraping (and programming in general) is as an
incremental, iterative process. Use the web developer tools to come up with a
basic strategy, try it out in R, check which parts don't work, and then repeat
to adjust the strategy. Expect to go back and forth between your web browser
and R several times when you're scraping.

Most scrapers follow the same four steps, regardless of the web page and the
language of the scraper:

1. Download pages with an HTTP request (usually `GET`)
2. Parse pages to extract text
3. Clean up extracted text with string methods or regex
4. Save cleaned results

In R, xml2's `read_xml` function takes care of step 1 for you, although you can
also use httr functions to make the request yourself.

[Firefox]: https://www.mozilla.org/en-US/firefox/new/
[Chrome]: https://www.google.com/chrome/


#### Being Polite

Making an HTTP request is not free! It has a real cost in CPU time and also
cash. Server administrators will not appreciate it if you make too many
requests or make requests too quickly. So:

* If you're making multiple requests, slow them down by using R's `Sys.sleep`
  function to make R do nothing for a moment. Aim for no more than 20-30
  requests per second, unless you're using an API that says more are okay.
* Avoid requesting the same page twice. One way to do this is by **caching**
  (saving) the results of the requests you make. You can do this manually, or
  use a package that does it automatically, like the [httpcache][] package.

[httpcache]: https://enpiar.com/r/httpcache/

::: {.callout-important}
Failing to be polite can get you banned from websites! Also check the website's
terms of service to make sure scraping is not explicitly forbidden.
:::


#### Case Study: CA Cities

[Wikipedia](https://en.wikipedia.org/) has many pages that are just tables of
data. For example, there's this [list of cities and towns in
California][wiki-cities]. Let's scrape the table to get a data frame.

[wiki-cities]: https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_California

**Step 1** is to download the page:
```{r wiki_scrape, echo=FALSE, results=FALSE}
wiki_url =
  "https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_California"
wiki_doc = .read_html_cache(wiki_url)
```
```r
wiki_url =
  "https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_California"

wiki_doc = read_html(wiki_url)
```

**Step 2** is to extract the table element from the page. We can use Firefox or
Chrome's web developer tools to identify the table. HTML tables usually use the
`table` tag. Let's see if it's the only table in the page:

```{r}
tables = xml_find_all(wiki_doc, "//table")
tables
```

The page has `r length(tables)` tables. We can either make our XPath more
specific, or use indexing to get the table we want. Refining the XPath makes
our scraper more robust, but indexing is easier.

For the sake of learning, let's refine the XPath. Going back to the browser, we
can see that the table includes `"wikitable"` and `"sortable"` in its `class`
attribute. So let's search for these among the table elements:

```{r}
tab = xml_find_all(tables, "//*[contains(@class, 'sortable')]")
tab
```

Now we get just two tables, and the second one is the one we want! Here we used
a second XPath applied only to the results from the first, but we also could've
done this all with one XPath: `//table[contains(@class, 'sortable')]`.

The next part of extracting the data is to extract the value from each
individual cell in the table. HTML tables have a strict layout order, with tags
to indicate rows and cells. We could extract each cell by hand and then
reassemble them into a data frame, but the rvest function `html_table` can do
it for us automatically:

```{r}
cities = html_table(tab, fill = TRUE)
cities = cities[[2]]
head(cities)
```

The `fill = TRUE` argument ensures that empty cells are filled with `NA`. We've
successfully imported the data from the web page into R, so we're done with
step 2.

**Step 3** is to clean up the data frame. The column names contain symbols, the
first row is part of the header, and the column types are not correct.

```{r}
# Fix column names.
names(cities) = c(
  "city", "type", "county", "population2020", "population2010",
  "population_change", "mi2", "km2", "density", "date"
)

# Remove fake first row.
cities = cities[-1, ]
# Reset row names.
rownames(cities) = NULL
```

How can we clean up the `date` column? The `as.Date` function converts a string
into a date R understands. The idea is to match the date string to a _format
string_ where the components of the date are indicated by codes that start with
`%`. For example, `%m` stands for the month as a two-digit number. You can read
about the different date format codes in `?strptime`.

Here's the code to convert the dates in the data frame:

```{r}
dates = as.Date(cities$date, "%B %m, %Y")
cities$date = dates
```

We can also convert the population to a number:

```{r}
class(cities$population2020)

# Remove commas and footnotes (e.g., [1]) before conversion
library(stringr)

pop = str_replace_all(cities$population2020, ",", "")
pop = str_replace_all(pop, "\\[[0-9]+\\]", "")
pop = as.numeric(pop)

# Check for missing values, which can mean conversion failed
any(is.na(pop))

cities$population2020 = pop
```

#### Case Study: The CA Aggie

Suppose we want to scrape The California Aggie.

In particular, we want to get all the links to news articles on the features
page <https://theaggie.org/category/features/>. This could be one part of a
larger scraping project where we go on to scrape individual articles.

First for **Step 1**, let's download the features page so we can extract the
links:

```{r aggie_features, echo=FALSE, results=FALSE}
url = "https://theaggie.org/category/features/"
doc = .read_html_cache(url)
```

```r
url = "https://theaggie.org/category/features/"
doc = read_html(url)
```

We know that links are in `a` tags, but we only want links to articles. Looking
at the features page with the web developer tools, the links to feature
articles are all inside of a `div` tag with class `td_block_inner`. So for
**Step 2**, let's get that tag:

```{r}
xml_find_all(doc, "//div[contains(@class, 'td_block_inner')]")
# OR html_nodes(doc, "div.td-block-inner")
```

That returns a lot of results, so let's try using the `id` attribute, which is
`"tdi_113"`, instead. Usually the id of an element is unique, so this ensures
that we get the right section.

We can also add in a part about getting links now:

```{r}
div = xml_find_all(doc, "//div[@id = 'tdi_113']")
# OR html_nodes(doc, "div#tdi_113")

links = xml_find_all(div, ".//a")
# OR html_nodes(div, "a")

length(links)
```

That gives us `r length(links)` links, but there are only 15 articles on the
page, so something's still not right. Inspecting the page again, there are
actually three links to each article: on the image, on the title, and on
"Continue Reading".

Let's focus on the title link. All of the title links are inside of an `h3`
tag. Generally it's more robust to rely on tags (structure) than to rely on
attributes (other than `id` and `class`). So let's use the `h3` tag here:

```{r}
links = xml_find_all(div, ".//h3/a")
# OR html_nodes(div, "h3 > a")

length(links)
```

Now we've got the 15 links, so let's get the URLs from the `href` attribute.

```{r}
feature_urls = xml_attr(links, "href")
```

The other article listings (Sports, Science, etc) on The Aggie have a similar
structure, so we can potentially reuse our code to scrape those.

So let's turn our code into a function. The input will be a downloaded page,
and the output will be the article links.

```{r}
parse_article_links = function(page) {
  div = xml_find_all(page, "//div[@id = 'tdi_113']")
  links = xml_find_all(div, ".//h3/a")
  xml_attr(links, "href")
}
```

We can test this out on the Sports page. First we download the page:

```{r aggie_sports, echo=FALSE, results=FALSE}
sports = .read_html_cache("https://theaggie.org/category/sports")
```
```r
sports = read_html("https://theaggie.org/category/sports")
```

Then we call the function on the document:

```{r}
sports_urls = parse_article_links(sports)
head(sports_urls)
```

It looks like the function works even on other pages! We can also set up the
function to extract the link to the next page, in case we want to scrape
multiple pages of links.

The link to the next page of features (an arrow at the bottom) is an `a` tag
with attribute `aria-label` in a `div` with class `page-nav`. Let's see if
that's specific enough to isolate the tag:

```{r}
nav = xml_find_all(doc, "//div[contains(@class, 'page-nav')]")
# OR html_nodes(doc, "div.page-nav")
next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
# OR html_nodes(nav, "a[aria-label *= 'next-page']")
```

It looks like it is. We use `contains` here rather than `=` because it is
common for the `class` attribute to have many parts. Using `contains` makes our
code robust against changes in the future.

We can now modify our parser function to return the link to the next page:

```{r}
parse_article_links = function(page) {
  # Get article URLs
  div = xml_find_all(page, "//div[@id = 'tdi_113']")
  links = xml_find_all(div, ".//h3/a")
  urls = xml_attr(links, "href")

  # Get next page URL
  nav = xml_find_all(page, "//div[contains(@class, 'page-nav')]")
  next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
  next_url = xml_attr(next_page, "href")

  # Using a list allows us to return two objects
  list(urls = urls, next_url = next_url)
}
```

Since our function gets URL for the next page, what happens on the last page?

Looking at the last page in the browser, there is no link to the next page.
Let's see what our scraper function does:

```{r aggie_last, echo=FALSE, results=FALSE}
last_page = .read_html_cache("https://theaggie.org/category/features/page/187/")
```
```r
last_page = read_html("https://theaggie.org/category/features/page/187/")
```
```{r}
parse_article_links(last_page)
```

We get an empty character vector as the URL for the next page. This is because
the `xml_find_all` function returns an empty node set for the next page URL, so
there aren't any `href` fields for `xml_attr` to extract. It's convenient that
the xml2 functions behave this way, but we could also add an if-statement to
the function to check (and possibly return `NA` as the next URL in this case).

Then the code becomes:

```{r}
parse_article_links = function(page) {
  # Get article URLs
  div = xml_find_all(page, "//div[@id = 'tdi_113']")
  links = xml_find_all(div, ".//h3/a")
  urls = xml_attr(links, "href")

  # Get next page URL
  nav = xml_find_all(page, "//div[contains(@class, 'page-nav')]")
  next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
  if (length(next_page) == 0) {
    next_url = NA
  } else {
    next_url = xml_attr(next_page, "href")
  }

  # Using a list allows us to return two objects
  list(urls = urls, next_url = next_url)
}
```

Now our function should work well even on the last page.

If we want to scrape links to all of the articles in the features section, we
can use our function in a loop:

```r
# NOTE: This code is likely to take a while to run, and is meant more for
# reading than for you to run and try out.

url = "https://theaggie.org/category/features/"
article_urls = list()
i = 1

# On the last page, the next URL will be `NA`.
while (!is.na(url)) {
  # Download and parse the page.
  page = read_html(url)
  result = parse_article_links(page)

  # Save the article URLs in the `article_urls` list. The variable `i` is the
  # page number.
  article_urls[[i]] = result$url
  i = i + 1

  # Set the URL to the next URL.
  url = result$next_url

  # Sleep for 1/30th of a second so that we never make more than 30 requests
  # per second.
  Sys.sleep(1/30)
}
```

Now we've got the basis for a simple scraper.


### CSS Selectors

**Cascading style sheets** (CSS) is a language used to control the formatting
of an XML or HTML document.

**CSS selectors** are the CSS way to write paths to elements. CSS selectors are
more concise than XPath, so many people prefer them. Even if you prefer CSS
selectors, it's good to know XPath because CSS selectors are less flexible.

Here's the basic syntax of CSS selectors:

CSS           | Description
------------- | --------------------------
`a`           | tags `a`
`a > b`       | tags `b` directly beneath `a`
`a b`         | tags `b` anywhere beneath `a`
`a, b`        | tags `a` or `b`
`#hi`         | tags with attribute `id="hi"`
`.hi`         | tags with attribute `class` that contains `"hi"`
`[foo="hi"]`  | tags with attribute `foo="hi"`
`[foo*="hi"]` | tags with attribute `foo` that contains `"hi"`

: {.striped .hover}

If you want to learn more, [CSS Diner](http://flukeout.github.io/) is an
interactive tutorial that covers the entire CSS selector language.

In Firefox, you can get CSS selectors from the web developer tool. Right-click
the tag you want a selector for and choose "Copy Unique Selector". Beware that
the selectors Firefox generates are often too specific to be useful for
anything beyond the simplest web scrapers.

The rvest package uses CSS selectors by default. Behind the scenes, the package
translates these into XPath and passes them to xml2.

Here are a few examples of CSS selectors, using rvest's `html_nodes` function:

```{r}
html = r"(
<html>
  <head>
    <title>This is the page title!</title>
  </head>
  <body>
    <h1>This is a header!</h1>
    <p>This is a paragraph.
      <a href="http://www.r-project.org/">Here's a website!</a>
    </p>
    <p id="hello">This is another paragraph.</p>
  </body>
</html> )"

doc = read_html(html)

# Get all p elements
html_nodes(doc, "p")

# Get all links
html_nodes(doc, "a")

# Get all tags with id="hello"
html_nodes(doc, "#hello")
```
