# Reshaping Data {#sec-reshaping-data}

:::{.callout-note title="Learning Goals" collapse="false"}
After this lesson, you should be able to:

* Use the tidyr package to reshape data
:::

:::{.callout-important title="Required Packages"}
This chapter uses the following packages:

* ggplot2
* lubridate
* readxl
* tidyr

@sec-packages explains how to install and load packages.
:::


The structure of a dataset---its shape and organization---has enormous
influence on how difficult it will be to analyze, so making structural changes
is an important part of the cleaning process. This chapter explains how to
**reshape** untidy data into tidy data (@sec-tidy-data). While reshaping can
seem tricky at first, making sure your dataset has the right structure before
you begin analysis saves time and frustration in the long run.


## The tidyr Package

The [tidyr][] package provides functions to reshape tabular datasets. It also
provides examples of tidy and untidy datasets. Like most Tidyverse packages, it
comes with detailed [documentation][tidyr] and a [cheatsheet][tidyr-cheat].

[tidyr]: https://tidyr.tidyverse.org/
[tidyr-cheat]: https://github.com/rstudio/cheatsheets/blob/main/tidyr.pdf

As usual, install the package if you haven't already, and then load it:

```{r}
# install.packages("tidyr")
library("tidyr")
```

## An Untidy Dataset

The City of Davis has two bike counters: one is at the intersection of 3rd
Street and University Avenue (the 3rd Street bike obelisk) and the other is at
the intersection of Loyola Drive and Pole Line Road. The City publishes data
from the bike counters online. DataLab combined the City's 2020 bike counts,
aggregated to the day level, with precipitation and wind data from the U.S.
National Oceanic and Atmospheric Administration's weather station at Sacramento
Metropolitan Airport (this was the nearest weather station with complete
records for 2020). We'll use this dataset to demonstrate how to transform
untidy data.

:::{.callout-important}
[Click here][davis-bikes] to download the 2020 Davis bike counts dataset.

[davis-bikes]: https://ucdavis.box.com/s/q9oox9arov97dvsocrsx6gghh7283z3i

If you haven't already, we recommend you create a directory for this workshop.
In your workshop directory, create a `data/` subdirectory. Download and save
the dataset in the `data/` subdirectory.
:::

:::{.callout-note title="Documentation for the 2020 Davis Bike Counts Dataset" collapse="true"}
Each row in the dataset contains measurements from one date-variable
combination.

Column     | Description
---------- | -----------
`date`     | The date of measurement
`variable` | What was measured: `third` and `loyola` are bike counts, `prcp` is total precipitation in millimeters, `awnd` is average daily wind speed in meters per second
`value`    | The measured value

The source for the bike counts is the City of Davis' [Bike and Pedestrian
Statistics][bike-ped-stats] web page. The source for the total precipitation
and average wind speed is NOAA's [weather station at Sacramento Metropolitan
Airport][noaa-sac].

[bike-ped-stats]: https://www.cityofdavis.org/city-hall/public-works-engineering-and-transportation/bike-pedestrian-program/bike-and-pedestrian-data-statistics
[noaa-sac]: https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00093225/detail
:::

The dataset is saved in an RDS file, which you can use the built-in `readRDS`
function to read:

```{r}
bikes = readRDS("data/2020_davis_bikes.rds")
head(bikes)
```

This data is not tidy, because it breaks rule 1. The `value` column contains
many different features---they even have different units! Soon we'll reshape
the dataset to make it tidy.

Before you reshape a dataset, you should also think about what role each column
serves:

* **Identifiers** (or indexes) are labels that distinguish observations from
  one another. They're often but not always categorical. Examples include names
  or identification numbers, treatment groups, and dates or times. In the bike
  counts dataset, the `date` column is an identifier.

* **Measurements** are the values collected for each observation and typically
  the values of research interest. In the bike counts dataset, the `value`
  column is a measurement.

A clear understanding of which columns are identifiers and which are
measurements makes it easier to write the code to reshape.


## Rows into Columns {#sec-rows-into-columns}

In order to make the Davis bike counts data tidy, the measurements in the
`value` column need to be moved into two separate columns, one for each of the
categories in the `variable` column.

You can use the `pivot_wider` function to **pivot** a data frame, creating new
columns from values in the rows. This makes the data frame wider (and shorter).
Let's pivot the `bikes` data frame on the `variable` column to create four new
columns filled with values from the `values` column.

The `pivot_wider` function's most important parameters are:

* `values_from` -- The column(s) that contains values for the new columns.
* `names_from` -- The column that contains names for the new columns.
* `id_cols` -- The identifier columns, which are not pivoted. This defaults to
  all columns except those in `values_from` and `names_from`.

Here's how to use the function to make `bikes` tidy:

```{r}
bikes2 = pivot_wider(bikes, values_from = value, names_from = variable)
head(bikes2)
```

The function automatically removes values from the `date` column as needed to
maintain the original correspondence with the pivoted values.

The new `bikes2` data frame contains all of the data from `bikes`, but now the
measurements for each date share a row. In other words, the observational units
for `bikes2` are dates, which is convenient for investigating how individual
features change over time, as well as making same-time comparisons between
features. To illustrate this, we can use ggplot2 to make a scatter plot of the
bike counts at 3rd Street against the counts at Loyola Drive:

```{r}
library("ggplot2")

ggplot(bikes2) +
  aes(x = third, y = loyola) +
  geom_point()
```

The plot shows that Loyola Drive occasionally has days with much higher traffic
than 3rd Street, but it's difficult to tell whether 3rd or Loyola is typically
busier (that is, whether there are more points above or below the $y = x$
line).


## Columns into Rows {#sec-columns-into-rows}

Suppose we want to try to get a better answer to whether Loyola or 3rd tends to
be busier. One way we can do it is by making a line plot with the counts for
each site over time. This way we're treating each site as a group within the
data and making a comparison between groups.

Comparing groups in a data frame is generally easier when each row corresponds
to an observation from one group. Let's reshape the `bikes2` data frame so that
the observational units are date-site combinations. To do this, the `third` and
`loyola` columns need to be transformed into two new columns: one for
measurements (the counts) and one for identifiers (the sites). It might help to
visualize this as stacking the two separate columns `third` and `loyola`
together, one on top of the other, and then adding a second column with the
corresponding site names.

You can use the `pivot_longer` function to **unpivot** a data frame, creating
new rows from values in the columns. This is the inverse of a pivot. It makes
the data frame longer (and narrower). We'll unpivot the `bikes2` data frame on
the `third` and `loyola` columns.

The `pivot_longer` function's parameters are:

* `cols` -- The columns to stack into a new column; the names of these columns
  will also go into a new column.
* `values_to` -- Name(s) for the new measurement column(s)
* `names_to` -- Name(s) for the new identifier column(s)

The code to unpivot `bikes2` is:

```{r}
bikes3 = pivot_longer(
    bikes2,
    cols = c(third, loyola),
    values_to = "count",
    names_to = "site"
)
head(bikes3)
```

For `bikes3`, the observational units are date-site combinations, as planned.
This is convenient for comparing the two sites to each other with statistics
and visualizations. We can use ggplot2 to make a line plot of the counts for
the two sites:

```{r}
ggplot(bikes3) +
  aes(x = date, y = count, color = site) +
  geom_line()
```

From this plot, we can see that Loyola Drive was generally busier for the first
3 months of 2020. Traffic dropped at both sites in mid-March, probably due to
the COVID-19 pandemic. The drop was sharper at Loyola Drive than 3rd Street, so
3rd Street was generally busier for the remaining 9 months of 2020.

:::{.callout-note}
We didn't use `prcp` and `awnd` (and they don't differ between sites anyway).
If we dropped them, the resulting data frame would have the same observational
units as `bikes3`, but would also be a subset of the original `bikes` data
frame (albeit with different column names).
:::


## Case Study: SMART Ridership

[Sonoma-Marin Area Rail Transit (SMART)][smart] is a relatively new single-line
passenger rail service between the San Francisco Bay and Santa Rosa. They
publish data about monthly ridership online, but the format is slightly messy.
Let's clean and reshape the data in order to make a plot of ridership over
time.

[smart]: http://sonomamarintrain.org/

:::{.callout-important}
[Click here][smart-riders] to download the SMART Ridership dataset (version
2026-01).

If you haven't already, we recommend you create a directory for this workshop.
In your workshop directory, create a `data/` subdirectory. Download and save
the dataset in the `data/` subdirectory.

[smart-riders]: https://ucdavis.box.com/s/u1f7zoycpdlfrom5pr7ihyaulvir2mm1
:::

:::{.callout-note title="Documentation for the SMART Ridership Dataset" collapse="true"}

The source for the dataset is the [SMART Ridership Reports][smart-reports] web
page.

[smart-reports]: https://www.sonomamarintrain.org/RidershipReports
:::

The dataset is saved as a Microsoft Excel file. Before reading an Excel file,
it's a good idea to manually inspect it with spreadsheet software to figure out
how the data are organized. The SMART dataset contains two tables on the left
side of the first sheet: one for total monthly ridership and one for average
weekday ridership (by month).

The [readxl][] package provides functions to read data from Excel files.
Install the package if you don't already have it installed, and then load it:

[readxl]: https://readxl.tidyverse.org/

```{r load-readxl}
# install.packages("readxl")
library("readxl")
```

We can use the package's `read_excel` function to read sheets from an Excel
file. It has a parameter `range` to control which cells it reads.

Let's focus on the total monthly ridership table, which occupies cells B4
to K16:

```{r read-smart-data}
smart = read_excel("data/2026-01_smart_ridership.xlsx", range = "B4:K16")
head(smart)
```

The dataset needs to be cleaned. The `FY18` column uses a hyphen `-` to
indicate a missing value and has the wrong data type. We can use indexing to
replace the hyphen with a missing value and then convert the column to an
appropriate type with the `as.numeric` function:

```{r}
smart$FY18[smart$FY18 == "-"] = NA
smart$FY18 = as.numeric(smart$FY18)
head(smart)
```

There's still a lot of cleaning to do. The identifiers in this dataset are the
months and years, and they're split between the row and column names. Each row
contains data from several different years, so the dataset is not tidy. In
addition, the years are indicated in fiscal years (FY), which begin in July
rather than January, so some of the years need to be adjusted.

To make the dataset tidy, it needs to be reshaped so that the values in the
various fiscal year columns are all in one column. In other words, the dataset
needs to be unpivoted (@sec-columns-into-rows) on all of the `FY` columns. The
result of the pivot will be easier to understand if we rename the columns as
their years first. Here's one way to do that:

```{r}
names(smart)[-1] = 2018:2026
head(smart)
```

Next, we use `pivot_longer` to pivot the dataset:

```{r pivot-smart-data}
smart = pivot_longer(
  smart,
  cols = -Month,
  values_to = "count",
  names_to = "fiscal_year"
)
head(smart)
```

In order to use the months and years in the data, we need to convert them to
dates. As a first step towards this, we'll cast the values in the new
`fiscal_year` column to integers:

```{r}
smart$fiscal_year = as.numeric(smart$fiscal_year)
head(smart)
```

Next, we can use the lubridate package's `fast_strptime` and `month` functions
to create a new of column month numbers:

```{r}
library("lubridate")

month_num = month(fast_strptime(smart$Month, "%m"))
```

Now we need to transform the fiscal years in the `fiscal_year` column into
calendar years. A SMART fiscal year extends from July to the following June and
is named after the calendar year at the end of the fiscal year. So from July to
December, the calendar year is the fiscal year minus 1. We can use the built-in
`ifelse` function to subtract either `1` or `0` depending on whether the
condition `month_num >= 7` is true or false:

```{r}
cal_year = smart$fiscal_year - ifelse(month_num >= 7, 1, 0)
```

:::{.callout-note}
Alternatively, we can compute the calendar year by taking advantage of implicit
coercion. The logical value `FALSE` corresponds to `0` and `TRUE` corresponds
to `1`, so we can just subtract the condition to get the calendar year:

```{r}
cal_year = smart$fiscal_year - (month_num >= 7)
```
:::

Finally, we can use the lubridate package's `make_date` function to construct
dates from the `cal_year` and `month_num` variables:

```{r}
smart$date = make_date(year = cal_year, month = month_num)
```

With the dates in the `date` column and the counts in the `count` column, we
have everything we need to make a plot of SMART ridership over time. We can use
ggplot2 to make the plot:

```{r}
ggplot(smart) +
  aes(x = date, y = count) +
  geom_line()
```

Notice the huge drop (more than 90%) in April 2020 due to the COVID-19
pandemic!


<!--
## Without `tidyr`

This section shows how to pivot datasets without the help of the tidyr package.
In practice, we recommend that you use the package, but the examples here may
make it easier to understand what's actually happening when you pivot a
dataset.

### Rows into Columns

The steps for pivoting `table2` wider are:

1. Subset rows to separate `cases` and `population` values.
2. Remove the `type` column from each.
3. Rename the `count` column to `cases` and `population`.
4. Merge the two subsets by matching `country` and `year`.

And the code is:

```{r}
# Step 1
cases = table2[table2$type == "cases", ]
pop = table2[table2$type == "population", ]
# Step 2
cases = cases[-3]
pop = pop[-3]
# Step 3
names(cases)[3] = "cases"
names(pop)[3] = "population"
# Step 4
merge(cases, pop)
```

### Columns into Rows

The steps for pivoting `table4a` longer are:

1. Subset columns to separate `1999` and `2000` into two data frames.
2. Add a `year` column to each.
3. Rename the `1999` and `2000` columns to `cases`.
4. Stack the two data frames with `rbind`.

And the code is:

```{r}
# Step 1
df99 = table4a[-3]
df00 = table4a[-2]
# Step 2
df99$year = "1999"
df00$year = "2000"
# Step 3
names(df99)[2] = "cases"
names(df00)[2] = "cases"
# Step 4
rbind(df99, df00)
```
-->
